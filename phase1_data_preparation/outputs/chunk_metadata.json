{
  "model_name": "all-MiniLM-L6-v2",
  "num_chunks": 406,
  "chunks": [
    {
      "chunk_id": 0,
      "paper_id": "openvla",
      "text": "Open VLA: An Open-Source Vision-Language-Action Model Moo Jin Kim\u2217,1 Karl Pertsch\u2217,1,2 Siddharth Karamcheti\u2217,1,3 Ted Xiao 4 Ashwin Balakrishna 3 Suraj Nair 3 Rafael Rafailov 1 Ethan Foster 1 Grace Lam Pannag Sanketi 4 Quan Vuong 5,\u2020 Thomas Kollar 3 Benjamin Burchfiel 3 Russ Tedrake 3,6 Dorsa Sadigh 1 Sergey Levine 2 Percy Liang 1 Chelsea Finn 1 https://openvla.github.io Large-Scale Robot Open VLA Closed-Loop Training Data Robot Control Policy Vision-Language-Action Model User: Wipe the table. Fine-tune VLM w/ Robot Actions: Open VLA: 970 k Robot [\u0394x, \u0394\u03b8, \u0394Grip] = \u2026 Llama 2 7 B Episodes Vi T Base VLM Multi-Robot Control & Efficient Fine-Tuning Fully Open-Source Data Weights Code Figure 1: Wepresent Open VLA,a 7 B-parameter open-sourcevision-language-action model(VLA),trained on 970 krobotepisodes from the Open X-Embodiment data set[1]. Open VLAsetsa new stateof the artfor generalistrobotmanipulationpolicies.Itsupportscontrollingmultiplerobotsoutof the boxand can bequickly adaptedto new robotdomainsviaparameter-efficientfine-tuning. The Open VLAcheckpoints and Py Torch trainingpipeline are fully open-source and models can bedownloaded and fine-tuned from Hugging Face. Abstract: Largepoliciespretrainedonacombinationof Internet-scalevision- language data and diverserobotdemonstrations have the potentialtochangehow we teachrobots new skills: ratherthantraining new behaviors from scratch,we can fine-tunesuchvision-language-action(VLA)modelstoobtainrobust,generalizable policies for visuomotorcontrol. Yet,widespreadadoptionof VLAs for robotics has been challengingas 1)existing VLAs are largelyclosed and inaccessibleto the public,and 2)priorworkfailstoexploremethods for efficientlyfine-tuning VLAs for new tasks,akeycomponent for adoption.Addressing the sechallenges,weintro- duce Open VLA,a 7 B-parameter open-source VLAtrainedonadiversecollection of 970 kreal-worldrobotdemonstrations. Open VLAbuildsona Llama 2 language modelcombined with avisualencoder that fusespretrainedfeatures from DINOv 2 and Sig LIP.Asaproductof the added data diversity and new model components, Open VLAdemonstratesstrongresults for generalistmanipulation,outper for ming closed model ssuchas RT-2-X(55 B)by 16.5%inabsolute task successrateacross 29 tasks and multiplerobotembodiments,with 7 xfewerparameters. Wefurther showthatwe caneffectivelyfine-tune Open VLA for newsettings,withespecially \u2217:denotesequalcontribution Correspondenceto:moojink@stanford.edu, pertsch@berkeley.edu, skaramcheti@stanford.edu 1 Stanford University,2 UCBerkeley,3 Toyota Research Institute,4 Google Deepmind,5 Physical Intelligence, 6 MIT,\u2020Workdoneinpartwhileat Google Deepmind 4202 pe S 5 ]OR.sc[ 3 v 64290.6042:vi Xra stronggeneralizationresultsinmulti-taskenvironmentsinvolvingmultipleobjects andstronglanguagegroundingabilities,andoutper for mexpressive from-scratch imitationlearningmethodssuchas Diffusion Policyby 20.4%Wealsoexplore compute efficiency; as a separate contribution, we show that Open VLA can be fine-tunedonconsumer GPUsviamodernlow-rankadaptationmethods and served efficientlyviaquantization with outahittodownstreamsuccessrate. Finally,we release model checkpoints,fine-tuningnotebooks,andour Py Torchcode base with built-insupport for training VLAsat scaleon Open X-Embodiment data sets. 1 Introduction Akeyweaknessoflearnedpolicies for roboticmanipulationis the irinabilitytogeneralizebeyond theirtraining data: whileexistingpoliciestrained for individualskillsorlanguageinstructions have thecapacitytoextrapolatebehaviorsto new initialconditionssuchasobjectpositionsorlighting [2,3],theylackrobustnesstoscenedistractorsornovelobjects[4,5]andstruggletoexecuteunseen taskinstructions[6,7]. Yetbeyondrobotics,existingfoundationmodels for vision and language suchas CLIP[8],Sig LIP[9],and Llama 2[10]arecapableof the setypesofgeneralization and more, stemming from the priorscapturedbytheir Internet-scalepretraining data sets. Whilereproducing this scale ofpretraining for roboticsisstillan open challenge\u2014even the largestrobotmanipulation datasets[1,11]only have 100 Kto 1 Mexamples\u2013thisimbalancesuggestsanopportunity: using existing foundation models for vision and language as a core building block for training robotic policies that can generalizetoobjects,scenes,and task sbeyond the irtraining data. Towards this goal,existingworkhasexploredintegratingpretrainedlanguage and vision-language models for roboticrepresentationlearning[12\u201314]andasacomponentinmodularsystems for task planning and execution[15,16]. Morerecently,they have beenused for directlylearningvision- language-action models [VLAs; 1, 7, 17, 18] for control. VLAs provide a direct instantiation of usingpretrainedvision-and-languagefoundationmodels for robotics,directlyfine-tuningvisually- conditionedlanguagemodels(VLMs)suchas Pa LI[19,20]togeneraterobotcontrolactions. By building off of strong foundation models trained on Internet-scale data, VLAs such as RT-2 [7] demonstrateimpressiverobustnessresults,aswellasanabilitytogeneralizetonovelobjects and tasks,settinganewst and ard for generalistrobotpolicies. Yet,there are twokeyreasonspreventing the widespread use of existing VLAs: 1) current models [1, 7, 17, 18] are closed, with limited visibilityinto model architecture,trainingprocedures,and data mixture,and 2)existingworksdo",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 1,
      "paper_id": "openvla",
      "text": "LI[19,20]togeneraterobotcontrolactions. By building off of strong foundation models trained on Internet-scale data, VLAs such as RT-2 [7] demonstrateimpressiverobustnessresults,aswellasanabilitytogeneralizetonovelobjects and tasks,settinganewst and ard for generalistrobotpolicies. Yet,there are twokeyreasonspreventing the widespread use of existing VLAs: 1) current models [1, 7, 17, 18] are closed, with limited visibilityinto model architecture,trainingprocedures,and data mixture,and 2)existingworksdo notprovidebestpractices for deploying and adapting VLAsto new robots,environments,andtasks \u2014 especially on commodity hardw are (e.g., consumer-grade GPUs). We argue that to develop a richfoundation for futureresearch and development,roboticsneeds open-source,generalist VLAs thatsupporteffectivefine-tuning and adaptation,akinto the existingecosystemaround open-source languagemodels[21\u201324]. To this end, we introduce Open VLA, a 7 B-parameter open-source VLA that establishes a new state of the art for generalist robot manipulation policies.1 Open VLA consists of a pretrained visually-conditionedlanguage model backbone that capturesvisualfeaturesatmultiplegranularities, fine-tuned on a large, diverse dataset of 970 k robot manipulation trajectories from the Open-X Embodiment[1]dataset\u2014adataset that spansawiderangeofrobotembodiments,tasks,andscenes. Asaproductofincreased data diversity and new model components,Open VLAoutperforms the 55 B-parameter RT-2-X model [1, 7], the prior state-of-the-art VLA, by 16.5% absolute success rateacross 29 evaluation task son the Widow Xand Google Robotembodiments. Weadditionally investigateefficientfine-tuningstrategies for VLAs,anewcontributionnotexploredinpriorwork, across 7 diversemanipulation task sspanningbehaviors from objectpick-and-placetocleaninga table. Wefind that fine-tuned Open VLApoliciesclearlyoutper for mfine-tunedpretrainedpolicies suchas Octo[5]. Comp are dtofrom-scratchimitationlearning with diffusionpolicies[3],fine-tuned Open VLAshowssubstantialimprovementon task sinvolvinggroundinglanguagetobehaviorin 1 Open VLAusesmultiplepretrained model components:Sig LIP[9]and Dino V 2[25]visionencodersanda Llama 2[10]language model backbone.Forallthreemodels,weights are open,butnot the irtrainingdataor code.Wereleasetraining data,code and modelweights for reproducing Open VLAontopof the secomponents. 2 multi-tasksettings with multipleobjects. Following the seresults,weare the firsttodemonstrate the effectivenessofcompute-efficientfine-tuningmethodsleveraginglow-rankadaptation[Lo RA;26] and model quantization[27]tofacilitateadapting Open VLA model sonconsumer-grade GPUsinstead oflargeservernodes with outcompromisingper for mance. Asafinalcontribution,weopen-source allmodels,deployment and fine-tuningnotebooks,andthe Open VLAcode base for training VLAs at scale, withthehope that the seres our cesenablefutureworkexploring and adapting VLAs for robotics. 2 Related Work Visually-Conditioned Language Models Visually-conditionedlanguagemodels(VLMs),which aretrainedon Internet-scale data togeneratenaturallanguage from inputimage(s)andlanguage prompts,have been adopted for myriadapplications from visualquestionanswering[28\u201331]toobject localization[32,33]. Oneof the keyadvancesfuelingrecent VLMs are modelarchitectures that bridgefeatures from pretrainedvisionencoders[8,9,25]withpretrainedlanguagemodels[10,23,34\u2013 36],directlybuildingonadvancesinbothcomputervision and naturallanguage model lingtocreate powerfulmultimodalmodels. Whileearlyworkexploredvariousarchitectures for cross-attending betweenvision and languagefeatures[37\u201341],newopen-source VLMs[20,42\u201344]haveconverged onasimpler\u201cpatch-as-token\u201dapproach,inwhichpatchfeatures from pretrainedvisualtrans for mers aretreatedastokens,and are thenprojectedinto the inputspaceofalanguage model. Thissimplicity makesiteasytorepurposeexistingtools for traininglanguage model sat scale for VLMtraining. We employ the setoolsin our workto scale VLAtraining,andspecificallyuse VLMs from Karamcheti etal.[44]asourpretrainedbackbone,asthey are trained from multi-resolutionvisualfeatures,fusing low-levelspatialin for mation from DINOv 2[25]withhigher-levelsemantics from Sig LIP[9]toaid invisualgeneralization. Generalist Robot Policies Arecenttrendinroboticsworkstowardstrainingmulti-task\u201cgeneralist\u201d robotpolicies[2,6,45\u201349]onlargediverserobot data sets[1,2,6,11,45,49\u201356],spanningmany differentrobotembodiments[1,5,53,57\u201366]. Notably,Octo[5]trainsageneralistpolicy that can controlmultiplerobotsout-of-the-box and allows for flexiblefine-tuningto new robotsetups. A keydifferencebetween the seapproaches and Open VLAis the modelarchitecture. Priorworkslike Octotypicallycomposepretrainedcomponentssuchaslanguageembeddingsorvisualencoders with additional model componentsinitialized from scratch[2,5,6],learningto\u201cstitch\u201dthemtogether during the course of policy training. Unlike these works, Open VLA adopts a more end-to-end approach, directly fine-tuning VLMs to generate robot actions by treating them as tokens in the language model vocabulary. Ourexperimentalevaluationshows that thissimpleyetscalablepipeline substantiallyboostsper for mance and generalizationabilityoverpriorgeneralistpolicies. Vision-Language-Action Models Anumberofworks have explored the useof VLMs for robotics, e.g.,forvisualstaterepresentations[12,13],objectdetection[67],high-levelplanning[16],andfor providingafeedbacksignal[68\u201371]. Othersintegrate VLMsdirectlyintoend-to-endvisuomotor manipulation policies [14, 15], but incorporate significant structure into the policy architecture or require calibrated cameras, which limits their applicability. A number of recent works have exploredsimilarrecipestoours and directlyfine-tunedlargepretrained VLMs for predictingrobot actions[1,7,17,18,72\u201374]. Suchmodels are oftenreferredtoasvision-language-actionmodels (VLAs), since they fuse robot control actions directly into VLM backbones. This has three key benefits: (1)itper",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 2,
      "paper_id": "openvla",
      "text": "15], but incorporate significant structure into the policy architecture or require calibrated cameras, which limits their applicability. A number of recent works have exploredsimilarrecipestoours and directlyfine-tunedlargepretrained VLMs for predictingrobot actions[1,7,17,18,72\u201374]. Suchmodels are oftenreferredtoasvision-language-actionmodels (VLAs), since they fuse robot control actions directly into VLM backbones. This has three key benefits: (1)itper for msalignmentofpretrainedvision and languagecomponentsonalarge,Internet- scalevision-language data set,(2)theuseofagenericarchitecture,notcustom-made for robotcontrol, allowsustoleverage the scalableinfrastructureunderlyingmodern VLMtraining[75\u201377]andscale totrainingbillion-parameterpolicies with minimalcodemodifications,and(3)itprovidesadirect pathway for roboticstobenefit from the rapidimprovementsin VLMs. Existingworkson VLAs eitherfocusontraining and evaluatinginsinglerobotorsimulatedsetups[72\u201374,78]andthuslack generality,orareclosed and donotsupportefficientfine-tuningto new robotsetups[1,7,17,18]. Mostcloselyrelated,RT-2-X[1]trainsa 55 B-parameter VLApolicyon the Open X-Embodiment dataset and demonstratesstate-of-the-artgeneralistmanipulationpolicyper for mance. However,our work differs from RT-2-X in multiple important aspects: (1) by combining a strong open VLM 3 Open VLA Action De-Tokenizer \u0394x 3 \u0394\u03b8 Llama 2 7 B \u0394Grip 7 D Robot Input Image Action 2 MLP Projector Llama Tokenizer \u201cPut eggplant Dino V 2 Sig LIP in bowl\u201d 1 Language Instruction \u201cWhat should the robot do to {task}? A:\u201d Figure 2:Open VLA model architecture.Givenanimageobservation and alanguageinstruction,themodel predicts 7-dimensionalrobotcontrolactions.Thearchitectureconsistsofthreekeycomponents:(1)avision encoder that concatenates Dino V 2[25]and Sig LIP[79]features,(2)aprojector that mapsvisualfeaturesto thelanguageembeddingspace,and(3)the LLMbackbone,a Llama 27 B-parameterlargelanguage model[10]. backbone with aricherrobotpretraining data set,Open VLAoutperforms RT-2-Xin our experiments whilebeinganorderofmagnitudesmaller;(2)wethoroughlyinvestigatefine-tuningof Open VLA modelsto new targetsetups,while RT-2-Xdoesnotinvestigate the fine-tuningsetting;(3)weare thefirsttodemonstrate the effectivenessofmodernparameter-efficientfine-tuning and quantization approaches for VLAs;and(4)Open VLAis the firstgeneralist VLA that isopen-source and thus supportsfutureresearchon VLAtraining,datamixtures,objectives,andinference. 3 The Open VLAModel Weintroduce the Open VLAmodel,a 7 B-parametervision-language-action model(VLA)trained on 970 krobotdemonstrations from the Open X-Embodiment data set[1]. There are many,largely unexplored, questions around best practices for developing VLA models, e.g., what are the best modelbackbones,datasets,andhyperparameterstouse for training. Below,wedetail our approach fordeveloping Open VLA and summarize our keylearnings. Concretely, wefirst provideabrief overviewofmodern VLMs, whichform the backboneof Open VLA(Section 3.1); thendescribe our basic training recipe and dataset (Section 3.2 and Section 3.3); discuss key design decisions (Section 3.4);andprovidedetailsof the usedinfrastructure for training and inference(Section 3.5). 3.1 Preliminaries: Vision-Language Models Thearchitectureofmostrecent VLMs[20,42\u201344]consistsofthreemainparts(see Fig.2): (1)a visualencoder that mapsimageinputstoanumberof\u201cimagepatchembeddings\u201d,(2)aprojector that takes the output embeddings of the visual encoder and maps them into the input space of a language model, and(3)alargelanguage model(LLM)backbone. During VLMtraining, the modelistrainedend-to-end with anexttexttokenpredictionobjectiveonpairedorinterleavedvision andlanguage data curated from various Internets our ces. Inthiswork,webuildon the Prismatic-7 BVLM[44]. Prismaticfollows the samest and ardarchitec- turedescribedabove,witha 600 M-parametervisualencoder,asmall 2-layer MLPprojector,anda 7 B-parameter Llama 2 language model backbone[10]. Notably,Prismaticusesatwo-partvisualen- coder,consistingofpretrained Sig LIP[79]and Dino V 2[25]models. Inputimagepatches are passed separatelythroughbothencoders and the resultingfeaturevectors are concatenatedchannel-wise. In contrastto the morecommonlyusedvisionencoderssuchas CLIP-[80]or Sig LIP-onlyencoders, theadditionof Dino V 2 featureshas been showntobehelpful for improvedspatialreasoning[44], which can beparticularlyhelpful for robotcontrol. Sig LIP,Dino V 2,and Llama 2 donotreleasedetailsabout the irtraining data,whichlikelyconsistsof trillionsoftokensof Internet-sourcedimage-text,image-only,andtext-onlydat are spectively. The Prismatic VLMisfine-tunedontopof the secomponentsusing the LLa VA 1.5 datamixture[43], whichcontainsatotalofapproximately 1 Mimage-text and text-only data samples from open-source datasets[29,42,81\u201383]. 4 3.2 Open VLATraining Procedure Totrain Open VLA,wefine-tuneapretrained Prismatic-7 BVLMbackbone for robotactionprediction (see Fig.2). Weformulate the actionpredictionproblemasa\u201cvision-language\u201dtask,whereaninput observationimage and anaturallanguage task instruction are mappedtoastringofpredictedrobot actions[7]. Toenable the VLM\u2019slanguage model backbonetopredictrobotactions,werepresent theactionsin the outputspaceof the LLMbymappingcontinuousrobotactionstodiscretetokens usedby the language model\u2019stokenizer. Following Brohanetal.[7],wediscretizeeachdimensionof therobotactionsseparatelyintooneof 256 bins. Foreachactiondimension,weset the binwidth touni for mlydivide the intervalbetween the 1 stand 99 thquantileoftheactionsin the training data. Usingquantilesinsteadof the min-maxbounds Brohanetal.[7]usedallowsustoignoreoutlier actionsinthe data that couldotherwisedrasticallyexp and the discretizationinterval",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 3,
      "paper_id": "openvla",
      "text": "instruction are mappedtoastringofpredictedrobot actions[7]. Toenable the VLM\u2019slanguage model backbonetopredictrobotactions,werepresent theactionsin the outputspaceof the LLMbymappingcontinuousrobotactionstodiscretetokens usedby the language model\u2019stokenizer. Following Brohanetal.[7],wediscretizeeachdimensionof therobotactionsseparatelyintooneof 256 bins. Foreachactiondimension,weset the binwidth touni for mlydivide the intervalbetween the 1 stand 99 thquantileoftheactionsin the training data. Usingquantilesinsteadof the min-maxbounds Brohanetal.[7]usedallowsustoignoreoutlier actionsinthe data that couldotherwisedrasticallyexp and the discretizationinterval and reduce the effectivegranularityof our actiondiscretization. Using this discretization,weobtain Ndiscreteintegers\u2208[0...255]foran N-dimensionalrobotac- tion. Unfortunately,thetokenizerusedby Open VLA\u2019slanguagebackbone,the Llamatokenizer[10], only reserves 100 \u201cspecial tokens\u201d for tokens newly introduced during fine-tuning, which is too fewfor the 256 tokensof our actiondiscretization. Instead,weagainopt for simplicity and follow Brohanetal.[7]\u2019sapproachbysimplyoverwriting the 256 leastusedtokensin the Llamatokenizer\u2019s vocabulary(whichcorrespondsto the last 256 tokens)with our actiontokens. Once the actions are processed into a sequence of tokens, Open VLA is trained with a standard next-token prediction objective, evaluating the cross-entropy loss on the predicted action tokens only. We discuss key designdecisions for implementing this trainingprocedurein Section 3.4. Next,wedescribe the robot datasetwe usefor Open VLAtraining. 3.3 Training Data The goal in constructing the Open VLA training dataset is to capture a large diversity of robot embodiments,scenes,andtasks. Thisenables the final model tocontrolvariousrobotsoutof the box and admits efficient fine-tuning to new robot setups. We leverage the Open X-Embodiment dataset[1](Open X)asa base tocurate our training data set. Thefull Open Xdataset,atthetimeof writing,consistsofmorethan 70 individualrobot data sets,withmorethan 2 Mrobottrajectories, that were pooledintoacoherent and easy-to-usedata for matinalargecommunityeffort. Tomake trainingon this datapractical,weapplymultiplestepsof data curationto the raw data set. The goals of this curation are to ensure (1) a coherent input and output space across all training datasets,and(2)abalancedmixofembodiments,tasks,andscenesin the finaltrainingmixture.2 To address(1),wefollow[1,5]andrestrict our trainingdatasettocontainonlymanipulation data sets withatleastone 3 rdpersoncamera and usesingle-armend-effectorcontrol. For(2),weleverage the datamixtureweightsof Octo[5]forall data sets that pass the firstroundoffiltering.Octoheuristically down-weightsorremoveslessdiverse data setsandup-weights data sets with larger task and scene diversity;see Octo Model Teametal.[5]fordetails. Wealsoexperimented with incorporatingafewadditional data setsinto our trainingmixture that were addedto the Open Xdatasetsince the releaseof Octo,including the DROID data set[11],althoughata conservativemixtureweightof 10%. Inpractice,wefound that the actiontokenaccuracyon DROID remainedlowthroughouttraining,suggestingalargermixtureweightor model mayberequiredtofit itsdiversityin the future. Tonotjeopardizethequalityof the final model,weremoved DROID from the data mixture for the finalthirdoftraining. Weprovideacompleteoverviewof the used data sets andmixtureweightsin Appendix A. 3.4 Open VLADesign Decisions When developing the Open VLA model, we explored various design decisions in smaller-scale experiments before starting the final model training run. Concretely, we trained and evaluated Open VLA model son Bridge Data V 2[6]for our initialexperiments,insteadoftrainingon the full 2 Octo[5]demonstratedtrainingacross data sets with heterogeneoussensoryinputs.Whileverypromising, weleaveaninvestigationof VLAtrainingacrossheterogeneoussensormodalities and actionspacestofuture work. 5 Open X mixture, to increase iteration speed and reduce computational cost. We summarize key learnings from the seexplorationsbelow. VLM Backbone. Initially, we experimented with multiple VLM backbones. Apart from Pris- matic[44],wetestedfine-tuning IDEFICS-1[84]and LLa VA[85]forrobotactionprediction. We found that LLa VAand IDEFICS-1 per for medcomparablyontasks with onlyoneobjectin the scene, but LLa VAdemonstratedstrongerlanguagegroundingintasks that involvedmultipleobjectsin the scene and requiredthepolicytomanipulate the correctobject,i.e.,theobjectspecifiedin the language instruction.Concretely,LLa VAimprovedupon IDEFICS-1 by 35%inabsolutesuccessrate,averaged acrossfivelanguagegrounding task sina Bridge Data V 2 sinkenvironment. Thefine-tuned Prismatic VLMpolicyachievedfur the rimprovements,outper for ming the LLa VApolicybyroughly 10%in absolutesuccessrateacrossbothsimplesingle-objecttasks and multi-object,languagegrounding tasks. Weattribute this performancedeltatoimprovedspatialreasoningcapabilitiesaf for dedby the fused Sig LIP-Dino V 2 backbones(see Section 3.1). Inadditionto the per for manceenhancements, Prismatic also provides a modular and easy-to-use code base, so we ultimately chose it to",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 4,
      "paper_id": "openvla",
      "text": "sinkenvironment. Thefine-tuned Prismatic VLMpolicyachievedfur the rimprovements,outper for ming the LLa VApolicybyroughly 10%in absolutesuccessrateacrossbothsimplesingle-objecttasks and multi-object,languagegrounding tasks. Weattribute this performancedeltatoimprovedspatialreasoningcapabilitiesaf for dedby the fused Sig LIP-Dino V 2 backbones(see Section 3.1). Inadditionto the per for manceenhancements, Prismatic also provides a modular and easy-to-use code base, so we ultimately chose it to be the backbone for the Open VLAmodel. Image Resolution. The resolution of input images has significant impact on the computational requirementsof VLAtraining, sincehigher-resolutionimagesresultinmoreimagepatchtokens andthuslongercontextlengths that quadraticallyincreasetrainingcompute. Wecompared VLAs with 224\u00d7224 pxand 384\u00d7384 pxinputs,butfoundnoper for mancedifferencein our evaluations, while the latter takes 3 x longer to train. We thus opt for a resolution of 224 \u00d7 224 px for the final Open VLAmodel. Note that onmany VLMbenchmarks,increasedresolutiondoesimprove per for mance[44,86,87],butwedidnotsee this trend(yet)for VLAs. Fine-Tuning Vision Encoder. Prior work on VLMs found that freezing vision encoders during VLMtrainingtypicallyleadstohigherper for mance[44]. Intuitively,afrozenvisionencodermay betterpreserve the robustfeatureslearned from its Internet-scalepretraining. However,wefound fine-tuning the visionencoderduring VLAtrainingtobecrucial for good VLAper for mance. We hypothesize that the pretrainedvisionbackbonemaynotcapturesufficientfine-grainedspatialdetails aboutimportantpartsof the scenetoenablepreciseroboticcontrol. Training Epochs. Typical LLMor VLMtrainingrunscompleteatmostoneortwoepochsthrough their training dataset. In contrast, we found it important for VLA training to iterate through the training data setsignifi can tlymoretimes,withrealrobotper for mancecontinuallyincreasinguntil trainingactiontokenaccuracysurpasses 95%. Ourfinaltrainingruncompletes 27 epochsthroughits training data set. Learning Rate. Weswept the learningrateacrossmultipleordersofmagnitude for VLAtraining, andachieved the bestresultsusingafixedlearningrateof 2 e-5(thesamelearningrateusedduring VLMpretraining[44]). Wedidnotfindlearningratewarmuptoprovidebenefits. 3.5 Infrastructure for Training and Inference The final Open VLA model is trained on a cluster of 64 A 100 GPUs for 14 days, or a total of 21,500 A 100-hours,usingabatchsizeof 2048. Duringinference,Open VLArequires 15 GBof GPU memorywhenloadedinbfloat 16 precision(i.e.,withoutquantization)andrunsatapproximately 6 Hzonone NVIDIARTX 4090 GPU(withoutcompilation,speculativedecoding,oro the rinference speed-uptricks). we canfurtherreduce the memoryfootprintof Open VLAduringinferencevia quantization,withoutcompromisingper for manceinreal-worldroboticstasks,asshownin Section 5.4. Wereportinferencespeedonvariousconsumer-andserver-grade GPUsin Fig.6.Forconvenience,we implement are mote VLAinferenceservertoallowreal-timeremotestreamingofactionpredictions to the robot \u2013 removing the requirement of having access to a powerful local compute device to control the robot. Werelease this remoteinferencesolutionaspartof our open-sourcecoderelease (Section 4). 4 The Open VLACode base Along with ourmodel,werelease the Open VLAcode base,amodular Py Torchcode base for training VLA models (see https://openvla.github.io). It scales from fine-tuning VLAs on individ- ual GPUstotrainingbillion-parameter VLAsonmulti-node GPUclusters, andsupportsmodern 6 87.0 85.0 90.0 70.6 76.7 60.0 50.6 52.0 55.0 38.836.3 40.0 18.520.0 29.0 25.0 20.0 26.7 26.3 30.0 8.0 7.5 10.0 0.0 (Un d s is a e t p e ra n p c e b t a o a r r c a s k n , g c o r e b o s j u e ) n c d t s, (Unseen o r o ie b n je ta c t t i o p n o s s ) itions & (Unseen s h o a b p je e c s t ) sizes & ( & U n c s o e n e c n e p o t b s j e fr c o t m s, t in h s e t r I u n c te ti r o n n e s t) , (Ab s ili p ty e c to ifi e m p d r a o i n m n i p",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 5,
      "paper_id": "openvla",
      "text": "e fr c o t m s, t in h s e t r I u n c te ti r o n n e s t) , (Ab s ili p ty e c to ifi e m p d r a o i n m n i p p la u t n ) la g t u e a o g b e j ect Put Yellow Corn Stack Blue Cup Put {Red Bottle, on Pink Plate Lift Eggplant Flip Pot Upright on Pink Cup Eggplant} into Pot Figure 3: Bridge Data V 2 Widow Xrobotevaluationtasks and results. Weevaluate Open VLA and prior state-of-the-artgeneralistrobotpoliciesonacomprehensivesuiteof task scoveringseveralaxesofgeneralization, aswellastasks that specificallyassesslanguageconditioningability.Open VLAachieveshighestoverallper for- mance and evenoutper for msclosed-source model RT-2-Xinallcategoriesexcept for semanticgeneralization. Averagesuccessrates\u00b1Std Err are computedacross 170 totalrolloutsperapproach.See Table 4 fordetailed results. techniquesforlargetrans for mer model trainingsuchasautomaticmixedprecision(AMP,Py Torch [75]),Flash Attention[76],and full ysharded data parallelism(FSDP,Zhaoetal.[77]). Outof the box, the Open VLAcode base has full support for trainingon the Open Xdataset, integrates with Hugging Face\u2019s[21]Auto Modelclass, andsupports Lo RAfine-tuning[26]andquantized model inference[27,88]. 5 Experiments Thegoalof our experimentalevaluationsistotest Open VLA\u2019sabilitytoserveasapowerfulmulti- robotcontrolpolicyoutof the box,aswellasbeagoodinitialization for fine-tuningto new robot tasks. Concretely,weaimtoanswer the followingquestions: 1. Howdoes Open VLAcomp are topriorgeneralistrobotpolicies,whenevaluatingonmultiple robots and varioustypesofgeneralization? 2. Can Open VLAbeeffectivelyfine-tunedona new robotsetup and task,andhowdoesit comp are tostate-of-the-artdata-efficientimitationlearningapproaches? 3. Canwe useparameter-efficientfine-tuning and quantizationtoreduce the computationalre- quirements for training and inferenceof Open VLAmodels and make the mmoreaccessible? What are the per for mance-computetrade-offs? 5.1 Direct Evaluationson Multiple Robot Platforms Robot Setups and Tasks. Weevaluate Open VLA\u2019sper for mance\u201cout-of-the-box\u201dontworobot embodiments: the Widow X robot from the Bridge Data V 2 evaluations [6] (see Fig. 1, left) and the mobile manipulation robot from the RT-1 and RT-2 evaluations [2, 7] (\u201cGoogle robot\u201d; see Fig.1,middle). Bothplatforms have beenextensivelyusedinpriorworks for evaluatinggeneralist robotpolicies[1,2,5,7]. Wedefineacomprehensivesetofevaluation task sineachenvironment thatcoversvariousaxesofgeneralization,suchasvisual(unseenbackgrounds,distractorobjects, colors/appearancesofobjects);motion(unseenobjectpositions/orientations);physical(unseenobject sizes/shapes); and semantic (unseen target objects, instructions, and concepts from the Internet) generalization. Wealsoassesslanguageconditioningabilityinscenes with multipleobjects,testing whether the policy can manipulate the correcttargetobject,asspecifiedin the user\u2019sprompt. See bottomrowof Fig.3 and Fig.4 forexample task imagesin the Bridge Data V 2 and Googlerobot evaluations,respectively. Overall,weevaluatedeachmethodin 170 rollouts(17 tasks with 10 trials each)for Bridge Data V 2 experiments and 60 rollouts(12 tasks with 5 trialseach)for Googlerobot experiments. A detailed breakdown of all tasks and how they differ from the training data is in 7 Appendix B.Allevaluationsin this and the followingsections are conductedas A/Bevaluations, usingthesametasks with the samesetsofinitialrobot and objectstates,toensurefaircomparison. Comparisons. Wecomp are Open VLA\u2019sper for mancetothreepriorgeneralistmanipulationpolicies: RT-1-X[1],RT-2-X[1],and Octo[5]. RT-1-X(35 Mparameters)and Octo(93 Mparameters)are trans for merpoliciestrained from scratchonsubsetsof the Open Xdataset;Octois the state-of-the-art model among open-source manipulation policies. RT-2-X (55 B parameters) is a state-of-the-art, closed-source VLA that leverages Internet-pretrainedvision and languagebackbones. Theresults are summarizedin Fig.3 for Bridge Data V 2 evaluations and Fig.4 for Googlerobot evaluations(per-taskbreakdownin Appendix,Table 4 and Table 6). Wefind that both RT-1-Xand Octo struggle on the tested tasks, often failing to manipulate the correct object, especially when distractors are present,andinsomecasescausing the robottowaveitsarmaroundaimlessly. Note that our evaluations test even larger degrees of generalization than the evaluations per for med in thosepriorworkstochallenge the Internet-pretrained VLAmodels. Thus,lowerper for manceof models with out Internetpretrainingisexpected. RT-2-Xclearlyoutper for msboth RT-1-Xand Octo, demonstrating the benefitsoflarge,pretrained VLMs for robotics.",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 6,
      "paper_id": "openvla",
      "text": "manipulate the correct object, especially when distractors are present,andinsomecasescausing the robottowaveitsarmaroundaimlessly. Note that our evaluations test even larger degrees of generalization than the evaluations per for med in thosepriorworkstochallenge the Internet-pretrained VLAmodels. Thus,lowerper for manceof models with out Internetpretrainingisexpected. RT-2-Xclearlyoutper for msboth RT-1-Xand Octo, demonstrating the benefitsoflarge,pretrained VLMs for robotics. Notably,Open VLAper for mscomparablyto RT- 2-X on Google robot evaluations and signifi- 78.3 85.0 88.0 82.982.9 cantlyoutperforms RT-2-Xon Bridge Data V 2 72.0 evaluations despite being an order of magni- tude smaller (7 B vs. 55 B parameters). Qual- 44.0 33.3 26.7 32.0 34.3 itatively, we find that both RT-2-X and Open- VLA exhibit markedly more robust behaviors 14.3 thantheo the rtestedmodels,suchasapproach- ing the correct object when distractor objects (Tasks & conditions seen in (Unseen objects, tasks, training data) backgrounds, & concepts) arepresent,properlyorienting the robot\u2019send- effector to align with the orientation of the target object, and even recovering from mis- Move Coke Can takes such as insecurely grasping objects (see Pick Coke Can to Taylor Swift https://openvla.github.ioforqualitative Figure 4:Googlerobotevaluationresults.Weevaluate rolloutexamples). RT-2-Xachieveshigherper- generalistrobotpoliciesonin-distribution and out-of- formance in semantic generalization tasks, as distribution(OOD)taskson the mobilemanipulatorused in RT-1 and RT-2 evaluations[2,7].Wefind that Open- shown in Fig. 3, which is expected given that VLAand RT-2-Xattaincomparableper for mance and ituseslarger-scale Internetpretraining data and signifi can tlyoutperform RT-1-Xand Octooverall.Aver- isco-fine-tuned with bothrobotaction data and agesuccessrates\u00b1Std Err are computedacross 60 total Internetpretraining data tobetterpreserve the rolloutsperapproach.See Table 6 fordetailedresults. pretraining knowledge, rather than being fine- tunedsolelyonrobot data,like Open VLA.However,Open VLAper for mscomparablyorbetterin all other task categories in both Bridge Data V 2 and Google robot evaluations. The per for mance difference can beattributedtoacombinationoffactors: wecuratedamuchlargertraining data set for Open VLAwith 970 ktrajectories(vs. 350 kfor RT-2-X);weper for medmorec are fulcleaningof thetraining data set and,e.g.,filteredoutall-zeroactionsin the Bridge data set(see Appendix Cfora detaileddiscussion);and Open VLAusesafusedvisionencoder that combinespretrainedsemantic andspatialfeatures. See Appendix Dforablationanalysesof the secomponents. 5.2 Data-Efficient Adaptationto New Robot Setups Whilepriorworksmainlyfocusedondirectlyevaluating VLAs\u201cout-of-the-box\u201d[1,7,16],effective fine-tuningof VLA model sto new tasks and robotsetupsislargelyunexplored,yetiskey for their widespreadadoption. Inthissection,weinvestigate Open VLA\u2019sabilitytobequicklyadaptedtoa newreal-worldrobotsetup. (See Appendix Eforfine-tuningexperimentsinsimulation.) Robotsetups and tasks. Wetestasimplefine-tuningrecipe for the Open VLAmodel: fullfine- tuningofall model parameters,usingsmall data sets with 10\u2013150 demonstrationsofatarget task(see Fig.5;weexploreparameter-efficientfine-tuningapproachesin Section 5.3). Wetest Open VLAin twosetups: Franka-Tabletop,astationary,table-mounted Franka Emika Panda 7-Do Frobotarm; and Franka-DROID,the Frankarobotarmsetup from the recentlyreleased DROID data set[11], 8 Franka-Tabletop Franka-DROID 43.437.141.535.2 63.8 66.7 53.5 33.3 46.7 60.0 93.3 80.0 13.3 53.3 83.3 63.3 26.7 70.0 93.3 19.427.830.6 16.7 69.4 27.822.2 66.769.4 77.8 16.725.0 91.7 44.450.0 35.0 26.7 38.3 21.7 58.3 0.0 Narrow Single-Instruction Tasks Diverse Multi-Instruction Tasks Visual Robustness Figure 5: Adapting to new robot setups. We evaluate the state-of-the-art Diffusion Policy trained from scratchonseven Franka Emika Pan data sks(10\u2013150 demonstrationseach),aswellasgeneralistrobotpolicies Octo and Open VLAfine-tunedon the same data. Diffusion Policyexhibitsstrongper for manceonnarrow single-instructiontasks,while Octo and Open VLAper for mbetterondiversefine-tuning task sinvolvingmultiple instructions and distractorobjects. Overall,Open VLAachieveshighestaggregateper for manceacrossboth setups,suggesting that itisaneffectivedefault for learningapolicyonadownstream task.Averagesuccessrates \u00b1Std Err are computedacross 129 rolloutsperapproach(99 for Franka-Tabletoptasks and 30 for Franka-DROID tasks).See Table 7 fordetailedresults. mounted on a movable standing desk. The setups use 5 Hz and 15 Hz non-blocking controllers, respectively. Wechoose Frankarobotarmsas the targetembodiment for ourfine-tuningexperiments sincethey are widelyusedin the robotlearningcommunity and thusalikely\u201ctarget\u201dof Open",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 7,
      "paper_id": "openvla",
      "text": "for learningapolicyonadownstream task.Averagesuccessrates \u00b1Std Err are computedacross 129 rolloutsperapproach(99 for Franka-Tabletoptasks and 30 for Franka-DROID tasks).See Table 7 fordetailedresults. mounted on a movable standing desk. The setups use 5 Hz and 15 Hz non-blocking controllers, respectively. Wechoose Frankarobotarmsas the targetembodiment for ourfine-tuningexperiments sincethey are widelyusedin the robotlearningcommunity and thusalikely\u201ctarget\u201dof Open VLA fine-tuning. Wetestonsetups with differentcontrolfrequenciestotest Open VLA\u2019sapplicabilitytoa rangeofusecases. Comparisons. Wecompareto Diffusion Policy[3],astate-of-the-artdata-efficientimitationlearning approach, trained from scratch. We also comp are to Diffusion Policy (matched), a version of Diffusion Policy that matches the input and outputspecificationsof Open VLA.3 Additionally,we evaluate Octo[5]fine-tunedon the target data set,sinceitiscurrently the bestgeneralistpolicy that supportsfine-tuning (fine-tuningof RT-2-X isnotsupported throughits inference API).Wealso fine-tune Open VLAon the sametarget data set,and the resultingpolicyisdenotedby Open VLA. Finally, as an ablation experiment, we comp are to Open VLA (scratch), where we directly fine- tune the underlying base Prismatic VLM on the target robot setup \u2013 rather than fine-tuning the Open X-pretrained Open VLAmodel\u2013toassess the benefitoflarge-scalerobotpretraining. Wepresent the resultsin Fig.5(per-taskbreakdownin Appendix,Table 7). Wefind that bothversions of Diffusion Policy are competitive with oroutperform the generalistpolicies Octo and Open VLA onnarrowersingle-instruction task slike\u201cPut Carrotin Bowl\u201dand\u201cPour Corninto Pot\u201d, butthe pretrainedgeneralistpoliciesper for mbetterinmorediversefine-tuningtasks that involvemultiple objectsin the scene and requirelanguageconditioning. Open Xpretraining for Octo and Open VLA enablesthe model stobetteradaptto the semorediverse task swherelanguagegroundingisimportant; weseeevidence for thisin the lowerper for manceof Open VLA(scratch). Overall,wefind that Open VLAachieves the highestaverageper for mance. Notably,mostpriorworks achievestrongper for manceonlyinei the rnarrowsingle-instructionordiversemulti-instructiontasks, resultinginwidelyvaryingsuccessrates. Open VLAis the onlyapproach that achievesatleast 50% successrateacrossalltestedtasks, suggestingthatit can beastrongdefaultoption for imitation learning tasks, particularly if they involve a diverse set of language instructions. For narrower but highly dexterous tasks, Diffusion Policy still shows smoother and more precise trajectories; incorporatingactionchunking and temporalsmoothing,asimplementedin Diffusion Policy,may help Open VLAattain the samelevelofdexterity and maybeapromisingdirection for futurework (see Section 6 foradetaileddiscussionofcurrentlimitations). 3 Thefull Diffusion Policyusesatwo-stepobservationhistory with bothimages and proprioceptivestate,and per for msrecedinghorizoncontrolbypredictingachunkof T futureactions and executing the first Xactionsin open-loopfashionbe for epredicting the nextchunk(for 15 Hzcontrol,weset T =16,X =8 likein the DROID priorwork[11];for 5 Hzcontrol,wereduce the chunksizesto T = 8,X = 3). Itisalso the onlymethod in Section 5.2 thatpredictsabsolute Cartesiancoordinatestocontrol the robot;allo the rmethodsuserelative positioncontrol.Diffusion Policy(matched)usesasingleimageasinput,hasnoproprioceptivein for mation and noobservationhistory,andpredictsasinglerelativepositioncontrolaction with outactionchunking. 9 5.3 Parameter-Efficient Fine-Tuning The full fine-tuningrunsof Open VLAin the previoussectionused 8 A 100 GPUs for 5-15 hoursper task(dependingon the datasetsize)toachievehighper for mance. While this issubstantiallyless computethanwhatisrequired for VLApretraining,inthissectionweexploreevenmorecompute- andparameter-efficientfine-tuningapproaches and investigate the ireffectiveness. Concretely, we comp are the follow- Table 1:Parameter-efficientfine-tuningevaluation.Lo RAfine- ingfine-tuningapproaches: fullfine- tuningachieves the bestper for mance-computetrade-off,matching tuningupdatesallweightsduringfine- fullfine-tuningper for mancewhiletrainingonly 1.4%ofthe model tuning, as described in Section 5.2; parameters.Meansuccess\u00b1Std Errcomputedacross 33 rolloutsper lastlayeronlyfine-tunesonly the last approachonselect Franka-Tabletoptasks(see Table 8 fordetails). \u2217:Shardedacross 2 GPUs with FSDP[77]. layerof Open VLA\u2019strans for merback- bone and the tokenembeddingmatrix; Strategy Success Rate Train Params(\u00d7106) VRAM(batch 16) frozen vision freezes the vision en- Full FT 69.7\u00b17.2% 7,188.1 163.3 GB* coderbutfine-tunesallo the rweights; Lastlayeronly 30.3\u00b16.1% 465.1 51.4 GB sandwich fine-tuning unfreezes the Frozenvision 47.0\u00b16.9% 6,760.4 156.2 GB* Sandwich 62.1\u00b17.9% 914.2 64.0 GB vision encoder,tokenembeddingma- Lo RA,rank=32 68.2\u00b17.5% 97.6 59.7 GB trix, and last layer; and Lo RA uses rank=64 68.2\u00b17.8% 195.2 60.5 GB thepopularlow-rankadaptationtech- niqueof Huetal.[26]withmultiplerankvaluesr,appliedtoalllinearlayersof the model. Wereportfine-tuningsuccessratesacrossmultiple Franka-Tabletoptasks,aswellastrainingparame- tercount and GPUmemoryrequirements,in Table 1.4",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 8,
      "paper_id": "openvla",
      "text": "465.1 51.4 GB sandwich fine-tuning unfreezes the Frozenvision 47.0\u00b16.9% 6,760.4 156.2 GB* Sandwich 62.1\u00b17.9% 914.2 64.0 GB vision encoder,tokenembeddingma- Lo RA,rank=32 68.2\u00b17.5% 97.6 59.7 GB trix, and last layer; and Lo RA uses rank=64 68.2\u00b17.8% 195.2 60.5 GB thepopularlow-rankadaptationtech- niqueof Huetal.[26]withmultiplerankvaluesr,appliedtoalllinearlayersof the model. Wereportfine-tuningsuccessratesacrossmultiple Franka-Tabletoptasks,aswellastrainingparame- tercount and GPUmemoryrequirements,in Table 1.4 Wefind that onlyfine-tuning the network\u2019s lastlayerorfreezing the visionencoderleadstopoorper for mance,suggesting that fur the radaptation of the visual features to the target scene is crucial. In contrast, \u201csandwich fine-tuning\u201d achieves betterper for mancesinceitfine-tunes the visionencoder,anditconsumesless GPUmemorysince it does not fine-tune the full LLM backbone. Lastly, Lo RA achieves the best trade-off between per for mance and trainingmemoryconsumption,outper for ming\u201csandwichfine-tuning\u201dandmatching fullfine-tuningper for mancewhilefine-tuningonly 1.4%oftheparameters. Wefind that the Lo RA rankhasnegligibleeffectonpolicyper for mance and thusrecommendusingadefaultrankofr =32. With Lo RA,we canfine-tune Open VLAona new task within 10-15 hoursonasingle A 100 GPU\u2013 an 8 xreductionincomputecomp are dto full fine-tuning. 5.4 Memory-Efficient Inferencevia Quantization Precision Bridge Success VRAM bfloat 16 71.3\u00b14.8% 16.8 GB int 8 58.1\u00b15.1% 10.2 GB int 4 71.9\u00b14.7% 7.0 GB N/A Table 2:Per for mance with quantizedin- ference.4-bitquantizationmatches the per- Figure 6: Open VLAinferencespeed for various GPUs.Both formanceofbfloat 16 inference(ourdefault bfloat 16 andint 4 quantizationachievehighthroughput,especially approach)whilereducing the GPUmemory on GPUs with Ada Lovelacearchitecture(RTX 4090,H 100).Fur- footprintbymorethanhalf.Meansuccess therspeed-ups are possible with modern LLMinferenceframe- \u00b1Std Errcomputedacross 8 representative workslike Tensor RT-LLM[89]. \u2660: Modelshardedacrosstwo Bridge Data V 2 tasks[6]and 80 rolloutsper GPUstofit. approach(see Table 5 fordetails). Open VLA,a 7 B-parameter model,consumesmorememoryatinferencetimethanprior open-source generalistpoliciessuchas Octo,whichhas<100 Mparameters. Wefollowbest-practices from LLM servingbysaving and loading Open VLAinbfloat 16 precision for inference(ourdefaultapproach), whichcuts the memoryfootprintinhalf,allowingustoserve Open VLAon GPUs with only 16 GB 4 In Section 5.3 and Section 5.4,weexperiment with aversionof the Open VLAmodel that ispretrained with asmallerrobot data mixture(thesame Open Xdatasetmixtureas Octo)andhasaslightlysmallerarchitecture whichonlyusesa Sig LIP[79]visionbackboneinsteadof the fused Dino Sig LIPencoder. Wefind that this simplerarchitecturestillachievesstrongper for manceinbothfine-tuningtasks and\u201cout-of-the-box\u201dtasks. 10 of GPUmemory. Inthissection,wetestwhe the rwe canfurtherreduce the requiredmemory for policyinference and broadenaccessibilityof VLApolicies,byusingmodernquantizationtechniques developed for serving LLMs[27,88]. Theseapproachesloadtheweightsof the networkatlower precision,therebytradingoffreducedmemoryrequirements for potentiallyreducedinferencespeed andaccuracy. Concretely,weinvestigateserving the Open VLAmodel with 8-bitand 4-bitprecisionon 8 represen- tative Bridge Data V 2 tasks. Wereportmemoryfootprint and rolloutper for mancein Table 2. We alsoreportachievablecontrolfrequenciesonvariousconsumer-andserver-grade GPUsin Fig.6. Weobserve that 8-bitquantizationslowsdowninferenceacrossmost GPUs,dueto the overheadof theaddedquantizationoperations. 4-bitinferenceachieveshigherthroughput,sincereduced GPU memorytransfercompensates for the quantizationoverhead. As a result of the reduced inference speed, we observe a substantial per for mance decrease with 8-bitquantization: onthe A 5000 GPUwe use for ourevaluations, we canonlyrun the modelat 1.2 Hz,whichsignifi can tlychangesthesystemdynamicscomp are dto the training data set for the 5 Hznon-blockingcontrollerusedin the Bridge Data V 2 tasks.5 Notably,4-bitquantizationresultsin similarper for manceasbfloat 16 half-precisioninferencedespiterequiringlessthanhalf the amount of GPUmemory. 4-bitquantizedmodels can runat 3 Hzon the A 5000,thusmorecloselymatching thesystemdynamicsduring data collection. 6 Discussion and Limitations Inthiswork,wepresented Open VLA,astate-of-the-art,open-sourcevision-language-action model thatobtainsstrongperformance for cross-embodimentrobotcontrolout-of-the-box. Wealsodemon- strated that Open VLAcan beeasilyadaptedto new robotsetupsviaparameter-efficientfine-tuning techniques. Thecurrent Open VLA model hasseverallimitations. First,itcurrentlyonlysupportssingle-image observations. Inreality,real-worldrobotsetups are heterogeneous,withawiderangeofpossible sensoryinputs[5]. Exp and ing Open VLAtosupportmultipleimage and proprioceptiveinputsaswell asobservationhistoryisanimportantavenue for futurework. Exploring the useof VLMspretrained oninterleavedimage and text data mayfacilitatesuchflexible-input VLAfine-tuning. Secondly,improving the inferencethroughputof Open VLAiscriticaltoenable VLAcontrol for high-frequency control setups such as ALOHA [90], which runs at 50 Hz. This will also enable testing VLAsonmoredexterous,bi-manualmanipulation task sthanwhatweinvestigatedin this work. Exploring",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 9,
      "paper_id": "openvla",
      "text": "Exp and ing Open VLAtosupportmultipleimage and proprioceptiveinputsaswell asobservationhistoryisanimportantavenue for futurework. Exploring the useof VLMspretrained oninterleavedimage and text data mayfacilitatesuchflexible-input VLAfine-tuning. Secondly,improving the inferencethroughputof Open VLAiscriticaltoenable VLAcontrol for high-frequency control setups such as ALOHA [90], which runs at 50 Hz. This will also enable testing VLAsonmoredexterous,bi-manualmanipulation task sthanwhatweinvestigatedin this work. Exploring the useofactionchunkingoralternativeinference-timeoptimizationtechniques suchasspeculativedecoding[91]offerpotentialremedies. Additionally,thereisroom for fur the rper for manceimprovements. While Open VLAoutperforms prior generalist policies, it does not yet offer very high reliability on the tested tasks, typically achieving<90%successrate. Finally, due to compute limitations, many VLA design questions remain underexplored: What effectdoesthesizeof the base VLMhaveon VLAper for mance? Doesco-trainingonrobotaction prediction data and Internet-scalevision-language data substantiallyimprove VLAper for mance? Whatvisualfeatures are best-suited for VLAmodels? Wehope that the releaseof the Open VLA model and code base will enablethecommunitytojointlyinvestigate the sequestions. Acknowledgments We are grateful to the Toyota Research Institute for providing significant funding and compute res our cesrequiredtocarryout this research. Wealsothank the Stanford Center for Researchon Foundation Models for providingadditionalcomputeres our ces and Google Deep Mind for alpha accessto the RT-2-XAPI for ourevaluations. Weacknowledgeadditionalsupport from Volkswagen, Physical Intelligence,ONRgrants N 00014-22-1-2621 and N 00014-22-1-2293,the National Science Foundationthrough IIS-2246811,and DARPAANSR. 5 Weattribute the per for mancelosstolowinferencespeed,sinceboth 8-bitand 4-bitquantizationachieve comparabletokenaccuracytobfloat 16 inferencewhenevaluatedofflineontraining data.See Appendix D.4 for supportingdetails. 11 References [1] Open X-Embodiment Collaboration,A.Padalkar,A.Pooley,A.Jain,A.Bewley,A.Herzog, A.Irpan,A.Khazatsky,A.Rai,A.Singh,A.Brohan,A.Raffin,A.Wahid,B.Burgess-Limerick, B.Kim,B.Sch\u00f6lkopf,B.Ichter,C.Lu,C.Xu,C.Finn,C.Xu,C.Chi,C.Huang,C.Chan, C.Pan,C.Fu,C.Devin,D.Driess,D.Pathak,D.Shah,D.B\u00fcchler,D.Kalashnikov,D.Sadigh, E.Johns,F.Ceola,F.Xia,F.Stulp,G.Zhou,G.S.Sukhatme,G.Salhotra,G.Yan,G.Schiavi, H. Su, H.-S. Fang, H. Shi, H. B. Amor, H. I. Christensen, H. Furuta, H. Walke, H. Fang, I.Mordatch,I.Radosavovic,I.Leal,J.Liang,J.Kim,J.Schneider,J.Hsu,J.Bohg,J.Bingham, J.Wu,J.Wu,J.Luo,J.Gu,J.Tan,J.Oh,J.Malik,J.Tompson,J.Yang,J.J.Lim,J.Silv\u00e9rio, J.Han,K.Rao,K.Pertsch,K.Hausman,K.Go,K.Gopalakrishnan,K.Goldberg,K.Byrne, K.Oslund,K.Kawaharazuka,K.Zhang,K.Majd,K.Rana,K.Srinivasan,L.Y.Chen,L.Pinto, L.Tan,L.Ott,L.Lee,M.Tomizuka,M.Du,M.Ahn,M.Zhang,M.Ding,M.K.Srirama, M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf, N. D. Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer, P. R. Sanketi, P. Wohlhart, P. Xu, P. Sermanet, P. Sund are san, Q. Vuong, R. Rafailov, R. Tian, R. Doshi, R. Mart\u00edn-Mart\u00edn, R.Mendonca,R.Shah,R.Hoque,R.Julian,S.Bustamante,S.Kirmani,S.Levine,S.Moore, S. Bahl, S. Dass, S. Song, S. Xu, S. Haldar, S. Adebola, S. Guist, S. Nasiriany, S. Schaal, S.Welker,S.Tian,S.Dasari,S.Belkhale,T.Osa,T.Harada,T.Matsushima,T.Xiao,T.Yu, T.Ding,T.Davchev,T.Z.Zhao,T.Armstrong,T.Darrell,V.Jain,V.Vanhoucke,W.Zhan, W.Zhou,W.Burgard,X.Chen,X.Wang,X.Zhu,X.Li,Y.Lu,Y.Chebotar,Y.Zhou,Y.Zhu, Y.Xu,Y.Wang,Y.Bisk,Y.Cho,Y.Lee,Y.Cui,Y.hua Wu,Y.Tang,Y.Zhu,Y.Li,Y.Iwasawa, Y.Matsuo,Z.Xu,and Z.J.Cui. Open X-Embodiment: Roboticlearning data sets and RT-X models. https://arxiv.org/abs/2310.08864,2023. [2] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus- man, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, R.Julian,D.Kalashnikov,Y.Kuang,I.Leal,K.-H.Lee,S.Levine,Y.Lu,U.Malla,D.Manju- nath,I.Mordatch,O.Nachum,C.Parada,J.Peralta,E.Perez,K.Pertsch,J.Quiambao,K.Rao, M.Ryoo,G.Salazar,P.Sanketi,K.Sayed,J.Singh,S.Sontakke,A.Stone,C.Tan,H.Tran, V.Vanhoucke,S.Vega,Q.Vuong,F.Xia,T.Xiao,P.Xu,S.Xu,T.Yu,and B.Zitkovich. Rt-1: Robotics trans for mer for real-world control at scale. In ar Xiv preprint ar Xiv:2212.06817, 2022. [3] C.Chi, S.Feng, Y.Du,Z.Xu,E.Cousineau,B.Burchfiel,and S.Song. Diffusionpolicy: Visuomotorpolicylearningviaactiondiffusion. In Proceedingsof Robotics: Science and Systems(RSS),2023. [4] A.Xie,L.Lee,T.Xiao,and C.Finn. Decomposing the generalizationgapinimitationlearning forvisualroboticmanipulation. ar Xivpreprintar Xiv:2307.03659,2023. [5] Octo Model Team,D.Ghosh,H.Walke,K.Pertsch,K.Black,O.Mees,S.Dasari,J.Hejna, C.Xu,J.Luo,T.Kreiman,Y.Tan,D.Sadigh,C.Finn,and S.Levine. Octo: Anopen-source generalistrobotpolicy. https://octo-models.github.io,2023. [6] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q.Vuong,A.He,V.Myers,K.Fang,C.Finn,and S.Levine. Bridgedatav 2: Adataset for robotlearningat scale,2023. [7] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess, A.Dubey,C.Finn,P.Florence,C.Fu,M.G.Arenas,K.Gopalakrishnan,K.Han,K.Hausman, A.Herzog,J.Hsu,B.Ichter,A.Irpan,N.Joshi,R.Julian,D.Kalashnikov,Y.Kuang,I.Leal, L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K.Reymann,M.Ryoo,G.Salazar,P.Sanketi,P.Sermanet,J.Singh,A.Singh,R.Soricut, H.Tran,V.Vanhoucke,Q.Vuong,A.Wahid,S.Welker,P.Wohlhart,J.Wu,F.Xia,T.Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-2: Vision-language-action models transfer web knowledgetoroboticcontrol. Inar Xivpreprintar Xiv:2307.15818,2023. 12 [8] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell, P.Mishkin,J.Clark,G.Krueger,and I.Sutskever. Learningtransferablevisualmodels from natural language supervision. In International Conference on Machine Learning (ICML), volume 139,pages 8748\u20138763,2021. [9] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre- training. In International Conferenceon Computer Vision(ICCV),2023. [10]",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 10,
      "paper_id": "openvla",
      "text": "12 [8] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell, P.Mishkin,J.Clark,G.Krueger,and I.Sutskever. Learningtransferablevisualmodels from natural language supervision. In International Conference on Machine Learning (ICML), volume 139,pages 8748\u20138763,2021. [9] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre- training. In International Conferenceon Computer Vision(ICCV),2023. [10] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra, P.Bhargava,S.Bhosale,etal. Llama 2: Openfoundation and fine-tunedchatmodels. ar Xiv preprintar Xiv:2307.09288,2023. [11] A.Khazatsky, K.Pertsch, S.Nair, A.Balakrishna, S.Dasari, S.Karamcheti, S.Nasiriany, M.K.Srirama,L.Y.Chen,K.Ellis,P.D.Fagan,J.Hejna,M.Itkina,M.Lepert,Y.J.Ma, P.T.Miller,J.Wu,S.Belkhale,S.Dass,H.Ha,A.Jain,A.Lee,Y.Lee,M.Memmel,S.Park, I.Radosavovic,K.Wang,A.Zhan,K.Black,C.Chi,K.B.Hatch,S.Lin,J.Lu,J.Mercat, A.Rehman,P.R.Sanketi,A.Sharma,C.Simpson,Q.Vuong,H.R.Walke,B.Wulfe,T.Xiao, J.H.Yang,A.Yavary,T.Z.Zhao,C.Agia,R.Baijal,M.G.Castro,D.Chen,Q.Chen,T.Chung, J.Drake,E.P.Foster,J.Gao,D.A.Herrera,M.Heo,K.Hsu,J.Hu,D.Jackson,C.Le,Y.Li, K.Lin,R.Lin,Z.Ma,A.Maddukuri,S.Mirch and ani,D.Morton,T.Nguyen,A.O\u2019Neill, R.Scalise,D.Seale,V.Son,S.Tian,E.Tran,A.E.Wang,Y.Wu,A.Xie,J.Yang,P.Yin, Y.Zhang,O.Bastani,G.Berseth,J.Bohg,K.Goldberg,A.Gupta,A.Gupta,D.Jayaraman, J.J.Lim,J.Malik,R.Mart\u00edn-Mart\u00edn,S.Ramamoorthy,D.Sadigh,S.Song,J.Wu,M.C.Yip, Y.Zhu,T.Kollar,S.Levine,and C.Finn. Droid: Alarge-scalein-the-wildrobotmanipulation dataset. 2024. [12] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,and A.Gupta. R 3 m: Auniversalvisualrepresenta- tion for robotmanipulation. In Co RL,2022. [13] S.Karamcheti,S.Nair,A.S.Chen,T.Kollar,C.Finn,D.Sadigh,and P.Liang. Language- driven representation learning for robotics. Ar Xiv, abs/2302.12766, 2023. URL https: //api.semanticscholar.org/Corpus ID:257205716. [14] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conferenceonrobotlearning,pages 894\u2013906.PMLR,2022. [15] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,B.Zitkovich, F. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language models. ar Xivpreprintar Xiv:2303.00905,2023. [16] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson, Q.Vuong,T.Yu,etal. Palm-e: Anembodiedmultimodallanguage model. ar Xivpreprint ar Xiv:2303.03378,2023. [17] A. S. et al. Introducing rfm-1: Giving robots human-like reason- ing capabilities, 2024. URL https://covariant.ai/insights/ introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/. [18] Wayve.Lingo-2:Driving with naturallanguage.2024.URLhttps://wayve.ai/thinking/ lingo-2-driving-with-language/. [19] X.Chen,X.Wang,S.Changpinyo,A.J.Piergiovanni,P.Padlewski,D.M.Salz,S.Goodman, A.Grycner,B.Mustafa,L.Beyer,A.Kolesnikov,J.Puigcerver,N.Ding,K.Rong,H.Akbari, G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali: Ajointly-scaledmultilinguallanguage-image model. Ar Xiv, abs/2209.06794, 2022. URL https://api.semanticscholar.org/Corpus ID:252222320. 13 [20] X.Chen,X.Wang,L.Beyer,A.Kolesnikov,J.Wu,P.Voigtlaender,B.Mustafa,S.Goodman, I.M.Alabdulmohsin, P.Padlewski, D.M.Salz, X.Xiong, D.Vlasic, F.Pavetic, K.Rong, T.Yu,D.Keysers,X.-Q.Zhai,and R.Soricut. Pa LI-3 visionlanguagemodels: Smaller,faster, stronger. ar Xivpreprintar Xiv:2310.09199,2023. [21] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf, M.Funtowicz,J.Davison,S.Shleifer,and... Trans for mers: State-of-the-artnaturallanguage processing. In Proceedingsof the 6 th International Conferenceon Learning Representations, 2020. URLhttps://arxiv.org/abs/1910.03771. [22] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N.Goyal,E.Hambro,F.Azhar,etal. Llama: Open and efficientfoundationlanguagemodels. ar Xivpreprintar Xiv:2302.13971,2023. [23] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bress and, G.Lengyel,G.Lample,L.Saulnier,etal. Mistral 7 b. ar Xivpreprintar Xiv:2310.06825,2023. [24] G.Team,T.Mesnard,C.Hardin,R.Dadashi,S.Bhupatiraju,S.Pathak,L.Sifre,M.Rivi\u00e8re, M.S.Kale,J.Love,etal. Gemma: Open model sbasedongeminiresearch and technology. ar Xivpreprintar Xiv:2403.08295,2024. [25] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D.Haziza, F.Massa, A.El-Nouby, etal. Dinov 2: Learningrobustvisualfeatures with out supervision. ar Xivpreprintar Xiv:2304.07193,2023. [26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rankadaptationoflargelanguagemodels. ar Xivpreprintar Xiv:2106.09685,2021. [27] T.Dettmers, A.Pagnoni, A.Holtzman, and L.Zettlemoyer. Qlora: Efficientfinetuningof quantizedllms. Advancesin Neural Information Processing Systems,36,2024. [28] Y.Goyal,T.Khot,D.Summers-Stay,D.Batra,and D.Parikh. Making the Vin VQAmatter: Elevating the roleofimageunderst and inginvisualquestionanswering. In Computer Vision and Pattern Recognition(CVPR),2017. [29] D.A.Hudson and C.D.Manning. GQA:Anew data set for real-worldvisualreasoning and compositional question answering. In Computer Vision and Pattern Recognition (CVPR), 2019. [30] A.Singh,V.Natarajan,M.Shah,Y.Jiang,X.Chen,D.Batra,D.Parikh,and M.Rohrbach. Towards VQAmodels that can read. In Computer Vision and Pattern Recognition(CVPR), 2019. [31] J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz, B.White,S.White,and T.Yeh. Viz Wiz: nearlyreal-timeanswerstovisualquestions. In User Interface Softw are and Technology(UIST),pages 333\u2013342,2010. [32] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Refer It Game: Referring to objects in photographs of natural scenes. In Empirical Methods in Natural Language Processing (EMNLP),pages 787\u2013798,2014. [33] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 11,
      "paper_id": "openvla",
      "text": "are and Technology(UIST),pages 333\u2013342,2010. [32] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Refer It Game: Referring to objects in photographs of natural scenes. In Empirical Methods in Natural Language Processing (EMNLP),pages 787\u2013798,2014. [33] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In European Conferenceon Computer Vision(ECCV),2016. [34] T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi\u00e8re, M. S. Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua, A.Botev,A.Castro-Ros,A.Slone,A.H\u00e9liou,A.Tacchetti,A.Bulanova,A.Paterson,B.Tsai, B. Shahriari, C. L. Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E.Buchatskaya, E.Ni, E.Noland, G.Yan, G.Tucker, G.-C.Muraru, G.Rozhdestvenskiy, 14 H.Michalewski,I.Tenney,I.Grishchenko,J.Austin,J.Keeling,J.Labanowski,J.-B.Lespiau, J.Stanway,J.Brennan,J.Chen,J.Ferret,J.Chiu,J.Mao-Jones,K.Lee,K.Yu,K.Milli can, L.L.Sjoesund,L.Lee,L.Dixon,M.Reid,M.Miku\u0142a,M.Wirth,M.Sharman,N.Chinaev, N.Thain,O.Bachem,O.Chang,O.Wahltinez,P.Bailey,P.Michel,P.Yotov,R.Chaabouni, R.Comanescu,R.Jana,R.Anil,R.Mc Ilroy,R.Liu,R.Mullins,S.L.Smith,S.Borgeaud, S.Girgin,S.Douglas,S.Pandya,S.Shakeri,S.De,T.Klimenko,T.Hennigan,V.Feinberg, W.Stokowiec,Y.hui Chen,Z.Ahmed,Z.Gong,T.Warkentin,L.Peran,M.Giang,C.Farabet, O.Vinyals,J.Dean,K.Kavukcuoglu,D.Hassabis,Z.Ghahramani,D.Eck,J.Barral,F.Pereira, E.Collins,A.Joulin,N.Fiedel,E.Senter,A.Andreev,and K.Kenealy. Gemma: Openmodels basedongeminiresearch and technology. ar Xivpreprintar Xiv:2403.08295,2024. [35] Y.Li,S.Bubeck,R.Eldan,A.D.Giorno,S.Gunasekar,and Y.T.Lee. Textbooks are allyou needii: phi-1.5 technicalreport. ar Xivpreprintar Xiv:2309.05463,2023. [36] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,etal. Qwen technicalreport. ar Xivpreprintar Xiv:2309.16609,2023. [37] J.Li,D.Li,C.Xiong,and S.C.H.Hoi. BLIP:Bootstrappinglanguage-imagepre-training forunifiedvision-languageunderstanding and generation. In International Conferenceon Machine Learning(ICML),2022. [38] J.Li,D.Li,S.Savarese,and S.C.H.Hoi. BLIP-2:Bootstrappinglanguage-imagepre-training with frozen image encoders and large language models. In International Conference on Machine Learning(ICML),2023. [39] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.A.Li,P.Fung,and S.C.H.Hoi. Instruct BLIP:Towardsgeneral-purposevision-languagemodels with instructiontuning. ar Xiv preprintar Xiv:2305.06500,2023. [40] H.H.Tanand M.Bansal. LXMERT:Learningcross-modalityencoderrepresentations from trans for mers. In Empirical Methodsin Natural Language Processing(EMNLP),2019. [41] H.Lauren\u00e7on,L.Saulnier,L.Tronchon,S.Bekman,A.Singh,A.Lozhkov,T.Wang,S.Karam- cheti,A.M.Rush,D.Kiela,M.Cord,and V.Sanh. OBELICS:Anopenweb-scalefiltered datasetofinterleavedimage-textdocuments. In Neural Information Processing Systems Track on Datasets and Benchmarks(Neur IPS Data sets and Benchmarks),2023. [42] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems(Neur IPS),2023. [43] H.Liu,C.Li,Y.Li,and Y.J.Lee. Improved base lines with visualinstructiontuning. ar Xiv preprintar Xiv:2310.03744,2023. [44] S.Karamcheti,S.Nair,A.Balakrishna,P.Liang,T.Kollar,and D.Sadigh. Prismaticvlms: Investigating the design space of visually-conditioned language models. ar Xiv preprint ar Xiv:2402.07865,2024. [45] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. QT-Opt: Scalable deep rein for cement learning for vision-basedroboticmanipulation. ar Xivpreprintar Xiv:1806.10293,2018. [46] D.Kalashnkov,J.Varley,Y.Chebotar,B.Swanson,R.Jonschkowski,C.Finn,S.Levine,and K.Hausman. Mt-opt: Continuousmulti-taskroboticrein for cementlearningat scale. ar Xiv, 2021. [47] F.Ebert,Y.Yang,K.Schmeckpeper,B.Bucher,G.Georgakis,K.Daniilidis,C.Finn,and S.Levine. Bridge data: Boostinggeneralizationofroboticskills with cross-domain data sets. ar Xivpreprintar Xiv:2109.13396,2021. 15 [48] K. Ehsani, T. Gupta, R. Hendrix, J. Salvador, L. Weihs, K.-H. Zeng, K. P. Singh, Y. Kim, W.Han,A.Herrasti,etal. Imitatingshortestpathsinsimulationenableseffectivenavigation andmanipulationin the realworld. ar Xivpreprintar Xiv:2312.02976,2023. [49] H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tulsiani, and V. Kumar. Roboagent: Generalizationandefficiencyinrobotmanipulationviasemanticaugmentations and action chunking. ar Xivpreprintar Xiv:2309.01918,2023. [50] L.Pinto and A.Gupta. Supersizingself-supervision: Learningtograsp from 50 ktries and 700 robothours. In 2016 IEEEinternationalconferenceonrobotics and automation(ICRA), pages 3406\u20133413.IEEE,2016. [51] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta, E. Orbay, et al. Roboturk: A crowds our cing platform for robotic skill learning through imitation. In Conferenceon Robot Learning,pages 879\u2013893.PMLR,2018. [52] A. Gupta, A. Murali, D. P. Gandhi, and L. Pinto. Robot learning in homes: Improving generalization and reducing data setbias. Advancesinneuralin for mationprocessingsystems, 31,2018. [53] S.Dasari,F.Ebert,S.Tian,S.Nair,B.Bucher,K.Schmeckpeper,S.Singh,S.Levine,and C.Finn. Robonet: Large-scalemulti-robotlearning. Co RL,2019. [54] S. Cabi, S. G. Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna, Y.Aytar,D.Budden,M.Vecerik,O.Sushkov,D.Barker,J.Scholz,M.Denil,N.de Freitas, and Z.Wang. Scaling data-drivenrobotics with rewardsketching and batchrein for cement learning. RSS,2019. [55] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C.",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 12,
      "paper_id": "openvla",
      "text": "Large-scalemulti-robotlearning. Co RL,2019. [54] S. Cabi, S. G. Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna, Y.Aytar,D.Budden,M.Vecerik,O.Sushkov,D.Barker,J.Scholz,M.Denil,N.de Freitas, and Z.Wang. Scaling data-drivenrobotics with rewardsketching and batchrein for cement learning. RSS,2019. [55] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with roboticimitationlearning. In Conferenceon Robot Learning,pages 991\u20131002.PMLR,2022. [56] H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and C. Lu. Rh 20 t: A comprehensive robotic dataset for learning diverse skills in one-shot. Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@Co RL 2023,3:5,2023. [57] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural net- workpolicies for multi-task and multi-robottransfer. In Proceedingsof IEEEInternational Conferenceon Robotics and Automation,2017. [58] E.S.Hu,K.Huang,O.Rybkin,and D.Jayaraman. Knowthyself: Transferablevisualcontrol policiesthroughrobot-awareness. In International Conferenceon Learning Representations, 2022. [59] J. H. Yang, D. Sadigh, and C. Finn. Polybot: Training one policy across robots while embracing variability. In 7 th Annual Conference on Robot Learning, 2023. URL https: //openreview.net/forum?id=HEIRj 51 lc S. [60] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-maron,M.Gim\u00e9nez, Y.Sulsky,J.Kay,J.T.Springenberg,T.Eccles,J.Bruce,A.Razavi,A.Edwards,N.Heess, Y.Chen,R.Hadsell,O.Vinyals,M.Bordbar,and N.de Freitas.Ageneralistagent.Transactions on Machine Learning Research,2022. ISSN 2835-8856. [61] G.Salhotra,I.-C.A.Liu,and G.Sukhatme. Bridgingactionspacemismatchinlearning from demonstrations. ar Xivpreprintar Xiv:2304.03833,2023. [62] I.Radosavovic, B.Shi, L.Fu, K.Goldberg, T.Darrell, and J.Malik. Robotlearning with sensorimotorpre-training. In Conferenceon Robot Learning,2023. 16 [63] D.Shah,A.Sridhar,A.Bhorkar,N.Hirose,and S.Levine. Gnm: Ageneralnavigation model to drive any robot. In 2023 IEEE International Conference on Robotics and Automation (ICRA),pages 7226\u20137233.IEEE,2023. [64] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou, A.Gupta,A.Raju,etal. Robocat: Aself-improvingfoundationagent for roboticmanipulation. ar Xivpreprintar Xiv:2306.11706,2023. [65] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,and S.Levine. Vi NT:A foundation model for visualnavigation. In 7 th Annual Conferenceon Robot Learning,2023. URLhttps://arxiv.org/abs/2306.14846. [66] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, D. Sadigh, and S. Levine. Pushing the limits of cross-embodiment learning for manipulation and navigation. ar Xiv preprintar Xiv:2402.19432,2024. [67] S.Y.Gadre,M.Wortsman,G.Ilharco,L.Schmidt,and S.Song. Cowsonpasture: Baselines and benchmarks for language-driven zero-shot object navigation. In Proceedings of the IEEE/CVFConferenceon Computer Vision and Pattern Recognition, pages 23171\u201323181, 2023. [68] Y.Du, K.Konyushkova, M.Denil, A.Raju, J.Landon, F.Hill, N.de Freitas, and S.Cabi. Vision-language model sassuccessdetectors. ar Xivpreprintar Xiv:2303.07280,2023. [69] Y.J.Ma,V.Kumar,A.Zhang,O.Bastani,and D.Jayaraman. Liv: Language-imagerepresen- tations and rewards for roboticcontrol. In International Conferenceon Machine Learning, pages 23301\u201323320.PMLR,2023. [70] X.Zhang,Y.Ding,S.Amiri,H.Yang,A.Kaminski,C.Esselink,and S.Zhang. Grounding classical task plannersviavision-languagemodels. ar Xivpreprintar Xiv:2304.08587,2023. [71] S. Sontakke, J. Zhang, S. Arnold, K. Pertsch, E. B\u0131y\u0131k, D. Sadigh, C. Finn, and L. Itti. Roboclip:Onedemonstrationisenoughtolearnrobotpolicies.Advancesin Neural Information Processing Systems,36,2024. [72] J.Huang,S.Yong,X.Ma,X.Linghu,P.Li,Y.Wang,Q.Li,S.-C.Zhu,B.Jia,and S.Huang. Anembodiedgeneralistagentin 3 dworld. In Proceedingsof the International Conferenceon Machine Learning(ICML),2024. [73] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu, et al. Vision-language foundation models as effective robot imitators. ar Xiv preprint ar Xiv:2311.01378,2023. [74] H.Zhen,X.Qiu,P.Chen,J.Yang,X.Yan,Y.Du,Y.Hong,and C.Gan. 3 d-vla: 3 dvision- language-actiongenerativeworld model. ar Xivpreprintar Xiv:2403.09631,2024. [75] Py Torch. Automatic mixed precision. URL https://pytorch.org/docs/stable/amp. html. [76] T.Dao. Flashattention-2: Fasterattention with betterparallelism and workpartitioning. ar Xiv preprintar Xiv:2307.08691,2023. [77] Y.Zhao,A.Gu,R.Varma,L.Luo,C.-C.Huang,M.Xu,L.Wright,H.Shojanazeri,M.Ott, S. Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. ar Xiv preprintar Xiv:2304.11277,2023. [78] N.Dorka, C.Huang, T.Welschehold, and W.Burgard. Whatmattersinemployingvision languagemodels for tokenizingactionsinrobotcontrol?",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 13,
      "paper_id": "openvla",
      "text": "[75] Py Torch. Automatic mixed precision. URL https://pytorch.org/docs/stable/amp. html. [76] T.Dao. Flashattention-2: Fasterattention with betterparallelism and workpartitioning. ar Xiv preprintar Xiv:2307.08691,2023. [77] Y.Zhao,A.Gu,R.Varma,L.Luo,C.-C.Huang,M.Xu,L.Wright,H.Shojanazeri,M.Ott, S. Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. ar Xiv preprintar Xiv:2304.11277,2023. [78] N.Dorka, C.Huang, T.Welschehold, and W.Burgard. Whatmattersinemployingvision languagemodels for tokenizingactionsinrobotcontrol? In First Workshopon Vision-Language Models for Navigation and Manipulationat ICRA 2024. 17 [79] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre- training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986,2023. [80] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell, P.Mishkin,J.Clark,etal. Learningtransferablevisualmodels from naturallanguagesupervi- sion. In Internationalconferenceonmachinelearning,pages 8748\u20138763.PMLR,2021. [81] P.Sharma,N.Ding,S.Goodman,and R.Soricut.Conceptualcaptions:Acleaned,hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56 th Annual Meetingof the Association for Computational Linguistics(Volume 1: Long Papers),pages 2556\u20132565,2018. [82] C.Schuhmann,R.Vencu,R.Beaumont,R.Kaczmarczyk,C.Mullis,A.Katta,T.Coombes, J.Jitsev,and A.Komatsuzaki.Laion-400 m:Open data setofclip-filtered 400 millionimage-text pairs. ar Xivpreprintar Xiv:2111.02114,2021. [83] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: a dataset for image captioning withreadingcomprehension. In Computer Vision\u2013ECCV 2020: 16 th European Conference, Glasgow,UK,August 23\u201328,2020,Proceedings,Part II 16,pages 742\u2013758.Springer,2020. [84] H.Face. Introducingidefics: Anopenreproductionofstate-of-the-artvisuallangage model. Hugging Face Blog,2024. [85] H.Liu,C.Li,Q.Wu,and Y.J.Lee. Visualinstructiontuning. Advancesinneuralin for mation processingsystems,36,2024. [86] B.Mc Kinzie,Z.Gan,J.-P.Fauconnier,S.Dodge,B.Zhang,P.Dufter,D.Shah,X.Du,F.Peng, F.Weers,etal. Mm 1: Methods,analysis&insights from multimodalllmpre-training. ar Xiv preprintar Xiv:2403.09611,2024. [87] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S.Han. Vila: Onpre-training for visuallanguagemodels. ar Xivpreprintar Xiv:2312.07533, 2023. [88] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt 3. int 8 (): 8-bit matrix multi- plicationfortrans for mersat scale. Advancesin Neural Information Processing Systems,35: 30318\u201330332,2022. [89] NVIDIA. Tensorrt-llm. URLhttps://github.com/NVIDIA/Tensor RT-LLM. [90] T.Z.Zhao,V.Kumar,S.Levine,and C.Finn. Learningfine-grainedbimanualmanipulation withlow-costhardw are. ar Xivpreprintar Xiv:2304.13705,2023. [91] Y.Leviathan,M.Kalman,and Y.Matias. Fastinference from trans for mersviaspeculative decoding. In International Conferenceon Machine Learning,pages 19274\u201319286.PMLR, 2023. [92] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus- man,A.Herzog,J.Hsu,etal. Rt-1: Roboticstransformer for real-worldcontrolat scale. ar Xiv preprintar Xiv:2212.06817,2022. [93] E.Rosete-Beas,O.Mees,G.Kalweit,J.Boedecker,and W.Burgard. Latentplans for task agnostic offline rein for cement learning. In Proceedings of the 6 th Conference on Robot Learning(Co RL),2022. [94] O.Mees,J.Borja-Diaz,and W.Burgard. Groundinglanguage with visualaf for dancesover unstructured data. In Proceedings of the IEEE International Conference on Robotics and Automation(ICRA),London,UK,2023. 18 [95] S.Dass,J.Yapeter,J.Zhang,J.Zhang,K.Pertsch,S.Nikolaidis,and J.J.Lim. CLVRjaco play data set,2023. URLhttps://github.com/clvrai/clvr_jaco_play_dataset. [96] J.Luo,C.Xu,X.Geng,G.Feng,K.Fang,L.Tan,S.Schaal,and S.Levine. Multi-stagecable routingthroughhierarchicalimitationlearning. ar Xivpreprintar Xiv:2307.08927,2023. [97] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta, E.Orbay,S.Savarese,and L.Fei-Fei. Robo Turk: Acrowds our cingplatform for roboticskill learningthroughimitation. Co RR,abs/1811.02790,2018. URLhttp://arxiv.org/abs/ 1811.02790. [98] Y.Zhu,A.Joshi,P.Stone,and Y.Zhu. Viola: Imitationlearning for vision-basedmanipulation withobjectproposalpriors,2023. [99] L. Y. Chen, S. Adebola, and K. Goldberg. Berkeley UR 5 demonstration dataset. https: //sites.google.com/view/berkeley-ur 5/home. [100] G.Zhou,V.Dean,M.K.Srirama,A.Rajeswaran,J.Pari,K.Hatch,A.Jain,T.Yu,P.Abbeel, L.Pinto,C.Finn,and A.Gupta. Trainoffline,testonline: Arealrobotlearningbenchmark, 2023. [101] C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,and P.Florence. Interactivelanguage: Talkingtorobotsinrealtime. IEEERobotics and Automation Letters, 2023. [102] S.Belkhale,Y.Cui,and D.Sadigh. Hydra: Hybridrobotactions for imitationlearning. arxiv, 2023. [103] Y.Zhu,P.Stone,and Y.Zhu. Bottom-upskilldiscovery from unsegmenteddemonstrations for long-horizonrobotmanipulation. IEEERobotics and Automation Letters,7(2):4126\u20134133, 2022. [104] Z. J. Cui, Y. Wang, N. M. M. Shafiullah, and L. Pinto. From play to policy: Conditional behaviorgeneration from uncuratedrobot data. ar Xivpreprintar Xiv:2210.10047,2022. [105] M.Heo,Y.Lee,D.Lee,and J.J.Lim. Furniturebench: Reproduciblereal-worldbenchmark forlong-horizoncomplexmanipulation. In Robotics: Science and Systems,2023. [106] G.Yan,K.Wu,and X.Wang. ucsdkitchens dataset. August 2023. [107] S.Nasiriany,T.Gao,A.Mandlekar,and Y.Zhu. Learning and retrieval from prior data for skill-basedimitationlearning. In Conferenceon Robot Learning(Co RL),2022. [108] H.Liu,S.Nasiriany,L.Zhang,Z.Bao,and Y.Zhu. Robotlearningon the job:",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 14,
      "paper_id": "openvla",
      "text": "to policy: Conditional behaviorgeneration from uncuratedrobot data. ar Xivpreprintar Xiv:2210.10047,2022. [105] M.Heo,Y.Lee,D.Lee,and J.J.Lim. Furniturebench: Reproduciblereal-worldbenchmark forlong-horizoncomplexmanipulation. In Robotics: Science and Systems,2023. [106] G.Yan,K.Wu,and X.Wang. ucsdkitchens dataset. August 2023. [107] S.Nasiriany,T.Gao,A.Mandlekar,and Y.Zhu. Learning and retrieval from prior data for skill-basedimitationlearning. In Conferenceon Robot Learning(Co RL),2022. [108] H.Liu,S.Nasiriany,L.Zhang,Z.Bao,and Y.Zhu. Robotlearningon the job: Human-in- the-loopautonomy and learningduringdeployment. In Robotics: Science and Systems(RSS), 2023. [109] G.Quere,A.Hagengruber,M.Iskandar,S.Bustamante,D.Leidner,F.Stulp,and J.Vogel. Shared Control Templates for Assistive Robotics. In 2020 IEEEInternational Conferenceon Robotics and Automation(ICRA),page 7,Paris,France,2020. [110] S. Saxena, M. Sharma, and O. Kroemer. Multi-resolution sensing for real-time control with vision-language models. In 7 th Annual Conference on Robot Learning, 2023. URL https://openreview.net/forum?id=Wu Bv 9-IGDUA. [111] R.Shah,R.Mart\u00edn-Mart\u00edn,and Y.Zhu. MUTEX:Learningunifiedpolicies from multimodal task specifications. In 7 th Annual Conference on Robot Learning, 2023. URL https: //openreview.net/forum?id=Pwqiqaa Ez J. 19 [112] X.Zhu,R.Tian,C.Xu,M.Ding,W.Zhan,and M.Tomizuka. Fanucmanipulation: Adataset forlearning-basedmanipulation with fanucmate 200 idrobot. 2023. [113] R.Mendonca,S.Bahl,and D.Pathak. Structuredworldmodels from humanvideos. Co RL, 2023. [114] J.Luo, C.Xu, F.Liu, L. Tan, Z. Lin, J.Wu, P. Abbeel, and S. Levine. Fmb: a functional manipulationbenchmark for generalizableroboticlearning. ar Xivpreprintar Xiv:2401.08553, 2024. [115] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chintala, and L. Pinto. On bringingrobotshome,2023. [116] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone. Libero: Benchmarking knowledgetransfer for lifelongrobotlearning. Advancesin Neural Information Processing Systems,36,2024. [117] V.Sanh,L.Debut,J.Chaumond,and T.Wolf. Distilbert,adistilledversionofbert: smaller, faster,cheaper and lighter. ar Xivpreprintar Xiv:1910.01108,2019. 20 A Data Mixture Details We list our used data mixture in Table 3. The mixture mostly follows [5], with a few additional datasets. Open VLATraining dataset Mixture Fractal[92] 12.7% Kuka[45] 12.7% Bridge[6,47] 13.3% Taco Play[93,94] 3.0% Jaco Play[95] 0.4% Berkeley Cable Routing[96] 0.2% Roboturk[97] 2.3% Viola[98] 0.9% Berkeley Autolab UR 5[99] 1.2% Toto[100] 2.0% Language Table[101] 4.4% Stanford Hydra dataset[102] 4.4% Austin Buds dataset[103] 0.2% NYUFranka Play dataset[104] 0.8% Furniture Bench dataset[105] 2.4% UCSDKitchen dataset[106] <0.1% Austin Sailor dataset[107] 2.2% Austin Sirius dataset[108] 1.7% DLREDANShared Control[109] <0.1% IAMLab CMUPickup Insert[110] 0.9% UTAustin Mutex[111] 2.2% Berkeley Fanuc Manipulation[112] 0.7% CMUStretch[113] 0.2% BC-Z[55] 7.5% FMB Data set[114] 7.1% Dobb E[115] 1.4% DROID[11] 10.0%6 Table 3:Open VLAtrainingdatamixtureusing data sets from the Open X-Embodiment data set[1],following[5] withafewadditions. B Evaluation Tasks and Detailed Results Inthissection,weprovidemoredetailson the Bridge Data V 2 Widow Xand Googlerobotevaluations discussedin Section 5.1,aswellas the Franka-Tabletop and Franka-DROIDfine-tuningevaluations discussedin Section 5.2. B.1 Bridge Data V 2 Widow XEvaluation Details Herewefocusspecificallyon Bridge Data V 2 evaluationsdiscussedin Section 5.1. B.1.1 Bridge Data V 2 Evaluation Tasks Asdescribedin Section 5.1,weevaluateeachgeneralistrobotmanipulationpolicyon 17 tasks with 10 trialseach. Inthissection,weprovidedetailson the taskcategories and individualtasks. In total, we evaluate on 5 visual generalization tasks, 2 motion generalization tasks, 3 physical generalizationtasks,4 semanticgeneralizationtasks,and 3 languagegroundingtasks. Note that all tasksweevaluateonintroducesome for mofdistributionshiftsincewe are unabletoprocure the exactobjectsusedin the original data set(otherdistributionshiftsnaturallyariseas were producea real-worldtestenvironmentoriginallyconstructe data differentlocation;see Appendix B.1.2 fora detaileddiscussiononsuchdistributionshifts). All 17 tasks are depictedin Fig.7. Eachrolloutis 6 Weremove DROID for the lastthirdoftrainingduetoslowlearningprogress(see Section 3.3)andre- distributeitsmixtureweightsacrossallo the rdatasets. 21 Put Eggplant into Pot (Easy Version) Put Eggplant into Pot Put Cup from Counter into Sink Put Eggplant into Pot (w/ Clutter) Put Yellow Corn on Pink Plate Lift Eggplant Put Carrot on Plate (w/ Height",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 15,
      "paper_id": "openvla",
      "text": "Fig.7. Eachrolloutis 6 Weremove DROID for the lastthirdoftrainingduetoslowlearningprogress(see Section 3.3)andre- distributeitsmixtureweightsacrossallo the rdatasets. 21 Put Eggplant into Pot (Easy Version) Put Eggplant into Pot Put Cup from Counter into Sink Put Eggplant into Pot (w/ Clutter) Put Yellow Corn on Pink Plate Lift Eggplant Put Carrot on Plate (w/ Height Change) Put Carrot on Plate Flip Pot Upright Lift AAA Battery Move Skull into Drying Rack Lift White Tape Take Purple Grapes out of Pot Stack Blue Cup on Pink Cup Put Eggplant into Pot Put Red Bottle into Pot Lift Cheese Lift Red Chili Pepper Put Blue Cup on Plate Put Pink Cup on Plate .ne G lausi V .ne G noito M .ne G lacisyh P .ne G citname S gnidnuor G egaugna L Figure 7: Bridge Data V 2 Widow Xrobotevaluationtasks. Weevaluateeverygeneralistrobotpolicyon 4 typesout-of-distribution(OOD)generalizationtasks: visual,motion,physical,andsemantic(asdefinedin Section 5.1).Everypairofimagesshowsthestartstate and anexampleendstateafter the robotcompletes the task.Wealsorigorouslyassesslanguagegroundingin the 3 tasksshownin the bottom 3 rows,bychanging the promptwhilefixingtheinitialstate and testingwhether the policy can approach the correcttargetobject. 22 markedasafailure(0)orsuccess(1). Insomemoredifficulttasks,werecordpartialsuccesses(0.5); wedescribetheconditions for partialcreditin the taskdescriptionsbelow. Belowwedescribeeachof the 17 tasks,intheordershownin Fig.7: 1. Put Eggplantinto Pot(Easy Version): Therobot\u2019sgoalistopickup the eggplant and dropitinto the pot. Thisisavisualgeneralization task becausewe useah and craftedpaper pot that hasadifferentappearancethanthepotusedin the original Bridge Data V 2 training dataset (since we are unable to procure the original pot). Unlike all 16 other tasks, for thisparticular task weinitialize the robot\u2019send-effectordirectlyabove the eggplantbefore rollingout the policy;hence,wecall this the\u201cEasy Version\u201dofthe\u201cPut Eggplantinto Pot\u201d task. 2. Put Eggplantinto Pot: Thisis the same task asdescribedabove,except that the robot\u2019s end-effector is not initialized directly above the eggplant. Instead, we initialize it in a position that isfixedacrossallrollouts,whichmeans that the robotmusthorizontallyreach for the eggplant first before manipulating it. (Note: The same applies to all other tasks describedbelow.) Thisisavisualgeneralization task for the samereasonasabove. 3. Put Cupfrom Counterinto Sink: Therobot\u2019sgoalistopickup the pinkcup from either thekitchencountertopordryingrack and placeitintothesinkon the right. Thisisavisual generalization task becausewe useapinkcupra the rthanabluecup(abluecupisusedin theoriginal Bridge Data V 2 dataset,butwefind that noneof the methodsweevaluateisable tomanipulateitreliably\u2013mostlikelybecausethecolorofthecupblendsin with the color ofthesink). 4. Put Eggplantinto Pot(w/Clutter): Thisis the sametaskas the\u201cPut Eggplantinto Pot\u201d task,except that itismoredifficultdueto the presenceofseveraldistractorobjects. Itisa visualgeneralization task for thesamereasondiscussedin the normal\u201cPut Eggplantinto Pot\u201dtask,andevenmoresogivenunseendistractorsin the scene. Partialcredit(0.5 outof 1)isrewardedwhentherobotmovestowards the correcttargetobject. 5. Put Yellow Cornon Pink Plate: Therobot\u2019sgoalistopickup the yellowcorn and place it on the pink plate. This is a visual generalization task due to the presence of unseen distractorobjectsin the scene,suchasagreendinosauronthecountertopin the backsection of the sink. Partial credit (0.5 out of 1) is rewarded when the robot moves towards the correcttargetobject. 6. Lift Eggplant:Therobot\u2019sgoalistograsp and lifttheeggplantinto the air. Thisisamotion generalization task because the eggplantisinitializedinunseenpositions and/ororienta- tions,and the robotis for cedtomovebeyonditstrainingdistributionofpositions and/or orientations and oftenper for mlong-rangereachinginordertocomplete the task. (Note: Long-rangereachingisnotdemonstratedin this environmentin the original Bridge Data V 2 demonstrations;see Appendix B.1.2 fordetails.) Wefind that this task,thoughseemingly simple,isdeceptivelychallenging for manypolicies. Partialcredit(0.5 outof 1)isrewarded whentherobotmakescontact with the eggplant. 7. Put Carroton Plate(w/Height Change):Therobot\u2019sgoalistopickup the carrot and place iton the yellowplate. Thisisamotiongeneralization task because the plateiselevated fromitsusualpositionatthebottomof the sink, and the robotmustadjustitstrajectory tocorrectlyplacethecarroton the elevatedplatform(withoutknockingdown the platein theprocess). Partialcredit(0.5 outof 1)isrewardedwhentherobotgrasps the carrot and touches the platewithit. 8. Put Carroton Plate: Thisis the same task asabove,except that the",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 16,
      "paper_id": "openvla",
      "text": "eggplant. 7. Put Carroton Plate(w/Height Change):Therobot\u2019sgoalistopickup the carrot and place iton the yellowplate. Thisisamotiongeneralization task because the plateiselevated fromitsusualpositionatthebottomof the sink, and the robotmustadjustitstrajectory tocorrectlyplacethecarroton the elevatedplatform(withoutknockingdown the platein theprocess). Partialcredit(0.5 outof 1)isrewardedwhentherobotgrasps the carrot and touches the platewithit. 8. Put Carroton Plate: Thisis the same task asabove,except that the plateisatitsnormal position(atthebottomof the sinkordryingrack). Weconsider this aphysicalgeneraliza- tion task becausethecarrothasadifferentsize and shapethantheoneusedin the original Bridge Data V 2 dataset,whichisshorter and narrower.(Note that the previousversionof this tasklistedabovewouldalsotechnicallybeaphysicalgeneralization task sinceitinvolves thesamecarrot,butwelistitunder the\u201cmotiongeneralization\u201dcategorysincethatis the focusthere.) 23 9. Flip Pot Upright: Therobot\u2019sgoalistomanipulate the potsuch that itisorientedupright inthesinkattheendof the episode. Thisisaphysicalgeneralization task because this pothasadifferentsize and shapethantheoneusedin the original Bridge Data V 2 training demonstrations(thepotwe useiswider and shorter). 10. Lift AAABattery:Therobot\u2019sgoalissimplytograsp the AAAbattery and liftitupinto the air. Thisisconsideredaphysicalgeneralization task because the batteryismuchsmaller andthinnerthantargetobjectsseenin the Bridge Data V 2 trainingdemonstrationsin this environment;see Appendix B.1.2 fordetails. (Note that thistargetobjectdoesnotexistin theoriginal Bridge Data V 2 demonstrationsin this environment,sothisisalsoaninstanceof \u201csemanticgeneralization\u201d,butweclassifyitsolelyas\u201cphysicalgeneralization\u201dsincethatis themainfocushere). 11. Move Skullinto Drying Rack: Therobot\u2019sgoalistograsp the skullwinduptoy and drop itintotheyellowdryingrackintheleftpartof the sink. Thisisasemanticgeneralization tasksince the skullisanunseentargetobject(doesnotappearin the Bridge Data V 2 training demonstrations). 12. Lift White Tape: Therobot\u2019sgoalistograsp and liftthewhiterolloftapeinto the air. Thisisasemanticgeneralization task since the whitetaperollisanunseentargetobject (doesnotappearin the Bridge Data V 2 trainingdemonstrations). (Note that this task may alsobeconsideredas\u201cphysicalgeneralization\u201dbecauseofitsshapebeingdifferentthan the objectsseenin the trainingdemonstrationsin this environment;mostpoliciesstruggleto graspobjects with thisringstructure,andtheyoftenmove the robot\u2019send-effectordirectly into the centerregion.) 13. Take Purple Grapesoutof Pot: Therobot\u2019sgoalistograsp the purplegrapeslyinginside thesteelpot and removeit from the pot(byliftingitout and/ordroppingitanywhereoutside thepot).Thisisasemanticgeneralization task becauseitisanunseenlanguageinstruction; therobothasneverseen this taskin the original Bridge Data V 2 training data set. 14. Stack Blue Cupon Pink Cup:Therobot\u2019sgoalistograsp the bluecup and placeitsecurely on top of the pink cup. This is a semantic generalization task because it is an unseen languageinstruction;therobothasneverseenthistaskin this environmentin the original Bridge Data V 2 training data set. Partialcredit(0.5 outof 1)isrewardedwhen the robot graspsthebluecup and touchesthepinkcup with the bluecup. 15. Put{Eggplant,Red Bottle}into Pot: Thisisalanguagegrounding task. Therobot\u2019sgoal istoputthespecifiedtargetobjectinto the pot. Both the eggplant and redbottle are present inthescene. Weconductpairedevaluations: for the sameinitialstate,weprompt the policy totarget the eggplantinoneepisode,andthentheredbottlein the nextepisode. Wetest eachmethod 5 times with the eggplant and 5 times with the redbottle,using the sameset of 5 initialstates for bothtargetobjects. Partialcredit(0.5 outof 1)isrewardedwhen the robotmovestowards the correcttargetobject. 16. Lift{Cheese,Red Chili Pepper}: Thisisalanguagegrounding task. Therobot\u2019sgoalis tograsp and lift the specifiedtargetobject. Weconductpairedevaluationsasdescribedin the task above. Partialcredit(0.5 outof 1)isrewardedwhen the robotmovestowards the correcttargetobject. 17. Put {Blue Cup, Pink Cup} on Plate: This is a language grounding task. The robot\u2019s goalistograspthespecifiedtargetobject and placeitonto the plate. Weconductpaired evaluationsasdescribedino the rlanguagegroundingtasks. Partialcredit(0.5 outof 1)is rewardedwhentherobotmovestowards the correcttargetobject. B.1.2 Comparing Evaluation Tasksto Original Bridge Data V 2 Training Data Weconduct our evaluationsinasinkenvironmentusedin the original Bridge Data V 2 dataset[6]. Wereproducetheenvironmenttomatch the originalenvironmentin the Bridge Data V 2 dataset with roughapproximations for the robot\u2019slocationrelativeto the sink,aswellas the camera\u2019splacement 24 relativeto the scene. Giventhelackofprecisemeasurementsofthesepositionsin the original data set, weareunabletoreproduce the exactenvironmentsetup,andnaturaldistributionshiftsarisedueto slightlydifferentrobot,sink,andcameraplacements. Inaddition,sinceweevaluaterobotpolicies in a different location than where the training demonstrations were collected from, other natural distributionshiftsarise. Forexample, thelightingconditions and background(e.g., visibleareas behind the sink)areinevitablydifferentthanwhatwasseenin the training data set. Fur the rmore,we areunabletoprocuretheexactsetofobjectsusedin the original Bridge Data V 2 dataset,sothere are distributionshiftsbetween the objectsusedattraintime and thoseusedattesttime. Despiteall the sechallenges,wefind that certaingeneralistpolicies,suchas Open VLAand RT-2-X,",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 17,
      "paper_id": "openvla",
      "text": "different location than where the training demonstrations were collected from, other natural distributionshiftsarise. Forexample, thelightingconditions and background(e.g., visibleareas behind the sink)areinevitablydifferentthanwhatwasseenin the training data set. Fur the rmore,we areunabletoprocuretheexactsetofobjectsusedin the original Bridge Data V 2 dataset,sothere are distributionshiftsbetween the objectsusedattraintime and thoseusedattesttime. Despiteall the sechallenges,wefind that certaingeneralistpolicies,suchas Open VLAand RT-2-X, can still generalize and perform various tasks fairly reliably \u201cout-of-the-box\u201d. Other generalist policies,suchas RT-1-Xand Octo,canalsocompletesometasks,though the ystrugglewhentested withmoredifficultgeneralization task sinour Bridge Data V 2 evaluationsuite. The original Bridge Data V 2 dataset includes demonstrations of the following seven tasks in this specificsinkenvironment: \u201cFlip Pot Upright\u201d,\u201cPut Carroton Plate\u201d,\u201cPut Cupfrom Counter(or Drying Rack)into Sink\u201d,\u201cPut Eggplantinto Pot\u201d,\u201cPut Knifeon Cutting Board\u201d,\u201cPut Spoonin Pot\u201d, and\u201cTurn Lever Verticalto Front\u201d. See Fig.8 forsamplesimagesofallthesetasks from the original dataset. Note that alltrainingdemonstrationscollectedin this environment are initializedsuch that therobot\u2019send-effectorispositioneddirectlyabovethetargetobjectinthebeginningof the episode. (However,thisisnot the caseacrossallenvironmentsin the Bridge Data V 2 dataset;insomeother environments,therobotisinitializedfartheraway from the targetobject,soitmusthorizontallyreach for the objectfirstbe for emanipulatingit.) Flip Pot Upright Put Carrot on Plate Put Cup from Counter into Sink Turn Lever Vertical to Front Put Eggplant into Pot Put Knife on Cutting Board Put Spoon in Pot Figure 8:Original Bridge Data V 2 sinkenvironmenttasks.Images from sampledemonstrationsin the sink environment from the original Bridge Data V 2 datasetreveal that alldemonstrationsin this environment were initializedsuch that the robot\u2019send-effectorwaspositionedimmediatelyabove the targetobject.Note that these initialstates are different from the initialstateswe usein our Bridge Data V 2 evaluation task sshownin Fig.7. Inourevaluations,wealwaysinitialize the robot\u2019send-effectortoafixedlocationabove the sink,ratherthan positioningitdirectlyabove the targetobject(except for onetask:\u201cPut Eggplantinto Pot(Easy Version)\u201d). Inour Bridge Data V 2 evaluationsuite,onlyone task\u2013\u201cPut Eggplantinto Pot(Easy Version\u201d)\u2013is initialized with the robot\u2019send-effectorhoveringdirectlyover the targetobject;inall 16 othertasks, theend-effectorisinitialize data fixedlocationabovethesinksuch that the robotmusthorizontally reach towards the object. This initial condition, in combination with the distribution shifts we introducein the varioustypesof OODgeneralizationin our evaluationsuite,challenges the generalist policies and requiresahighdegreeofrobustnessinordertocomplete the taskssuccessfully. Hence, thesuccessrates for policieslike RT-1-Xand Octo are lowerthanwhatisreportedinpriorworks. However,wefindthato the rpoliciessuchas RT-2-Xand Open VLAstillachieverelativelystrong per for mancedespiteall the sedistributionshifts and challenges. 25 B.1.3 Detailed Bridge Data V 2 Evaluation Results See Table 4 for the full Bridge Data V 2 Widow Xevaluationresults. Thenumberofsuccesses for each method,outof 10 trials,islisted for eachof 17 tasks. Open VLAachievesstrongestper for mancein themajorityofthetasks and hasthehighestaggregatesuccessrateamong the generalistpolicies. RT-2-Xalsoshowsgoodper for mance,outper for ming RT-1-Xand Octo,thoughitdoesnotperform aswellas Open VLA.RT-1-Xand Octogenerallyexperiencedifficultyin the segeneralizationtasks. Table 4:Detailed Bridge Data V 2 Widow Xevaluationresults.Wereportper for manceon the fullevaluation suiteof 17 tasks(discussedin Section 5.1),includingvisual/motion/physical/semanticgeneralizationtasks and languagegroundingtasks.Note that partialsuccess(scoreof 0.5)ispossible for sometasks;see Appendix B.1.1 fordetails. Wefind that Open VLAper for msbestinmosttasks and achieveshighestper for manceoverall, followedby RT-2-X.Ontheo the rhand,RT-1-Xand Octostrugglein the evaluations,onlygetting 0\u20132 successes inseveraltasks.See Fig.7 forillustrationsofalltasks. RT-1-X Octo RT-2-X Open VLA(ours) Category Task #Trials #Successes #Successes #Successes #Successes Visualgen Put Eggplantinto Pot(Easy Version) 10 1 5 7 10 Visualgen Put Eggplantinto Pot 10 0 1 5 10 Visualgen Put Cupfrom Counterinto Sink 10 1 1 0 7 Visualgen Put Eggplantinto Pot(w/Clutter) 10 1 3.5 6 7.5 Visualgen Put Yellow Cornon Pink Plate 10 1 4 8 9 Motiongen Lift Eggplant 10 3 0.5 6.5 7.5 Motiongen Put Carroton Plate(w/Height Change) 10 2 1 4.5 4.5 Physicalgen Put Carroton Plate 10 1 0 1 8 Physicalgen Flip Pot Upright 10 2 6 5",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 18,
      "paper_id": "openvla",
      "text": "1 3.5 6 7.5 Visualgen Put Yellow Cornon Pink Plate 10 1 4 8 9 Motiongen Lift Eggplant 10 3 0.5 6.5 7.5 Motiongen Put Carroton Plate(w/Height Change) 10 2 1 4.5 4.5 Physicalgen Put Carroton Plate 10 1 0 1 8 Physicalgen Flip Pot Upright 10 2 6 5 8 Physicalgen Lift AAABattery 10 0 0 2 7 Semanticgen Move Skullinto Drying Rack 10 1 0 5 5 Semanticgen Lift White Tape 10 3 0 0 1 Semanticgen Take Purple Grapesoutof Pot 10 6 0 5 4 Semanticgen Stack Blue Cupon Pink Cup 10 0.5 0 5.5 4.5 Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 2.5 4 8.5 7.5 Languagegrounding Lift{Cheese,Red Chili Pepper} 10 1.5 2.5 8.5 10 Languagegrounding Put{Blue Cup,Pink Cup}on Plate 10 5 5.5 8.5 9.5 Mean Success Rate 18.5\u00b12.7% 20.0\u00b12.6% 50.6\u00b13.5% 70.6\u00b13.2% Additionally,in Table 5,weprovidethe full evaluationresults for the quantizedinferenceexperi- ments that weresummarizedin Table 2. For the seevaluations,wetestpolicieson 8 representative Bridge Data V 2 tasksspanningall task categoriesin the fullevaluationsuite. Table 5:Fullquantizedinferenceresults.Herewepresentthedetailedversionof the resultsshownin Table 2. bfloat 16 int 8 int 4 Category Task #Trials #Successes #Successes #Successes Visualgen Put Eggplantinto Pot(Easy Version) 10 9 7 9 Visualgen Put Eggplantinto Pot 10 7 7 7 Visualgen Put Cupfrom Counterinto Sink 10 5 3 7 Motiongen Lift Eggplant 10 6 4 7.5 Physicalgen Put Carroton Plate 10 6 5 7 Physicalgen Lift AAABattery 10 7 5 3 Semanticgen Take Purple Grapesoutof Pot 10 8 8 9 Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 9 7.5 8 Mean Success Rate 71.3\u00b14.8% 58.1\u00b15.1% 71.9\u00b14.7% B.2 Google Robot Evaluation Details Inthissection,weprovidemoredetailson the Googlerobotevaluationsintroducedin Section 5.1. B.2.1 Google Robot Evaluation Tasks Onthe Googlerobot,weevaluateeachgeneralistrobotpolicyon 12 tasks with 5 rolloutseach,fora totalof 60 rollouts. Thefirstfive task stestonin-distributionconditions,and the lastseven task stest onmoredifficultout-of-distribution(OOD)conditions. Alltasks are depictedin Fig.9. Eachrollout ismarkedasafailure(0)orsuccess(1). Wedescribe the 12 tasksbelow: 1. Pick Coke Can(in-distribution): Therobotispositionedinfrontofaplat for mwitha can of Cokeontopofit. Therobot\u2019sgoalistograsp and lift the Coke can. 26 Figure 9: Googlerobotevaluationtasks. Weevaluateeverygeneralistrobotpolicyonin-distributiontasks andout-of-distribution(OOD)generalizationtasks. OOD task sinvolveunseenbackgrounds,targetobjects, instructions/objectrelations,andsemanticconcepts(e.g.,photos from the Internet that donotappearinrobot action data). 2. Move Applenear Green Can(in-distribution):Therobotispositionedinfrontofaplatform withanapple and agreensoda can ontopofit. Therobot\u2019sgoalistograsp the apple and moveitnextto the green can. 3. Move Blue Chip Bagnear Apple(in-distribution): Therobotispositionedinfrontofa plat for mwithabluebagofchips and anappleontopofit. Therobot\u2019sgoalistograsp the bluebagofchips and moveitcloseto the apple. 4. Place Coke Can Upright(in-distribution): Therobotispositionedinfrontofaplatform withacanof Cokeontopofit,andthe can isorientedhorizontallyonitsside. Therobot\u2019s goalistograsp the Coke can andorientittobeinaverticalposition. 5. Open Middle Drawer(in-distribution): Therobotispositionedinfrontofasetofthree drawers. Therobot\u2019sgoalistograspthemiddledrawerhandle and pull the drawer open. 6. Move Orangenear Brown Chip Bag(OOD):Therobotispositionedinfrontofaplatform withabrownbagofchips and anorangeontopofit. Atablecloth with bluesky and white cloudpatternscoverstheplat for munderneath the objects. Therobot\u2019sgoalistograsp the orange and bringitnextto the bagofchips. Thistaskis OODbecause the orangeisan unseenobjectrelativeto the training data set,and the tableclo this anunseenbackground.7 7 See Appendixof Brohanetal.[7]foradetailedlistof OODconditionsin Googlerobotevaluations. 27 7. Pick Pepsi Can(OOD):Therobotispositionedinfrontofaplat for mwithacanof Pepsi ontopofit. Atablecloth with brightyellow/brownpatternscovers the plat for munderneath thecan. Therobot\u2019sgoalistograsp and lift the can. Thistaskis OODbecause the Pepsi canisanunseenobject,and the tableclo this anunseenbackground. 8. Pick Banana(OOD):Therobotispositionedinfrontofaplat for mwithanapple,acan of Coke,andabanana. Therobot\u2019sgoalistograsp and lift the banana. Thistaskis OOD because the bananaisanunseentargetobject. 9. Pick Green Cup(OOD):Therobotispositionedinfrontofaplat for mwithabanana,acan of Pepsi,andagreencup. Therobot\u2019sgoalistograsp and lift the greencup. Thistaskis OODbecauseallobjectsinthescene are unseenin the training data. 10. Place Appleon Plate(OOD):Therobotispositionedinfrontofaplat for mwithaplate and anapple.",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 19,
      "paper_id": "openvla",
      "text": "canisanunseenobject,and the tableclo this anunseenbackground. 8. Pick Banana(OOD):Therobotispositionedinfrontofaplat for mwithanapple,acan of Coke,andabanana. Therobot\u2019sgoalistograsp and lift the banana. Thistaskis OOD because the bananaisanunseentargetobject. 9. Pick Green Cup(OOD):Therobotispositionedinfrontofaplat for mwithabanana,acan of Pepsi,andagreencup. Therobot\u2019sgoalistograsp and lift the greencup. Thistaskis OODbecauseallobjectsinthescene are unseenin the training data. 10. Place Appleon Plate(OOD):Therobotispositionedinfrontofaplat for mwithaplate and anapple. Therobot\u2019sgoalistograsptheapple and moveitonto the plate. Thistaskis OOD becauseitisanovelinstructiondescribinganunseenobjectrelation:trainingdemonstrations onlycovermovingtheapplenear the plate,ratherthanplacingitontopof theplate. 11. Place Bananain Pan(OOD):Therobotispositionedinfrontofaplat for mwithapan and abanana. Therobot\u2019sgoalistograspthebanana and moveitinto the pan. Thistaskis OOD because the bananaisanunseentargetobject,anditisanovelinstructiondescribingan unseenobjectrelation,asexplainedin the previous task. 12. Move Coke Canto Taylor Swift(OOD):Therobotispositionedinfrontofaplat for mwith acanof Coke and photosofthreedifferentcelebrities,including Taylor Swift. Therobot\u2019s goalistograsp the can andmoveitto the photoof Taylor Swift. Thistaskis OODbecause thephotosofthecelebrities are unseenin the robotinteraction data. B.2.2 Detailed Google Robot Evaluation Results Table 6:Detailed Googlerobotevaluationresults.Wereport full evaluationresults for Googlerobotevaluations discussedin Section 5.1. Eachgeneralistpolicyisevaluated with 60 rolloutsacross 12 tasks,coveringboth in-distribution and out-of-distribution(OOD)testingconditions.Inthebottomrow,wereportmeansuccessrate \u00b1Std Err for eachpolicy.Open VLAand RT-2-Xbothsignifi can tlyoutperform RT-1-Xand Octooverall(we bold the meansuccessrate for bothduetooverlappingerrorbars).See Fig.9 forillustrationsofalltasks. RT-1-X Octo RT-2-X Open VLA(ours) Category Task #Trials #Successes #Successes #Successes #Successes In-distribution Pick Coke Can 5 5 1 5 5 In-distribution Move Applenear Green Can 5 3 3 3 5 In-distribution Move Blue Chip Bagnear Apple 5 0 3 4 5 In-distribution Place Coke Can Upright 5 0 0 4 4 In-distribution Open Middle Drawer 5 0 4 2 3 OOD Move Orangenear Brown Chip Bag 5 1 2 5 5 OOD Pick Pepsi Can 5 3 0 5 4 OOD Pick Banana 5 5 3 5 5 OOD Pick Green Cup 5 1 0 5 5 OOD Place Appleon Plate 5 0 0 4 4 OOD Place Bananain Pan 5 0 0 2 4 OOD Move Coke Cannear Taylor Swift 5 2 0 3 2 Mean Success Rate 33.3\u00b16.1% 26.7\u00b15.8% 78.3\u00b15.4% 85.0\u00b14.6% Fullresults for the Googlerobotevaluations are shownin Table 6. Overall,wefind that RT-1-Xand Octoexperiencedifficultyon the evaluationtasks;they are oftenunabletoachieveasinglesuccess out of five trials in several tasks. On the other hand, RT-2-X and Open VLA demonstrate strong per for mance, completing every task at least two times out of five trials; these two VLA policies per for mcomparably with eacho the ron this particul are valuationsuite. B.3 Data-Efficient Adaptation Experiment Details Inthissection,weprovidemoredetailson the data-efficientadaptationexperimentsdiscussedin Section 5.2,whereweinvestigate the effectivenessoffine-tuned Open VLApolicieson new robot setupssuchas Franka-Tabletop and Franka-DROID. 28 B.3.1 Franka-Tabletop and Franka-DROIDTasks Wecollect 10\u2013150 demonstrationsofeachofseventasks. Thefirstsix task scorrespondtoarobot setupwhichwedenoteas\u201cFranka-Tabletop\u201d(Franka Emika Pandarobotmountedontopofatable), and the final task correspondstoarobotsetupwhichwecall\u201cFranka-DROID\u201d. Inthe Franka-Tabletopsetup,thefirstthreeofsix task scorrespondtosingle-instructiontasks and arenarrow,while the lastthree task scorrespondtomulti-instruction task sinwhichmultipleobjects arepresentinthescene and therobotmustmanipulatethecorrectonedependingon the language instruction. In-Distribution Out-of-Distribution Put Carrot in Bowl Pour Corn into Pot Flip Pot Upright Move <object> onto Plate Knock <object> Over Cover <object> with Towel sksa T noitcurtsn I-elgni S worra N sksa T noitcurtsn I-itlu M esrevi D Figure 10: Franka-Tabletopfine-tuningtasks. Franka-Tabletop task susedin the data-efficientadaptation experimentsin Section 5.2 anddescribedindetailin Fig.10 aredepictedabove. Thefirstthreeofsixtasks, shownin the topthreerows,onlyinvolveasingleinstruction,whilethelastthree task sin the bottomthree rowsinvolvemultipleobjects and instructions(theinstructionsspecify the targetobjectortargetlocation). Thefirstcolumnshowssampleinitialstatesmatching the training data distribution,while the secondcolumn showsout-of-distribution(OOD)initialstates(e.g.,unseenbackgrounds,targetobjects,distractors,andobject positions/orientations).Everypolicyin Section 5.2 isevaluated with 10\u201312 rolloutsonin-distributiontasks and 5\u20136 rolloutson OODtasks. 29 Belowwedescribeeachof the six Franka-Tabletop task sshownin Fig.10: 1. Put Carrotin Bowl(single-instruction): Therobot\u2019sgoalistograsp the carrot and placeit into",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 20,
      "paper_id": "openvla",
      "text": "Thefirstthreeofsixtasks, shownin the topthreerows,onlyinvolveasingleinstruction,whilethelastthree task sin the bottomthree rowsinvolvemultipleobjects and instructions(theinstructionsspecify the targetobjectortargetlocation). Thefirstcolumnshowssampleinitialstatesmatching the training data distribution,while the secondcolumn showsout-of-distribution(OOD)initialstates(e.g.,unseenbackgrounds,targetobjects,distractors,andobject positions/orientations).Everypolicyin Section 5.2 isevaluated with 10\u201312 rolloutsonin-distributiontasks and 5\u20136 rolloutson OODtasks. 29 Belowwedescribeeachof the six Franka-Tabletop task sshownin Fig.10: 1. Put Carrotin Bowl(single-instruction): Therobot\u2019sgoalistograsp the carrot and placeit into the bowl. Wecollect 50 demonstrationsof this task for the training data set,randomly placingthecarrot and thebowlatdifferentlocationson the tableineveryepisode.Thecarrot isalwaysinitializedontheleftsideof the bowl. Duringevaluation,eachtrialisrecordedas asuccess(1)orfailure(0);thereisnopartialcredit. 2. Pour Corninto Pot(single-instruction): Therobot\u2019sgoalistograsp the redbowl,move towards the steel pot, and pour the contents (a yellow corn) into the pot. We collect 50 demonstrationsof this task for the training data set,randomlyplacingthebowl and the potat differentlocationson the tableineveryepisode. Thebowlisalwaysinitializedon the right sideof the pot. Duringevaluation,eachtrialisrecordedasasuccess(1)orfailure(0);there isnopartialcredit. 3. Flip Pot Upright (single-instruction): The robot\u2019s goal is to grasp the steel pot (which is initially oriented vertically), rotate it to be in the upright position, and place it back onto the table. We collect only 10 demonstrations of this task for the training dataset, randomly placing the steel pot at various locations within a small section of the table. Duringevaluation,eachtrialisrecordedasasuccess(1),failure(0),orpartialsuccess(0.5). Partialsuccessesincludegrasping the potbutnotorientingitupright,orknockingitoverto theuprightpositionbutnotc are fullyguidingit. Therobotmustreleasethepotat the endof theepisode for fullcredit. 4. Move <object> onto Plate (multi-instruction): The robot\u2019s goal is to grasp one out of threeobjects(dependingonthetargetspecifiedin the languageinstruction)andplaceit ontheplateontherightsideof the table. Wecollect 150 demonstrationsof this task for thetraining data set,randomlyplacingdifferentcombinationsofthreeobjectson the table andselectingoneas the target. Theplateisalwaysinitializedontherightsideof the table. Duringevaluation,eachtrialisrecordedasasuccess(1),failure(0),orpartialsuccess(0.5). Partialsuccessisrecordedwhenthefirstobject that the robotmakescontactwi this the correcttargetobject(i.e.,theobjectspecifiedin the languageinstruction),but the robotdoes notcomplete the task. 5. Knock<object>Over(multi-instruction): Therobot\u2019sgoalistoapproachoneoutofthree objects (depending on the target specified in the language instruction) and push it until itfallsover. Wecollect 70 demonstrationsof this task for the training data set,randomly placingdifferentcombinationsofthreeobjectsonthetable and selectingoneas the target. Duringevaluation,eachtrialisrecordedasasuccess(1),failure(0),orpartialsuccess(0.5). Partialsuccessisrecordedwhenthefirstobject that the robotmakescontactwi this the correcttargetobject(i.e.,theobjectspecifiedin the languageinstruction),but the robotdoes notcomplete the task. 6. Cover<object>with Towel(multi-instruction): Therobot\u2019sgoalistograsp the bluetowel andplaceitononeoutofthreeobjects(dependingonthetargetspecifiedin the language instruction). Wecollect 45 demonstrationsof this task for the training data set,randomly placingdifferentcombinationsofthreeobjectson the table. Duringevaluation,eachtrial isrecordedasasuccess(1),failure(0),orpartialsuccess(0.5). Partialsuccessisrecorded whenthefirstobject that therobottouches with thetowelis the correcttargetobject(i.e., theobjectspecifiedin the languageinstruction),buttherobotdoesnotcomplete the task (e.g.,itdropsthetowelontothetableinsteadofontopof the targetobject). Fullcreditis givenwhenanypartofthetowelisrestingoverthetopsurfaceof the targetobject,i.e.,the objectdoesnotneedtobe full ycovered. Forevery Franka-Tabletop task,weevaluateeachmethod with 10\u201312 in-distributiontrials and 5\u20136 OOD generalization trials. The in-distribution and OOD test conditions are depicted in Fig. 10 (secondcolumn). Wedescribe the OODtestconditions for eachof the six task sbelow: 30 1. Put Carrotin Bowl(OOD):Aneggplant(unseenobject)replaces the carrot. 2. Pour Corninto Pot(OOD):Anunseenbrowntableclothcovers the tabletop. 3. Flip Pot Upright(OOD):Anunseenwhitetableclothcovers the tabletop 4. Move<object>onto Plate(OOD):Asetofthreeunseenobjects are placedon the table. 5. Knock<object>Over(OOD):Twounseendistractorobjects(redplasticcup and brown box)arepositionedbehind the setofthreeseenobjects. 6. Cover<object>with Towel(OOD):Thethreeobjectson the table are placedupside-down andatunseenpositions. Finally,inthe Franka-DROIDenvironment,weexperiment with onetask and variantsofit: Wipe Table(see Fig.11). Inthis task,therobot\u2019sgoalistograb the brush and sweepallthreesmallbrown objectsinto the dustpan. Wecollect 70 demonstrations for this task for the training data set,varying thepositionsofall the objects. Figure 11: Franka-DROIDfine-tuning task. The\u201cWipe Table\u201dtaskshownhereis the final task usedin thedata-efficientadaptationexperimentsin Section 5.2. Theleftimageshows the initialconditionsforan in-distributiontrial.Therightimageshowsanout-of-distributiontrialinwhichunseendistractorobjects are presenton the table.Tofullycomplete the task,therobotmustgrab the brush and sweepallthreeobjectsinto thedustpan. Attesttime,weevaluateonin-distributionconditionsmatching the training data(Fig.11,left),as wellasout-of-distribution(OOD)conditionsinwhichdistractorobjects are alsopresentin the scene onthetable(Fig.11,right). Since the rearevariouspossibleoutcomes for eachtrial,wedefinea scoringrubricasfollows: Themaximumscore for eachtrialis 2 points. Thepolicyreceives the full 2 pointsiftherobotsweepsallthreeobjectsinto the dustpan. Itreceives 1 point for successfully sweepingoneortwoobjectsinto the dustpan. Otherwise, itreceives",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 21,
      "paper_id": "openvla",
      "text": "in-distributiontrial.Therightimageshowsanout-of-distributiontrialinwhichunseendistractorobjects are presenton the table.Tofullycomplete the task,therobotmustgrab the brush and sweepallthreeobjectsinto thedustpan. Attesttime,weevaluateonin-distributionconditionsmatching the training data(Fig.11,left),as wellasout-of-distribution(OOD)conditionsinwhichdistractorobjects are alsopresentin the scene onthetable(Fig.11,right). Since the rearevariouspossibleoutcomes for eachtrial,wedefinea scoringrubricasfollows: Themaximumscore for eachtrialis 2 points. Thepolicyreceives the full 2 pointsiftherobotsweepsallthreeobjectsinto the dustpan. Itreceives 1 point for successfully sweepingoneortwoobjectsinto the dustpan. Otherwise, itreceives 0 points. Weevaluateeach policy with 18 in-distributiontrials and 12 OODtrials,soeachpolicyreceivesanaggregatescoreout of 60 points. B.3.2 Detailed Franka-Tabletop and Franka-DROIDEvaluation Results Fullevaluationresults for both Franka-Tabletop and Franka-DROIDevaluations are shownin Table 7. We evaluate the methods discussed in Section 5.2. We find that Diffusion Policy demonstrates strongper for manceon the single-instruction Franka-Tabletoptasks(e.g.,\u201cPut Carrotin Bowl\u201dand \u201cPour Cornin Pot\u201d),outper for mingo the rmethods. However,Open VLAand Octoachievehigher per for mance in the more diverse multi-instruction tasks (\u201cMove <object> onto Plate\u201d, \u201cKnock <object>Over\u201d,and\u201cCover<object>with Towel\u201d). Inthe Franka-DROIDenvironment,Open VLA obtainsbestresults. Overall,wefind that Open VLAachieves the highestaverageper for manceacross bothtasks. Additionally,in Table 8,weshowthedetailedversionof the parameter-efficientfine-tuningexperiment resultssummarizedin Table 1. Intheseexperiments,we use are presentativesubsetoftwo Franka- Tabletoptasks,withbothin-distribution and OODvariants: onenarrowsingle-instruction task(\u201cPut Carrotin Bowl\u201d)andonediversemulti-instruction task(\u201cMove<object>onto Plate\u201d). we usethe samenumberoftrainingdemonstrationsusedin Section 5.2(50 and 150,respectively),whichis delineatedin Appendix B.3.1. 31 Table 7:Detailed data-efficientadaptationexperimentresults.Herewepresent the fullbreakdownofresults summarizedin Fig.5.Wereport the per for manceof Diffusion Policytrained from scratchon new robottasks, aswellasgeneralistpoliciesfine-tunedon the same data. Eachpolicyistestedagainstbothin-distribution andout-of-distribution(OOD)generalizationconditions(see Fig.10 for Franka-Tabletoptasks and Fig.11 for Franka-DROIDtasks).Wefind that nosinglepolicyper for msbestonalltasks:Diffusion Policyachieveshigh successratesonsingle-instructiontasks,while Open VLAand Octoper for mswellondiversemulti-instruction tasks.Intermsofaggregateper for mance,however,Open VLAobtains the highestaveragesuccessrateacross bothenvironments. Diffusion Policy Open VLA Open VLA #trials Diffusion Policy Octo (matched) (scratch) (ours) Franka-Tabletop(5 Hz) \u201cPut Carrotin Bowl\u201d(in-distribution) 10 90.0% 80.0% 40.0% 70.0% 70.0% \u201cPut Carrotin Bowl\u201d(OOD) 5 20.0% 0.0% 20.0% 0.0% 40.0% \u201cPour Corninto Pot\u201d(in-distribution) 10 100.0% 90.0% 0.0% 10.0% 50.0% \u201cPour Corninto Pot\u201d(OOD) 5 80.0% 60.0% 0.0% 20.0% 60.0% \u201cFlip Pot Upright\u201d(in-distribution) 10 100.0% 85.0% 40.0% 85.0% 100.0% \u201cFlip Pot Upright\u201d(OOD) 5 50.0% 20.0% 0.0% 40.0% 80.0% \u201cMove<object>onto Plate\u201d(in-distribution) 12 25.0% 25.0% 41.7% 8.3% 75.0% \u201cMove<object>onto Plate\u201d(OOD) 6 8.3% 33.3% 8.3% 33.3% 58.3% \u201cKnock<object>Over\u201d(in-distribution) 12 33.3% 25.0% 83.3% 75.0% 75.0% \u201cKnock<object>Over\u201d(OOD) 6 16.7% 16.7% 33.3% 58.3% 83.3% \u201cCover<object>with Towel\u201d(in-distribution) 12 16.7% 20.8% 91.7% 41.7% 50.0% \u201cCover<object>with Towel\u201d(OOD) 6 16.7% 33.3% 91.7% 50.0% 50.0% Average 48.5\u00b14.9% 43.4\u00b14.7% 43.4\u00b14.4% 43.4\u00b14.6% 67.2\u00b14.0% Franka-DROID(15 Hz) \u201cWipe Table\u201d(in-distribution) 18 50.0% 27.8% 52.8% 25.0% 55.6% \u201cWipe Table\u201d+Distractors(OOD) 12 12.5% 25.0% 16.7% 16.7% 62.5% Average 35.0\u00b18.0% 26.7\u00b17.5% 38.3\u00b18.5% 21.7\u00b16.6% 58.3\u00b17.2% Table 8: Detailedparameter-efficientfine-tuningexperimentresults. Herewepresent the detailed task per for manceresultssummarizedin Table 1. #trials Full FT Lastlayeronly Frozenvision Sandwich Lo RA,r=32 Lo RA,r=64 Franka-Tabletop(5 Hz) \u201cPut Carrotin Bowl\u201d(in-distribution) 10 90.0 40.0 40.0 90.0 60.0 90.0 \u201cPut Carrotin Bowl\u201d(OOD) 5 40.0 0.0 40.0 0.0 60.0 40.0 \u201cMove<object>onto Plate\u201d(in-distribution) 12 79.2 33.3 50.0 75.0 75.0 62.5 \u201cMove<object>onto Plate\u201d(OOD) 6 41.7 33.3 58.3 41.7 75.0 66.7 Average 69.7\u00b17.2% 30.3\u00b16.1% 47.0\u00b16.9% 62.1\u00b17.9% 68.2\u00b17.5% 68.2\u00b17.8% C RT-2-Xvs. Open VLAin Bridge Data V 2 Evaluations Inthissection,weprovideadditionaldetailson RT-2-Xvs. Open VLAcomparisonsin Bridge Data V 2 evaluationsdiscussedin Section 5.1. Asdiscussedpreviously,Open VLAispretrainedonalarger subsetof Open Xdatathan RT-2-Xandusesafused Sig LIP-Dino V 2 visionbackbonera the rthana singlevisualencoder. However,inadditionto the sefactors,webelieve that Open VLA\u2019ssignificant improvementupon RT-2-Xspecificallyin Bridge Data V 2 evaluations(asshownin Fig.3)alsostems frommorec are fulpreprocessingof the Bridge data set. During the development of the Open VLA model, we discovered that the original version of the Bridge Data V 2 datasetcontainedmanytransitions with",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 22,
      "paper_id": "openvla",
      "text": "LIP-Dino V 2 visionbackbonera the rthana singlevisualencoder. However,inadditionto the sefactors,webelieve that Open VLA\u2019ssignificant improvementupon RT-2-Xspecificallyin Bridge Data V 2 evaluations(asshownin Fig.3)alsostems frommorec are fulpreprocessingof the Bridge data set. During the development of the Open VLA model, we discovered that the original version of the Bridge Data V 2 datasetcontainedmanytransitions with all-zero(no-op)actions. Forinstance,in everydemonstration,anall-zeroactionwasrecordedas the ground-truthactionin the firsttimestep. Consequently, training a highly expressive VLA model on the original dataset without any data preprocessingledtoapolicy that frequentlypredictedall-zeroactions and frozeduringevaluations. Therefore, we simply filtered out the first transition in every demonstration when training the Open VLAmodel,and this wassufficient for mitigating the freezingbehaviorinmostcases. However, the RT-2-X model was trained without such data preprocessing, so it often suffers the aforementionedfreezingbehaviorifdeployedoutofthebox with outmodifying the modelquerying procedure\u2013whichseverelydeterioratesrolloutper for mance. Since this isaproprietary model that isinfeasible for ustore-train(e.g.,with our preprocessedversionof the Bridge Data V 2 dataset), wemitigated this issuebysimplyquerying the second-most-likelyaction from the model,since the first-most-likelyactionwasoftenallzeroswhile the second-most-likelyactionwasnot.(Note that this isthesameworkaround that wasappliedby the developersof the RT-2-Xmodel for Bridge Data V 2 evaluationsreportedin the Open X-Embodimentexperiments[1].) Thisworkaroundledtomuch stronger RT-2-X per for mance on Bridge Data V 2 evaluations \u2013 though we believe that it is still suboptimalcomp are dtore-trainingthe model onthepreprocessedversionof the dataset. Wealsotriedtodynamicallyquery RT-2-X,i.e.,byfirstsampling the first-most-likelyaction and thensampling the second-most-likelyactionif the firsto new asallzeros. However,weempirically 32 found that dynamicqueryingledtoworseper for mancethansimplyquerying the second-most-likely actionatalltimes. Wehypothesize that thisisduetoachangein the robot\u2019sdynamics that arises fromdynamicquerying: pausingin the middleofatrajectorytore-query the modelleadstoslight interruptionsin the robot\u2019smovementduetonon-negliblelatencyin the queryingpipeline,andthis leadstosubtleper for mancedegradation. Therefore, wereport the per for manceof RT-2-Xwhen alwaysquerying the second-most-likelyaction,asdonein the Open X-Embodimentproject[1]. D Additional Experiments and Ablations In this section, we conduct several additional experiments to analyze the effects of individual componentsof the Open VLA model architecture and trainingscheme,aswellasprovidequantitative evidence for claimsmadeinearliersectionsof this work. Weaimtoanswer the followingquestions: 1. Howimportantis Open Xtraining and howdoesitimpact Open VLA\u2019sper for mance(Ap- pendix D.1)? 2. Whateffectdoesusingafused Sig LIP-Dino V 2 visionencoderhaveon Open VLA\u2019sper for- mance,comp are dtousinga Sig LIP-onlyvisionencoder(Appendix D.2)? 3. Isitbettertofine-tuneorfreeze the visionencoderin Open VLA(Appendix D.3)? 4. Howdo the quantizedinferenceresultsdiscussedin Section 5.3 changewhenpolicyper for- manceisdisentangled from modelinferencespeed(Appendix D.4)? Wediscusstheexperimentalsetup and resultsaddressingeachof the abovequestionssequentiallyin thefollowingsections. D.1 Open XTraining Data Ablation Experiments Asdiscussedin Section 3.3,Open VLAistrainedonalarge data setofrobotembodiments,scenes,and tasks from the Open X-Embodiment data set[1](Open X).Inthissection,weablate the Open Xmixture andtraina VLApolicysolelyononerobot data set,toassess the impactof Open Xtrainingonpolicy per for mance.Note that wehavealreadyobserved the negativeeffectofablating Open Xtrainingin the fine-tuningregime,asdiscussedin Section 5.2(see Open VLA(Scratch)),butwediscussadditional experimentsonano the rrobotembodimentin this sectiontoprovidemoresupportingevidence. Experimentalsetup and tasks. Wecomp are the original Open VLAmodel with Open VLA-Bridge, which is produced by taking the same pretrained VLM as Open VLA (Prismatic VLM [44]) and fine-tuningitsolelyon Bridge Data V 2[6]ratherthan the entire Open Xtrainingmixturediscussed in Appendix A.Weevaluate Open VLAand Open VLA-Bridgeonasubsetof 8 representativetasks from the Bridge Data V 2 Widow Xrobotevaluationsuitediscussedin Appendix B.1.1. Thetasks are listedin Table 9. Results. Results for the Open X training mixture ablation are shown in Table 9. By comparing Open VLAwith Open VLA-Bridge,wesee that per for mancedropsdrastically(reductionof 30 percent inabsolutesuccessrate),whichdemonstrates the importanceof Open Xpretrainingonfinalpolicy per for mance.Although the languagegroundingper for manceisnotimpacted,weobserveper for mance reductionacrossallgeneralizationcategories. Thisresultsuggests that the largediversityofscenes, objects,and task sin the Open Xtrainingmixtureisessential for unlockingimprovedgeneralization capabilitiesin the Open VLAmodel. D.2 Dualvs. Single Vision Encoder",
      "start_pos": 10164,
      "end_pos": 10676
    },
    {
      "chunk_id": 23,
      "paper_id": "openvla",
      "text": "Table 9. By comparing Open VLAwith Open VLA-Bridge,wesee that per for mancedropsdrastically(reductionof 30 percent inabsolutesuccessrate),whichdemonstrates the importanceof Open Xpretrainingonfinalpolicy per for mance.Although the languagegroundingper for manceisnotimpacted,weobserveper for mance reductionacrossallgeneralizationcategories. Thisresultsuggests that the largediversityofscenes, objects,and task sin the Open Xtrainingmixtureisessential for unlockingimprovedgeneralization capabilitiesin the Open VLAmodel. D.2 Dualvs. Single Vision Encoder Experiments The Open VLA model architectureconsistsofafusedvisionbackbone that combines the Sig LIP[9] and Dino V 2[25]encoders. Inthissection,weablate the Dino V 2 componenttoassess the importance ofusingadualvisionencoder. Experimentalsetup and tasks. Weinstantiatea model, Open VLA-Bridge-Sig LIP,whichisa versionof Open VLA that istrainedonlyon Bridge Data V 2 andconsistsofonly the Sig LIPencoder as the vision backbone. We comp are this model with the Open VLA-Bridge model discussed in the previous section (Appendix D.1), which shares the same model architecture as the original Open VLAmodel and isonlytrainedon Bridgerobot data. Therefore,theonlydifferencebetween Open VLA-Bridge-Sig LIPand Open VLA-Bridgeis that the formeromits the Dino V 2 encoderin the 33 Table 9:Bridge Data V 2 Widow Xablationexperimentresults.Weevaluatevariousmethodsonasubsetof 8 representative task stoassess the importanceofdifferentcomponentsof the Open VLA model architecture andtrainingscheme.Open VLA-Bridgeisaversionof Open VLA with out Open Xtraining(itistrainedonlyon Bridge Data V 2),and Open VLA-Bridge-Sig LIPadditionallyablates the fusedvisionbackbonebyremoving the Dino V 2 encoder(itsvisionbackboneonlyconsistsof the Sig LIPencoder).Weobserve that both Open X training and the fusedvisionencoderimprovepolicyper for mance,though the formerhasamuchgreatereffect than the latter. Open VLA Open VLA-Bridge Open VLA-Bridge-Sig LIP Category Task #Trials #Successes #Successes #Successes Visualgen Put Eggplantinto Pot(Easy Version) 10 10 8 8 Visualgen Put Eggplantinto Pot 10 10 2 3 Visualgen Put Cupfrom Counterinto Sink 10 7 4 2 Motiongen Lift Eggplant 10 7.5 5.5 6.5 Physicalgen Put Carroton Plate 10 8 4 1 Physicalgen Lift AAABattery 10 7 2 2 Semanticgen Take Purple Grapesoutof Pot 10 4 3 3 Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 7.5 8 7 Mean Success Rate 76.3\u00b14.8% 45.6\u00b15.6% 40.6\u00b15.5% visionbackbone. Weevaluatethese model son the samesubsetof 8 Bridge task sdescribedin the previoussection. Results. Results for the dualvisionencoderablation are shownin Table 9. Thedropinper for mance from Open VLA-Bridgeto Open VLA-Bridge-Sig LIPimplies that additionallyincluding the Dino V 2 encoderin the visionbackboneimprovespolicyper for mance. However,the 5 percentreductionin per for mancehereisnotassignifi can tasthe 30 percentdropinper for manceobserved from ablating Open Xtraining. Thelow-levelspatialfeaturesrepresentedin Dino V 2 appeartoaidgeneralizationin onlysomecases. D.3 Fine-Tunedvs. Frozen Vision Encoder Experiments Asdiscussedin Section 3.4,priorworkon VLMsobservedhigherper for mance from freezing the visionencoderthanfine-tuningitsparameters[44].However,whentraining Open VLA,wefine-tuned all 7 Bparametersin the model,including the Sig LIP-Dino V 2 visionbackbone,aswediscovered earlyonduringdevelopment that fine-tuning the visionencoderledtohigher-per for ming VLAs\u2014a findingwhichheldacrossvariouspretrained VLMs and modelarchitectures. Wediscussdetailsof suchfindingsbelow. Experimentalsetup and tasks. Inthissection, wereport the per for manceoftwo VLApolicies producedbyfine-tuningtwodifferentpretrainedmodels from the Prismatic VLMs[44]repositoryon Bridge Data V 2. Thetwopretrainedmodels are named Sig LIPVi T-SO 224 pxand LLa Vav 1.57 B (Reproduction);see Karamchetietal.[44]fordetailson the irarchitectures and trainingmixtures. We evaluate both policies on various Bridge tasks shown in Table 10. Note that the evaluation configurationsherediffer from previouslydiscussed Bridgeevaluations,sotheresults are notdirectly comparabletoresultsino the rsimil are xperiments. Results. Results for the fine-tunedvs. frozenvisionencoderexperiments are shownin Table 10. We find that for both VLAstested,fine-tuning the visionencoderleadstosignifi can tlyhighersuccess ratesacrossvarioustasks. Qualitatively,insomecases,deploying the frozenvisionencoderpolicies leadstounstablerobotbehaviors that are clearlysuboptimal. Consequently,wedecidedearlyon duringdevelopmenttonotconductfur the rexperimentation with frozenvisionencoders. D.4 Additional Quantized Inference Experiments: Disentangling Policy Per for mance and Model Inference Speed In Section 5.3, we evaluated",
      "start_pos": 10626,
      "end_pos": 11138
    },
    {
      "chunk_id": 24,
      "paper_id": "openvla",
      "text": "the fine-tunedvs. frozenvisionencoderexperiments are shownin Table 10. We find that for both VLAstested,fine-tuning the visionencoderleadstosignifi can tlyhighersuccess ratesacrossvarioustasks. Qualitatively,insomecases,deploying the frozenvisionencoderpolicies leadstounstablerobotbehaviors that are clearlysuboptimal. Consequently,wedecidedearlyon duringdevelopmenttonotconductfur the rexperimentation with frozenvisionencoders. D.4 Additional Quantized Inference Experiments: Disentangling Policy Per for mance and Model Inference Speed In Section 5.3, we evaluated Open VLA with different levels of precision at inference time: half precision (bfloat 16), 8-bit quantization, and 4-bit quantization. 8-bit quantization led to lower Bridge Data V 2 per for mance relative to the other two approaches, and we hypo the sized that the reductioninper for mancewascausedbylower model inferencespeed from the operationsusedin 8-bitquantization. Inthissection,weconductexperimentstoassess the veracityof this claim. Specifically,weevaluate Open VLAagain with the threedifferentlevelsofprecisionlistedabove, but now with blocking control. In otherwords, each actionis fullyexecuted onthe robotbefore thenextoneispredictedbythepolicy and executedby the controller. Thisschemecontrolssystem 34 Table 10:Fine-tunedvs.frozenvisionencoderexperimentresults.Weevaluate the per for manceoffine-tuning (\u201cFine-Tuned\u201d)vs. freezing the visionencoder(\u201cFrozen Vision\u201d)intwo VLApoliciesbuiltontopoftwo differentpretrained VLMs from the Prismatic VLMs[44]repository.Bridge Data V 2 Widow Xtasksshownhere areper for medin the samesinkenvironmentused for other Bridgeexperimentsin this work(however,theinitial environmentconfigurationsherediffer,astheseevaluations were conducte data nearlierstagein the project). Wefind that fine-tuning the visionencoderiscrucialtoobtaingoodpolicyper for mance.Certainfrozenvision encoderevaluations were discontinuedduetoverypoor(near-zero)per for mance and unstablerobotbehaviors. Among the evaluationswherebothfrozenvision and fine-tunedapproaches are tested,fine-tuning the vision encoderleadsto 80.0%averagesuccessversus 46.7%averagesuccess from leavingitfrozen. Sig LIPVi T-SO 224 px LLa Vav 1.57 B(Reproduction) Frozen Vision Fine-Tuned Frozen Vision Fine-Tuned Task #Trials #Successes #Successes #Successes #Successes Put Eggplantinto Pot 10 7 10 5 9 Put Cornon Plate 10 10 9 0 9 Mean Success Rate 85 95 25 90 Put{Eggplant,Red Bottle}into Pot 4 2 4 \u2013 3 Put{Blue Cup,Pink Cup}on Plate 4 0 0 \u2013 0 Lift{Cheese,Red Chili Pepper} 4 0 3 \u2013 2 Put{Strawberry,Lime}into Pot 4 1 0 \u2013 3 Move{Sushi,Grapes} 4 3 4 \u2013 3 Mean Success Rate 30 55 \u2013 55 dynamicsacrossmethods with varyingamountsoflatency and thusallowsustotest the qualityof apolicy\u2019sactionpredictions,independentofitspredictionspeed. Effectively,theprecisionlevels that have higherthroughput\u2013bfloat 16 and 4-bitquantization\u2013are for cedtorunslowertomatch the dynamicsobservedwhendeploying Open VLAwith 8-bitprecision.Therefore,weexpect Open VLA\u2019s per for mance with 8-bitprecisiontomatch the per for manceofbfloat 16 and 4-bitprecisionunder blockingcontrol. Experimentalsetup and tasks. Wereport the per for manceof Open VLA with blockingcontrol andquantizedinferenceon the samesubsetof 8 Bridge Data V 2 tasksusedin Appendix D.1 and Appendix D.2. Results. Quantizedinferenceexperimentresults with blockingcontrol are shownin Table 11. Unlike in Table 2, where 8-bit quantization led to the worst rollout per for mance due to low inference speed,hereweobserve that 8-bitquantizationper for mscomparablytobfloat 16 precision and 4-bit quantizationgiven that weevaluate with blockingcontroltoremove the influenceofvaryinginference speedson task per for mance. Thisconfirms our hypothesisabout the effectofinferencespeedon 8-bit quantizationper for manceinpreviousexperiments(whenusingnon-blockingcontrol). Wealsosee nosubstantialper for mancedegradationwhenusing the lowestprecision,4-bit,asalsoobservedin Section 5.3. Table 11: Quantizedinferenceexperimentresults with blockingcontrol. Wereport the successrate and standarderrorof Open VLAonvarious Bridge Data V 2 Widow Xtasks with bfloat 16 precision(thedefault approach),8-bitquantization(int 8),and 4-bitquantization(int 4)atinferencetime.Allaveragesuccessrates haveoverlappingerrorbars,whichsuggests that allmethodsper for mcomparably. bfloat 16 int 8 int 4 Category Task #Trials #Successes #Successes #Successes Visualgen Put Eggplantinto Pot(Easy Version) 10 10 10 10 Visualgen Put Eggplantinto Pot 10 9 10 10 Visualgen Put Cupfrom Counterinto Sink 10 5 5 3 Motiongen Lift Eggplant 10 8 7 7.5 Physicalgen Put Carroton Plate 10 10 10 10 Physicalgen Lift AAABattery 10 3 6 4 Semanticgen Take Purple",
      "start_pos": 11088,
      "end_pos": 11600
    },
    {
      "chunk_id": 25,
      "paper_id": "openvla",
      "text": "Put Eggplantinto Pot(Easy Version) 10 10 10 10 Visualgen Put Eggplantinto Pot 10 9 10 10 Visualgen Put Cupfrom Counterinto Sink 10 5 5 3 Motiongen Lift Eggplant 10 8 7 7.5 Physicalgen Put Carroton Plate 10 10 10 10 Physicalgen Lift AAABattery 10 3 6 4 Semanticgen Take Purple Grapesoutof Pot 10 2 2 2 Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 9 9.5 8.5 Mean Success Rate 70.0\u00b15.1% 74.4\u00b14.9% 68.8\u00b15.2% 35 E LIBEROSimulation Experiments Ourpreviousdiscussionsin Section 5.2 and Section 5.3 focusedonadapting Open VLAtonovel real-worldrobotsetups and tasks.Thissectionexploresadapting Open VLAtosimulatedrobotsetups andtasks,specificallyutilizing the LIBERObenchmark[116]. Ourexperimentationinsimulation offerstwokeyadvantages: 1. Demonstration of versatility: We show that Open VLA, despite having been pretrained exclusivelyonreal-worldrobot data,caneffectivelyadapttosimulateddomains,overcoming potentialdisparitiesbetweenreal-worldandsimulatedenvironments and dynamics. 2. Enhancedaccessibility and reproducibility:Integrationof Open VLAintoapubliclyavailable simulationplat for mmakes our model moreaccessibletoo the rresearchers,especiallythose whomaynot have accesstorobotichardw are. Additionally,simulatedexperiments are more easilyreproducedthan the irreal-worldcounterparts. Wediscuss the experimentalsetupin Appendix E.1 and the resultsin Appendix E.2. Werelease the materialsrequiredtoreproduce the experimentsalong with the Open VLAcode base. E.1 LIBEROSimulation Experimental Setup Simulationsetup and tasks. The LIBERObenchmark[116]consistsoff our task suitesdesigned forstudyinglifelonglearninginroboticmanipulation,andtheoriginalpaper the reforeinvestigates both for ward and backwardtransfertoavarietyoftasks. Inourexperiments,wefocussolelyon supervisedfine-tuningon the target task suite,measuring the per for manceofvariouspoliciestrained viabehavioralcloningonsuccessfuldemonstrationsof the tasks. Weper for mexperiments with the followingf our task suites,whicheachcontain 10 tasks with 50 human-teleoperateddemonstrationseach: \u2022 LIBERO-Spatial consists of the same set of objects but different layouts, and tests the model\u2019sunderst and ingofspatialrelationships. \u2022 LIBERO-Object consists of the same scene layouts but different objects, and tests the model\u2019sunderst and ingofobjecttypes. \u2022 LIBERO-Goalconsistsof the sameobjects and layoutsbutdifferent task goals,andtests themodel\u2019sknowledgeofdifferent task-orientedbehaviors. \u2022 LIBERO-Long (also called LIBERO-10) consists of long-horizon tasks with diverse objects,layouts,andtasks. Wemakethefollowingmodificationstoeachof the training data setsabove: 1. To accommodate methods requiring higher-resolution images (such as 256\u00d7256 px or 224\u00d7224 px),weregeneratealldemonstrationsatanincreasedresolutionof 256\u00d7256 px. Originally, the data setprovidedby the benchmarkconsistsof 128\u00d7128 pximages. We find that simply upscaling these images to 256 \u00d7 256 px results in poor image quality. Therefore,wechoosetobegin with higher-resolutionimages,which can bedownscaled asnecessary,ensuringhigherimagequalityacrossvariousresolutionrequirements. These higher-resolutionimages were obtainedbysteppingthrough the simulationenvironments with the actions stored in the provided human-collected demonstrations and saving the imagesrenderedby the simulator. 2. Wefilteroutall\u201cno-op\u201dactions from the dataset,i.e.,actions that havenear-zeromagnitude inthetranslationandrotationcomponents and donotchangethestateof the robot\u2019sgripper. Wefind that thissimple data cleaningstepiscrucial for highlyexpressivesingle-steppolicies suchas Open VLA,whichotherwiselearntoimitate the seno-opactions and consequently freezeindefinitelyatcertainstatesduringevaluation. 3. Werotateallthird-personimagesatbothtrain and testtimeby 180 degreesbecausewe observe that the LIBEROenvironmentsreturnimages that are upsidedownon our hardw are. 36 4. Sincewetrainpoliciesviaimitationlearning,whichexpectsdemonstrationstobesuccessful, wereplayalldemonstrationsin the correspondingsimulationenvironments and filterout the demonstrations that failtocomplete the task(asdeterminedby the environments\u2019success criteria). As a result, we remove 68 of 500 LIBERO-Spatial demonstrations, 46 of 500 LIBERO-Objectdemonstrations,72 of 500 LIBERO-Goaldemonstrations,and 121 of 500 LIBERO-Longdemonstratinos. 5. Forallmethodsin our comparisons,weonlyutilize the staticthird-personcameraimages; wedonotusethewristcameraimages that are additionallyprovidedin the original data sets. Thisis for sakeofhavingfaircomparisons, as Open VLA\u2019svisualinputsonlyconsistof third-personcameraimages. Comparisons. Themethods that wecomp are include Diffusion Policy 8 [3]trained from scratch, Octo[5]fine-tunedon the target data set,and Open VLAfine-tunedon the target data setvia Lo RA (r =32)asdescribedin Section 5.3. Eachpolicyistrainedindependentlyoneachof the tasksuites above(ratherthantrainingasinglepolicyonallf our suitescombined). Allpolicies are trained with thesamesetofdemonstrations,soallmethodsbenefit from the datacleaningstepsdescribedabove. Evaluationdetails. Toensurelowervariancein the experimentalresults,allmethods are evaluated across 500 trials for each task suite,andthereportedper for manceis the averagesuccessrateover threer and omseeds(resultingin 1500 totaltrialsperstatistic). Althoughwemodify the training datasets,asdescribedearlier,wedonotchangethetestenvironmentsbutratheruse the sameinitial environmentconfigurationsprovidedby the original LIBERObenchmark. E.2 LIBEROSimulation Experimental",
      "start_pos": 11550,
      "end_pos": 12062
    },
    {
      "chunk_id": 26,
      "paper_id": "openvla",
      "text": "5.3. Eachpolicyistrainedindependentlyoneachof the tasksuites above(ratherthantrainingasinglepolicyonallf our suitescombined). Allpolicies are trained with thesamesetofdemonstrations,soallmethodsbenefit from the datacleaningstepsdescribedabove. Evaluationdetails. Toensurelowervariancein the experimentalresults,allmethods are evaluated across 500 trials for each task suite,andthereportedper for manceis the averagesuccessrateover threer and omseeds(resultingin 1500 totaltrialsperstatistic). Althoughwemodify the training datasets,asdescribedearlier,wedonotchangethetestenvironmentsbutratheruse the sameinitial environmentconfigurationsprovidedby the original LIBERObenchmark. E.2 LIBEROSimulation Experimental Results Wepresent the LIBEROexperimentalresultsin Table 12. Importantly,weobserve that Open VLAcan beeffectivelyadaptedto task sin the LIBEROsimulationenvironments,asitobtainshighestaverage successrate and rankamong the testedmethods. However,wefind that the overallmarginbetween Open VLA and the other methods are tighter here than in the real-world fine-tuning experiments discussed in Section 5.2. We attribute this to the fact that Open VLA was pretrained with purely real-worldrobot data and nosimulation data,whichsuggests that fine-tuning the modelonsimulated robot task smaynotbeaseffectiveasfine-tuningitonreal-world task sdueto the domaingapbetween simulated and real-worldenvironments and dynamics. Weseeevidence for thisnotionin the results obtained by Octo \u2013 another policy pretrained on large amounts of real-world robot data \u2013 which alsoonlyachievesasmallboostinoverallper for mancerelativetoasimple,strong base linesuchas Diffusion Policytrained from scratch. Weexpectincreasedgainsinper for mance for the pretrained and fine-tunedmethodsifsimulation data isaddedto the pretraining data mixture. Table 12:LIBEROsimulationbenchmarkresults.Wereport the successrate(SR)andst and arderrorofeach method for the four task suitesin the LIBERObenchmark,averagedoverthreer and omseeds with 500 trials each.Inaddition,weshow the rankingofeachmethod with ineach task suite,wherearankof 1 indicates the strongestmethodin the suite and arankof 3 indicates the weakestmethod.(Theaveragerankingisimportantto notesinceitinformswhichmethodmaybemostsuitabletouseasadefault for avarietyoftasks;itismore informativethan the averagesuccessrate,whichisnotnormalizedbyindividual task suitedifficulty.)Overall, wefind that fine-tuned Open VLAachieveshighestaveragesuccessrate and rank,followedbyfine-tuned Octo andthen Diffusion Policytrained from scratch. LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average SR(\u2191) Rank(\u2193) SR(\u2191) Rank(\u2193) SR(\u2191) Rank(\u2193) SR(\u2191) Rank(\u2193) SR(\u2191) Rank(\u2193) Diffusion Policy from scratch 78.3\u00b11.1% 3 92.5\u00b10.7% 1 68.3\u00b11.2% 3 50.5\u00b11.3% 3 72.4\u00b10.7% 2.5 Octofine-tuned 78.9\u00b11.0% 2 85.7\u00b10.9% 3 84.6\u00b10.9% 1 51.1\u00b11.3% 2 75.1\u00b10.6% 2 Open VLAfine-tuned(ours) 84.7\u00b10.9% 1 88.4\u00b10.8% 2 79.2\u00b11.0% 2 53.7\u00b11.3% 1 76.5\u00b10.6% 1.5 8 we use the implementationof Diffusion Policy that isdescribedin the DROID data setpaper[11],which conditionsactiongenerationon Distil BERT[117]languageembeddingsof the tasklabel. 37",
      "start_pos": 12012,
      "end_pos": 12313
    },
    {
      "chunk_id": 27,
      "paper_id": "openxembodiment",
      "text": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models Open X-Embodiment Collaboration 0 robotics-trans for mer-x.github.io Abby O\u2019Neill 34,Abdul Rehman 37,Abhinav Gupta 4,Abhiram Maddukuri 45,Abhishek Gupta 46,Abhishek Padalkar 10,Abraham Lee 34, Acorn Pooley 11,Agrim Gupta 28,Ajay Mandlekar 22,Ajinkya Jain 15,Albert Tung 28,Alex Bewley 11,Alex Herzog 11,Alex Irpan 11, Alexander Khazatsky 28,Anant Rai 23,Anchit Gupta 19,Andrew Wang 34,Andrey Kolobov 20,Anikait Singh 11,34,Animesh Garg 9, Aniruddha Kembhavi 1,Annie Xie 28,Anthony Brohan 11,Antonin Raffin 10,Archit Sharma 28,Arefeh Yavary 35,Arhan Jain 46,Ashwin Balakrishna 32, Ayzaan Wahid 11,Ben Burgess-Limerick 25,Beomjoon Kim 17,Bernhard Scho\u00a8lkopf 18,Blake Wulfe 32,Brian Ichter 11,Cewu Lu 27,8,Charles Xu 34, Charlotte Le 34,Chelsea Finn 11,28,Chen Wang 28,Chenfeng Xu 34,Cheng Chi 5,28,Chenguang Huang 38,Christine Chan 11, Christopher Agia 28,Chuer Pan 28,Chuyuan Fu 11,Coline Devin 11,Danfei Xu 9,Daniel Morton 28,Danny Driess 11,Daphne Chen 46,Deepak Pathak 4, Dhruv Shah 34,Dieter Bu\u00a8chler 18,Dinesh Jayaraman 42,Dmitry Kalashnikov 11,Dorsa Sadigh 11,Edward Johns 14,Ethan Foster 28, Fangchen Liu 34,Federico Ceola 16,Fei Xia 11,Feiyu Zhao 13,Felipe Vieira Frujeri 20,Freek Stulp 10,Gaoyue Zhou 23,Gaurav S.Sukhatme 43, Gautam Salhotra 43,15,Ge Yan 36,Gilbert Feng 34,Giulio Schiavi 7,Glen Berseth 41,21,Gregory Kahn 34,Guangwen Yang 33, Guanzhi Wang 3,22,Hao Su 36,Hao-Shu Fang 27,Haochen Shi 28,Henghui Bao 43,Heni Ben Amor 2,Henrik IChristensen 36,Hiroki Furuta 31, Homanga Bharadhwaj 4,19,Homer Walke 34,Hongjie Fang 27,Huy Ha 5,28,Igor Mordatch 11,Ilija Radosavovic 34,Isabel Leal 11, Jacky Liang 11,Jad Abou-Chakra 25,Jaehyung Kim 17,Jaimyn Drake 34,Jan Peters 29,Jan Schneider 18,Jasmine Hsu 11,Jay Vakil 19, Jeannette Bohg 28,Jeffrey Bingham 11,Jeffrey Wu 34,Jensen Gao 28,Jiaheng Hu 30,Jiajun Wu 28,Jialin Wu 12,Jiankai Sun 28,Jianlan Luo 34, Jiayuan Gu 36,Jie Tan 11,Jihoon Oh 31,Jimmy Wu 24,Jingpei Lu 36,Jingyun Yang 28,Jitendra Malik 34,Joa\u02dco Silve\u00b4rio 10,Joey Hejna 28, Jonathan Booher 28,Jonathan Tompson 11,Jonathan Yang 28,Jordi Salvador 1,Joseph J.Lim 17,Junhyek Han 17,Kaiyuan Wang 36, Kanishka Rao 11,Karl Pertsch 34,28,Karol Hausman 11,Keegan Go 15,Keerthana Gopalakrishnan 11,Ken Goldberg 34,Kendra Byrne 11, Kenneth Oslund 11,Kento Kawaharazuka 31,Kevin Black 34,Kevin Lin 28,Kevin Zhang 4,Kiana Ehsani 1,Kiran Lekkala 43,Kirsty Ellis 41, Krishan Rana 25,Krishnan Srinivasan 28,Kuan Fang 34,Kunal Pratap Singh 6,Kuo-Hao Zeng 1,Kyle Hatch 32,Kyle Hsu 28,Laurent Itti 43, Lawrence Yunliang Chen 34,Lerrel Pinto 23,Li Fei-Fei 28,Liam Tan 34,Linxi\u201dJim\u201dFan 22,Lionel Ott 7,Lisa Lee 11,Luca Weihs 1, Magnum Chen 13,Marion Lepert 28,Marius Memmel 46,Masayoshi Tomizuka 34,Masha Itkina 32,Mateo Guaman Castro 46,Max Spero 28,Maximilian Du 28, Michael Ahn 11,Michael C.Yip 36,Mingtong Zhang 39,Mingyu Ding 34,Minho Heo 17,Mohan Kumar Srirama 4,Mohit Sharma 4, Moo Jin Kim 28,Muhammad Zubair Irshad 32,Naoaki Kanazawa 31,Nicklas Hansen 36,Nicolas Heess 11,Nikhil JJoshi 11,Niko Suenderhauf 25, Ning Liu 13,Norman Di Palo 14,Nur Muhammad Mahi Shafiullah 23,Oier Mees 38,Oliver Kroemer 4,Osbert Bastani 42,Pannag RSanketi 11, Patrick\u201dTree\u201dMiller 32,Patrick Yin 46,Paul Wohlhart 11,Peng Xu 11,Peter David Fagan 37,Peter Mitrano 40,Pierre Sermanet 11,Pieter Abbeel 34, Priya Sund are san 28,Qiuyu Chen 46,Quan Vuong 11,Rafael Rafailov 11,28,Ran Tian 34,Ria Doshi 34,Roberto Mart\u2019in-Mart\u2019in 30, Rohan Baijal 46,Rosario Scalise 46,Rose Hendrix 1,Roy Lin 34,Runjia Qian 13,Ruohan Zhang 28,Russell Mendonca 4,Rutav Shah 30, Ryan Hoque 34,Ryan Julian 11,Samuel Bustamante 10,Sean Kirmani 11,Sergey Levine 11,34,Shan Lin 36,Sherry Moore 11,Shikhar Bahl 4, Shivin Dass 43,30,Shubham Sonawani 2,Shubham Tulsiani 4,Shuran Song 5,Sichun Xu 11,Siddhant Haldar 23,Siddharth Karamcheti 28, Simeon Adebola 34,Simon Guist 18,Soroush Nasiriany 30,Stefan Schaal 15,Stefan Welker 11,Stephen Tian",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 28,
      "paper_id": "openxembodiment",
      "text": "Zhang 28,Russell Mendonca 4,Rutav Shah 30, Ryan Hoque 34,Ryan Julian 11,Samuel Bustamante 10,Sean Kirmani 11,Sergey Levine 11,34,Shan Lin 36,Sherry Moore 11,Shikhar Bahl 4, Shivin Dass 43,30,Shubham Sonawani 2,Shubham Tulsiani 4,Shuran Song 5,Sichun Xu 11,Siddhant Haldar 23,Siddharth Karamcheti 28, Simeon Adebola 34,Simon Guist 18,Soroush Nasiriany 30,Stefan Schaal 15,Stefan Welker 11,Stephen Tian 28,Subramanian Ramamoorthy 37, Sudeep Dasari 4,Suneel Belkhale 28,Sungjae Park 17,Suraj Nair 32,Suvir Mirch and ani 28,Takayuki Osa 31,Tanmay Gupta 1,Tatsuya Harada 31,26, Tatsuya Matsushima 31,Ted Xiao 11,Thomas Kollar 32,Tianhe Yu 11,Tianli Ding 11,Todor Davchev 11,Tony Z.Zhao 28, Travis Armstrong 11,Trevor Darrell 34,Trinity Chung 34,Vidhi Jain 11,4,Vikash Kumar 4,Vincent Vanhoucke 11,Vitor Guizilini 32,Wei Zhan 34, Wenxuan Zhou 11,4,Wolfram Burgard 44,Xi Chen 11,Xiangyu Chen 13,Xiaolong Wang 36,Xinghao Zhu 34,Xinyang Geng 34,Xiyuan Liu 13, Xu Liangwei 13,Xuanlin Li 36,Yansong Pang 11,Yao Lu 11,Yecheng Jason Ma 42,Yejin Kim 1,Yevgen Chebotar 11,Yifan Zhou 2, Yifeng Zhu 30,Yilin Wu 4,Ying Xu 11,Yixuan Wang 39,Yonatan Bisk 4,Yongqiang Dou 33,Yoonyoung Cho 17,Youngwoon Lee 34,Yuchen Cui 28, Yue Cao 13,Yueh-Hua Wu 36,Yujin Tang 11,31,Yuke Zhu 30,Yunchu Zhang 46,Yunfan Jiang 28,Yunshuang Li 42,Yunzhu Li 39, Yusuke Iwasawa 31,Yutaka Matsuo 31,Zehan Ma 34,Zhuo Xu 11,Zichen Jeff Cui 23,Zichen Zhang 1,Zipeng Fu 28,Zipeng Lin 34 Fig. 1: We propose an open, large-scale dataset for robot learning curated from 21 institutions across the globe. The dataset represents diverse behaviors, robot embodiments and environments, and enables learning generalized robotic policies. Abstract\u2014Large, high-capacity models trained on diverse for many applications. Can such a consolidation happen in datasets have shownremarkablesuccessesonefficientlytackling robotics? Conventionally, robotic learning methods train a downstream applications. In domains from NLP to Computer separate model for every application, every robot, and even Vision, this has led to a consolidation of pretrained models, every environment. Can we instead train \u201cgeneralist\u201d X-robot with general pretrained backbones serving as a starting point policy that can be adapted efficiently to new robots, tasks, 5202 ya M 41 ]OR.sc[ 9 v 46880.0132:vi Xra and environments? In this paper, we provide datasets in in environments and robots. Learning generalizable robot standardized data formats and models to make it possible to policies requires developing methods that can utilize X- explore this possibility in the context of robotic manipulation, embodiment data, tapping into datasets from many labs, alongside experimental results that provide an example of robots, and settings. Even if such datasets in their current effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 size and coverage are insufficient to attain the impressive institutions, demonstrating 527 skills (160266 tasks). We show generalization results that have been demonstrated by large that a high-capacity model trained on this data, which we call language models, in the future, the union of such data can RT-X,exhibitspositivetransfer and improves the capabilitiesof potentiallyprovide this kindofcoverage.Becauseof this,we multiplerobotsbyleveragingexperiencefromo the rplatforms. believe that enablingresearchinto X-embodimentrobotic The project website is robotics-trans for mer-x.github.io. learning is critical at the present juncture. I. INTRODUCTION Following this rationale, we have two goals: (1) Evaluate A central lesson from advances in machine learning and whether policies trained on data from many different robots artificial intelligence is that large-scale learning",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 29,
      "paper_id": "openxembodiment",
      "text": "X-embodimentrobotic The project website is robotics-trans for mer-x.github.io. learning is critical at the present juncture. I. INTRODUCTION Following this rationale, we have two goals: (1) Evaluate A central lesson from advances in machine learning and whether policies trained on data from many different robots artificial intelligence is that large-scale learning from di- and environments enjoy the benefits of positive transfer, verse datasets can enable capable AI systems by providing attaining better per for mance than policies trained only on for general-purpose pretrained models. In fact, large-scale data from each evaluation setup. (2) Organize large robotic general-purpose models typically trained on large and di- datasetstoenablefutureresearchon X-embodimentmodels. verse datasets can often outperform their narrowly targeted We focus our work on robotic manipulation. Addressing counterparts trained on smaller but more task-specific data. goal (1), our empirical contribution is to demonstrate that For instance, open-vocab classifiers (e.g., CLIP [1]) trained several recent robotic learning methods, with minimal mod- on large datasets scraped from the web tend to outperform ification, can utilize X-embodiment data and enable positive fixed-vocabulary models trained on more limited datasets, transfer. Specifically, we train the RT-1 [8] and RT-2 [9] and large language models [2, 3] trained on massive text models on 9 different robotic manipulators. We show that corpora tend to outperform systems that are only trained on the resulting models, which we call RT-X, can improve over narrow task-specific data sets.Increasingly,themosteffective policies trained only on data from the evaluation domain, way to tackle a given narrow task (e.g., in vision or NLP) exhibiting better generalization and new capabilities. Ad- is to adapt a general-purpose model. However, these lessons dressing (2), we provide the Open X-Embodiment (OXE) are difficult to apply in robotics: any single robotic domain Repository,whichincludesa data set with 22 differentrobotic mightbetoonarrow,andwhilecomputervision and NLPcan embodiments from 21 different institutions that can enable leverage large datasets sourced from the web, comparably the robotics community to pursue further research on X- large and broad datasets for robotic interaction are hard to embodiment models, along with open-source tools to facili- come by. Even the largest data collection efforts still end tatesuchresearch.Ouraimisnottoinnovateintermsof the up with datasets that are a fraction of the size and diversity particular architectures and algorithms, but rather to provide of benchmark datasets in vision (5-18 M) [4, 5] and NLP the model that we trained together with data and tools to (1.5 B-4.5 B)[6,7].Moreimportantly,such data sets are often energize research around X-embodiment robotic learning. still narrow along some axes of variation, either focusing on II. RELATEDWORK a single environment, a single set of objects, or a narrow Transfer across embodiments. A number of prior works range of tasks. How can we overcome these challenges in have studied methods for transfer across robot embodiments robotics and move the field of robotic learning toward large in simulation [10\u201322] and on real robots [23\u201329]. These data regime that has been so successful in other domains? methodsoftenintroducemechanismsspecificallydesignedto Inspired by the generalization made possible by pretrain- address the embodiment gap between different robots, such ing large vision or language models on diverse data, we as",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 30,
      "paper_id": "openxembodiment",
      "text": "learning toward large in simulation [10\u201322] and on real robots [23\u201329]. These data regime that has been so successful in other domains? methodsoftenintroducemechanismsspecificallydesignedto Inspired by the generalization made possible by pretrain- address the embodiment gap between different robots, such ing large vision or language models on diverse data, we as shared action representations [14, 30], incorporating rep- take the perspective that the goal of training generalizable resentation learning objectives [17, 26], adapting the learned robot policies requires X-embodiment training, i.e., with policy on embodiment information [11, 15, 18, 30, 31], and data from multiple robotic platforms. While each individual decouplingrobot and environmentrepresentations[24].Prior robotic learning dataset might be too narrow, the union of work has provided initial demonstrations of X-embodiment all such datasets provides a better coverage of variations training [27] and transfer [25, 29, 32] with trans for mer 01 Allen Institute for AI;2 Arizona State University;3 Cali for nia Instituteof Tech- models. We investigate complementary architectures and nology;4 Carnegie Mellon University;5 Columbia University;6 EPFL;7 ETHZu\u00a8rich; providecomplementaryanalyses,and,inparticular,study the 8 Flexiv Robotics; 9 Georgia Institute of Technology; 10 German Aerospace Center; 11 Google Deep Mind;12 Google Research;13 IO-AITECH;14 Imperial College Lon- interaction between X-embodiment transfer and web-scale don;15 Intrinsic LLC;16 Istituto Italianodi Tecnologia;17 Korea Advanced Institute pretraining. Similarly, methods for transfer across human of Science & Technology; 18 Max Planck Institute; 19 Meta AI; 20 Microsoft Re- search;21 Mila Quebec;22 NVIDIA;23 New York University;24 Princeton University; and robot embodiments also often employ techniques for 25 Queensl and Universityof Technology;26 RIKEN;27 Shanghai Jiao Tong University; reducing the embodiment gap, i.e. by translating between 28 Stanford University;29 Technische Universita\u00a8t Darmstadt;30 The Universityof Texas at Austin; 31 The University of Tokyo; 32 Toyota Research Institute; 33 Tsinghua domainsorlearningtransferablerepresentations[33\u201343].Al- University; 34 University of Cali for nia, Berkeley; 35 University of Cali for nia, Davis; ternatively, some works focus on sub-aspects of the problem 36 University of Cali for nia, San Diego; 37 University of Edinburgh; 38 University of Freiburg; 39 University of Illinois Urbana-Champaign; 40 University of Michigan; such as learning transferable reward functions [17, 44\u201348], 41 University of Montreal; 42 University of Pennsylvania; 43 University of Southern goals [49, 50], dynamics models [51], or visual representa- Cali for nia;44 Universityof Technology,Nuremberg;45 Universityof Texasat Austin; 46 Universityof Washington tions [52\u201359] from human video data. Unlike most of these Google Franka Robot Google x Arm Franka Robot Kuka iiwa x Arm Franka x Ar m S G a o w o y g e l r e Rob K o u t ka ii wa UR 5 Wi H do e w llo X Stretch P D R L 2 R SARA Jac U o n 2 x it A re r m e A B 1 i manua C l obo D t L ta R EDAN PA K M in Y o 2 va G F e a n n 3 uc Mate Jackal RC T C u a r r tle Bot 2 Baxter Widow X Jackal Hel K lo S in a S o w t v r y a e e t G",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 31,
      "paper_id": "openxembodiment",
      "text": "PA K M in Y o 2 va G F e a n n 3 uc Mate Jackal RC T C u a r r tle Bot 2 Baxter Widow X Jackal Hel K lo S in a S o w t v r y a e e t G c r h en 3 Widow X Sawyer (a) # Datasets per Robot Embodiment (b) # Scenes per Embodiment (c) # Trajectories per Embodiment Shapes Containers Food Furniture Appliances Utensils picking moving pushing placing sliding puttin n g avigat s in e g parating pointing opening nudging closin i g nsertin k g nockin d g ragging dropping wip a i s n s g e mbli t n u g rning on keeping hexag t o r n iangle heart cube tray bo wl pot box cu b p asket count d er ra wer tab c l a e binet door chair app o le rang b e an c a o n k a e c c a h n ip bag fauce f t ridge sink m st i o c v ro e wave oven for s k poon kni s f p e atula (d) Common dataset Skills (e) Common dataset Objects Fig.2:The Open X-Embodiment dataset.(a):the data setconsistsof 60 individual data setsacross 22 embodiments.(b):the Frankarobot has the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): x Arm and Google Robot contribute the most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects. prior works, we directly train a policy on X-embodiment \u2022 Open X-Embodiment dataset: robot learning dataset data,withoutanymechanismstoreduce the embodimentgap, with 1 M+ robot trajectories from 22 robot embodi- and observe positive transfer by leveraging that data. ments. Large-scale robot learning datasets. The robot learning \u2022 Pre-Trained Checkpoints: a selection of RT-X model community has created open-source robot learning datasets, checkpoints ready for inference and fine tuning. spanninggrasping[60\u201371],pushinginteractions[23,72\u201374], We intend for these res our ces to form a foundation for X- setsofobjects and models[75\u201385],andteleoperateddemon- embodiment research in robot learning, but they are just strations [8, 86\u201395]. With the exception of Robo Net [23], thestart.Open X-Embodimentisacommunity-driveneffort, these datasets contain data of robots of the same type, currently involving 21 institutions from around the world, whereas we focus on data spanning multiple embodiments. and we hope to further broaden participation and grow the The goal of our data repository is complementary to these initial Open X-Embodiment dataset over time. In this sec- efforts: we process and aggregate a large number of prior tion, we summarize the dataset and X-embodiment learning datasets into a single, standardized repository, called Open framework, before discussing the specific models we use to X-Embodiment, which shows how robot learning datasets evaluate our dataset and our experimental results. can be shared in a meaningul and useful way. A. The Open X-Embodiment dataset Language-conditioned robot learning. Prior work has The Open X-Embodiment Datasetcontains 1 M+realrobot aimed to endow robots and other agents with the ability",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 32,
      "paper_id": "openxembodiment",
      "text": "X-Embodiment, which shows how robot learning datasets evaluate our dataset and our experimental results. can be shared in a meaningul and useful way. A. The Open X-Embodiment dataset Language-conditioned robot learning. Prior work has The Open X-Embodiment Datasetcontains 1 M+realrobot aimed to endow robots and other agents with the ability to trajectories spanning 22 robot embodiments, from single underst and and follow language instructions [96\u2013101], often robot arms to bi-manual robots and quadrupeds. The dataset by learning language-conditioned policies [8, 40, 45, 102\u2013 was constructed by pooling 60 existing robot datasets from 106]. We train language-conditioned policies via imitation 34 robotic research labs around the world and converting learning like many of these prior works but do so using them into a consistent data format for easy download and large-scalemulti-embodimentdemonstration data.Following usage.we usethe RLDS data for mat[119],whichsaves data previous works that leverage pre-trained language embed- inserializedtfrecordfiles and accommodates the various dings [8, 40, 45, 103, 107\u2013112] and pre-trained vision- action spaces and input modalities of different robot setups, language models [9, 113\u2013115] in robotic imitation learning, such as differing numbers of RGB cameras, depth cameras we study both forms of pre-training in our experiments, and point clouds. It also supports efficient, parallelized data specifically following the recipes of RT-1 [8] and RT-2 [9]. loading in all major deep learning frameworks. For more III. THEOPENX-EMBODIMENTREPOSITORY details about the data storage format and a breakdown of all 60 datasets, see robotics-trans for mer-x.github.io. We introduce the Open X-Embodiment Repository (robotics-trans for mer-x.github.io) \u2013 an open-source reposi- B. dataset Analysis torywhichincludeslarge-scale data along with pre-trained Fig.2 analyzes the Open X-Embodiment dataset.Fig.2(a) model checkpoints for X-embodiedrobotlearningresearch. shows the breakdown of datasets by robot embodiments, More specifically, we provide and maintain the following with the Franka robot being the most common. This is open-source res our ces to the broader community: reflected in the number of distinct scenes (based on dataset Pick apple from top drawer and place on counter Fi LM Efficient Net Trans for mer Discrete Action \u03b2 )\u03b3+1( 10 Hz Route cable DDiissccrreettee Closed Gripper Instruction AAccttiioonn Velocity \u00b7 RT-1-X Z-Rot. Velocity Images + 3 Hz Gripper Position Delta Rotation Delta Discrete Instruction Action 5 Hz Pick up the orange fruit Image RT-2-X Gripper Vi T LLM De-Tokenizer Position Delta No Rotation Fig. 3: RT-1-X and RT-2-X both take images and a text instruction as input and output discretized end-effector actions. RT-1-X is an architecture designed for robotics, with a Fi LM [116] conditioned Efficient Net [117] and a Trans for mer [118]. RT-2-X builds on a VLM backbone by representing actions as another language, and training action text tokens together with vision-language data. meta data) per embodiment (Fig. 2(b)), where Franka dom- B. Policy architectures inates. Fig. 2(c) shows the breakdown of trajectories per We consider two model architectures in our experiments: embodiment. To further analyze the diversity, we use the (1) RT-1 [8], an efficient Trans for mer-based architecture language annotations present in our data. We use the Pa LM designed for roboticcontrol,and(2)RT-2[9]alargevision- language model [3] to extract objects and behaviors from",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 33,
      "paper_id": "openxembodiment",
      "text": "per We consider two model architectures in our experiments: embodiment. To further analyze the diversity, we use the (1) RT-1 [8], an efficient Trans for mer-based architecture language annotations present in our data. We use the Pa LM designed for roboticcontrol,and(2)RT-2[9]alargevision- language model [3] to extract objects and behaviors from language model co-fine-tuned to output robot actions as the instructions. Fig. 2(d,e) show the diversity of skills and natural language tokens. Both models take in a visual input objects. While most skills belong to the pick-place family, and natural language instruction describing the task, and the long tail of the dataset contains skills like \u201cwiping\u201d or output a tokenized action. For each model, the action is \u201cassembling\u201d.Additionally,the data coversarangeofhouse- tokenized into 256 bins uni for mly distributed along each of hold objects, from appliances to food items and utensils. eightdimensions;onedimension for terminating the episode IV. RT-XDESIGN and seven dimensions for end-effector movement. Although both architectures are described in detail in their original To evaluate how much X-embodiment training can im- papers [8, 9], we provide a short summary of each below: prove the per for mance of learned policies on individual RT-1 [8] is a 35 M parameter network built on a Trans- robots, we require models that have sufficient capacity to former architecture [118] and designed for robotic control, productively make use of such large and heterogeneous as shown in Fig. 3. It takes in a history of 15 images datasets. To that end, our experiments will build on two along with the natural language. Each image is processed recently proposed Trans for mer-based robotic policies: RT- through an Image Net-pretrained Efficient Net [117] and the 1[8]and RT-2[9].Webrieflysummarize the designofthese naturallanguageinstructionistrans for medintoa USE[120] models in this section, and discuss how we adapted them to embedding.Thevisual and languagerepresentations are then the X-embodiment setting in our experiments. interwoven via Fi LM [116] layers, producing 81 vision- A. Data format consolidation language tokens. These tokens are fed into a decoder-only One challenge of creating X-embodiment models is that Trans for mer, which outputs the tokenized actions. observation and action spaces vary signifi can tly across RT-2 [9] is a family of large vision-language-action robots. We use a coarsely aligned action and observation models(VLAs)trainedon Internet-scalevision and language space across datasets. The model receives a history of dataalong with roboticcontrol data.RT-2 casts the tokenized recent images and language instructions as observations and actions to text tokens, e.g., a possible action may be \u201c1 128 predicts a 7-dimensional action vector controlling the end- 91 241 5 101 127\u201d. As such, any pretrained vision-language effector (x, y, z, roll, pitch, yaw, and gripper opening or the model(VLM[121\u2013123])can befinetuned for roboticcontrol, rates of these quantities). We select one canonical camera thusleveraging the backboneof VLMs and transferringsome view from each data setas the inputimage,resizeittoacom- of their generalization properties. In this work, we focus on mon resolution and convert the original action set into a 7 the RT-2-Pa LI-Xvariant[121]builtonabackboneofavisual Do Fend-effectoraction.Wenormalizeeach data set\u2019sactions model, Vi T [124], and a language model, UL 2 [125], and prior to discretization. This way,",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 34,
      "paper_id": "openxembodiment",
      "text": "each data setas the inputimage,resizeittoacom- of their generalization properties. In this work, we focus on mon resolution and convert the original action set into a 7 the RT-2-Pa LI-Xvariant[121]builtonabackboneofavisual Do Fend-effectoraction.Wenormalizeeach data set\u2019sactions model, Vi T [124], and a language model, UL 2 [125], and prior to discretization. This way, an output of the model can pretrained primarily on the Web LI [121] dataset. be interpreted (de-normalized) differently depending on the embodimentused. Itshould benoted thatdespite this coarse C. Training and inference details alignment, the camera observations still vary substantially Both models use a standard categorical cross-entropy across datasets, e.g. due to differing camera poses relative objective over their output space (discrete buckets for RT- to the robot or differing camera properties, see Figure 3. 1 and all possible language tokens for RT-2). Similarly,for the actionspace,wedonotalign the coordinate We define the robotics data mixture used across all of framesacross data setsinwhich the end-effectoriscontrolled, the experiments as the data from 9 manipulators, and taken andallowactionvaluestorepresentei the rabsoluteorrelative from RT-1 [8], QT-Opt [66], Bridge [95], Task Agnostic positions or velocities, as per the original control scheme Robot Play[126,127],Jaco Play[128],Cable Routing[129], chosen for each robot. Thus, the same action vector may Robo Turk [86], NYU VINN [130], Austin VIOLA [131], induce very different motions for different robots. Berkeley Autolab UR 5 [132], TOTO [133] and Language Fig.4:RT-1-Xmeansuccessrateis 50%higherthan that ofei the rthe Original Methodor RT-1.RT-1 and RT-1-Xhave the samenetwork architecture. Therefore the per for mance increase can be attributed to co-training on the robotics data mixture. The lab logos indicate the physical location of real robot evaluation, and the robot pictures indicate the embodiment used for the evaluation. tasks. We split our evaluation into two types: evaluation on Evaluation Setting Bridge Bridge RT-1 paper 6 skills domains that have small-scale datasets (Fig. 4), where we Evaluation Location IRIS(Stanford) RAILLab(UCB) Google Robotic Lab would expect transfer from larger datasets to signifi can tly Robot Embodiment Widow X Widow X Google Robot Original Method LCBC[95] LCBC[95] - improve per for mance, and evaluation on domains that have Original Method 13% 13% - large-scale datasets (Table I), where we expect further im- RT-1 40% 30% 92% provement to be more challenging. Note that we use the RT-1-X 27% 27% 73% RT-2-X(55 B) 50% 30% 91% samerobotics data trainingmixture(definedin Sec.IV-C)for all the evaluations presented in this section. For small-scale TABLEI:Parametercountscalingexperimenttoassess the impact dataset experiments, we use Kitchen Manipulation [128], of capacity on absorbing large-scale diverse embodiment data. For these large-scale datasets (Bridge and RT-1 paper data), RT-1-X Cable Routing [129], NYU Door Opening [130], AUTOLab underfits and performs worse than the Original Method and RT-1. UR 5 [132], and Robot Play [134]. We use the same evalua- RT-2-Xmodel with significantlymanymoreparameters can obtain tion and robot embodiment as in the respective publications. strong per for mance in these two evaluation scenarios. Forlarge-scale data setexperiments,weconsider Bridge[95] Table [91] datasets. RT-1-X is trained on only robotics and RT-1 [8] for in-distribution evaluation and use their mixture data defined above, whereas RT-2-X is trained via respective robots: Widow X and Google Robot. co-fine-tuning (similarly",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 35,
      "paper_id": "openxembodiment",
      "text": "publications. strong per for mance in these two evaluation scenarios. Forlarge-scale data setexperiments,weconsider Bridge[95] Table [91] datasets. RT-1-X is trained on only robotics and RT-1 [8] for in-distribution evaluation and use their mixture data defined above, whereas RT-2-X is trained via respective robots: Widow X and Google Robot. co-fine-tuning (similarly to the original RT-2 [9]), with an approximately one to one split of the original VLM data For each small dataset domain, we comp are the perfor- and the robotics data mixture. Note that the robotics data mance of the RT-1-X model, and for each large dataset mixture used in our experiments includes 9 embodiments we consider both the RT-1-X and RT-2-X models. For which is fewer than the entire Open X-Embodiment dataset all experiments, the models are co-trained on the full X- (22)\u2013thepracticalreason for thisdifferenceis that wehave embodiment data set.Throughout this evaluationwecomp are continued to extend the dataset over time, and at the time with two baseline models: (1) The model developed by of the experiments, the dataset above represented all of the the creators of the dataset trained only on that respective data. In the future, we plan to continue training policies on dataset. This constitutes a reasonable baseline insofar as the extended versions of the dataset as well as continue to it can be expected that the model has been optimized to growthe data settogether with the robotlearningcommunity. work well with the associated data; we refer to this baseline At inference time, each model is run at the rate required model as the Original Method model. (2) An RT-1 model for the robot (3-10 Hz), with RT-1 run locally and RT-2 trained on the dataset in isolation; this baseline allows us to hosted on a cloud service and queried over the network. assess whether the RT-X model architectures have enough capacity to represent policies for multiple different robot V. EXPERIMENTALRESULTS platforms simultaneously, and whether co-training on multi- Our experiments answer three questions about the effect embodiment data leads to higher per for mance. of X-embodiment training: (1) Can policies trained on our X-embodiment dataset effectively enable positive transfer, Small-scale dataset domains (Fig. 4). RT-1-X outper- such that co-training on data collected on multiple robots forms Original Method trained on each of the robot-specific improves per for mance on the training task? (2) Does co- datasets on 4 of the 5 datasets, with a large average im- training models on data from multiple platforms and tasks provement, demonstrating domains with limited data benefit improvegeneralizationto new,unseentasks?(3)Whatis the substantially from co-training on X-embodiment data. influenceofdifferentdesigndimensions,suchas model size, model architecture or dataset composition, on per for mance Large-scale dataset domains (Table I). In the large- and generalization capabilities of the resulting policy? To dataset setting, the RT-1-X model does not outperform answerthesequestionsweconduct the totalnumberof 3600 the RT-1 baseline trained on only the embodiment-specific evaluation trials across 6 different robots. dataset, which indicates underfitting for that model class. However, the larger RT-2-X model outperforms both the A. In-distributionper for manceacrossdifferentembodiments Original Method and RT-1 suggesting that X-robot training To assess the ability of RT-X",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 36,
      "paper_id": "openxembodiment",
      "text": "totalnumberof 3600 the RT-1 baseline trained on only the embodiment-specific evaluation trials across 6 different robots. dataset, which indicates underfitting for that model class. However, the larger RT-2-X model outperforms both the A. In-distributionper for manceacrossdifferentembodiments Original Method and RT-1 suggesting that X-robot training To assess the ability of RT-X models to learn from X- can improve per for mance in the data-rich domains, but only embodiment data,weevaluateper for manceonin-distribution when utilizing a sufficiently high-capacity architecture. Row Model Size History Length dataset Co-Trainedw/Web Initial Checkpoint Emergent Skills Evaluation RT-2 Generalization Evaluation (1) RT-2 55 B none Google Robotaction Yes Web-pretrained 27.3% 62% (2) RT-2-X 55 B none Robotics data Yes Web-pretrained 75.8% 61% (3) RT-2-X 55 B none Robotics data except Bridge Yes Web-pretrained 42.8% 54% (4) RT-2-X 5 B 2 Robotics data Yes Web-pretrained 44.4% 52% (5) RT-2-X 5 B none Robotics data Yes Web-pretrained 14.5% 30% (6) RT-2-X 5 B 2 Robotics data No Fromscratch 0% 1% (7) RT-2-X 5 B 2 Robotics data No Web-pretrained 48.7% 47% TABLE II: Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and emergent skills (skills from other datasets on the Google Robot), showing the importance of Web-pretraining, model size, and history. B. Improved generalization to out-of-distribution settings We now examine how X-embodiment training can enable better generalization to out-of-distribution settings and more complex and novel instructions. These experiments focus on the high-data domains, and use the RT-2-X model. Unseen objects, backgrounds and environments. We firstconduct the sameevaluationofgeneralizationproperties as proposed in [9], testing for the ability to manipulate unseen objects in unseen environments and against unseen backgrounds.Wefind that RT-2 and RT-2-Xper for mroughly on par (Table II, rows (1) and (2), last column). This is not unexpected, since RT-2 already generalizes well (see [9]) along these dimensions due to its VLM backbone. Emergent skills evaluation. To investigate the transfer of knowledge across robots, we conduct experiments with the Google Robot, assessing the per for mance on tasks like the ones shown in Fig. 5. These tasks involve objects and Fig. 5: To assess transfer between embodiments, we evaluate the skills that are notpresentin the RT-2 datasetbutoccurin the RT-2-X model on out-of-distribution skills. These skills are in Bridge data set[95]foradifferentrobot(the Widow Xrobot). the Bridge dataset, but not in the Google Robot dataset (the embodiment they are evaluated on). Results are shown in Table II, Emergent Skills Evaluation across robotic datasets. Contrary to previous RT-2 findings, column. Comparing rows (1) and (2), we find that RT-2-X co-fine-tuning and fine-tuning have similar per for mance in outperforms RT-2 by \u223c 3\u00d7, suggesting that incorporating both the Emergent Skills and Generalization Evaluation(row data from other robots into the training improves the range (4)vsrow(7)),whichweattributetothefact that the robotics of tasks that can be per for med even by a robot that already datausedin RT-2-Xismuchmorediversethan the previously has large amounts of data available. Our results suggest that used robotics datasets. co-training with data from other platforms imbues the RT-2- X controller with additional skills for the platform that are VI. DISCUSSION,FUTUREWORK,AND OPEN PROBLEMS",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 37,
      "paper_id": "openxembodiment",
      "text": "be per for med even by a robot that already datausedin RT-2-Xismuchmorediversethan the previously has large amounts of data available. Our results suggest that used robotics datasets. co-training with data from other platforms imbues the RT-2- X controller with additional skills for the platform that are VI. DISCUSSION,FUTUREWORK,AND OPEN PROBLEMS We presented a consolidated dataset that combines data not present in that platform\u2019s original dataset. from 22 robotic embodiments collected through a collab- Our next ablation involves removing the Bridge dataset oration between 21 institutions, demonstrating 527 skills from RT-2-X training: Row (3) shows the results for RT-2- (160266 tasks). We also presented an experimental demon- X that includes all data used for RT-2-X except the Bridge stration that Trans for mer-based policies trained on this data dataset. This variation signifi can tly reduces per for mance on canexhibitsignifi can tpositivetransferbetween the different thehold-outtasks,suggesting that transfer from the Widow X robots in the dataset. Our results showed that the RT-1- data may indeed be responsible for the additional skills that X policy has a 50% higher success rate than the original, can be per for med by RT-2-X with the Google Robot. state-of-the-art methods contributed by different collabo- C. Design decisions rating institutions, while the bigger vision-language-model- Lastly, we perform ablations to measure the influence of based version (RT-2-X) demonstrated \u223c 3\u00d7 generalization different design decisions on the generalization capabilities improvements over a model trained only on data from the of our most per for mant RT-2-X model, which are presented evaluation embodiment. In addition, we provided multiple in Table II. We note that including a short history of im- res our ces for the robotics community to explore the X- ages signifi can tly improves generalization per for mance (row embodiment robot learning research, including: the unified (4) vs row (5)). Similarly to the conclusions in the RT-2 X-robot and X-institution dataset, sample code showing paper [9], Web-based pre-training of the model is critical how to use the data, and the RT-1-X model to serve as a to achieving a high per for mance for the large models (row foundation for future exploration. (4) vs row (6)). We also note that the 55 B model has While RT-X demonstrates a step towards a X-embodied signifi can tlyhighersuccessratein the Emergent Skillscom- robot generalist, many more steps are needed to make this pared to the 5 B model (row (2) vs row (4)), demonstrating future a reality. Our experiments do not consider robots that higher model capacity enables higher degree of transfer with very different sensing and actuation modalities. They do not study generalization to new robots, and provide a Advances in Neural Information Processing Systems, decision criterion for when positive transfer does or does 2018, pp. 9355\u20139366. not happen. Studying these questions is an important future [12] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, workdirection.Thisworkservesnotonlyasanexample that J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia, X-robot learning is feasible and practical, but also provide \u201cGraph networks as learnable physics engines for the tools to advance research in this direction in the future.",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 38,
      "paper_id": "openxembodiment",
      "text": "is an important future [12] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, workdirection.Thisworkservesnotonlyasanexample that J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia, X-robot learning is feasible and practical, but also provide \u201cGraph networks as learnable physics engines for the tools to advance research in this direction in the future. inference and control,\u201d in Proceedings of the 35 th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy REFERENCES and A. Krause, Eds., vol. 80. PMLR, 10\u201315 Jul [1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, 2018, pp. 4470\u20134479. [Online]. Available: https:// G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, proceedings.mlr.press/v 80/sanchez-gonzalez 18 a.html J. Clark et al., \u201cLearning transferable visual models [13] D.Pathak,C.Lu,T.Darrell,P.Isola,and A.A.Efros, from natural language supervision,\u201d in International \u201cLearning to control self-assembling morphologies: a conference on machine learning. PMLR, 2021, pp. study of generalization via modularity,\u201d Advances in 8748\u20138763. Neural Information Processing Systems,vol.32,2019. [2] Open AI, \u201cGPT-4 technical report,\u201d 2023. [14] R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, M. Lee, R. Gardner, S. Savarese, [3] R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin, J. Bohg, and A. Garg, \u201cVariable impedance control in A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen end-effector space. an action space for rein for cement et al., \u201cPa LM 2 technical report,\u201d ar Xiv preprint learning in contact rich tasks,\u201d in Proceedings of the ar Xiv:2305.10403, 2023. International Conference of Intelligent Robots and [4] T. Weyand, A. Araujo, B. Cao, and J. Sim, \u201cGoogle Systems (IROS), 2019. landmarks dataset v 2 - a large-scale benchmark for [15] W.Huang,I.Mordatch,and D.Pathak,\u201cOnepolicyto instance-level recognition and retrieval,\u201d in Proceed- control them all: Shared modular policies for agent- ingsof the IEEE/CVFConferenceon Computer Vision agnostic control,\u201d in ICML, 2020. and Pattern Recognition (CVPR), June 2020. [16] V. Kurin, M. Igl, T. Rockta\u00a8schel, W. Boehmer, and [5] B. Wu, W. Chen, Y. Fan, Y. Zhang, J. Hou, J. Liu, S. Whiteson, \u201cMy body is a cage: the role of mor- and T. Zhang, \u201cTencent ML-images: A large-scale phology in graph-based incompatible control,\u201d ar Xiv multi-label image data base for visual representation preprint ar Xiv:2010.01856, 2020. learning,\u201d IEEE Access, vol. 7, 2019. [17] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, [6] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, and D. Dwibedi, \u201cXIRL: Cross-embodiment inverse D. Kontokostas, P. N. Mendes, S. Hellmann, rein for cement learning,\u201d Conference on Robot Learn- M. Morsey, P. van Kleef, S. Auer, and C. Bizer, ing (Co RL), 2021. \u201cDBpedia - a large-scale, multilingual knowledge [18] A. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, base extracted from wikipedia.\u201d Semantic Web, M.Bjo\u00a8rkman,and D.Kragic,\u201cBayesianmeta-learning vol. 6, no. 2, pp. 167\u2013195, 2015. [Online]. for few-shot policy adaptation across robotic plat- Available: http://dblp.uni-trier.de/db/journals/semweb/ forms,\u201d in 2021 IEEE/RSJ International Conference semweb 6.html#Lehmann IJJKMHMK 15 on Intelligent Robots and Systems (IROS). IEEE, [7] H. Mu\u00a8hleisen and C. Bizer, \u201cWeb data commons- 2021, pp. 1274\u20131280. extracting structured data from two large web cor- [19] A. Gupta, L. Fan, S. Ganguli, and L. Fei-Fei, \u201cMeta- pora.\u201d LDOW, vol. 937, pp. 133\u2013145, 2012. morph:Learninguniversalcontrollers with transform- [8] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, ers,\u201d",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 39,
      "paper_id": "openxembodiment",
      "text": "H. Mu\u00a8hleisen and C. Bizer, \u201cWeb data commons- 2021, pp. 1274\u20131280. extracting structured data from two large web cor- [19] A. Gupta, L. Fan, S. Ganguli, and L. Fei-Fei, \u201cMeta- pora.\u201d LDOW, vol. 937, pp. 133\u2013145, 2012. morph:Learninguniversalcontrollers with transform- [8] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, ers,\u201d in International Conference on Learning Repre- J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, sentations, 2021. A. Herzog, J. Hsu et al., \u201cRT-1: Robotics trans for mer [20] I. Schubert, J. Zhang, J. Bruce, S. Bechtle, forreal-worldcontrolat scale,\u201dRobotics:Science and E. Parisotto, M. Riedmiller, J. T. Springenberg, Systems (RSS), 2023. A. Byravan, L. Hasenclever, and N. Heess, \u201cA gen- [9] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, eralist dynamics model for control,\u201d 2023. X. Chen, K. Choromanski, T. Ding, D. Driess, [21] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and A. Dubey, C. Finn et al., \u201cRT-2: Vision-language- S.Levine,\u201cGNM:Ageneralnavigation model todrive action model stransferwebknowledgetoroboticcon- anyrobot,\u201din 2023 IEEEInternational Conferenceon trol,\u201d ar Xiv preprint ar Xiv:2307.15818, 2023. Robotics and Automation (ICRA). IEEE, 2023, pp. [10] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and 7226\u20137233. S. Levine, \u201cLearning modular neural network policies [22] Y. Zhou, S. Sonawani, M. Phielipp, S. Stepputtis, and formulti-task and multi-robottransfer,\u201din 2017 IEEE H. Amor, \u201cModularity through attention: Efficient international conference on robotics and automation training and transfer of language-conditioned policies (ICRA). IEEE, 2017, pp. 2169\u20132176. for robot manipulation,\u201d in Proceedings of The 6 th [11] T. Chen, A. Murali, and A. Gupta, \u201cHardw are con- Conference on Robot Learning, ser. Proceedings ditioned policies for multi-robot transfer learning,\u201d in of Machine Learning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205. PMLR, 14\u2013 controller,\u201d Advances in Neural Information Process- 18 Dec 2023, pp. 1684\u20131695. [Online]. Available: ing Systems, vol. 32, 2019. https://proceedings.mlr.press/v 205/zhou 23 b.html [36] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and [23] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, S.Levine,\u201cAvid:Learningmulti-stage task sviapixel- K. Schmeckpeper, S. Singh, S. Levine, and C. Finn, level translation of human videos,\u201d ar Xiv preprint \u201cRobo Net: Large-scale multi-robot learning,\u201d in Con- ar Xiv:1912.04443, 2019. ferenceon Robot Learning(Co RL),vol.100. PMLR, [37] A. Bonardi, S. James, and A. J. Davison, \u201cLearning 2019, pp. 885\u2013897. one-shot imitation from humans without humans,\u201d [24] E. S. Hu, K. Huang, O. Rybkin, and D. Jayaraman, IEEE Robotics and Automation Letters, vol. 5, no. 2, \u201cKnow thyself: Transferable visual control policies pp. 3533\u20133539, 2020. throughrobot-awareness,\u201din International Conference [38] K.Schmeckpeper,O.Rybkin,K.Daniilidis,S.Levine, on Learning Representations, 2022. and C. Finn, \u201cRein for cement learning with videos: [25] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Combining offline observations with interaction,\u201d in Lee, M. Bauza, T. Davchev, Y. Zhou, A. Gupta, Conference on Robot Learning. PMLR, 2021, pp. A. Raju et al., \u201cRobo Cat: A self-improving founda- 339\u2013354. tion agent for robotic manipulation,\u201d ar Xiv preprint [39] H.Xiong,Q.Li,Y.-C.Chen,H.Bharadhwaj,S.Sinha, ar Xiv:2306.11706, 2023. and A. Garg, \u201cLearning by watching: Physical imita- [26] J. Yang, D. Sadigh, and C. Finn, \u201cPolybot: Training tion of manipulation skills from human videos,\u201d in one policy across robots while embracing variability,\u201d 2021",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 40,
      "paper_id": "openxembodiment",
      "text": "self-improving founda- 339\u2013354. tion agent for robotic manipulation,\u201d ar Xiv preprint [39] H.Xiong,Q.Li,Y.-C.Chen,H.Bharadhwaj,S.Sinha, ar Xiv:2306.11706, 2023. and A. Garg, \u201cLearning by watching: Physical imita- [26] J. Yang, D. Sadigh, and C. Finn, \u201cPolybot: Training tion of manipulation skills from human videos,\u201d in one policy across robots while embracing variability,\u201d 2021 IEEE/RSJ International Conference on Intelli- ar Xiv preprint ar Xiv:2307.03719, 2023. gent Robots and Systems (IROS). IEEE, 2021, pp. [27] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, 7827\u20137834. A. Novikov, G. Barth-maron, M. Gime\u00b4nez, Y. Sul- [40] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, sky, J. Kay, J. T. Springenberg, T. Eccles, J. Bruce, C. Lynch, S. Levine, and C. Finn, \u201cBC-Z: Zero-shot A. Razavi, A. Edwards, N. Heess, Y. Chen, R. Had- task generalization with robotic imitation learning,\u201d sell, O. Vinyals, M. Bordbar, and N. de Freitas, \u201cA in Conference on Robot Learning (Co RL), 2021, pp. generalist agent,\u201d Transactions on Machine Learning 991\u20131002. Research, 2022. [41] S. Bahl, A. Gupta, and D. Pathak, \u201cHuman-to-robot [28] G.Salhotra,I.-C.A.Liu,and G.Sukhatme,\u201cBridging imitation in the wild,\u201d Robotics: Science and Systems action space mismatch in learning from demonstra- (RSS), 2022. tions,\u201d ar Xiv preprint ar Xiv:2304.03833, 2023. [42] M. Ding, Y. Xu, Z. Chen, D. D. Cox, P. Luo, [29] I.Radosavovic,B.Shi,L.Fu,K.Goldberg,T.Darrell, J. B. Tenenbaum, and C. Gan, \u201cEmbodied concept and J. Malik, \u201cRobot learning with sensorimotor pre- learner:Self-supervisedlearningofconcepts and map- training,\u201d in Conference on Robot Learning, 2023. ping through instruction following,\u201d in Conference on [30] L. Shao, F. Ferreira, M. Jorda, V. Nambiar, J. Luo, Robot Learning. PMLR, 2023, pp. 1743\u20131754. E. Solowjow, J. A. Ojea, O. Khatib, and J. Bohg, [43] S. Bahl, R. Mendonca, L. Chen, U. Jain, and \u201cUni Grasp: Learning a unified model to grasp with D. Pathak, \u201cAffordances from human videos as a multifingered robotic hands,\u201d IEEE Robotics and Au- versatile representation for robotics,\u201d in Proceedings tomation Letters, vol. 5, no. 2, pp. 2286\u20132293, 2020. ofthe IEEE/CVFConferenceon Computer Vision and [31] Z. Xu, B. Qi, S. Agrawal, and S. Song, \u201cAdagrasp: Pattern Recognition (CVPR), June 2023, pp. 13778\u2013 Learning an adaptive gripper-aware grasping policy,\u201d 13790. in 2021 IEEE International Conference on Robotics [44] P.Sermanet,K.Xu,and S.Levine,\u201cUnsupervisedper- and Automation(ICRA). IEEE,2021,pp.4620\u20134626. ceptualrewards for imitationlearning,\u201dar Xivpreprint [32] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, ar Xiv:1612.06699, 2016. K. Black, N. Hirose, and S. Levine, \u201cVi NT: A Foun- [45] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and dation Model for Visual Navigation,\u201d in 7 th Annual J.Bohg,\u201cConcept 2 Robot:Learningmanipulationcon- Conference on Robot Learning (Co RL), 2023. cepts from instructionsandhum and emonstrations,\u201din [33] Y.Liu,A.Gupta,P.Abbeel,and S.Levine,\u201cImitation Proceedings of Robotics: Science and Systems (RSS), from observation: Learning to imitate behaviors from 2020. raw video via context translation,\u201d in 2018 IEEE [46] A.S.Chen,S.Nair,and C.Finn,\u201cLearninggeneraliz- International Conferenceon Robotics and Automation able robotic reward functions from \u201cin-the-wild\u201d hu- (ICRA). IEEE, 2018, pp. 1118\u20131125. man videos,\u201d ar Xiv preprint ar Xiv:2103.16817, 2021. [34] T.Yu,C.Finn,S.Dasari,A.Xie,T.Zhang,P.Abbeel, [47] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and and S.Levine,\u201cOne-shotimitation from observinghu- X.Wang,\u201cGraphinverserein for cementlearning from mans via domain-adaptive meta-learning,\u201d Robotics: diverse videos,\u201d in Conference on Robot Learning. Science",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 41,
      "paper_id": "openxembodiment",
      "text": "functions from \u201cin-the-wild\u201d hu- (ICRA). IEEE, 2018, pp. 1118\u20131125. man videos,\u201d ar Xiv preprint ar Xiv:2103.16817, 2021. [34] T.Yu,C.Finn,S.Dasari,A.Xie,T.Zhang,P.Abbeel, [47] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and and S.Levine,\u201cOne-shotimitation from observinghu- X.Wang,\u201cGraphinverserein for cementlearning from mans via domain-adaptive meta-learning,\u201d Robotics: diverse videos,\u201d in Conference on Robot Learning. Science and Systems XIV, 2018. PMLR, 2023, pp. 55\u201366. [35] P. Sharma, D. Pathak, and A. Gupta, \u201cThird-person [48] M. Alakuijala, G. Dulac-Arnold, J. Mairal, J. Ponce, visual imitation learning via decoupled hierarchical and C.Schmid,\u201cLearningrewardfunctions for robotic manipulation by observing humans,\u201d in 2023 IEEE 2015. International Conferenceon Robotics and Automation [62] D. Kappler, J. Bohg, and S. Schaal, \u201cLeveraging big (ICRA). IEEE, 2023, pp. 5006\u20135012. data for grasp planning,\u201d in ICRA, 2015, pp. 4304\u2013 [49] Y. Zhou, Y. Aytar, and K. Bousmalis, \u201cManipulator- 4311. independent representations for visual imitation,\u201d [63] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, 2021. X. Liu, J. A. Ojea, and K. Goldberg, \u201cDex-Net 2.0: [50] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu, Deep learning to plan robust grasps with syn the tic Y. Zhu, and A. Anandkumar, \u201cMimicplay: Long- point clouds and analytic grasp metrics,\u201d in Robotics: horizon imitation learning by watching human play,\u201d Science and Systems (RSS), 2017. in Conference on Robot Learning, 2023. [64] A. Depierre, E. Dellandre\u00b4a, and L. Chen, \u201cJacquard: [51] K. Schmeckpeper, A. Xie, O. Rybkin, S. Tian, A large scale dataset for robotic grasp detection,\u201d in K. Daniilidis, S. Levine, and C. Finn, \u201cLearning pre- 2018 IEEE/RSJ International Conference on Intelli- dictive models from observation and interaction,\u201d in gent Robots and Systems (IROS). IEEE, 2018, pp. European Conference on Computer Vision. Springer, 3511\u20133516. 2020, pp. 708\u2013725. [65] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and [52] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and D. Quillen, \u201cLearning hand-eye coordination for A.Gupta,\u201cR 3 m:Auniversalvisualrepresentation for robotic grasping with deep learning and large-scale robot manipulation,\u201d in Co RL, 2022. data collection,\u201d The International journal of robotics [53] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik, research, vol. 37, no. 4-5, pp. 421\u2013436, 2018. \u201cMasked visual pre-training for motor control,\u201d ar Xiv [66] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Her- preprint ar Xiv:2203.06173, 2022. zog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, [54] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Ma- V. Vanhoucke et al., \u201cQT-Opt: Scalable deep rein- lik, and T. Darrell, \u201cReal-world robot learning with forcement learning for vision-based robotic manipu- masked visual pre-training,\u201d in Conference on Robot lation,\u201d ar Xiv preprint ar Xiv:1806.10293, 2018. Learning, 2022. [67] S. Brahmbhatt, C. Ham, C. Kemp, and J. Hays, [55] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, \u201cContactdb: Analyzing and predicting grasp contact V. Kumar, and A. Zhang, \u201cVip: Towards universal via thermal imaging,\u201d 04 2019. visual reward and representation via value-implicit [68] H.-S. Fang, C. Wang, M. Gou, and C. Lu, \u201cGraspnet- pre-training,\u201d ar Xiv preprint ar Xiv:2210.00030, 2022. 1 billion: a large-scale benchmark for general object [56] A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen, grasping,\u201din Proceedingsof the IEEE/CVFconference S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik oncomputervision and patternrecognition,2020,pp. etal.,\u201cWhere",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 42,
      "paper_id": "openxembodiment",
      "text": "and representation via value-implicit [68] H.-S. Fang, C. Wang, M. Gou, and C. Lu, \u201cGraspnet- pre-training,\u201d ar Xiv preprint ar Xiv:2210.00030, 2022. 1 billion: a large-scale benchmark for general object [56] A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen, grasping,\u201din Proceedingsof the IEEE/CVFconference S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik oncomputervision and patternrecognition,2020,pp. etal.,\u201cWhere are wein the search for anartificialvi- 11444\u201311453. sualcortex for embodiedintelligence?\u201dar Xivpreprint [69] C. Eppner, A. Mousavian, and D. Fox, \u201cACRONYM: ar Xiv:2303.18240, 2023. A large-scale grasp dataset based on simulation,\u201d in [57] S.Karamcheti,S.Nair,A.S.Chen,T.Kollar,C.Finn, 2021 IEEE Int. Conf. on Robotics and Automation, D. Sadigh, and P. Liang, \u201cLanguage-driven represen- ICRA, 2020. tation learning for robotics,\u201d Robotics: Science and [70] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kel- Systems (RSS), 2023. cey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, [58] Y. Mu, S. Yao, M. Ding, P. Luo, and C. Gan, \u201cEC 2: K. Konolige, S. Levine, and V. Vanhoucke, \u201cUsing Emergent communication for embodied control,\u201d in simulation and domain adaptation to improve effi- Proceedings of the IEEE/CVF Conference on Com- ciency of deep robotic grasping,\u201d in ICRA, 2018, pp. puter Vision and Pattern Recognition,2023,pp.6704\u2013 4243\u20134250. 6714. [71] X. Zhu, R. Tian, C. Xu, M. Huo, W. Zhan, [59] S. Bahl, R. Mendonca, L. Chen, U. Jain, and M. Tomizuka, and M. Ding, \u201cFanuc manipulation: D. Pathak, \u201cAffordances from human videos as a A dataset for learning-based manipulation with fanuc versatile representation for robotics,\u201d in Proceedings mate 200 i D robot,\u201d https://sites.google.com/berkeley. ofthe IEEE/CVFConferenceon Computer Vision and edu/fanuc-manipulation, 2023. Pattern Recognition, 2023, pp. 13778\u201313790. [72] K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, [60] Y. Jiang, S. Moseson, and A. Saxena, \u201cEfficient \u201cMore than a million ways to be pushed. a high- grasping from RGBD images: Learning using a new fidelity experimental dataset of planar pushing,\u201d in rectangle representation,\u201d in 2011 IEEE International 2016 IEEE/RSJinternationalconferenceonintelligent conference on robotics and automation. IEEE, 2011, robots and systems (IROS). IEEE, 2016, pp. 30\u201337. pp. 3304\u20133311. [73] C.Finn and S.Levine,\u201cDeepvisualforesight for plan- [61] L. Pinto and A. K. Gupta, \u201cSupersizing self- ning robot motion,\u201d in 2017 IEEE International Con- supervision: Learning to grasp from 50 k tries and ference on Robotics and Automation (ICRA). IEEE, 700 robothours,\u201d2016 IEEEInternational Conference 2017, pp. 2786\u20132793. on Robotics and Automation (ICRA), pp. 3406\u20133413, [74] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine, \u201cVisual foresight: Model-based deep rein- E. Orbay, S. Savarese, and L. Fei-Fei, \u201cRobo Turk: forcement learning for vision-based robotic control,\u201d A crowds our cing platform for robotic skill learning ar Xiv preprint ar Xiv:1812.00568, 2018. through imitation,\u201d Co RR, vol. abs/1811.02790, 2018. [75] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser, [Online]. Available: http://arxiv.org/abs/1811.02790 \u201cTheprincetonshapebenchmark,\u201din Shape Modeling [87] P. Sharma, L. Mohan, L. Pinto, and A. Gupta, \u201cMul- Applications, 2004, pp. 167\u2013388. tiple interactions made easy (MIME): Large scale [76] W. Wohlkinger, A. Aldoma Buchaca, R. Rusu, and demonstrations data for imitation,\u201d in Conference on M. Vincze, \u201c3 DNet: Large-Scale Object Class Recog- robot learning. PMLR, 2018, pp. 906\u2013915. nition from CADModels,\u201din IEEEInternational Con- [88] A. Mandlekar, J. Booher, M.",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 43,
      "paper_id": "openxembodiment",
      "text": "pp. 167\u2013388. tiple interactions made easy (MIME): Large scale [76] W. Wohlkinger, A. Aldoma Buchaca, R. Rusu, and demonstrations data for imitation,\u201d in Conference on M. Vincze, \u201c3 DNet: Large-Scale Object Class Recog- robot learning. PMLR, 2018, pp. 906\u2013915. nition from CADModels,\u201din IEEEInternational Con- [88] A. Mandlekar, J. Booher, M. Spero, A. Tung, ference on Robotics and Automation (ICRA), 2012. A. Gupta, Y. Zhu, A. Garg, S. Savarese, and L. Fei- [77] A. Kasper, Z. Xue, and R. Dillmann, \u201cThe kit object Fei, \u201cScaling robot supervision to hundreds of hours models data base:Anobject model data base for object with Robo Turk:Roboticmanipulation data setthrough recognition, localization and manipulation in service human reasoning and dexterity,\u201d in 2019 IEEE/RSJ robotics,\u201d The International Journal of Robotics Re- International Conference on Intelligent Robots and search, vol. 31, no. 8, pp. 927\u2013934, 2012. Systems (IROS). IEEE, 2019, pp. 1048\u20131055. [78] A. Singh, J. Sha, K. S. Narayan, T. Achim, and [89] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, P. Abbeel, \u201cBig BIRD: A large-scale 3 D data base of G. Georgakis, K. Daniilidis, C. Finn, and S. Levine, object instances,\u201d in IEEE International Conference \u201cBridge data:Boostinggeneralizationofroboticskills on Robotics and Automation (ICRA), 2014, pp. 509\u2013 withcross-domain data sets,\u201din Robotics:Science and 516. Systems (RSS) XVIII, 2022. [79] B. Calli, A. Walsman, A. Singh, S. Srinivasa, [90] A.Mandlekar,D.Xu,J.Wong,S.Nasiriany,C.Wang, P. Abbeel, and A. M. Dollar, \u201cBenchmarking in ma- R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and nipulation research: Using the Yale-CMU-Berkeley R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, \u201cWhat matters in learning from object and model set,\u201d IEEE Robotics & Automation offlinehum and emonstrations for robotmanipulation,\u201d Magazine, vol. 22, no. 3, pp. 36\u201352, 2015. in ar Xiv preprint ar Xiv:2108.03298, 2021. [80] Zhirong Wu,S.Song,A.Khosla,Fisher Yu,Linguang [91] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, Zhang, Xiaoou Tang, and J. Xiao, \u201c3 D Shape Nets: A R. Baruch, T. Armstrong, and P. Florence, \u201cInterac- deep representation for volumetric shapes,\u201d in IEEE tive language: Talking to robots in real time,\u201d IEEE Conference on Computer Vision and Pattern Recogni- Robotics and Automation Letters, 2023. tion (CVPR), 2015, pp. 1912\u20131920. [92] H.-S.Fang,H.Fang,Z.Tang,J.Liu,J.Wang,H.Zhu, [81] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, and C. Lu, \u201cRH 20 T: A robotic dataset for learning R. Mottaghi, L. Guibas, and S. Savarese, \u201cObject- diverse skills in one-shot,\u201d in RSS 2023 Workshop on Net 3 D: A large scale data base for 3 d object recog- Learning for Task and Motion Planning, 2023. nition,\u201d in European Conference on Computer Vision [93] H.Bharadhwaj,J.Vakil,M.Sharma,A.Gupta,S.Tul- (ECCV). Springer, 2016, pp. 160\u2013176. siani, and V. Kumar, \u201cRobo Agent: Towards sample [82] D. Morrison, P. Corke, and J. Leitner, \u201cEgad! an efficient robot manipulation with semantic augmenta- evolved grasping analysis dataset for diversity and re- tions and action chunking,\u201d arxiv, 2023. producibility in robotic manipulation,\u201d IEEE Robotics [94] M. Heo, Y. Lee, D. Lee, and J. J. Lim, \u201cFurni- and Automation Letters, vol. 5, no. 3, pp. 4368\u20134375, turebench: Reproducible real-world benchmark for 2020. long-horizoncomplexmanipulation,\u201din Robotics:Sci- [83] R. Gao, Y.-Y. Chang, S. Mall, L. Fei-Fei, and J. Wu, ence and Systems, 2023. \u201cObject",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 44,
      "paper_id": "openxembodiment",
      "text": "robotic manipulation,\u201d IEEE Robotics [94] M. Heo, Y. Lee, D. Lee, and J. J. Lim, \u201cFurni- and Automation Letters, vol. 5, no. 3, pp. 4368\u20134375, turebench: Reproducible real-world benchmark for 2020. long-horizoncomplexmanipulation,\u201din Robotics:Sci- [83] R. Gao, Y.-Y. Chang, S. Mall, L. Fei-Fei, and J. Wu, ence and Systems, 2023. \u201cObject Folder: A dataset of objects with implicit vi- [95] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, sual, auditory, and tactile representations,\u201d in Confer- C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, ence on Robot Learning, 2021, pp. 466\u2013476. A. He, V. Myers, K. Fang, C. Finn, and S. Levine, [84] L.Downs,A.Francis,N.Koenig,B.Kinman,R.Hick- \u201cBridgedatav 2:Adataset for robotlearningat scale,\u201d man, K. Reymann, T.B. Mc Hugh, and V. Vanhoucke, 2023. \u201cGoogles can nedobjects:Ahigh-quality data setof 3 D [96] T. Winograd, \u201cUnderst and ing natural language,\u201d scannedhouseholditems,\u201din 2022 International Con- Cognitive Psychology, vol. 3, no. 1, pp. 1\u2013191, ference on Robotics and Automation (ICRA). IEEE, 1972. [Online]. Available: https://www.sciencedirect. 2022, pp. 2553\u20132560. com/science/article/pii/0010028572900023 [85] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swan- [97] M. Mac Mahon, B. Stankiewicz, and B. Kuipers, son, R. Jonschkowski, C. Finn, S. Levine, and \u201cWalk the talk: Connecting language, knowledge, and K.Hausman,\u201cMT-Opt:Continuousmulti-taskrobotic action in route instructions,\u201d in Proceedings of the rein for cement learning at scale,\u201d ar Xiv preprint Twenty-First AAAI Conference on Artificial Intelli- ar Xiv:2104.08212, 2021. gence, 2006. [86] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, [98] T. Kollar, S. Tellex, D. Roy, and N. Roy, \u201cToward M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, underst and ing natural language directions,\u201d in 2010 5 th ACM/IEEE International Conference on Human- [112] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and Robot Interaction (HRI), 2010, pp. 259\u2013266. L. Fei-Fei, \u201cVox Poser: Composable 3 d value maps [99] D. L. Chen and R. J. Mooney, \u201cLearning to interpret forroboticmanipulation with languagemodels,\u201dar Xiv naturallanguagenavigationinstructions from observa- preprint ar Xiv:2307.05973, 2023. tions,\u201d in Proceedings of the Twenty-Fifth AAAI Con- [113] M. Shridhar, L. Manuelli, and D. Fox, \u201cCliport: What ference on Artificial Intelligence, 2011, p. 859\u2013865. and where pathways for robotic manipulation,\u201d in [100] F. Duvallet, J. Oh, A. Stentz, M. Walter, T. Howard, Conference on Robot Learning. PMLR, 2022, pp. S. Hemachandra, S. Teller, and N. Roy, \u201cInferring 894\u2013906. maps and behaviors from natural language instruc- [114] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. tions,\u201d in International Symposium on Experimental Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia, Robotics (ISER), 2014. C.Finnetal.,\u201cOpen-worldobjectmanipulationusing [101] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foer- pre-trained vision-language models,\u201d ar Xiv preprint ster, J. Andreas, E. Grefenstette, S. Whiteson, and ar Xiv:2303.00905, 2023. T. Rockta\u00a8schel, \u201cA survey of rein for cement learning [115] Y. Mu, Q. Zhang, M. Hu, W. Wang, M. Ding, J. Jin, informed by natural language,\u201d in IJCAI, 2019. B.Wang,J.Dai,Y.Qiao,and P.Luo,\u201cEmbodied GPT: [102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, Vision-language pre-training via embodied chain of C. Baral, and H. Ben Amor, \u201cLanguage-conditioned thought,\u201d ar Xiv preprint ar Xiv:2305.15021, 2023. imitation learning for robot manipulation tasks,\u201d Ad- [116] E. Perez, F.",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 45,
      "paper_id": "openxembodiment",
      "text": "informed by natural language,\u201d in IJCAI, 2019. B.Wang,J.Dai,Y.Qiao,and P.Luo,\u201cEmbodied GPT: [102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, Vision-language pre-training via embodied chain of C. Baral, and H. Ben Amor, \u201cLanguage-conditioned thought,\u201d ar Xiv preprint ar Xiv:2305.15021, 2023. imitation learning for robot manipulation tasks,\u201d Ad- [116] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and vances in Neural Information Processing Systems, A. Courville, \u201cFilm: Visual reasoning with a general vol. 33, pp. 13139\u201313150, 2020. conditioning layer,\u201d 2017. [103] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn [117] M. Tan and Q. Le, \u201cEfficient Net: Rethinking model et al., \u201cLearning language-conditioned robot behavior scaling for convolutional neural networks,\u201d in Inter- from offline data and crowd-sourced annotation,\u201d in national conference on machine learning. PMLR, Conference on Robot Learning. PMLR, 2022, pp. 2019, pp. 6105\u20136114. 1303\u20131315. [118] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, [104] O. Mees, L. Hermann, E. Rosete-Beas, and L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, W. Burgard, \u201cCALVIN: A benchmark for language- \u201cAttention is all you need,\u201d Advances in neural infor- conditioned policy learning for long-horizon robot mation processing systems, vol. 30, 2017. manipulation tasks,\u201d IEEE Robotics and Automation [119] S. Ramos, S. Girgin, L. Hussenot, D. Vincent, Letters, 2022. H. Yakubovich, D. Toyama, A. Gergely, P. Stanczyk, [105] O.Mees,L.Hermann,and W.Burgard,\u201cWhatmatters R. Marinier, J. Harmsen, O. Pietquin, and N. Mom- in language conditioned robotic imitation learning chev,\u201cRLDS:anecosystemtogenerate,share and use over unstructured data,\u201d IEEE Robotics and Automa- datasets in rein for cement learning,\u201d 2021. tion Letters, vol. 7, no. 4, pp. 11205\u201311212, 2022. [120] D. Cer, Y. Yang, S. yi Kong, N. Hua, N. Limti- [106] M. Shridhar, L. Manuelli, and D. Fox, \u201cPerceiver- aco, R. S. John, N. Constant, M. Guajardo-Cespedes, actor: A multi-task trans for mer for robotic manipu- S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and lation,\u201d Conference on Robot Learning (Co RL), 2022. R. Kurzweil, \u201cUniversal sentence encoder,\u201d 2018. [107] F. Hill, S. Mokra, N. Wong, and T. Harley, \u201cHuman [121] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, instruction-following with deep rein for cement learn- S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, ing via transfer-learning from text,\u201d ar Xiv preprint X. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, ar Xiv:2005.09382, 2020. M.Lucic,M.Tschannen,A.Nagrani,H.Hu,M.Joshi, [108] C. Lynch and P. Sermanet, \u201cGrounding language in B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, play,\u201d Robotics: Science and Systems (RSS), 2021. A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters, [109] M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu, A.Herzogetal.,\u201cDoas Ican,notas Isay:Grounding K. Rong, A. Kolesnikov, M. Seyedhosseini, A. An- languageinroboticaf for dances,\u201dConferenceon Robot gelova, X. Zhai, N. Houlsby, and R. Soricut, \u201cPali- Learning (Co RL), 2022. x: On scaling up a multilingual vision and language [110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, model,\u201d 2023. Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and [122] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I.",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 46,
      "paper_id": "openxembodiment",
      "text": "Houlsby, and R. Soricut, \u201cPali- Learning (Co RL), 2022. x: On scaling up a multilingual vision and language [110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, model,\u201d 2023. Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and [122] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, L. Fan, \u201cVIMA: General robot manipulation with Y. Hasson, K. Lenc, A. Mensch, K. Milli can, multimodal prompts,\u201d International Conference on M.Reynolds,R.Ring,E.Rutherford,S.Cabi,T.Han, Machine Learning (ICML), 2023. Z. Gong, S. Samangooei, M. Monteiro, J. Menick, [111] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, S. Borgeaud, A. Brock, A. Nematzadeh, S. Shar- \u201cChat GPT for robotics: Design principles and model ifzadeh, M. Binkowski, R. Barreira, O. Vinyals, abilities,\u201d Microsoft Auton. Syst. Robot. Res, vol. 2, A. Zisserman, and K. Simonyan, \u201cFlamingo: a visual p. 20, 2023. language model for few-shot learning,\u201d 2022. [123] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q.Vuong,T.Yu,W.Huang,Y.Chebotar,P.Sermanet, D.Duckworth,S.Levine,V.Vanhoucke,K.Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence, \u201cPa LM-E: An embodied multimodal lan- guage model,\u201d 2023. [124] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis- senborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16 x 16 words: Trans for mers for image recognition at scale,\u201d 2021. [125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X.Wang,H.W.Chung,S.Shakeri,D.Bahri,T.Schus- ter,H.S.Zheng,D.Zhou,N.Houlsby,and D.Metzler, \u201cUL 2: Unifying language learning paradigms,\u201d 2023. [126] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W.Burgard,\u201cLatentplans for taskagnosticoffline rein for cement learning,\u201d in Proceedings of the 6 th Conference on Robot Learning (Co RL), 2022. [127] O. Mees, J. Borja-Diaz, and W. Burgard, \u201cGrounding language with visual affordances over unstructured data,\u201d in Proceedings of the IEEE International Con- ference on Robotics and Automation (ICRA), London, UK, 2023. [128] S. Dass, J. Yapeter, J. Zhang, J. Zhang, K. Pertsch, S. Nikolaidis, and J. J. Lim, \u201cCLVR jaco play dataset,\u201d 2023. [Online]. Available: https://github. com/clvrai/clvr jaco play dataset [129] J. Luo, C. Xu, X. Geng, G. Feng, K. Fang, L. Tan, S. Schaal, and S. Levine, \u201cMulti-stage cable rout- ing through hierarchical imitation learning,\u201d ar Xiv preprint ar Xiv:2307.08927, 2023. [130] J. Pari, N. M. Shafiullah, S. P. Arunachalam, and L. Pinto, \u201cThe surprising effectiveness of representa- tion learning for visual imitation,\u201d 2021. [131] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, \u201cViola: Imitation learning for vision-based manipulation with object proposal priors,\u201d 2023. [132] L. Y. Chen, S. Adebola, and K. Goldberg, \u201cBerkeley UR 5 demonstration dataset,\u201d https://sites.google.com/ view/berkeley-ur 5/home. [133] G. Zhou, V. Dean, M. K. Srirama, A. Rajeswaran, J. Pari, K. Hatch, A. Jain, T. Yu, P. Abbeel, L. Pinto, C. Finn, and A. Gupta, \u201cTrain offline, test online: A real robot learning benchmark,\u201d 2023. [134] \u201cTask-agnostic real world robot play,\u201d https://www. kaggle.com/datasets/oiermees/taco-robot.",
      "start_pos": 8778,
      "end_pos": 9257
    },
    {
      "chunk_id": 47,
      "paper_id": "rt2",
      "text": "https://robotics-trans for mer 2.github.io 2023-8-1 RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control Anthony Brohan,Noah Brown,Justice Carbajal,Yevgen Chebotar,Xi Chen,Krzysztof Choromanski, Tianli Ding,Danny Driess,Avinava Dubey,Chelsea Finn,Pete Florence,Chuyuan Fu, Montse Gonzalez Arenas,Keerthana Gopalakrishnan,Kehang Han,Karol Hausman,Alexander Herzog, Jasmine Hsu,Brian Ichter,Alex Irpan,Nikhil Joshi,Ryan Julian,Dmitry Kalashnikov,Yuheng Kuang, Isabel Leal,Lisa Lee,Tsang-Wei Edward Lee,Sergey Levine,Yao Lu,Henryk Michalewski,Igor Mordatch, Karl Pertsch,Kanishka Rao,Krista Reymann,Michael Ryoo,Grecia Salazar,Pannag Sanketi, Pierre Sermanet,Jaspiar Singh,Anikait Singh,Radu Soricut,Huong Tran,Vincent Vanhoucke,Quan Vuong, Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Jialin Wu,Fei Xia,Ted Xiao,Peng Xu,Sichun Xu,Tianhe Yu, and Brianna Zitkovich Google Deep Mind.Authorslistedinalphabeticalorder,withcontributionslistedin Appendix A. Westudyhowvision-language model strainedon Internet-scale data can beincorporateddirectlyinto end-to-endroboticcontroltoboostgeneralization and enableemergentsemanticreasoning. Ourgoalis toenableasingleend-to-endtrained model tobothlearntomaprobotobservationstoactions and enjoy thebenefitsoflarge-scalepretrainingonlanguage and vision-language data from the web. Tothisend, weproposetoco-fine-tunestate-of-the-artvision-language model sonbothrobotictrajectory data and Internet-scalevision-languagetasks,suchasvisualquestionanswering. Incontrasttoo the rapproaches, weproposeasimple,generalrecipetoachieve this goal: inordertofitbothnaturallanguageresponses androboticactionsinto the same for mat,weexpress the actionsastexttokens and incorporatethem directly into the training set of the model in the same way as natural language tokens. We refer to suchcategoryof model sasvision-language-actionmodels(VLA)andinstantiateanexampleofsuch amodel,whichwecall RT-2. Ourextensiveevaluation(6 kevaluationtrials)shows that ourapproach leadstoper for mantroboticpolicies and enables RT-2 toobtainarangeofemergentcapabilities from Internet-scaletraining. Thisincludessignifi can tlyimprovedgeneralizationtonovelobjects,theability tointerpretcomm and snotpresentin the robottraining data(suchasplacinganobjectontoaparticular numberoricon),and the abilitytoper for mrudimentaryreasoninginresponsetousercommands(such aspickingup the smallestorlargestobject,ortheoneclosesttoano the robject). Wefur the rshow that incorporatingchainofthoughtreasoningallows RT-2 toper for mmulti-stagesemanticreasoning,for examplefiguringoutwhichobjecttopickup for useasanimprovisedhammer(arock),orwhichtype ofdrinkisbestsuited for someo new hoistired(anenergydrink). 1. Introduction High-capacity models pretrained on broad web-scale datasets provide an effective and powerful platform for awiderangeofdownstreamtasks: largelanguagemodels can enablenotonlyfluenttext generation(Aniletal.,2023;Brohanetal.,2022;Open AI,2023)butemergentproblem-solving(Cobbe et al., 2021; Lewkowycz et al., 2022; Polu et al., 2022) and creative generation of prose (Brown et al., 2020; Open AI, 2023) and code (Chen et al., 2021), while vision-language models enable open-vocabulary visual recognition (Kirillov et al., 2023; Minderer et al., 2022; Radford et al., 2021) and can evenmakecomplexinferencesaboutobject-agentinteractionsinimages(Alayracetal.,2022; Chenetal.,2023 a,b;Driessetal.,2023;Haoetal.,2022;Huangetal.,2023;Wangetal.,2022). Such semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously useful for generalist robots that must perform a variety of tasks in real-world environments. However, Correspondingauthor(s):chebotar@google.com,tianheyu@google.com,karolhausman@google.com \u00a9 2023 Google Deep Mind.Allrightsreserved 3202 lu J 82 ]OR.sc[ 1 v 81851.7032:vi Xra RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control Internet-Scale VQA + Robot Action Data Vision-Language-Action Models for Robot Control Closed-Loop Robot Control Q: What is happening Q: What should the robot RT-2 in the image? do to <task>? A: \u2026 Large Language Model A: 311 423 170 55 244 A grey donkey walks down the street. Put the strawberry Q: Que puis-je faire avec Vi T into the correct bowl ces objets? A: 3455 1144 189 25673 Faire cuire un g\u00e2teau. \u0394T = [0.1, -0.2, 0] Q: What should the robot A: 132 114 128 5 25 156 De-Tokenize \u0394R = [10\u2218 , 25\u2218 , -7\u2218 ] Pick the nearly falling bag do to <task>? Robot Action A: 132 114 128 5 25 156 \u0394 Translation = [0.1, -0.2, 0] \u0394Rotation = [10\u2218 , 25 \u2218, -7 \u2218 ] Co-Fine-Tune Deploy Pick object that is different Figure 1|RT-2 overview:werepresentrobotactionsasano the rlanguage,which can becastintotexttokens and trainedtoge the rwith Internet-scalevision-language data sets. Duringinference,thetexttokensarede-tokenized into robot actions, enabling closed loop control. This allows us to leverage the backbone and pretraining of vision-language models in learning robotic policies, transferring some of their generalization, semantic underst and",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 48,
      "paper_id": "rt2",
      "text": "object that is different Figure 1|RT-2 overview:werepresentrobotactionsasano the rlanguage,which can becastintotexttokens and trainedtoge the rwith Internet-scalevision-language data sets. Duringinference,thetexttokensarede-tokenized into robot actions, enabling closed loop control. This allows us to leverage the backbone and pretraining of vision-language models in learning robotic policies, transferring some of their generalization, semantic underst and ing,andreasoningtoroboticcontrol. Wedemonstrateexamplesof RT-2 executionon the project website: robotics-trans for mer 2.github.io. it is unclear how robots should acquire such capabilities. While a brute force approach might entail collectingmillionsofroboticinteractiontrials,themostcapablelanguage and vision-languagemodels are trained on billions of tokens and images from the web (Alayrac et al., 2022; Chen et al., 2023 a,b; Huang et al., 2023) \u2013 an amount unlikely to be matched with robot data in the near future. On the other hand, directly applying such models to robotic tasks is also difficult: such models reason about semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such as Cartesian end-effector commands. While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess etal.,2023;Vempralaetal.,2023),suchmethodsgenerallyaddressonly the\u201chigherlevel\u201daspectsof robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects), which are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training. Therefore, in this paper we ask: can large pretrained vision- language models be integrated directly into low-level robotic control to boost generalization and enable emergent semantic reasoning? To this end, we explore an approach that is both simple and surprisingly effective: we directly train vision-language models designed for open-vocabulary visual question answering and visual dialogue to output low-level robot actions, along with solving other Internet-scale vision-language tasks. Although such models are typically trained to produce natural language tokens, we can train them on robotic trajectories by tokenizing the actions into text tokens and creating \u201cmultimodal sentences\u201d(Driessetal.,2023)that\u201crespond\u201dtoroboticinstructionspaired with cameraobservations by producing corresponding actions. In this way, vision-language models can be directly trained to actasinstructionfollowingroboticpolicies. Thissimpleapproachisincontrast with prioralternatives for incorporating VLMs into robot policies (Shridhar et al., 2022 a) or designing new vision-language- action architectures from scratch (Reed et al., 2022): instead, pre-existing vision-language models, with already-amortized significant compute investment, are trained without any new parameters to output text-encoded actions. We refer to this category of models as vision-language-action (VLA) models. We instantiate VLA models by building on the protocol proposed for RT-1 (Brohan et al., 2022), using a similar dataset, but exp and ing the model to use a large vision-language backbone. Hence we refer to our model as RT-2 (Robotics Trans for mer 2). We provide an overview in Figure 1. 2 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control We observe that robotic policies derived from such vision-language models exhibit a range of remarkable capabilities, combining the physical motions learned from the robot data with the ability to interpret images and text learned from web data into a single model. Besides the expected benefit of dramatically improving generalization to novel objects and semantically varied instructions, we",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 49,
      "paper_id": "rt2",
      "text": "such vision-language models exhibit a range of remarkable capabilities, combining the physical motions learned from the robot data with the ability to interpret images and text learned from web data into a single model. Besides the expected benefit of dramatically improving generalization to novel objects and semantically varied instructions, we observe a number of emergent capabilities. While the model\u2019s physical skills are still limited to the distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in new ways by interpreting images and language commands using knowledge gleaned from the web. Some example highlights are shown in Figure 2. The model is able to re-purpose pick and place skills learned from robot data to place objects near semantically indicated locations, such as specific numbersoricons,despitethosecuesnotbeingpresentin the robot data. Themodel can alsointerpret relations between objects to determine which object to pick and where to place it, despite no such relations being provided in the robot demonstrations. Fur the rmore, if we augment the comm and with chain of thought prompting, the model is able to make even more complex semantic inferences, such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink). Our main contribution is RT-2, a family of models derived from fine-tuning large vision-language models trained on web-scale data to directly act as generalizable and semantically aware robotic policies. Our experiments investigate models with up to 55 B parameters trained on Internet data and instruction-annotated robotic trajectories from previous work (Brohan et al., 2022). Over the courseof 6 kroboticevaluations,weshow that RT-2 enablesignifi can timprovementstogeneralization over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from web-scale vision-language pretraining. 2. Related Work Vision-language models. There are several categories of Vision-Language Models (VLMs) (Gan et al., 2022), with perhaps two most relevant: (1) representation-learning models, e.g. CLIP (Radford et al., 2021), which learn common embeddings for both modalities, and (2) visual language models of the form {vision,text} \u2192 {text} which learn to take vision and language as input and provide free-form text. Both categories have been used to provide pretraining for a wide variety of applied to downstream applications such as object classification (Radford et al., 2021), detection (Gu et al., 2021), and segmentation (Ghiasi et al., 2021). In this work, we focus on the latter category (Alayrac et al., 2022; Chen et al., 2023 a,b; Driess et al., 2023; Hao et al., 2022; Li et al., 2023, 2019; Lu et al., 2019). These models are generally trained on many different tasks, such as image captioning, vision-question answering (VQA), and general language tasks on multiple datasets at the same time. While prior works study VLMs for a wide range of problems and settings including in robotics, our focus is on how the capabilities of VLMs can be extended to robotics closed-loop control by endowing them with the abilitytopredictrobotactions,thusleveraging the knowledgealreadypresentin VLMs to enable new levels of generalization. Generalization in robot learning. Developing",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 50,
      "paper_id": "rt2",
      "text": "works study VLMs for a wide range of problems and settings including in robotics, our focus is on how the capabilities of VLMs can be extended to robotics closed-loop control by endowing them with the abilitytopredictrobotactions,thusleveraging the knowledgealreadypresentin VLMs to enable new levels of generalization. Generalization in robot learning. Developing robotic controllers that can broadly succeed in a variety of scenarios is a long-standing goal in robotics research (Kaelbling, 2020; Smith and Coles, 1973). A promising approach for enabling generalization in robotic manipulation is by learning from large anddiverse datasets(Dasari etal.,2019;Levineetal.,2018; Pinto and Gupta,2016). Bydoing so, prior methods have demonstrated how robots can generalize to novel object instances (Finn and Levine,2017;Levineetal.,2018;Mahleretal.,2017;Pinto and Gupta,2016;Youngetal.,2021),to tasks involving novel combinations of objects and skills (Dasari and Gupta, 2021; Finn et al., 2017; James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022 a; Pong et al., 3 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control 2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and tounseenenvironments(Cuietal.,2022;Duetal.,2023 a;Hansenetal.,2020). Unlikemostofthese prior works, we aim to develop and study a single model that can generalize to unseen conditions along all of these axes. A key ingredient of our approach is to leverage pre-trained models that have been exposed to data that is much broader than the data seen by the robot. Pre-training for robotic manipulation. Pre-training has a long history in robotic learning. Most works focus on pre-trained visual representations that can be used to initialize the encoder of the robot\u2019s camera observations, either via supervised Image Net classification (Shah and Kumar, 2021), data augmentation (Kostrikov et al., 2020; Laskin et al., 2020 a,b; Pari et al., 2021) or objectives that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al., 2023 b; Nair et al., 2022 b; Xiao et al., 2022 b). Other works have incorporated pre-trained language models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al., 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022 a; Shridhar et al., 2022 b) or for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023; Singh et al., 2023; Wu et al., 2023). Rather than using pre-training vision models or pre-trained language models, we specifically consider the use of pre-trained vision-language models (VLMs), which provide rich, grounded knowledge about the world. Prior works have studied the use of VLMs for robotics (Driess et al., 2023; Du et al., 2023 b; Gadre et al., 2022; Karamcheti et al., 2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work. Thesepriorapproaches use VLMs forvisualstaterepresentations(Karamchetietal.,2023), for identifyingobjects(Gadreetal.,2022;Stoneetal.,2023),forhigh-levelplanning(Driessetal.,2023), or for providing supervision or success detection (Du et al., 2023 b; Ma et al., 2023; Sumers et al., 2023; Xiao et",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 51,
      "paper_id": "rt2",
      "text": "2023; Shah et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this work. Thesepriorapproaches use VLMs forvisualstaterepresentations(Karamchetietal.,2023), for identifyingobjects(Gadreetal.,2022;Stoneetal.,2023),forhigh-levelplanning(Driessetal.,2023), or for providing supervision or success detection (Du et al., 2023 b; Ma et al., 2023; Sumers et al., 2023; Xiao et al., 2022 a; Zhang et al., 2023). While CLIPort (Shridhar et al., 2021) and MOO (Stone et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both incorporate significant structure into the policy that limits their applicability. Notably, our work does notrelyon are stricted 2 Dactionspace and doesnotrequireacalibratedcamera. Moreover,acritical distinction is that, unlike these works, we leverage VLMs that generate language, and the unified output space of our formulation enables model weights to be entirely shared across language and action tasks, without introducing action-only model layer components. 3. Vision-Language-Action Models In this section, we present our model family and the design choices for enabling training VLMs to directly perform closed-loop robot control. First, we describe the general architecture of our models and how they can be derived from models that are commonly used for vision-language tasks. Then, we introduce the recipe and challenges of fine-tuning large VLMs that are pre-trained on web-scale data to directly output robot actions, becoming VLA models. Finally, we describe how to make these modelspractical for robottasks,addressingchallenges with modelsize and inferencespeedtoenable real-time control. 3.1. Pre-Trained Vision-Language Models The vision-language models (Chen et al., 2023 a; Driess et al., 2023) that we build on in this work take as input one or more images and produce a sequence of tokens, which conventionally represents natural language text. Such models can perform a wide range of visual interpretation and reasoning tasks, from inferring the composition of an image to answering questions about individual objects and their relations to other objects (Alayrac et al., 2022; Chen et al., 2023 a; Driess et al., 2023; Huang et al., 2023). Representing the knowledge necessary to perform such a wide range of tasks 4 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control requires large models and web-scale datasets. In this work, we adapt two previously proposed VLMs to act as VLA models: Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023). We will refer to vision-language-action versions of these models as RT-2-Pa LI-X and RT-2-Pa LM-E. We leverage instantiations of these models that range in size from billions to tens of billions of parameters. We provide a detailed description of the architecture of these two models in Appendix D. Figure 2 | RT-2 is able to generalize to a variety of real-world situations that require reasoning, symbol underst and ing,andhumanrecognition. Westudy the sechallengingscenariosindetailin Section 4. 3.2. Robot-Action Fine-tuning To enable vision-language models to control a robot, they must be trained to output actions. We take a direct approach to this problem, representing actions as tokens in the model\u2019s output, which are treated in the same way as language tokens. We base our action encoding on the discretization proposed by Brohan et al. (2022) for the RT-1 model. The action space",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 52,
      "paper_id": "rt2",
      "text": "to output actions. We take a direct approach to this problem, representing actions as tokens in the model\u2019s output, which are treated in the same way as language tokens. We base our action encoding on the discretization proposed by Brohan et al. (2022) for the RT-1 model. The action space consists of 6-Do F positional and rotational displacement of the robot end-effector, as well as the level of extension of the robot gripper and a special discrete comm and for terminating the episode, which should be triggered by the policy to signal successful completion. The continuous dimensions (all dimensions except for the discrete termination comm and) are discretized into 256 bins uni for mly. Thus, the robot action can be represented using ordinals of the discrete bins as 8 integer numbers. In order to use these discretized actions to fine tune a vision-language into a vision-language-action model, we need to associate tokens from the model\u2019s existing tokenization with the discrete action bins. This requires 5 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control reserving 256 tokens to serve as action tokens. Which tokens to choose depends on the particular tokenization used by each VLM, which we discuss later in this section. In order to define a target for VLM fine-tuning we convert the action vector into a single string by simply concatenating action tokens for each dimension with a space character: \u201cterminate \u0394pos \u0394pos \u0394pos \u0394rot \u0394rot \u0394rot gripper_extension\u201d. \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc65 \ud835\udc66 \ud835\udc67 A possible instantiation of such a target could be: \u201c1 128 91 241 5 101 127\u201d. The two VLMs that we fine tune in our experiments, Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023), use different tokenizations. For Pa LI-X, integers up to 1000 each have a unique token, so we simply associate the action bins to the token representing the corresponding integer. For the Pa LM-E model, which does not provide this convenient representation of numbers, we simply overwrite the 256 least frequently used tokens to represent the action vocabulary. It is worth noting that training VLMs to override existing tokens with action tokens is a form of symbol tuning (Wei et al., 2023), which has been shown to work well for VLMs in prior work. Taking the action representation described above, we convert our robot data to be suitable for VLM model fine-tuning, where our inputs include robot camera image and textual task description (usingst and ard VQA for mat\u201cQ:whatactionshould the robottaketo[taskinstruction]? A:\u201d),andour output is formatted as a string of numbers/least frequently used tokens representing a robot action. Co-Fine-Tuning. As we will show in our experiments, a key technical detail of the training recipe that improves robot per for mance is co-fine-tuning robotics data with the original web data instead of na\u00efvefinetuningonrobotdataonly. Wenoticethatco-fine-tuningleadstomoregeneralizablepolicies since the policies are exposedtobothabstractvisualconcepts from webscale data and lowlevelrobot actions during fine-tuning, instead of just robot actions. During co-fine-tuning we balance the ratios of robot and web data in each training batch by increasing the sampling weight on the robot dataset. Output Constraint. One important distinction",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 53,
      "paper_id": "rt2",
      "text": "of na\u00efvefinetuningonrobotdataonly. Wenoticethatco-fine-tuningleadstomoregeneralizablepolicies since the policies are exposedtobothabstractvisualconcepts from webscale data and lowlevelrobot actions during fine-tuning, instead of just robot actions. During co-fine-tuning we balance the ratios of robot and web data in each training batch by increasing the sampling weight on the robot dataset. Output Constraint. One important distinction between RT-2 and standard VLMs is that RT-2 is required to output valid action tokens for execution on the real robot. Thus, to ensure that RT-2 outputs valid action tokens during decoding, we constrain its output vocabulary via only sampling valid action tokens when the model is prompted with a robot-action task, whereas the model is still allowed to output the full range of natural language tokens on standard vision-language tasks. 3.3. Real-Time Inference The size of modern VLMs can reach tens or hundreds of billions of parameters (Chen et al., 2023 a; Driess et al., 2023). The largest model trained in this work uses 55 B parameters. It is infeasible to directlyrunsuch model son the standarddesktop-stylemachinesoron-robot GPUscommonlyused for real-time robot control. To the best of our knowledge, our model is the largest ever, by over an order ofmagnitude,used for directclosed-looproboticcontrol,and the reforerequiresa new setofsolutions to enable efficient real-time inference. We develop a protocol that allows us to run RT-2 models on robots by deploying them in a multi-TPU cloud service and querying this service over the network. With this solution,we canachieveasuitablefrequencyofcontrol and alsoservemultiplerobotsusing the same cloud service. The largest model we evaluated, the 55 B parameter RT-2-Pa LI-X-55 B model, can run at a frequency of 1-3 Hz. The smaller version of that model, consisting of 5 B parameters, can run at a frequency of around 5 Hz. 4. Experiments Our experiments focus on real-world generalization and emergent capabilities of RT-2 and aim to answer the following questions: 6 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control 1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects, backgrounds, and environments? 2. Can we observe and measure any emergent capabilities of RT-2? 3. How does the generalization vary with parameter count and other design decisions? 4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models? We evaluate our approach and several baselines with about 6,000 evaluation trajectories in a variety of conditions, which we describe in the following sections. Unless specified otherwise, we use a 7 Do F mobile manipulator with the action space described in Sec. 3.2. We also demonstrate examples of RT-2 execution on the project website: robotics-trans for mer 2.github.io. We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-Pa LI-X is built from 5 B and 55 B Pa LI-X (Chen et al., 2023 a), and (2) RT-2-Pa LM-E is built from 12 B Pa LM-E (Driess et al., 2023). For training, we leverage the original web scale data from Chen et al. (2023 a) and Driess et al. (2023), which consists of visual question answering, captioning, and unstructured interwoven image and text examples. We combine it with the robot demonstration data from Brohan et al. (2022), which was",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 54,
      "paper_id": "rt2",
      "text": "2023). For training, we leverage the original web scale data from Chen et al. (2023 a) and Driess et al. (2023), which consists of visual question answering, captioning, and unstructured interwoven image and text examples. We combine it with the robot demonstration data from Brohan et al. (2022), which was collected with 13 robots over 17 months in an office kitchen environment. Each robot demonstration trajectory is annotated with a natural language instruction that describes the task per for med,consistingofaverbdescribing the skill(e.g.,\u201cpick\u201d,\u201dopen\u201d,\u201cplaceinto\u201d)andoneormore nouns describing the objects manipulated (e.g., \u201c7 up can\u201d, \u201cdrawer\u201d, \u201cnapkin\u201d) (see Appendix B for moredetailson the used data sets). Forall RT-2 trainingrunsweadopt the hyperparameters from the original Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023) papers, including learning rate schedules and regularizations. More training details can be found in Appendix E. Baselines. We comp are our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data. To comp are against a state-of-the-art policy, we use RT-1 (Brohan et al., 2022), a 35 M parameter trans for mer-based model. Tocomp are againststate-of-the-artpretrainedrepresentations,we use VC-1(Majumd are tal.,2023 a) and R 3 M (Nair et al., 2022 b), with policies implemented by training an RT-1 backbone to take their representationsasinput. Tocomp are againsto the rarchitectures for using VLMs,we use MOO(Stone et al., 2023), which uses a VLM to create an additional image channel for a semantic map, which is then fed into an RT-1 backbone. More information is provided in Appendix C. 4.1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects, backgrounds, and environments? (a) Unseen Objects (b) Unseen Backgrounds (c) Unseen Environments Figure 3 | Examplegeneralizationscenariosused for evaluationin Figures 4 and 6 band Tables 4 and 6. To evaluate in-distribution per for mance as well as generalization capabilities, we comp are the RT-2-Pa LI-X and RT-2-Pa LM-E models to the four baselines listed in the previous sections. For the seen tasks category, we use the same suite of seen instructions as in RT-1 (Brohan et al., 2022), which include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for pickingoutof and placingobjectsintodrawers. Note,however,thatthese\u201cin-distribution\u201devaluations still vary the placement of objects and factors such as time of day and robot position, requiring the skills to generalize to realistic variability in the environment. 7 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control Figure 3 showsexamplegeneralizationevaluations,which are splitintounseencategories(objects, backgrounds and environments),and are additionallysplitintoeasy and hardcases. Forunseenobjects, hard cases include harder-to-grasp and more unique objects (such as toys). For unseen backgrounds, hardcasesincludemorevariedbackgrounds and novelobjects. Lastly,forunseenenvironments,hard cases correspond to a more visually distinct office desk environment with monitors and accessories, while the easierenvironmentisakitchensink. Theseevaluationsconsistsofover 280 tasks that focus primarily on pick and placing skills in many diverse scenarios. The list of instructions for unseen categories is specified in Appendix F.2. Figure 4 | Overallper for manceoftwoinstantiationsof RT-2 and base linesacrossseentraining task",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 55,
      "paper_id": "rt2",
      "text": "visually distinct office desk environment with monitors and accessories, while the easierenvironmentisakitchensink. Theseevaluationsconsistsofover 280 tasks that focus primarily on pick and placing skills in many diverse scenarios. The list of instructions for unseen categories is specified in Appendix F.2. Figure 4 | Overallper for manceoftwoinstantiationsof RT-2 and base linesacrossseentraining task saswellas unseenevaluationsmeasuringgeneralizationtonovelobjects,novelbackgrounds,andnovelenvironments. Appendix Table 4 details the fullresults. The evaluation results are shown in Figure 4 and Appendix Table 4. The per for mance on seen tasks is similar between the RT-2 models and RT-1, with other baselines attaining a lower success rate. The difference between the RT-2 models and the baseline is most pronounced in the various generalization experiments, suggesting that the strength of vision-language-action models lies in transferring more generalizable visual and semantic concepts from their Internet-scale pretraining data. Here, on average, both instantiations of RT-2 perform similarly, resulting in \u223c2 x improvement over the next two baselines, RT-1 and MOO, and \u223c6 x better than the other baselines. The Pa LM-E version of RT-2 seems to perform better than the RT-2-Pa LI-X in harder versions of generalization scenarios while under-per for ming on easier ones, resulting in a similar average per for mance. Open Source Language Table Benchmark. To provide an additional point of comparison using open-source baselines and environments, we leverage the open-source Language-Table simulation environment from Lynchetal.(2022). Weco-fine-tuneasmaller Pa LI 3 Bmodelonseveralprediction tasks, including in-domain VQA tasks, for the Language-Table dataset, and evaluate the resulting policy in simulation. For the action prediction task, we discretize and encode actions as text in the format\u201cX Y\u201d,where Xand Yrangebetween{-10,-9,...,+9,+10},andrepresentdelta 2 Dcartesian setpointsof the endeffector. Duetoitsreducedsize,theresulting model can runinferenceatasimilar rate(5 Hz)astheo the rbaselines. Theresultsof this experiment are presentedin Table 1. Weobserve a significant per for mance boost when using our model compared to the baselines, indicating that the VLM-based pre-training together with the expressiveness of the large Pa LI model can be beneficial in other scenarios, in this case, simulation with a different robot. We also show qualitative real-world out-of-distribution behaviors behaviors in Figure 5, demonstrating novel pushing tasks and targeting objects not before seen in this environment. More details about the Language Table experiments can be found in Appendix B and D. 4.2. Can we observe and measure any emergent capabilities of RT-2? Inadditiontoevaluating the generalizationcapabilitiesofvision-language-actionmodels,wealsoaim to evaluate the degree to which such models can enable new capabilities beyond those demonstrated 8 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control Model Language-Table BC-Zero(Jangetal.,2021) 72\u00b13 RT-1(Brohanetal.,2022) 74\u00b113 LAVA(Lynchetal.,2022) 77\u00b14 RT-2-Pa LI-3 B(ours) 90\u00b110 Figure 5 | Real-world out-of-distribution behaviors in the Table 1 | Per for mance on the simulated Language Tableenvironment. Identical RT-2-Pa LI-3 Bmodel Language-Table tasks (Lynch and Ser- checkpointisusedasin Tab.1. manet,2020). intherobot data bytransferringknowledge from the web. Werefertosuchcapabilitiesasemergent,in the sense that they emerge by transferring Internet-scale pretraining. We do not expect such transfer to enable new robotic motions, but we do expect semantic and visual concepts, including relations andnouns,totransfereffectively, evenincaseswherethoseconcepts were notseenin the robot data. Qualitative Evaluations. First, we experiment with our RT-2-Pa LI-X model to determine various emergent capabilities transferred from vision-language concepts. We demonstrate some examples of",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 56,
      "paper_id": "rt2",
      "text": "not expect such transfer to enable new robotic motions, but we do expect semantic and visual concepts, including relations andnouns,totransfereffectively, evenincaseswherethoseconcepts were notseenin the robot data. Qualitative Evaluations. First, we experiment with our RT-2-Pa LI-X model to determine various emergent capabilities transferred from vision-language concepts. We demonstrate some examples of such interactions in Figure 2. We find through our explorations that RT-2 inherits novel capabilities in terms of semantic underst and ing and basic reasoning in the context of the scene. For example accomplishing the task \u201cput strawberry into the correct bowl\u201d requires a nuanced underst and ing of not only what a strawberry and bowl are, but also reasoning in the context the scene to know the strawberry should go with the like fruits. For the task \u201cpick up the bag about to fall off the table,\u201d RT-2 demonstrates physical underst and ing to disambiguate between two bags and recognize the precariously placed object. All the interactions tested in these scenarios have never been seen in the robot data, which points to the transfer of semantic knowledge from vision-language data. Quantitative Evaluations. Toquantify the seemergentcapabilities,wetake the toptwo base lines from the previousevaluations,RT-1 and VC-1,andcomp are the magainst our twomodels: RT-2-Pa LI-X and RT-2-Pa LM-E. To reduce the variance of these experiment, we evaluate all of the methods using the A/B testing framework (Fisher, 1936), where all four models are evaluated one after another in the exact same conditions. We\u2019 split the emergent capabilities of RT-2 into three categories covering axes of reasoning and semantic underst and ing (with examples of each shown in Appendix Figure 8). The first we term symbol underst and ing, which explicitly tests whether the RT-2 policy transfers semantic knowledge from vision-language pretraining that was not present in any of the robot data. Example instructions in this category are \u201cmove apple to 3\u201d or \u201cpush coke can on top of heart\u201d. The second category we termreasoning,whichdemonstratestheabilitytoapplyvariousaspectsofreasoningof the underlying VLMtocontroltasks. These task srequirevisualreasoning(\u201cmove the appletocup with samecolor\u201d), math (\u201cmove X near the sum of two plus one\u201d), and multilingual underst and ing (\u201cmueve la manzana al vaso verde\u201d). We refer to the last category as human recognition tasks, which include tasks such as \u201cmove the coke can to the person with glasses\u201d, to demonstrate human-centric underst and ing and recognition. The full list of instructions used for this evaluation is specified in Appendix F.2. Wepresent the resultsof this experimentin Figure 6 awithall the numericalresultsin Appendix H.2. We observe that our VLA models signifi can tly outperform the baselines across all categories, with our best RT-2-Pa LI-X model achieving more than 3 x average success rate over the next best baseline (RT-1). Wealsonote that while the larger Pa LI-X-based model resultsinbettersymbolunderst and ing, reasoning and person recognition per for mance on average, the smaller Pa LM-E-based model has an edge on tasks that involve math reasoning. We attribute this interesting result to the different pre-trainingmixtureusedin Pa LM-E,whichresultsina model that ismorecapableatmathcalculation than the mostly visually pre-trained Pa LI-X. 9 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control (a)Per for mancecomparisononvariousemergentskillevalu-(b)Ablationsof RT-2-Pa",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 57,
      "paper_id": "rt2",
      "text": "mance on average, the smaller Pa LM-E-based model has an edge on tasks that involve math reasoning. We attribute this interesting result to the different pre-trainingmixtureusedin Pa LM-E,whichresultsina model that ismorecapableatmathcalculation than the mostly visually pre-trained Pa LI-X. 9 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control (a)Per for mancecomparisononvariousemergentskillevalu-(b)Ablationsof RT-2-Pa LI-Xshowcasing the impactofparam- ations(Figure 8)between RT-2 andtwo base lines. etercount and trainingstrategyongeneralization. Figure 6 | Quantitativeper for manceof RT-2 across(6 a)emergentskills and(6 b)size and trainingablations. Appendix Tables 5 and 6 detail the fullnumericalresults. 4.3. How does the generalization vary with parameter count and other design decisions? For this comparison, we use RT-2-Pa LI-X model because of its flexibility in terms of the model size (dueto the natureof Pa LM-E,RT-2-Pa LM-Eisrestrictedtoonlycertainsizesof Pa LMand Vi Tmodels). In particular, we comp are two different model sizes, 5 B and 55 B, as well as three different training routines: training a model from scratch, without using any weights from the VLM pre-training; fine-tuning a pre-trained model using robot action data only; and co-fine-tuning (co-training with fine-tuning), the primary method used in this work where we use both the original VLM training data as well as robotic data for VLM fine-tuning. Since we are mostly interested in the generalization aspects of these models, we remove the seen tasks evaluation from this set of experiments. The results of the ablations are presented in Figure 6 b and Appendix Table 6. First, we observe that training a very large model from scratch results in a very poor per for mance even for the 5 B model. Given this result, we decide to skip the evaluation of an even bigger 55 B Pa LI-X model when trained from scratch. Second, we notice that co-fine-tuning a model (regardless of its size) results in a better generalization per for mance than simply fine-tuning it with robotic data. We attribute this to the fact that keeping the original data around the fine-tuning part of training, allows the model to not for getitspreviousconceptslearnedduring the VLMtraining. Lastly,somewhatunsurprisingly,we notice that the increased size of the model results in a better generalization per for mance. 4.4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models? Inspiredby the chain-of-thoughtpromptingmethodin LLMs(Weietal.,2022),wefine-tuneavariant of RT-2 with Pa LM-E for just a few hundred gradient steps to increase its capability of utilizing language and actions jointly with the hope that it will elicit a more sophisticated reasoning behavior. We augment the data to include an additional \u201cPlan\u201d step, which describes the purpose of the action that the robot is about to take in natural language first, which is then followed by the actual action tokens, e.g. \u201cInstruction: I\u2019m hungry. Plan: pick rxbar chocolate. Action: 1 128 124 136 121 158 111 255.\u201d This data augmentation scheme acts as a bridge between VQA datasets (visual reasoning) and manipulation datasets (generating actions). We qualitatively observe that RT-2 with chain-of-thought reasoning is able to answer more so- phisticated commands due to the fact that it is given a place to plan its actions in natural language first. This is a promising direction that",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 58,
      "paper_id": "rt2",
      "text": "between VQA datasets (visual reasoning) and manipulation datasets (generating actions). We qualitatively observe that RT-2 with chain-of-thought reasoning is able to answer more so- phisticated commands due to the fact that it is given a place to plan its actions in natural language first. This is a promising direction that provides some initial evidence that using LLMs or VLMs as planners(Ahnetal.,2022;Driessetal.,2023)can becombined with low-levelpoliciesinasingle VLA model. Rollouts of RT-2 with chain-of-thought reasoning are shown in Figure 7 and in Appendix I. 10 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control Prompt: Prompt: Prompt: Prompt: Given <img> Instruction: Given <img> Instruction: Given <img> Instruction: Given <img> Instruction: Bring me a drink. Move all the objects Pick the object that is Move the green objects Prediction: together. different from all other together. Plan: pick 7 up can. Prediction: objects Prediction: Action: 1 143 129 123 145 Plan: move green can near Prediction: Plan: move green can near 114 115 127 green rice chip bag. Plan: pick rxbar green rice chip bag. Action: 1 128 126 127 135 chocolate. Action: 1 128 Action: 1 130 129 121 131 123 119 127 129 125 131 125 128 127 127 128 127 Prompt: Given <img> I need to hammer a nail, what object from the scene might be useful? Prediction: Rocks. Action: 1 129 138 122 132 135 106 127 Figure 7 | Rolloutsof RT-2 withchain-of-thoughtreasoning,where RT-2 generatesbothaplan and anaction. 5. Limitations Even though RT-2 exhibits promising generalization properties, there are multiple limitations of this approach. First,althoughweshow that includingweb-scalepretrainingvia VLMsboostsgeneralization over semantic and visual concepts, the robot does not acquire any ability to perform new motions by virtue of including this additional experience. The model\u2019s physical skills are still limited to the distribution of skills seen in the robot data (see Appendix G), but it learns to deploy those skills in new ways. We believe this is a result of the dataset not being varied enough along the axes of skills. An exciting direction for future work is to study how new skills could be acquired through new data collection paradigms such as videos of humans. Second, although we showed we could run large VLA models in real time, the computation cost of these models is high, and as these methods are applied to settings that demand high-frequency control,real-timeinferencemaybecomeamajorbottleneck. Anexcitingdirection for futureresearch is to explore quantization and distillation techniques that might enable such models to run at higher ratesoronlower-costhardw are. Thisisalsoconnectedtoanothercurrentlimitationin that the reare onlyasmallnumberofgenerallyavailable VLMmodels that can beusedtocreate RT-2. Wehope that more open-sourced models will become available (e.g. https://llava-vl.github.io/) and the proprietary ones will open up their fine-tuning APIs, which is a sufficient requirement to build VLA models. 6. Conclusions In this paper, we described how vision-language-action (VLA) models could be trained by combining vision-language model (VLM) pretraining with robotic data. We then presented two instantiations of VLAs based on Pa LM-E and Pa LI-X, which we call RT-2-Pa LM-E and RT-2-Pa LI-X. These models are co- fine-tuned with robotic trajectory data to output robot actions, which are represented as text tokens. We showed that our approach",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 59,
      "paper_id": "rt2",
      "text": "pretraining with robotic data. We then presented two instantiations of VLAs based on Pa LM-E and Pa LI-X, which we call RT-2-Pa LM-E and RT-2-Pa LI-X. These models are co- fine-tuned with robotic trajectory data to output robot actions, which are represented as text tokens. We showed that our approach results in very per for mant robotic policies and, more importantly, leads to a signifi can tly better generalization per for mance and emergent capabilities inherited from 11 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control web-scale vision-language pretraining. We believe that this simple and general approach shows a promise of robotics directly benefiting from better vision-language models, which puts the field of robot learning in a strategic position to further improve with advancements in other fields. Acknowledgments Wewouldliketoacknowledge Fred Alcober,Jodi Lynn Andres,Carolina Parada,Joseph Dabis,Rochelle Dela Cruz, Jessica Gomez, Gavin Gonzalez, John Guilyard, Tomas Jackson, Jie Tan, Scott Lehrer, Dee M, Utsav Malla, Sarah Nguyen, Jane Park, Emily Perez, Elio Prado, Jornell Quiambao, Clayton Tan, Jodexty Therlonge, Eleanor Tomlinson, Wenxuan Zhou, and the greater Google Deep Mind team for their feedback and contributions. 12 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control References M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,K.Gopalakrishnan,K.Hausman, A.Herzog,etal. Doas Ican,notas Isay: Groundinglanguageinroboticaf for dances. ar Xivpreprint ar Xiv:2204.01691, 2022. J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Milli can, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. ar Xiv preprint ar Xiv:2204.14198, 2022. R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin,A.Passos,S.Shakeri,E.Taropa,P.Bailey,Z.Chen, et al. Palm 2 technical report. ar Xiv preprint ar Xiv:2305.10403, 2023. A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. Rt-1: Robotics trans for mer for real-world control at scale. ar Xiv preprint ar Xiv:2212.06817, 2022. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. D.Cer,Y.Yang,S.Kong,N.Hua,N.Limtiaco,R.S.John,N.Constant,M.Guajardo-Cespedes,S.Yuan, C. Tar, Y. Sung, B. Strope, and R. Kurzweil. Universal sentence encoder. Co RR, abs/1803.11175, 2018. URL http://arxiv.org/abs/1803.11175. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. ar Xiv preprint ar Xiv:2107.03374, 2021. X.Chen,J.Djolonga,P.Padlewski,B.Mustafa,S.Changpinyo,J.Wu,C.R.Ruiz,S.Goodman,X.Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu, M. Joshi, B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu, K. Rong, A. Kolesnikov, M. Seyedhosseini, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali-x: On scaling up a multilingual vision and language model, 2023 a. X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia,",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 60,
      "paper_id": "rt2",
      "text": "vision and language model, 2023 a. X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner, A.Angelova,X.Zhai,N.Houlsby,and R.Soricut. Pali: Ajointly-scaledmultilinguallanguage-image model, 2023 b. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. ar Xiv preprint ar Xiv:2110.14168, 2021. Z.J.Cui,Y.Wang,N.Muhammad,L.Pinto,etal. Fromplaytopolicy: Conditionalbehaviorgeneration from uncurated robot data. ar Xiv preprint ar Xiv:2210.10047, 2022. S. Dasari and A. Gupta. Trans for mers for one-shot visual imitation. In Conference on Robot Learning, pages 2071\u20132084. PMLR, 2021. S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn. Robonet: Large-scale multi-robot learning. In Conference on Robot Learning, 2019. 13 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control M.Dehghani,J.Djolonga,B.Mustafa,P.Padlewski,J.Heek,J.Gilmer,A.Steiner,M.Caron,R.Geirhos, I.Alabdulmohsin,R.Jenatton,L.Beyer,M.Tschannen,A.Arnab,X.Wang,C.Riquelme,M.Minderer, J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink, A.Kolesnikov,F.Paveti\u0107,D.Tran,T.Kipf,M.Lu\u010di\u0107,X.Zhai,D.Keysers,J.Harmsen,and N.Houlsby. Scaling vision trans for mers to 22 billion parameters, 2023. D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. ar Xiv preprint ar Xiv:2303.03378, 2023. M. Du, S. Nair, D. Sadigh, and C. Finn. Behavior retrieval: Few-shot imitation learning by querying unlabeled datasets. ar Xiv preprint ar Xiv:2304.08742, 2023 a. Y.Du,K.Konyushkova,M.Denil,A.Raju,J.Landon,F.Hill,N.de Freitas,and S.Cabi. Vision-language models as success detectors. ar Xiv preprint ar Xiv:2303.07280, 2023 b. C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786\u20132793. IEEE, 2017. C.Finn,T.Yu,T.Zhang,P.Abbeel,and S.Levine. One-shotvisualimitationlearningviameta-learning. In Conference on robot learning, pages 357\u2013368. PMLR, 2017. R. A. Fisher. Design of experiments. British Medical Journal, 1(3923):554, 1936. S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Clip on wheels: Zero-shot object navigation as object localization and exploration. ar Xiv preprint ar Xiv:2203.10421, 2022. Z.Gan,L.Li,C.Li,L.Wang,Z.Liu,J.Gao,etal. Vision-languagepre-training: Basics,recentadvances, and future trends. Foundations and Trends\u00ae in Computer Graphics and Vision, 14(3\u20134):163\u2013352, 2022. G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Open-vocabulary image segmentation. ar Xiv preprint ar Xiv:2112.12143, 2021. K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M.",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 61,
      "paper_id": "rt2",
      "text": "C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Sou the rland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Z. Zhao, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu, C.V.Jawahar,H.Joo,K.Kitani,H.Li,R.Newcombe,A.Oliva,H.S.Park,J.M.Rehg,Y.Sato,J.Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik. Ego 4 d: Around the world in 3,000 hours of egocentric video, 2022. X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language knowledge distillation. ar Xiv preprint ar Xiv:2104.13921, 2021. N. Hansen, R. Jangir, Y. Sun, G. Aleny\u00e0, P. Abbeel, A. A. Efros, L. Pinto, and X. Wang. Self-supervised policy adaptation during deployment. ar Xiv preprint ar Xiv:2007.04309, 2020. Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. Language models are general-purpose interfaces. ar Xiv preprint ar Xiv:2206.06336, 2022. 14 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep rein for cement learning via transfer-learning from text. ar Xiv preprint ar Xiv:2005.09382, 2020. S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu, et al. Language is not all you need: Aligning perception with language models. ar Xiv preprint ar Xiv:2302.14045, 2023. W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionableknowledge for embodiedagents. In International Conferenceon Machine Learning,pages 9118\u20139147. PMLR, 2022. S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation learning. In Conference on robot learning, pages 783\u2013795. PMLR, 2018. E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero- shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991\u20131002. PMLR, 2021. Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,and L.Fan. Vima: General robot manipulation with multimodal prompts. ar Xiv preprint ar Xiv:2210.03094, 2022. L. P. Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915\u2013916, 2020. S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang. Language-driven representation learning for robotics. ar Xiv preprint ar Xiv:2302.12766, 2023. A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Roll and, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. ar Xiv preprint ar Xiv:2304.02643, 2023. I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep rein for cement learning from pixels. ar Xiv preprint ar Xiv:2004.13649, 2020. M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Rein for cement learning with augmented data. Advances in neural information processing systems, 33:19884\u201319895, 2020 a. M.Laskin,A.Srinivas,and P.Abbeel.Curl: Contrastiveunsupervisedrepresentationsforrein for cement learning. In International Conference on Machine Learning,",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 62,
      "paper_id": "rt2",
      "text": "learning from pixels. ar Xiv preprint ar Xiv:2004.13649, 2020. M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Rein for cement learning with augmented data. Advances in neural information processing systems, 33:19884\u201319895, 2020 a. M.Laskin,A.Srinivas,and P.Abbeel.Curl: Contrastiveunsupervisedrepresentationsforrein for cement learning. In International Conference on Machine Learning, pages 5639\u20135650. PMLR, 2020 b. S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,and D.Quillen. Learningh and-eyecoordination for robotic grasping with deep learning and large-scale data collection. The International journal of robotics research, 37(4-5):421\u2013436, 2018. A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. ar Xiv preprint ar Xiv:2206.14858, 2022. J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ar Xiv preprint ar Xiv:2301.12597, 2023. L.H.Li,M.Yatskar,D.Yin,C.-J.Hsieh,and K.-W.Chang. Visualbert: Asimple and per for mant base line for vision and language. ar Xiv preprint ar Xiv:1908.03557, 2019. H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained vision- language models. ar Xiv preprint ar Xiv:2210.13431, 2022. 15 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. ar Xiv preprint ar Xiv:2005.07648, 2020. C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,and P.Florence.Interactive language: Talking to robots in real time. ar Xiv preprint ar Xiv:2210.06407, 2022. Y.J.Ma,S.Sodhani,D.Jayaraman,O.Bastani,V.Kumar,and A.Zhang. Vip: Towardsuniversalvisual reward and representation via value-implicit pre-training. ar Xiv preprint ar Xiv:2210.00030, 2022. Y. J. Ma, W. Liang, V. Som, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. Liv: Language-image representations and rewards for robotic control. ar Xiv preprint ar Xiv:2306.00958, 2023. J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with syn the tic point clouds and analytic grasp metrics. ar Xiv preprint ar Xiv:1703.09312, 2017. A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,J.Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? ar Xiv preprint ar Xiv:2303.18240, 2023 a. A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,J.Malik, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? ar Xiv preprint ar Xiv:2303.18240, 2023 b. O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation learning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205\u201311212, 2022. M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection with vision trans for mers. ar Xiv preprint ar Xiv:2205.06230, 2022. Y.Mu,Q.Zhang,M.Hu,W.Wang,M.Ding,J.Jin,B.Wang,J.Dai,Y.Qiao,and P.Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. ar Xiv preprint ar Xiv:2305.15021, 2023. S.Nair,E.Mitchell,K.Chen,S.Savarese,C.Finn,etal. Learninglanguage-conditionedrobotbehavior fromoffline data and crowd-sourcedannotation. In Conferenceon Robot Learning,pages 1303\u20131315. PMLR, 2022 a. S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R 3 m: A universal visual representation for robot manipulation. ar Xiv preprint ar Xiv:2203.12601, 2022 b. Open AI. Gpt-4 technical report, 2023. J.Pari,N.M.Shafiullah,S.P.Arunachalam,and L.Pinto.",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 63,
      "paper_id": "rt2",
      "text": "S.Nair,E.Mitchell,K.Chen,S.Savarese,C.Finn,etal. Learninglanguage-conditionedrobotbehavior fromoffline data and crowd-sourcedannotation. In Conferenceon Robot Learning,pages 1303\u20131315. PMLR, 2022 a. S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R 3 m: A universal visual representation for robot manipulation. ar Xiv preprint ar Xiv:2203.12601, 2022 b. Open AI. Gpt-4 technical report, 2023. J.Pari,N.M.Shafiullah,S.P.Arunachalam,and L.Pinto. Thesurprisingeffectivenessofrepresentation learning for visual imitation. ar Xiv preprint ar Xiv:2112.01511, 2021. L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50 k tries and 700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406\u20133413. IEEE, 2016. S.Polu,J.M.Han,K.Zheng,M.Baksys,I.Babuschkin,and I.Sutskever. Formalma the maticsstatement curriculum learning. ar Xiv preprint ar Xiv:2202.01344, 2022. 16 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-fit: State-covering self-supervised rein for cement learning. ar Xiv preprint ar Xiv:1903.03698, 2019. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Interna- tional Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021. S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,Y.Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. ar Xiv preprint ar Xiv:2205.06175, 2022. M.Ryoo,A.Piergiovanni,A.Arnab,M.Dehghani,and A.Angelova. Tokenlearner: Adaptivespace-time tokenization for videos. Advancesin Neural Information Processing Systems,34:12786\u201312797,2021. D.Shah,B.Osi\u0144ski,b.ichter,and S.Levine. Lm-nav: Roboticnavigation with largepre-trainedmodels of language, vision, and action. In K. Liu, D. Kulic, and J. Ichnowski, editors, Proceedings of The 6 th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 492\u2013 504.PMLR,14\u201318 Dec 2023. URLhttps://proceedings.mlr.press/v 205/shah 23 b.html. R. Shah and V. Kumar. Rrl: Resnet as representation for rein for cement learning. ar Xiv preprint ar Xiv:2107.03380, 2021. M.Shridhar,L.Manuelli,and D.Fox. Cliport: What and wherepathways for roboticmanipulation. In Proceedings of the 5 th Conference on Robot Learning (Co RL), 2021. M.Shridhar,L.Manuelli,and D.Fox. Cliport: What and wherepathways for roboticmanipulation. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022 a. M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task trans for mer for robotic manipula- tion. ar Xiv preprint ar Xiv:2209.05451, 2022 b. I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. In ICRA, 2023. M. H. Smith and L. S. Coles. Design of a low cost, general purpose robot. In IJCAI, pages 324\u2013336, 1973. A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language models. ar Xiv preprint ar Xiv:2303.00905, 2023. T. Sumers, K. Marino, A. Ahuja, R. Fergus, and I. Dasgupta. Distilling internet-scale vision-language models into embodied agents. ar Xiv preprint ar Xiv:2301.12507, 2023. Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri, T. Schuster, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler. Ul 2: Unifying language learning paradigms, 2023. S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot.",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 64,
      "paper_id": "rt2",
      "text": "Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri, T. Schuster, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler. Ul 2: Unifying language learning paradigms, 2023. S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot. Res, 2:20, 2023. J.Wang,Z.Yang,X.Hu,L.Li,K.Lin,Z.Gan,Z.Liu,C.Liu,and L.Wang.Git: Agenerativeimage-to-text trans for mer for vision and language. ar Xiv preprint ar Xiv:2205.14100, 2022. J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. ar Xiv preprint ar Xiv:2201.11903, 2022. 17 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le. Symbol tuning improves in-context learning in language models, 2023. J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser. Tidybot: Personalizedrobotassistance with largelanguagemodels.ar Xivpreprintar Xiv:2305.05658, 2023. T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson. Robotic skill acquisition via instruction augmentation with vision-language models. ar Xiv preprint ar Xiv:2211.11736, 2022 a. T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control. ar Xiv preprint ar Xiv:2203.06173, 2022 b. S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. In Conference on Robot Learning, pages 1992\u20132005. PMLR, 2021. K.-T.Yu,M.Bauza,N.Fazeli,and A.Rodriguez. Morethanamillionwaystobepushed.ahigh-fidelity experimental dataset of planar pushing. In 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 30\u201337. IEEE, 2016. T.Yu,C.Finn,A.Xie,S.Dasari,T.Zhang,P.Abbeel,and S.Levine. One-shotimitation from observing humans via domain-adaptive meta-learning. ar Xiv preprint ar Xiv:1802.01557, 2018. X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision trans for mers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113, 2022. X.Zhang,Y.Ding,S.Amiri,H.Yang,A.Kaminski,C.Esselink,and S.Zhang. Groundingclassical task planners via vision-language models. ar Xiv preprint ar Xiv:2304.08587, 2023. 18 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control A. Contributions \u2022 Training and Evaluations (designing and executing procedures for training models, evalu- ating models in simulation and the real world, running ablations for algorithm design choices): Yevgen Chebotar, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Alexander Herzog, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Paul Wohlhart, Fei Xia, Ted Xiao, and Tianhe Yu. \u2022 Network Architecture (designing and implementing model network modules, working on tokenization of actions, enabling inference of the model networks during experiments): Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Danny Driess, Pete Florence, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee, Henryk Michalewski, Igor Mordatch, Kanishka Rao, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Jialin Wu, Fei Xia, Ted Xiao, and Tianhe Yu. \u2022 Data Collection (collecting data on real robots, running real robot evaluations, executing operations required for running real robots): Noah Brown, Justice",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 65,
      "paper_id": "rt2",
      "text": "Irpan, Isabel Leal, Lisa Lee, Henryk Michalewski, Igor Mordatch, Kanishka Rao, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Jialin Wu, Fei Xia, Ted Xiao, and Tianhe Yu. \u2022 Data Collection (collecting data on real robots, running real robot evaluations, executing operations required for running real robots): Noah Brown, Justice Carbajal, Tianli Ding, Krista Reymann, Grecia Salazar, Pierre Sermanet, Jaspiar Singh, Huong Tran, Stefan Welker, and Sichun Xu. \u2022 Leadership (leading the project efforts, managing the project staff, advising on project directions): Yevgen Chebotar, Chelsea Finn, Karol Hausman, Brian Ichter, Sergey Levine, Yao Lu, Igor Mordatch, Kanishka Rao, Pannag Sanketi, Radu Soricut, Vincent Vanhoucke, and Tianhe Yu. \u2022 Paper (working on the paper manuscript, designing paper visualizations and figures): Yevgen Chebotar, Danny Driess, Chelsea Finn, Pete Florence, Karol Hausman, Brian Ichter, Lisa Lee,Sergey Levine,Igor Mordatch,Karl Pertsch,Quan Vuong,Fei Xia,Ted Xiao,and Tianhe Yu. \u2022 Infrastructure (working on infrastructure and code base backbone needed for training models, running experiments, storing and accessing data): Anthony Brohan,Yevgen Chebo- tar,Danny Driess,Kehang Han,Jasmine Hsu,Brian Ichter,Alex Irpan,Nikhil Joshi,Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Yao Lu, Igor Mordatch, Quan Vuong, Ayzaan Wahid, Fei Xia, Ted Xiao, Peng Xu, and Tianhe Yu. B. Datasets The vision-language datasets are based on the dataset mixtures from Chen et al. (2023 b) and Driess et al. (2023). The bulk of this data consists of the Web LI dataset, which is around 10 B image-text pairs across 109 languages, filtered to the top 10% scoring cross-modal similarity examples to give 1 B training examples. Many other captioning and vision question answering datasets are included as well, and more info on the dataset mixtures can be found in Chen et al. (2023 b) for RT-2-Pa LI-X, and Driessetal.(2023)for RT-2-Pa LM-E.Whenco-fine-tuning RT-2-Pa LI-X,wedonotuse the Episodic Web LI dataset described by Chen et al. (2023 a). The robotics dataset is based on the dataset from Brohan et al. (2022). This consists of demon- stration episodes collected with a mobile manipulation robot. Each demonstration is annotated with anaturallanguageinstruction from oneofsevenskills: \"Pick Object\",\"Move Object Near Object\", \"Place Object Upright\",\"Knock Object Over\",\"Open Drawer\",\"Close Drawer\",\"Place Objectinto Receptacle\", and \"Pick Object from Receptacle and place on the counter\". Further details can be found in Brohan et al. (2022). RT-2-Pa LI-X weights the robotics dataset such that it makes up about 50% of the training mixture 19 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control for co-fine-tuning. RT-2-Pa LM-E weights the robotics dataset to be about 66% of the training mixture. For the resultson Language-Tablein Table 1,our model istrainedon the Language-Table data sets from Lynch et al. (2022). Our model is co-fine-tuned on several prediction tasks: (1) predict the action, given two consecutive image frames and a text instruction; (2) predict the instruction, given image frames; (3) predict the robot arm position, given image frames; (4) predict the number of timesteps between given image frames; and (5)predict whether the task wassuccessful, given image frames and the instruction. C. Baselines We comp are our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 66,
      "paper_id": "rt2",
      "text": "robot arm position, given image frames; (4) predict the number of timesteps between given image frames; and (5)predict whether the task wassuccessful, given image frames and the instruction. C. Baselines We comp are our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data. \u2022 RT-1: Robotics Trans for mer 1 Brohan et al. (2022) is a trans for mer-based model that achieved state-of-the-art per for mance on a similar suite of tasks when it was published. The model does not use VLM-based pre-training so it provides an important data point demonstrating whether VLM-based pre-training matters. \u2022 VC-1: VC-1 Majumdar et al. (2023 a) is a visual foundation model that uses pre-trained visual representations specifically designed for robotics tasks. We use pre-trained representations from the VC-1 Vi T-L model. Since VC-1 does not include language conditioning, we add this by separatelyembedding the languagecomm and via Universal Sentence Encoder Ceretal.(2018) to enable comparison to our method. In particular, we concatenate the resulting language embedding tokens to the image tokens produced by VC-1, and pass the concatenated token sequences through token learner Ryoo et al. (2021). The token sequences produced by token learner are then consumed by an RT-1 decoder-only trans for mer model to predict robot action tokens. We train the VC-1 baseline end-to-end and unfreeze the VC-1 weights during training, since this led to far better results than using frozen VC-1 weights. \u2022 R 3 M: R 3 M Nair et al. (2022 b) is a similar method to VC-1 in that R 3 M uses pre-trained visual-language representations to improve policy training. In this case the authors use Ego 4 D dataset Graumanetal.(2022)ofhumanactivitiestolearn the representation that isusedby the policy. Both VC-1 and R 3 M test different state-of-the-art representation learning methods as an alternative to using a VLM. To obtain a language-conditioned policy from the R 3 M pretrained representation, we follow the same procedure as described above for VC-1, except we use the R 3 M Res Net 50 model to obtain the image tokens, and unfreeze it during training. \u2022 MOO: MOO Stone et al. (2023) is an object-centric approach, where a VLM is first used to specify the objectofinterestina for mofasingle,coloredpixelin the originalimage. Thispixel- modified image is then trained with an end-to-end policy to accomplish a set of manipulation tasks. This baseline corresponds to a situation where a VLM is used as a separate module that enhances perception but its representations are not used for policy learning. D. VLMs for RT-2 The Pa LI-X model architecture consists of a Vi T-22 B Dehghani et al. (2023) to process images, which canacceptsequencesof\ud835\udc5bimages,leadingto\ud835\udc5b\u00d7\ud835\udc58tokensperimage,where\ud835\udc58isthenumberofpatches perimage. Theimagetokenspassingoveraprojectionlayeris the nconsumedbyanencoder-decoder backbone of 32 B parameters and 50 layers, similar to UL 2 Tay et al. (2023), which jointly processes text and images as embeddings to generate output tokens in an auto-regressive manner. The text 20 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control inputusuallyconsistsof the typeof task and anyadditionalcontext(e.g.,\"Generatecaptionin \u27e8lang\u27e9\" for captioning tasks or \"Answer in \u27e8lang\u27e9: question\" for VQA tasks). The",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 67,
      "paper_id": "rt2",
      "text": "2 Tay et al. (2023), which jointly processes text and images as embeddings to generate output tokens in an auto-regressive manner. The text 20 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control inputusuallyconsistsof the typeof task and anyadditionalcontext(e.g.,\"Generatecaptionin \u27e8lang\u27e9\" for captioning tasks or \"Answer in \u27e8lang\u27e9: question\" for VQA tasks). The Pa LI-3 B model trained on Language-Table (Table 1) uses a smaller Vi T-G/14 (Zhai et al., 2022) (2 B parameters) to process images, and UL 2-3 B (Tay et al., 2023) for the encoder-decoder network. The Pa LM-E model is based on a decoder-only LLM that projects robot data such as images and text into the language token space and outputs text such as high-level plans. In the case of the used Pa LM-E-12 B, the visual model used to project images to the language embedding space is a Vi T-4 B Chen et al. (2023 b). The concatenation of continuous variables to textual input allows Pa LM-E to be fully multimodal, accepting a wide variety of inputs such as multiple sensor modalities, object-centric representations, scene representations and object entity referrals. E. Training Details We perform co-fine-tuning on pre-trained models from the Pa LI-X (Chen et al., 2023 a) 5 B & 55 B model, Pa LI (Chen et al., 2023 b) 3 B model and the Pa LM-E (Driess et al., 2023) 12 B model. For RT-2-Pa LI-X-55 B, we use learning rate 1 e-3 and batch size 2048 and co-fine-tune the model for 80 K gradient steps whereas for RT-2-Pa LI-X-5 B, we use the same learning rate and batch size and co-fine-tune the model for 270 K gradient steps. For RT-2-Pa LM-E-12 B, we use learning rate 4 e-4 and batch size 512 to co-fine-tune the model for 1 M gradient steps. Both models are trained with the next token prediction objective, which corresponds to the behavior cloning loss in robot learning. For RT-2-Pa LI-3 B model used for Language-Table results in Table 1, we use learning rate 1 e-3 and batch size 128 to co-fine-tune the model for 300 K gradient steps. F. Evaluation Details F.1. Evaluation Scenarios Forstudying the emergentcapabilitiesof RT-2 inaquantitativemanner, westudyvariouschallenging semanticevaluationscenarios that aimtomeasurecapabilitiessuchasreasoning,symbolunderst and- ing, and human recognition. A visual overview of a subset of these scenes is provided in Figure 8, and the full list of instructions used for quantiative evalution is shown in Table 3. F.2. Evaluation Instructions Table 2 listsnaturallanguageinstructionsusedin model evaluations for unseenobjects,backgrounds, and environments. Each instruction was run between 1-5 times, depending on the number of total instructions in that evaluation set. Table 3 lists natural language instructions used to evaluate quantitative emergent evals. Each instruction was run 5 times. 21 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control (a) Reasoning \u201cmove coke can to \u201cmove apple to cup with \u201cmove banna near the \u201cd\u00e9placer les frites verts \u201cpick a healthy drink\u201d Taylor Swift\u201d same color\u201d sum of two plus one\u201d dans la tasse rouge\u201d \u201cmove coke can to person with glasses\u201d \u201cmove coke can \u201cput coke can close \u201cmove banana to \u201cmove apple to tree\u201d near Y\u201d to",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 68,
      "paper_id": "rt2",
      "text": "with \u201cmove banna near the \u201cd\u00e9placer les frites verts \u201cpick a healthy drink\u201d Taylor Swift\u201d same color\u201d sum of two plus one\u201d dans la tasse rouge\u201d \u201cmove coke can to person with glasses\u201d \u201cmove coke can \u201cput coke can close \u201cmove banana to \u201cmove apple to tree\u201d near Y\u201d to dog\u201d android\u201d (c) Human (b) Symbol Underst and ing Recognition Figure 8 | Anoverviewofsomeoftheevaluationscenariosusedtostudy the emergentcapabilitiesof RT-2. They focus on three broad categories, which are (a) reasoning, (b) symbol underst and ing, and (c) human recognition. Thevisualizedinstructions are asubsetof the fullinstructions,which are listedin Appendix F.2. Task Group Tasks Symbol Underst and- movecoke can near X,movecoke can near 3,movecoke can near Y ing: Symbol 1 Symbol Underst and- moveappletotree,moveappletoduck,moveappletoapple,moveapple ing: Symbol 2 tomatchingcard Symbol Underst and- putcoke can closetodog,pushcoke can ontopofheart,placecoke can ing: Symbol 3 abovestar Reasoning: Math move banana to 2, move banna near the sum of two plus one, move banana near the answer of three times two, move banana near the smallestnumber Reasoning: Logos movecuptogoogle,movecupto and roid,movecuptoyoutube,move cuptoasearchengine,movecuptoaphone Reasoning: Nutrition getmeahealthysnack,pickahealthydrink,pickupasweetdrink,move thehealthysnackto the healthydrink,pickupasaltysnack Reasoning: Color and move apple to cup with same color, move apple to cup with different Multilingual color,movegreenchipstomatchingcolorcup,moveappletovasoverde, Bewegen Sieden Apfelindierote Tasse,movegreenchipstovasorojo, muevelamanzanaalvasoverde,d\u00e9placerlesfritesvertsdanslatasse rouge Person Recognition: movecoke can totaylorswift,movecoke can totomcruise,movecoke Celebrities cantosnoopdog Person Recognition: movecoke can toperson with glasses,movecoke can totheman with Celeb A whitehair,movecoke can tothebrunettelady Table 3 | Naturallanguageinstructionsused for quantitativeemergentevalutions. 22 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control G. Example Failure Cases In Fig.9 weprovideexamplesofanotabletypeoffailurecasein the Language Tablesetting,with the RT-2 model not generalizing to unseen object dynamics. In these cases, although the model is able to correctly attend to the language instruction and move to the first correct object, it is not able to control the challenging dynamics of these objects, which are signifi can tly different than the small set ofblockobjects that have been seenin this environment Lynchetal.(2022). Thenpensimplyrollsoff the table (Fig. 9, left), while the banana\u2019s center-of-mass is far from where the robot makes contact (Fig. 9, right). We note that pushing dynamics are notoriously difficult to predict and control Yu et al. (2016). We hypo the size that greater generalization in robot-environment interaction dynamics may be possible by further scaling the datasets across diverse environments and objects \u2013 for example, in this case, datasets that include similar types of more diverse pushing dynamics Dasari et al. (2019). In addition, despite RT-2\u2019s promising per for mance on real world manipulation tasks in qualitative and quantitative emergent evaluations, we still find numerous notable failure cases. For example, with the current training dataset composition and training method, RT-2 seemed to perform poorly at: \u2022 Grasping objects by specific parts, such as the handle \u2022 Novel motions beyond what was seen in the robot data, such as wiping with a towel or tool use \u2022 Dexterous or precise motions, such as folding a towel \u2022 Extended reasoning requiring multiple layers of indirection Push the red marker to the video game controller Push the banana to the apple Figure 9 | Qualitativeexamplefailurecasesin the real-worldfailingtogeneralizetounseenobjectdynamics. H. Quantitative Experimental Results H.1. Overall Per for mance, for",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 69,
      "paper_id": "rt2",
      "text": "use \u2022 Dexterous or precise motions, such as folding a towel \u2022 Extended reasoning requiring multiple layers of indirection Push the red marker to the video game controller Push the banana to the apple Figure 9 | Qualitativeexamplefailurecasesin the real-worldfailingtogeneralizetounseenobjectdynamics. H. Quantitative Experimental Results H.1. Overall Per for mance, for Section 4.1 Table 4 lists our quantitative overall evaluation results. We find that RT-2 performs as well or better than baselines on seen tasks and signifi can tly outperforms baselines on generalization to unseen objects, backgrounds, and environments. Model Seen Tasks Unseen Objects Unseen Backgrounds Unseen Environments Unseen Average Easy Hard Easy Hard Easy Hard R 3 M(Nairetal.,2022 b) 45 32 14 13 9 0 2 12 VC-1(Majumd are tal.,2023 a) 63 34 10 13 3 0 0 10 RT-1(Brohanetal.,2022) 92 31 43 71 9 26 14 32 MOO(Stoneetal.,2023) 75 58 48 38 41 19 3 35 RT-2-Pa LI-X-55 B(ours) 91 70 62 96 48 63 35 62 RT-2-Pa LM-E-12 B 1(ours) 93 84 76 75 71 36 33 62 Table 4 | Overallper for manceoftwoinstantiationsof RT-2 and base linesacrossseentraining task saswellas unseenevaluationsmeasuringgeneralizationtonovelobjects,novelbackgrounds,andnovelenvironments. 23 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control H.2. Emergent Evaluation, for Section 4.2 Table 5 lists all of our quantitative emergent evaluation results. We find that RT-2 performs 2 x to 3 x better than RT-1 on these new instructions, without any additional robotic demonstrations. This showcases how our method allows us to leverage capabilities from pretraining on web-scale vision-language datasets. Model Symbol Underst and ing Reasoning Person Recognition Average Symbol 1 Symbol 2 Symbol 3 Average Math Logos Nutrition Color/Multilingual Average Celebrities Celeb A Average VC-1(Majumd are tal.,2023 a) 7 25 0 11 0 8 20 13 10 20 7 13 11 RT-1(Brohanetal.,2022) 27 20 0 16 5 0 32 28 16 20 20 20 17 RT-2-Pa LI-X-55 B(ours) 93 60 93 82 25 52 48 58 46 53 53 53 60 RT-2-Pa LM-E-12 B(ours) 67 20 20 36 35 56 44 35 43 33 53 43 40 Table 5 | Per for manceof RT-2 and base linesonquantitativeemergentevaluations. H.3. Size and Training Ablations, for Section 4.3 Table 6 detailsquantitativeresults for ablationsacross model size and trainingapproach. Acrosseach, we see that model size plays an important role in per for mance and that co-fine-tuning outperforms fine-tuning, which outperforms training from scratch. Model Size Training Unseen Objects Unseen Backgrounds Unseen Environments Average Easy Hard Easy Hard Easy Hard RT-2-Pa LI-X 5 B fromscratch 0 10 46 0 0 0 9 RT-2-Pa LI-X 5 B fine-tuning 24 38 79 50 36 23 42 RT-2-Pa LI-X 5 B co-fine-tuning 60 38 67 29 44 24 44 RT-2-Pa LI-X 55 B fine-tuning 60 62 75 38 57 19 52 RT-2-Pa LI-X 55 B co-fine-tuning 70 62 96 48 63 35 63 Table 6 | Ablationsof RT-2 showcasing the impactofparametercount and trainingstrategyongeneralization. I. Additional Chain-Of-Thought Reasoning Results Wepresentadditionalexamplesofchain-of-thoughtreasoningrolloutsaccomplished with RT-2-Pa LM- E, as described in Sec. 4.4, in Figure 10. 1 Theoriginalpre-training data mixtureusedin Pa LM-E-12 B(asdescribedin Driessetal.(2023))includesrobotimages forhigh-level VQAplanningtasks that can besimilartoimagesencounteredingeneralizationscenarios.However,noneof thosetrainingexamplesincludelow-levelactions that are evaluatedin this experiment. 24 RT-2:Vision-Language-Action Models Transfer Web",
      "start_pos": 10164,
      "end_pos": 10676
    },
    {
      "chunk_id": 70,
      "paper_id": "rt2",
      "text": "| Ablationsof RT-2 showcasing the impactofparametercount and trainingstrategyongeneralization. I. Additional Chain-Of-Thought Reasoning Results Wepresentadditionalexamplesofchain-of-thoughtreasoningrolloutsaccomplished with RT-2-Pa LM- E, as described in Sec. 4.4, in Figure 10. 1 Theoriginalpre-training data mixtureusedin Pa LM-E-12 B(asdescribedin Driessetal.(2023))includesrobotimages forhigh-level VQAplanningtasks that can besimilartoimagesencounteredingeneralizationscenarios.However,noneof thosetrainingexamplesincludelow-levelactions that are evaluatedin this experiment. 24 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control User Pick up the object that is different from all other objects Plan: Pick rxbar chocolate User Move the green objects together Plan: Move green can near green rice chip bag User Move the blue objects together Plan: Move blue chip bag near pepsi User I am sleepy, bring me a drink to help. Plan: Pick redbull can Figure 10 | Additionalexamplesof RT-2 withchain-of-thoughtreasoning 25 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control Task Group Tasks Unseen Objects pickbanana,movebanananearcoke can,moveorange can nearbanana, (Easy) pickoreo,moveoreonearapple,moveredbull can nearoreo,pickpear, pickcoconutwater,movepearnearcoconutwater,movepepsi can near pear Unseen Objects pickcoldbrew can,picklargeorangeplate,pickchewtoy,picklargeten- (Hard) nisball,pickbirdornament,pickfishtoy,pickgingerlemonkombucha, pick egg separator, pick wrist watch, pick green sprite can, pick blue microfibercloth,pickyellowpear,pickpretzelchipbag,pickdisinfectant wipes,pickpineapplehintwater,pickgreencup,pickpicklesnack,pick small blue plate, pick small orange rolling pin, pick octopus toy, pick catniptoy Unseen Back- pickgreenjalapenochipbag,pickorange can,pickpepsi can,pick 7 up grounds(Easy) can, pick apple, pick blue chip bag, pick orange, pick 7 up can, move orangenearsink,pickcoke can,picksponge,pickrxbarblueberry Unseen Back- pick wrist watch, pick egg separator, pick green sprite can, pick blue grounds(Hard) microfibercloth,pickyellowpear,pickpretzelchipbag,pickdisinfectant wipes,pickpineapplehintwater,pickgreencup,pickpicklesnack,pick small blue plate, pick small orange rolling pin, pick octopus toy, pick catniptoy,pickswedishfishbag,picklargegreenrollingpin,pickblack sunglasses Unseen Environ- pickcoke can,pickapple,pickrxbarblueberry,moveapplenearcoke can, ments(Easy) moverxbarblueberrynearapple,movecoke can nearrxbarblueberry, pickblueplasticbottle,picksponge,pickbluechipbag,movesponge near blue plastic bottle, move blue chip bag near sponge, move blue plasticbottlenearbluechipbag,movecoke can nearwhitemug,move spongenearwhitemug,movecoke can nearyellowbowl,movesponge nearyellowbowl,movecoke can neargreencloth,movespongenear greencloth,movecoke can nearplate,movespongenearplate,move coke can near spoon, move sponge near spoon, move coke can near orangecup,movespongenearorangecup,pickwhitemug,pickyellow bowl,pickgreencloth,movewhitemugnearsponge,moveyellowbowl nearsponge,movegreenclothnearsponge,pickplate,pickspoon,pick orange cup, move plate near sponge, move spoon near sponge, move orangecupnearsponge,putcoke can intosink,dropcoke can intosink, push coke can into sink, put sponge into sink, drop sponge into sink, pushspongeintosink,putgreenclothintosink,dropgreenclothinto sink,pushgreenclothintosink Unseen Environ- pickcoke can,pickapple,pickrxbarblueberry,moveapplenearcoke can, ments(Hard) moverxbarblueberrynearapple,movecoke can nearrxbarblueberry, movecoke can nearstapler,moveapplenearstapler,movecoke can near keyboard, move apple near keyboard, move coke can near tissue box, moveappleneartissuebox,movecoke can nearpapers,moveapplenear papers,movecoke can nearmouse,moveapplenearmouse,movecoke can near book, move apple near book, pick marker, pick stapler, pick mouse,movemarkernearapple,movestaplernearapple,movemouse nearapple,pushcoke can totheleft,pushcoke can totheright,push spongeto the left,pushspongeto the right,pushtissueboxto the left, pushtissueboxto the right,pointatcoke can,pointatsponge,pointat tissuebox Table 2 | Natural language instructions used for evaluations testing controlled distribution shifts along the dimension of novel objects, novel environments, and novel backgrounds. For each category, we introduce evaluationsettings with smallerdistributionshiftsaswellaslargerdistributionshifts. Avisualizationofthese scenariosifshownin Figure 3. 26",
      "start_pos": 10626,
      "end_pos": 11034
    },
    {
      "chunk_id": 71,
      "paper_id": "rt1",
      "text": "Preprint RT-1: ROBOTICS TRANS FOR MER FOR REAL-WORLD CONTROL AT SCALE 1 Anthony Brohan\u2217,Noah Brown\u2217,Justice Carbajal\u2217,Yevgen Chebotar\u2217,Joseph Dabis\u2217, Chelsea Finn\u2217,Keerthana Gopalakrishnan\u2217,Karol Hausman\u2217,Alex Herzog\u2020,Jasmine Hsu\u2217, Julian Ibarz\u2217,Brian Ichter\u2217,Alex Irpan\u2217,Tomas Jackson\u2217,Sally Jesmonth\u2217,Nikhil JJoshi\u2217, Ryan Julian\u2217,Dmitry Kalashnikov\u2217,Yuheng Kuang\u2217,Isabel Leal\u2217,Kuang-Huei Lee\u2021,Sergey Levine\u2217, Yao Lu\u2217,Utsav Malla\u2217,Deeksha Manjunath\u2217,Igor Mordatch\u2021,Ofir Nachum\u2021,Carolina Parada\u2217, Jodilyn Peralta\u2217,Emily Perez\u2217,Karl Pertsch\u2217,Jornell Quiambao\u2217,Kanishka Rao\u2217,Michael Ryoo\u2217, Grecia Salazar\u2217,Pannag Sanketi\u2217,Kevin Sayed\u2217,Jaspiar Singh\u2217,Sumedh Sontakke\u2021,Austin Stone\u2217, Clayton Tan\u2217,Huong Tran\u2217,Vincent Vanhoucke\u2217,Steve Vega\u2217,Quan Vuong\u2217,Fei Xia\u2217,Ted Xiao\u2217, Peng Xu\u2217,Sichun Xu\u2217,Tianhe Yu\u2217,Brianna Zitkovich\u2217 \u2217Roboticsat Google,\u2020Everyday Robots,\u2021Google Research,Brain Team ABSTRACT Bytransferringknowledge from large,diverse,task-agnostic data sets,modernma- chinelearningmodels can solvespecificdownstream task sei the rzero-shotor with small task-specific data setstoahighlevelofper for mance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the dif- ficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training,combined with high-capacityarchitectures that can absorballofthedi- verse, robotic data. In this paper, we present a model class, dubbed Robotics Trans for mer, that exhibits promising scalable model properties. We verify our conclusionsinastudyofdifferent model classes and the irabilitytogeneralizeas a function of the data size, model size, and data diversity based on a large-scale datacollectiononrealrobotsper for mingreal-worldtasks. Theproject\u2019swebsite andvideos can befoundatrobotics-trans for mer 1.github.io 1 INTRODUCTION End-to-end robotic learning, with either imitation or rein for cement, typically involves collecting task-specific data in either single-task (Kalashnikov et al., 2018; Zhang et al., 2018) or multi- task (Kalashnikov et al., 2021 b; Jang et al., 2021) settings that are narrowly tailored to the tasks that the robotshouldperform. Thisworkflowmirrors the classicapproachtosupervisedlearningin otherdomains,suchascomputervision and NLP,where task-specific data setswouldbecollected, labeled, and deployed to solve individual tasks, with little interplay between the tasks themselves. Recentyears have seenatrans for mationinvision,NLP,ando the rdomains,away from siloed,small- scale datasets and models and towards large, general models pre-trained on broad, large datasets. Thekeysto the successofsuch model slie with open-ended task-agnostictraining,combined with high-capacityarchitectures that can absorballof the knowledgepresentinlarge-scale data sets. Ifa model can \u201csponge up\u201d experience to learn general patterns in language or perception, then it can bring them to bear on individual tasks more efficiently. While removing the need for large task- specific datasets is appealing generally in supervised learning, it is even more critical in robotics, where data setsmightrequireengineering-heavyautonomousoperationorexpensivehum and emon- strations. Wethere for eask: canwetrainasingle,capable,largemulti-taskbackbonemodelon data consistingofawidevarietyofrobotictasks? Anddoessucha model enjoy the benefitsobservedin otherdomains,exhibitingzero-shotgeneralizationto new tasks,environments,andobjects? Buildingsuch model sinroboticsisnoteasy. Althoughrecentyears have seenseverallargemulti- taskrobotpoliciesproposedin the literature(Reedetal.,2022;Jangetal.,2021),such model soften havelimitedbreadthofreal-worldtasks,aswith Gato(Reedetal.,2022),orfocusontrainingtasks ratherthangeneralizationto new tasks,aswithrecentinstructionfollowingmethods(Shridh are tal., 2021;2022),orattaincomparativelylowerper for manceon new tasks(Jangetal.,2021). 1 Authorslistedinalphabeticalorder.Contributionsin Appendix A. Correspondingemails:{keerthanapg,kanishkarao,karolhausman}@google.com. 1 3202 gu A 11 ]OR.sc[ 2 v 71860.2122:vi Xra Instruction Pick apple from top drawer and place on counter Mode Arm Base Images Fi LM Efficient Net Token Learner Trans for mer Preprint \u2026 RT-1 3 Hz \u03b2 )\u03b3+1( Action \u00b7 + Instruction Pick apple from top drawer and place on counter Mode Arm Base Images Fi LM Efficient Net Token Learner Trans for mer \u2026 RT-1 3 Hz \u03b2 )\u03b3+1( Action \u00b7 + (a)RT-1 takesimagesandnaturallanguageinstructionsandoutputsdiscretized base",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 72,
      "paper_id": "rt1",
      "text": "Token Learner Trans for mer Preprint \u2026 RT-1 3 Hz \u03b2 )\u03b3+1( Action \u00b7 + Instruction Pick apple from top drawer and place on counter Mode Arm Base Images Fi LM Efficient Net Token Learner Trans for mer \u2026 RT-1 3 Hz \u03b2 )\u03b3+1( Action \u00b7 + (a)RT-1 takesimagesandnaturallanguageinstructionsandoutputsdiscretized base and armactions. Despite itssize(35 Mparameters),itdoesthisat 3 Hz,duetoitsefficientyethigh-capacityarchitecture:a Fi LM(Perez et al., 2018) conditioned Efficient Net (Tan & Le, 2019), a Token Learner (Ryoo et al., 2021), and a Trans- former(Vaswanietal.,2017). (b)RT-1\u2019slarge-scale,real-worldtraining(130 kdemonstrations)andevaluation(3000 real-worldtrials)show impressivegeneralization,robustness,andabilitytolearn from diverse data. Figure 1: Ahigh-leveloverviewof RT-1\u2019sarchitecture,dataset,andevaluation. Thetwomainchallengesliein assemblingtheright data set and designing the right model. While data collection and curation is often the \u201cunsung hero\u201d of many large-scale machine learning projects(Rad for detal.,2021;Rameshetal.,2021),thisisespeciallytrueinrobotics,where data sets areoftenrobot-specifi can dga the redmanually(Dasarietal.,2019;Ebertetal.,2021). Aswe will showin our evaluations,goodgeneralizationrequires data sets that combineboth scale and breadth, coveringavarietyoftasks and settings. Atthesametime, the task sin the datasetshould besuffi- ciently well-connected to enable generalization, such that the model can discover the patterns be- tweenstructuralsimilartasks and per for mnewtasks that combinethosepatternsinnovelways. We utilizea data set that wegatheredover the courseof 17 months with afleetof 13 robots,containing \u223c130 kepisodes and over 700 tasks,andweablatevariousaspectsof this datasetin our evaluation. The second challenge lies in the design of the model itself. Effective robotic multi-task learning requiresahighcapacity model,and Trans for mer(Vaswanietal.,2017)modelsexcelin this regard, particularlywhenitisnecessarytolearnmany task sconditioned,asin our case,onlanguageinstruc- tions. However,roboticcontrollersmustalsobeefficientenoughtoruninrealtime,whichpresents amajorchallenge for Trans for mersinparticular. Weproposeanovelarchitecture that wecall RT-1 (Robotics Trans for mer 1),whichbyencodinghigh-dimensionalinputs and outputs,includingcam- eraimages,instructionsandmotorcomm and sintocompacttokenrepresentationstobeusedby the Trans for mer,allows for efficientinferenceatruntimetomakereal-timecontrolfeasible. Ourcontributionis the RT-1 modelandexperiments with this model onalarge and broad data setof real-worldrobotictasks. Ourexperimentsnotonlydemonstrate that RT-1 canexhibitsignifi can tly improvedgeneralization and robustnesscomp are dtopriortechniques,butalsoevaluate and ablate manydesignchoicesinboththe model and inthecompositionof the trainingset. Ourresultsshow that RT-1 canper for mover 700 traininginstructionsat 97%successrate,and can generalizeto new tasks, distractors, and backgrounds 25%, 36% and 18% better than the next best baseline, respec- tively. Thislevelofper for manceallowsustoexecuteverylong-horizon task sin the Say Can(Ahn etal.,2022)framework,withasmanyas 50 stages. Wefur the rshow that RT-1 canincorporate data fromsimulationoreveno the rrobottypes,retainingper for manceon the originaltasks and improving generalizationto new scenarios. Ashortoverviewof RT-1 capabilitiesispresentedin Fig.1 b 2. 2 Helperrobotsshownin Fig.1-5 arefrom Everyday Robots 2 Preprint 2 RELATED WORK A number of recent works have proposed Trans for mer-based policies for robotic control. As in RT-1, several works use language commands processed with Trans for mers as a robust framework for specifying and generalizing to new tasks (Zhang & Chai, 2021; Pashevich et al., 2021; Silva et al., 2021; Jang et al., 2021; Ahn et al., 2022; Nair et al., 2022). Our work takes the application of Trans for mersastepfurther and treats the mappingoflanguage and visionobservationstorobot actions as a sequence modelling problem, using a Trans for mer to learn this mapping. This idea is directly inspired by successes in game-playing (Chen et al., 2021; Lee et al., 2022 a) as well as simulated robot navigation (Fang et al., 2019), locomotion (Janner et al., 2021; Gupta et al., 2022),andmanipulation(Jiangetal.,2022)environments. Wenote that severalof the seworksgo beyond only text conditioning and use Trans for mers to also generalize across robot morphologies (e.g.,Guptaetal.(2022))ando the rmodalities for taskspecifications(e.g.,Jangetal.(2021);Jiang etal.(2022)). Theseextensions are promisingfuturedirections for",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 73,
      "paper_id": "rt1",
      "text": "a) as well as simulated robot navigation (Fang et al., 2019), locomotion (Janner et al., 2021; Gupta et al., 2022),andmanipulation(Jiangetal.,2022)environments. Wenote that severalof the seworksgo beyond only text conditioning and use Trans for mers to also generalize across robot morphologies (e.g.,Guptaetal.(2022))ando the rmodalities for taskspecifications(e.g.,Jangetal.(2021);Jiang etal.(2022)). Theseextensions are promisingfuturedirections for RT-1. Beyond Trans for mer-basedpolicies,thefocusof our workisongeneralizable and robustreal-world roboticmanipulationat scale.Existingworksonreal-world Trans for mer-basedroboticmanipulation focus on efficiently learning tasks from a set of demonstrations per task (Shridhar et al., 2022). Behavior Trans for mer (Shafiullah et al., 2022) and Gato (Reed et al., 2022) advocate for training a single model on large-scale robotic and non-robotic datasets. However, these works are limited intheirreal-worldrobotictasks;e.g.,Gatolearnseffectivelyasingle task(coloredblockstacking) withoutevaluatinggeneralizationto new task soravarietyofreal-worldsettings. Onthetechnical side,ourworkexamineshow Trans for mer-basedpolicies can bebuiltsoastocombinehighcapacity andgeneralization with the computationalefficiencynecessary for real-timecontrol. While the useofhigh-capacity Trans for mer model stolearnroboticcontrolpoliciesisafairlyrecent innovation, robotics has a long history of multi-task and language-conditioned learning, and RT-1 buildson the sefoundations. Asignifi can tbodyofworkdeals with learningpolicies and predictive models for robotic grasping (Saxena et al., 2006; Lenz et al., 2015; Pinto & Gupta, 2016; Gupta et al., 2018; Viereck et al., 2017), with the aim of generalizing to new objects. Prior works have sought to address robotic language underst and ing through pipelined approaches that combine lan- guageparsing,vision,androboticcontrol(Mac Mahonetal.,2006;Koll are tal.,2010;Tellexetal., 2011)and with end-to-endapproaches(Meietal.,2016;Stepputtisetal.,2020;Lynch&Sermanet, 2020;Ahnetal.,2022). Multi-taskroboticlearninghasalso been approached from the perspective of learning to reach goals (Chung et al., 2015; Raffin et al., 2019; Jurgenson et al., 2020; Huang etal.,2020), aswellaslearningpolicies that can per for mtasksinadiscretesetorsomeo the rpa- rameterizedform(Deisenro the tal.,2014;Devinetal.,2017;Foxetal.,2019;Kalashnikovetal., 2021 a). A number of prior works in robotics have also focused on collecting datasets containing demonstrationsortrials that illustrateavarietyofdifferenttasks(Sharmaetal.,2018;Dasarietal., 2019; Yu et al., 2020; Singh et al., 2020; James et al., 2020). Our work adds further evidence in supportof the powerofmulti-task,language-conditionedroboticlearning,presentingexperimental results at a larger scale and with a greater variety of behaviors, objects, and scenes and proposing newarchitectures and designchoices that enableroboticlearningatasignifi can tlylarger scale. 3 PRELIMINARIES Robotlearning. Weaimtolearnrobotpoliciestosolvelanguage-conditionedtasks from vision. Formally,weconsiderasequentialdecision-makingenvironment. Attimestept = 0,thepolicy\u03c0 ispresented with alanguageinstructioni and aninitialimageobservationx . Thepolicyproduces 0 an action distribution \u03c0(\u00b7 | i,x ) from which an action a is sampled and applied to the robot. 0 0 Thisprocesscontinues,with the policyiterativelyproducingactionsa bysampling from alearned t distribution\u03c0(\u00b7|i,{x }t )andapplyingthoseactionsto the robot. Theinteractionendswhena j j=0 terminationconditionisachieved. The full interactioni,{(x ,a )}T from from the startingstep j j j=0 t = 0 to terminating step T is referred to as an episode. At the end of an episode, the agent will begivenabinaryrewardr \u2208 {0,1}indicatingwhethertherobotper for med the instructioni. The goalistolearnapolicy\u03c0 thatmaximizes the averagereward,inexpectationoveradistributionof instructions,startingstatesx ,andtransitiondynamics. 0 3 Preprint Trans for mers. RT-1 usesa Trans for mer(Vaswanietal.,2017)toparameterizethepolicy\u03c0. Gener- allyspeaking,a Trans for merisasequence model mappinganinputsequence{\u03be }H toanoutput h h=0 sequence{y }K usingcombinationsofself-attentionlayers and fully-connectedneuralnetworks. k k=0 While Transformers were originallydesigned for textsequences,whereeachinput\u03be andoutputy j k represents a text token, they have been extended to images (Parmar et al., 2018) as well as other modalities (Lee et al., 2022 a; Reed et al., 2022). As detailed in the next section, we parameterize \u03c0 by first mapping inputs i,{x }t to a sequence {\u03be }H and action outputs",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 74,
      "paper_id": "rt1",
      "text": "a text token, they have been extended to images (Parmar et al., 2018) as well as other modalities (Lee et al., 2022 a; Reed et al., 2022). As detailed in the next section, we parameterize \u03c0 by first mapping inputs i,{x }t to a sequence {\u03be }H and action outputs a to a sequence j j=0 h h=0 t {y }K beforeusinga Trans for mertolearn the mapping{\u03be }H \u2192{y }K . k k=0 h h=0 k k=0 Imitation learning. Imitation learning methods train the policy \u03c0 on a dataset D of demonstra- tions (Pomerleau, 1988; Zhang et al., 2018; Jang et al., 2021). Specifically, we assume access to adataset D = {(i(n),{(x(n),a(n))}T(n))}N ofepisodes,allofwhich are successful(i.e.,havea t t t=0 n=0 finalrewardof 1). Welearn\u03c0 usingbehavioralcloning(Pomerleau,1988), whichoptimizes\u03c0 by minimizing the negativelog-likelihoodofactionsa given the images and languageinstructions. t 4 SYSTEM OVERVIEW The goal of this work is to build and demonstrate a general robot learning system that can ab- sorblargeamountsof data and generalizeeffectively. we usemobilemanipulators from Everyday Robots 3, which have a 7 degree-of-freedom arm, a two-fingered gripper, and a mobile base (see Fig.2(d)). Tocollect data and evaluate our method,we usethreekitchen-basedenvironments: two real office kitchens and a training environment modelled off these real kitchens. The training en- vironment, shown in Fig. 2 (a), consists of partial counters and is constructed for large scale data collection. Thetworealenvironments,shownin Fig.2(b,c),havesimilarcountertopsto the train- ingenvironment,butvaryinlighting,background,and full kitchengeometry(e.g.,theremaybea cabinetinsteadofadrawerorasinkmaybevisible). Weevaluate the per for manceof our policies across the sedifferentenvironments,measuring the policy\u2019sper for mance and abilitytogeneralize. Ourtraining data consistsofhuman-provideddemonstrations,andweannotateeachepisodewitha textualdescriptionoftheinstruction that the robotjustper for med. Theinstructionsusuallycontain averb and oneormorenounsdescribing the targetobjects. Togroup the seinstructionstogether,we split the mintoanumberofskills(e.g.,verbssuchas\u201cpick\u201d,\u201copen\u201dor\u201cplaceupright\u201d)andobjects (e.g., nounssuchas\u201ccoke can\u201d, \u201capple\u201d, or\u201cdrawer\u201d). Wedescribe the detailsof our data collec- tionstrategyat scalein Sec.5.2. Ourlargest data setcontainsover 130 kindividualdemonstrations constitutingover 700 distinct task instructionsusingalargevarietyofobjects(see Fig.2(f)). We describethedetailsof the datacollectedin Sec.5.2. One of the main contributions of our system is the network architecture, Robotics Trans for mer 1 (RT-1),anefficient model that can absorblargeamountsof data,effectivelygeneralize,andoutput actions at real-time rates for practical robotic control. RT-1 takes a short sequence of images and a natural language instruction as input and outputs an action for the robot at each time step. To this end, the architecture (shown in Figure 1 a) leverages several elements: first the images and text are processedviaan Image Netpretrainedconvolutionalnetwork(Tan&Le,2019)conditioned on a pretrained embedding of the instruction via Fi LM (Perez et al., 2018), followed by a Token Learner(Ryooetal.,2021)tocomputeacompactsetoftokens,andfinallya Trans for mer(Vaswani etal.,2017)toattendover the setokens and producediscretizedactiontokens. Theactionsconsist ofsevendimensions for the armmovement(x, y, z, roll, pitch, yaw, openingof the gripper), three dimensions for basemovement(x,y,yaw)andadiscretedimensiontoswitchbetweenthreemodes: controlling the arm, the base, or terminating the episode. RT-1 performs closed-loop control and comm and sactionsat 3 Hzuntilitei the ryieldsa\u201cterminate\u201dactionorhitsapre-settimesteplimit. 5 RT-1: ROBOTICS TRANS FOR MER Inthissection,wedescribehowwetokenize the images,text,andactions,and the ndiscuss the RT-1 modelarchitecture.Wethendescribehowweattain the runtimespeedrequired for real-timecontrol. Lastly,wedescribethe data collectionprocedure and the skills and instructionsin our data set. 3 everydayrobots.com 4 Preprint Figure 2:(a)Robotclassroomwherewecollectdataat scale;(b)arealofficekitchen,oneof the two realisticenvironmentsused for evaluation(named Kitchen 1 intherestof the paper);(c)adifferent officekitchenused for evaluation(named Kitchen 2 intherestof the paper);(d)mobilemanipulator usedthroughout the paper; (e)asetofobjectsused for mostof the skillstoexp and skilldiversity; (f)amorediversesetofobjectsusedmostlytoexp and objectdiversityof the pickingskill. 5.1 MODEL Our model",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 75,
      "paper_id": "rt1",
      "text": "skills and instructionsin our data set. 3 everydayrobots.com 4 Preprint Figure 2:(a)Robotclassroomwherewecollectdataat scale;(b)arealofficekitchen,oneof the two realisticenvironmentsused for evaluation(named Kitchen 1 intherestof the paper);(c)adifferent officekitchenused for evaluation(named Kitchen 2 intherestof the paper);(d)mobilemanipulator usedthroughout the paper; (e)asetofobjectsused for mostof the skillstoexp and skilldiversity; (f)amorediversesetofobjectsusedmostlytoexp and objectdiversityof the pickingskill. 5.1 MODEL Our model isbuiltona Trans for merarchitecture(Vaswanietal.,2017)andtakesahistoryofimages and task descriptionasinput and directlyoutputstokenizedactions,asshownin Fig.1 aandindetail in Fig. 3. In the following we describe the components of the model, following the top-to-bottom orderin Fig.3. Moredetailon model selectionat scale are providedin Appendix C.3. Instruction and imagetokenization. The RT-1 architecturereliesona data-efficient and compact tokenizationofimages and languageinstruction. RT-1 tokenizesahistoryof 6 imagesbypassing images through an Image Net pretrained Efficient Net-B 3 (Tan & Le, 2019) model, which takes 6 imagesofresolution 300\u00d7300 asinput and outputsaspatialfeaturemapofshape 9\u00d79\u00d7512 from the final convolutional layer. Unlike Reed et al. (2022), we do not patchify the images into visual tokenspriortofeeding the mtoour Trans for merbackbone.Weinsteadflatten the outputfeaturemap from the Efficient Netinto 81 visualtokenswhich are passedontothelaterlayersof the network. To include the language instruction, we condition the image tokenizer on the natural language in- structionin the formofapretrainedlanguageembedding,allowingextractionof task-relevantimage featuresearlyon and improvingper for manceof RT-1. Theinstructionisfirstembeddedvia Univer- sal Sentence Encoder(Ceretal.,2018). Thisembeddingis the nusedasinputtoidentity-initialized Fi LM layers (Perez et al., 2018) added to the pretrained Efficient Net to condition the image en- coder. Normally,insertinga Fi LMlayerinto the interiorofapretrainednetworkwoulddisrupt the intermediate activations and negate the benefit of using pretrained weights. To overcome this, we initialize the weights of the dense layers (f and h ) which produce the Fi LM affine transforma- c C tiontozero,allowing the Fi LMlayertoinitiallyactasanidentity and preserve the functionof the pretrainedweights. Wefind that identity-initialized Fi LMalsoproducesbetterresultswhentraining withan Efficient Netinitialized from scratch,without Image Netpretraining,butitdoesnotsurpass theinitializationdescribedabove. Thearchitectureof the imagetokenizerispresentedin Fig.3. RT-1\u2019simage and instructiontokenizationvia Fi LMEfficient Net-B 3 isatotalof 16 Mparameters, with 26 layersof MBConvblocks and Fi LMlayers,whichoutput 81 vision-languagetokens. Token Learner. Tofurthercompress the numberoftokens that RT-1 needstoattendover and thus speed up inference, RT-1 uses Token Learner (Ryoo et al., 2021). Token Learner is an element- wise attention module that learns to map a large number of tokens into a much smaller number of tokens. This allows us to soft-select image tokens based on their information, passing only the importanttokencombinationsto the subsequent Trans for merlayers. Theinclusionof Token Learner subsamples the 81 visualtokens that comeoutof the pre-trained Fi LM-Efficient Netlayerstojust 8 finaltokens that are the npassedonto our Trans for merlayers. 5 Preprint 1 \u03b3) \u03b2 \u00b7 + \u2026 1 \u03b3) \u03b2 1 \u03b3) \u03b2 1 \u03b3) \u03b2 Figure 3: Thearchitecturediagramof RT-1. Theinstructionistrans for medintoa USEembedding and used to condition a pre-trained Efficient Net via Fi LM layers. The resulting vision-language tokens are reduced by the Token Learner and fed into a decoder-only Trans for mer, which outputs tokenizedactions. Trans for mer. These 8 tokensper-image are thenconcatenated with theotherimagesin the history, forming 48 totaltokens(withaddedpositionencoding)tobefedinto the Trans for merbackboneof RT-1. The Trans for merisadecoder-onlysequence model with 8 self-attentionlayers and 19 Mtotal parameters that outputsactiontokens. Action tokenization. To tokenize actions, each action dimension in RT-1 is discretized into \ud835\udf38\ud835\udf37 256 bins. As mentioned previously, the action dimensions we consider include seven variables for the arm movement",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 76,
      "paper_id": "rt1",
      "text": "the Trans for merbackboneof RT-1. The Trans for merisadecoder-onlysequence model with 8 self-attentionlayers and 19 Mtotal parameters that outputsactiontokens. Action tokenization. To tokenize actions, each action dimension in RT-1 is discretized into \ud835\udf38\ud835\udf37 256 bins. As mentioned previously, the action dimensions we consider include seven variables for the arm movement (x, y, z, roll, pitch, yaw, opening of the gripper), three variables for base movement(x,y,yaw)andadiscretevariabletoswitchbetweenthreemodes: controllingarm,base orterminating the episode. Foreachvariable,wemap the targettooneof the 256 bins,where the bins are uni for mlydistributedwithin the boundsofeachvariable. Loss. We use a standard categorical cross-entropy entropy objective and causal masking that was utilizedinprior Trans for mer-basedcontrollers(Reedetal.,2022;Leeetal.,2022 a). Inference speed. In contrast to many applications of large models, such as natural language or imagegeneration,oneof the uniquerequirements for amodel that needstorunonrealrobotsinreal time is fast and consistent inference speed. Given the human speeds of executing the instructions 6 Preprint consideredin this work(whichwemeasuredtobein the 2\u22124 secsrange),wewant the modeltobe notsignifi can tlyslowerthan that. Basedon our experiments this requirementcorrespondstoatleast 3 Hzcontrolfrequency and theresultinginferencetimebudget for the model,giveno the rlatencies inthesystem,tobelessthan 100 ms. This requirement limits the size of the model that we can use. We further explore the impact of modelsizeoninferencespeedin the experiments. Weemploytwotechniquestospeedupinference: (i) reduce the number of tokens generated by a pre-trained Efficient Net model by using Token- Learner(Ryooetal.,2021), (ii)computethesetokensonlyonce and reusethem for the following windows that overlap for the futureinferences. Bothoftheseallowustospeedup the modelinfer- enceby 2.4 and 1.7 times,respectively. Additionaldetailson model inferencearein Appendix C.1. 5.2 DATA Skill Count Description Example Instruction Pick Object 130 Lifttheobjectoff the surface pickicedtea can Move Object Near Object 337 Movethefirstobjectnear the second movepepsi can nearrxbarblueberry Place Object Upright 8 Placeanelongatedobjectupright placewaterbottleupright Knock Object Over 8 Knockanelongatedobjectover knockredbull can over Open Drawer 3 Openanyof the cabinetdrawers open the topdrawer Close Drawer 3 Closeanyof the cabinetdrawers close the middledrawer Place Objectinto Receptacle 84 Placeanobjectinto are ceptacle placebrownchipbagintowhitebowl Pick Object from Receptacle 162 Pickanobjectup from alocation and then pickgreenjalapenochipbag from paper and Placeon the Counter placeiton the counter bowl and placeoncounter Section 6.3 and 6.4 tasks 9 Skillstrained for realistic,longinstructions open the largeglassjarofpistachios pullnapkinoutofdispenser grabscooper Total 744 Table 1: The list of skills collected for RT-1 together with their descriptions and example instruc- tions. Our goal is to build a system that exhibits high per for mance, generalization to new tasks, and ro- bustnesstodistractors and backgrounds. Wethere for eaimtocollectalarge,diverse data setofrobot trajectories that includesmultipletasks,objects and environments. Ourprimary data setconsistsof \u223c130 krobotdemonstrations,collected with afleetof 13 robotsover the courseof 17 months. We conducted this large-scale data collectioninaseriesofofficekitchensegments,which were fertoas robotclassrooms,shownin Fig.2. Moredetailson data collectionarein Appendix C.2. Skills and instructions. While the definition of a task remains inconsistent in the literature, in this work we count the number of language instructions that the system can perform, where an instructioncorrespondstoaverbsurroundedbyoneormultiplenouns,suchas\u201cplacewaterbottle upright\u201d,\u201cmove the coke can tothegreenchipbag\u201dor\u201copen the drawer\u201d. RT-1 isabletoperform over 700 languageinstructionsinmultiplerealisticofficekitchenenvironments that weevaluate and describeindetailin the experiments. Inordertogroup the evaluations and drawconclusionson the per for manceof the system,wegrouptheinstructionsby the verbsusedinthem,which were ferto asskills. Amoredetailedlistofinstructionsisshownin Table 1,withexamples and the numberof instructionsperskill. Thecurrentsetofskillsincludespicking,placing,opening and closingdrawers,gettingitemsin and outdrawers,placingelongateditemsup-right,knocking the mover,pullingnapkins and openingjars. Theskills were chosentodemonstratemultiplebehaviors with manyobjects(seenin Fig.2(e))to testaspectsof RT-1 suchasgeneralizationto new instructions and abilitytoper for",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 77,
      "paper_id": "rt1",
      "text": "and describeindetailin the experiments. Inordertogroup the evaluations and drawconclusionson the per for manceof the system,wegrouptheinstructionsby the verbsusedinthem,which were ferto asskills. Amoredetailedlistofinstructionsisshownin Table 1,withexamples and the numberof instructionsperskill. Thecurrentsetofskillsincludespicking,placing,opening and closingdrawers,gettingitemsin and outdrawers,placingelongateditemsup-right,knocking the mover,pullingnapkins and openingjars. Theskills were chosentodemonstratemultiplebehaviors with manyobjects(seenin Fig.2(e))to testaspectsof RT-1 suchasgeneralizationto new instructions and abilitytoper for mmanytasks.We thengreatlyexpanded the objectdiversity for the\u201cpick\u201dskilltomakesure that the skillsgeneralize to varied objects (see the expanded set of objects in Fig. 2(f)). The skills were further expanded while we conducted the ablations to include instructions added in the last row of Table 1, which were used for the experiments described in Sec. 6.4 and 6.3. These additional skills focused on realistic,long-horizoninstructionsinanofficekitchen. Theentireprocessofaddingtasks and data is described in the Appendix C.4. Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide morediverse data toimproveitscapabilities. 7 Preprint 6 EXPERIMENTS Ourexperimentsseektoanswer the followingquestions: 1. Can an RT-1 learn to perform a large number of instructions, as well as to generalize in zeroshotto new tasks,objects and environments? (Section 6.2) 2. Canwepushtheresulting model evenfur the rbyincorporatingheterogeneous data sources, suchassimulated data ordata from differentrobots? (Section 6.3) 3. Howdovariousmethodsgeneralizetolong-horizonroboticscenarios? (Section 6.4) 4. How do generalization metrics change with varying amounts of data quantity and data diversity? (Section 6.5) 5. What are theimportant and practicaldecisionsinthedesignof the model and howdothey affectper for mance and generalization? (Appendix Section D.4) Throughout this sectionwe will comp are totwo base linestateof the artarchitectures, Gato(Reed etal.,2022)and BC-Z(Jangetal.,2021).Importantlybothof the searetrainedon our data described in detail in Sec. 5.2 (which is an important part of our system) since the original models in these publicationswouldnotexhibitgeneralizationpropertiesrequired for ourevaluationtasks. Gatois, similarly to RT-1, based on a Trans for mer architecture, but varies from RT-1 in multiple aspects. First,itcomputesimagetokens with out the notionoflanguage and eachimagetokenembeddingis computed separately for each image patch, as opposed to early language fusion and global image embedding in our model. Second, it does not use a pre-trained text embedding to encode the lan- guagestring. Italsodoesnotincludeinferencetimeconsiderations that are necessary for realrobots asdiscussedin Sec.5.1 suchas Token Learner and the removalofauto-regressiveactions.Inorderto run Gatoonrealrobotsatahighenoughfrequency,wealsolimitthesizeof the modelcomp are dto theoriginalpublication,whichwas 1.2 Bparameters(resultinginonrobotinferencetimeof 1.9 s), to be of similar size to RT-1 (37 M parameters for Gato vs. 35 M for RT-1). BC-Z is based on a Res Netarchitecture,andwasusedin Say Can(Ahnetal.,2022). BC-Zdiffers from RT-1 inthatitis afeed for ward model that doesnotuseprevioustimesteps,anditusescontinuousactionsra the rthan discrete action tokens. In addition to the original BC-Z model size, we also comp are our method toalargerversionof BC-Zthathasasimilarnumberofparametersto RT-1 andrefertoitas BC-Z XL. We study and analyze how each of these design decisions changes per for mance in Appendix Sections D.4 and D.5. Weevaluate the successrateinexperimentstomeasureper for manceontraininginstructions, gen- eralization to unseen instructions, robustness to backgrounds and distractors, and per for mance in long-horizon scenarios, as detailed below. Throughout this section, we evaluate our approach and baselines with over 3000 real-world trials, making one of the largest scale evaluation of a robot learningsystemto-date. 6.1 EXPERIMENTALSETUP As mentioned in Section 4, we evaluate RT-1 with a set of mobile manipulators from Everyday Robotsinthreeenvironments:tworealofficekitchens and atrainingenvironment model ledoffthese realkitchens. Thetrainingenvironment,",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 78,
      "paper_id": "rt1",
      "text": "Throughout this section, we evaluate our approach and baselines with over 3000 real-world trials, making one of the largest scale evaluation of a robot learningsystemto-date. 6.1 EXPERIMENTALSETUP As mentioned in Section 4, we evaluate RT-1 with a set of mobile manipulators from Everyday Robotsinthreeenvironments:tworealofficekitchens and atrainingenvironment model ledoffthese realkitchens. Thetrainingenvironment, shownin Fig.2(a), consistsofpartialcounterswhile the tworealenvironments,shownin Fig.2(b,c),havesimilarcountertopsto the trainingenvironment, butvaryinlighting,background,and full kitchengeometry(e.g.,theremaybeacabinetinsteadof a drawer or a sink may be visible). The policies are evaluated for per for mance on training tasks as well as generalization to new tasks, robustness to unseen environments, and per for mance when chainedtoge the rforlong-horizontasks,asdetailedbelow. Seen task per for mance.Toevaluateper for manceonseeninstructions,weevaluateper for manceon instructionssampled from the trainingset. Note,however,that this evaluationstillinvolvesvarying theplacementofobjects and otherfactorsof the setup(e.g.,timeofday,robotposition),requiring the skills to generalize to realistic variability in the environment. In all, we test over 200 tasks in thisevaluation: 36 forpickingobjects,35 forknockingobjects,35 forplacingthingsupright,48 for movingobjects,18 for open ing and closingvariousdrawers,and 36 forpickingoutof and placing objectsintodrawers. Unseen task sgeneralization. Toevaluategeneralizationtounseentasks,wetest 21 novel,unseen instructions. These instructions are distributed across skills and objects. This ensures that at least 8 Preprint someinstancesofeachobject and skill were presentinthetrainingsetbut the ywill becombinedin novelways. Forexample,if\u201cpickup the apple\u201disheldout,thenthereareo the rtraininginstructions thatinclude the apple. Thelistofallunseeninstructions can befoundin the Appendix D.1. Robustness. Toevaluaterobustness, weperform 30 real-worldtasks for distractorrobustness and 22 tasks for background robustness. The background robustness was tested by evaluating in new kitchens(which have differentlighting and backgroundvisuals)and with differentcountersurfaces (e.g., a patterned table cloth). Example configurations of the robustness evaluation scenarios are depictedin Fig.4. Long-horizonscenarios. Wealsoevaluategeneralizationtomorerealisticlong-horizonscenarios, whicheachrequireexecutingasequenceofskills.Thegoalof this evaluationistocombinemultiple generalization axes such as new tasks, objects, environments and test the overall generalization capabilitiesinrealisticsettings.Theseevaluationsconsistof 15 long-horizoninstructionsintworeal kitchens, which require executing sequences of skills consisting of \u223c 10 distinct steps, with each stepofroughlycomparablescopeas the traininginstructions.Thesesteps are obtainedautomatically fromhigherlevelinstructions,suchas\u201chowwouldyouthrowawayalltheitemson the table?\u201d by using the Say Cansystem(Ahnetal.,2022),asdescribedindetailin Section 6.4 and Appendix D.3. Figure 4: Evaluation scenarios for distractors (first row), from left to right: easy (0-5 distractors), medium (9 distractors), hard (9 distractors and occluded object); background (second row), from lefttoright: originalenvironment,patternedtablecloth,newkitchen;andrealisticscenariosin the realkitchen(thirdrow),generalizationlevels from lefttoright: L 1,L 2 and L 3. 6.2 CANRT-1 LEARNTOPER FOR MALARGENUMBEROFINSTRUCTIONS,ANDTO GENERALIZETO NEW TASKS,OBJECTS AND ENVIRONMENTS? To answer our first question, we analyze the overall per for mance, generalization, and robustness capabilities of RT-1 compared to previously proposed models. Specifically, we comp are to the model architectures used by Gato (Reed et al., 2022) and BC-Z (Jang et al., 2021), as well as a largerversionof BC-Z,which were fertoas BC-ZXL.Note, however, thatallmodels are trained on the same data as RT-1, and the evaluation only compares the model architectures, not the task sets,datasets,oroverallroboticsystems. Thecapabilitiesof RT-1 aredeterminedtoalargeextent by the dataset and task set, which we believe improves signifi can tly over prior works (e.g. BC-Z uses 100 tasks and the original Gato model trainsastacking task with variousshapes),andthus this comparison should be viewed as rather favorable to the prior models, which also benefit from the largeanddiverse data set and taskset that wecollected. The results are shown in Table 2. Across each category, we find that RT-1 outperforms the prior models signifi can tly. On seen tasks, RT-1 is able to perform 97% of the",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 79,
      "paper_id": "rt1",
      "text": "rather favorable to the prior models, which also benefit from the largeanddiverse data set and taskset that wecollected. The results are shown in Table 2. Across each category, we find that RT-1 outperforms the prior models signifi can tly. On seen tasks, RT-1 is able to perform 97% of the more than 200 instruc- 9 Preprint Model Seen Tasks Unseen Tasks Distractors Backgrounds Gato(Reedetal.,2022) 65 52 43 35 BC-Z(Jangetal.,2021) 72 19 47 41 BC-ZXL 56 43 23 35 RT-1(ours) 97 76 83 59 Table 2: Overall per for mance of RT-1 and baselines across seen tasks, generalization to unseen tasks,androbustnesstodistractors and backgrounds. tionssuccessfully,whichis 25%morethan BC-Zand 32%morethan Gato. Onunseentasks,RT-1 showsitiscapableofgeneralizingtonovelinstructions,per for ming 76%ofthenever-before-seen instructions,24%morethan the nextbest base line. Whilesuchgeneralizationtonovelinstructions ismadepossibleduetonaturallanguageconditioningof the policy, asthepolicyisabletounder- stand new combinations of previously seen concepts, all of the baselines are also conditioned on naturallanguage and inprincipleenjoy the samebenefits. Wefur the rablatedifferentcomponents of RT-1 inthenextsectiontobetterunderst and whataspectsof our methodcontribute the mostto thisdifference. Ondistractors and backgrounds,wefind that RT-1 isquiterobust,success full yexe- cuting 83%ofthedistractorrobustnesstasks and 59%ofthebackgroundrobustnesstasks(36%and 18%higherthan the nextbestalternative,respectively). Overall,wefind that RT-1 hashighgeneral per for mance,whileexhibitingimpressivedegreesofgeneralization and robustness. Weshowexam- pletrajectoriesof the RT-1 agentincludinginstructions that coverdifferentskills,environments and objectsin Fig.5. Wealsopresentadditionaltrajectoryexamples for differentgeneralizationtestsin the Appendix,whichincludebackgrounds(Fig.10),anddistractors(Fig.12). Generalization to realistic instructions. Next, we test whether our method generalizes enough across all the different axes that we evaluated previously to be deployed in a real kitchen, which poses multiple distribution shifts all at once such as new tasks combinations, object distractors as wellasanovelenvironment. To evaluate our algorithm in realistic scenarios in a real kitchen, we construct task sequences to accomplish a number of realistic goals. The robot restocks several snacks in drawers, tidies up knocked over condiment bottles and closes drawers left open by humans, prepares a snack with an orange and a napkin and fetches lost sunglasses and an octopus toy from several places in the kitchen. The detailed instructions used in these scenarios are listed in the Appendix D.1. The officekitcheninvolvesadramaticshift from the trainingenvironment and wecategorize task sacross thesescenarios with varyinglevelsofgeneralization: L 1 forgeneralizationto the newcounter-top layout and lighting conditions, L 2 for additionally generalization to unseen distractor objects, L 3 foradditionalgeneralizationtodrastically new task settings, new task objectsorobjectsinunseen locations such as near a sink. The three levels that correspond to the three tasks of restocking, preparingasnack and fetchingalostobjectintherealkitchen are depictedin the lastrowof Fig.4. Exampletrajectories for differentlevels are presentedin the Appendixin Fig.11. Wereport the per-tasksuccessrateintheserealisticscenariosalong with the varyinggeneralization levelsin Table 3 andfind RT-1 tobe the mostrobustonalllevels. Gatogeneralizesfairlywellat the first level but it performs signifi can tly drops for the more difficult generalization scenarios. BC-Z andits XLequivalentper for mfairlywellat L 2 level and betterthan Gatoat L 3 but the yarestillnot atthegeneralizationlevelof RT-1. 6.3 CANWEPUSHTHERESULTING MODEL FUR THE RBYINCORPORATINGHETEROGENEOUS DATAS OUR CESSUCHASSIMULATIONOR DATA FROM DIFFERENTROBOTS? Next,weexplore the limitsof RT-1 forutilizinghighlyheterogeneous data.Wedemonstratehow RT- 1 canincorporateandlearn from vastlydifferent data sources and improve from such data with out sacrificingitsoriginal-tasksper for manceacross the varied task sinherentin this data.Tothisend,we conducttwoexperiments: (1)RT-1 trainedandtestedonbothreal data and simulation data and(2) 10 Preprint \u201cpick water bottle from the bottom drawer and put it on the counter\u201d \u201cmove sponge to green jalapeno chips\u201d \u201cplace red bull can",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 80,
      "paper_id": "rt1",
      "text": "sources and improve from such data with out sacrificingitsoriginal-tasksper for manceacross the varied task sinherentin this data.Tothisend,we conducttwoexperiments: (1)RT-1 trainedandtestedonbothreal data and simulation data and(2) 10 Preprint \u201cpick water bottle from the bottom drawer and put it on the counter\u201d \u201cmove sponge to green jalapeno chips\u201d \u201cplace red bull can in middle drawer\u201d \u201cpull napkin out of dispenser\u201d \u201cplace coke can upright\u201d \u201copen top drawer\u201d \u201cpick apple from bowl\u201d Figure 5: Exampleevaluationtrajectories for RT-1 acrossvariousinstructions. Generalization Scenario Levels Models All L 1 L 2 L 3 Gato Reedetal.(2022) 30 63 25 0 BC-ZJangetal.(2021) 45 38 50 50 BC-ZXL 55 63 75 38 RT-1(ours) 70 88 75 50 Table 3: Realistic generalization scenarios: we comp are model success rate in a realistic Google kitchen scenariosacrossthreelevelsofgeneralization:L 1 forgeneralizationto the newcounter-toplayout and lighting conditions,L 2 foradditionallygeneralizationtounseendistractorobjects,L 3 foradditionallygeneralization todrastically new task settings,new task objectsorinunseenlocationslikenearasink. RT-1 trainedacrosslarge data setsofdifferenttasks, originallycollectedbydifferentrobots. More informationoneachisprovidedin Appendix D.2. Absorbingsimulation data. Table 4 shows the abilityof RT-1, and base lines,toabsorbbothreal andsimulation data. Totest this,wetakeallof the realdemonstration data butwealsoprovidead- 11 Preprint 60% 50% Real Objects Sim Objects(notseeninreal) 40% Seen Skill Seen Skill Unseen Skill Models Training Data w/Objects w/Objects w/Objects 30% RT-1 Real Only 92 23 7 20% RT-1 Real+Sim 90(-2) 87(+64) 33(+26) 10% 0% Sim-seen Objects Sim-seen Objects Real Tasks w/ Skills w/o Skills ot derapmo C eta R sseccu S ylno lae R Real +Sim Data +64% +26% -2% Table 4: Experimental results for incorporating simulation data in RT-1. Adding simulation data doesnotimpact the per for manceonrealobjects,whilesignifi can tlyimprovingrealper for manceon objects that wereonlyintroducedinsimulation(+64%). Italsoimprovesreal-worldgeneralization onsimulatedobjectsused with skillsseenonlyin the realworld(+26%),e.g. \u201cmove Xto Y\u201dwhere Xonlyappe are dinsimulated\u201cpick X\u201dtask. ditionalsimulation data thatincludesobjects that therobothasneverseenin the realworld. Specifi- cally,wespecifydifferentgeneralizationscenarios: forseenskills with realobjects the training data hasrealdataof that instruction(i.e.,per for manceonseentasks),forseenskills with simobjects the training data hassimdataof that instruction(e.g. \u201cpickupasimobject\u201d,whichwaspresentinsim), and for unseenskills with simobjectsthetraining data hassimdataof that objectbut the reareno examplesoftheinstructiondescribingtheskill with thatobjectei the rinsimorinreal(e.g.,\u201cmove asimobjecttoapple\u201d,eventhough the robothasonlypracticedinpicking that simobject and not movingitnearo the robjects). Allevaluations are doneintherealworldbuttolimit the numberof instructionsevaluated,wefocusonpick and move-toskills. We find in Table 4 that for RT-1, we do not lose per for mance adding simulation data compared to the Real Only dataset. We do however, see a significant increase in per for mance (from 23% to 87%)onobjects and tasksseenonlyinsimulation, toapproximatelytheper for manceof the those in real, demonstrating an impressive degree of domain transfer. We also see a significant increase inper for manceonunseeninstructions from 7%to 33%;impressivegiven the objectinquestionhas never been seen in real and the instruction never seen at all. Overall, we find that RT-1 is able to efficientlyabsorb new data,even from averydifferentdomain. Absorbing data from different robots. To push the data absorption limits of RT-1, we conduct an additional set of experiments where we combine two data sources that originate from different robots: Kuka IIWA as well as the Everyday Robots mobile manipulators used in the experiments sofar. The Kuka data containsall the successfulexamplescollectedin QT-Opt(Kalashnikovetal., 2018),whichcorrespondsto 209 kepisodes,where the robotwasindiscriminatelygraspingobjects inabin(seeanexampleofa Kukaepisodein Table.5). Totestwhether RT-1 caneffectivelyabsorb thesetwoverydifferent data sets,which were fertoas the standard\u201cClassroomeval\u201d,aswellas the per for mance on the newly constructed tasks that reflect the bin-picking setup present",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 81,
      "paper_id": "rt1",
      "text": "the Everyday Robots mobile manipulators used in the experiments sofar. The Kuka data containsall the successfulexamplescollectedin QT-Opt(Kalashnikovetal., 2018),whichcorrespondsto 209 kepisodes,where the robotwasindiscriminatelygraspingobjects inabin(seeanexampleofa Kukaepisodein Table.5). Totestwhether RT-1 caneffectivelyabsorb thesetwoverydifferent data sets,which were fertoas the standard\u201cClassroomeval\u201d,aswellas the per for mance on the newly constructed tasks that reflect the bin-picking setup present in the Kuka data,which were fertoas the\u201cBin-pickingeval\u201d(see Fig.6). Wewouldliketoemphasizethedifficultyof this settingbynoting the majordifferencesbetween the datasets. Notonly are therobots that collected the datadifferentinappearance and actionspace,but alsotheenvironment the yweredeployedinhasdifferentappearance and dynamics. Inaddition the QT-Opt data presentsacompletelydifferentactiondistribution\u2013itwascollectedbyan RLagentas opposedtohum and emonstrationspresentin our data set. The results are presented in Table 5. We observe that the model that mixes the RT-1 data and the Kuka data hasonlyaminimaldecreasein the originaltasks\u2019per for mance(i.e. Classroomeval),i.e. 2%. Even more importantly, in the Bin-picking eval, we observe that the model trained on multi- robot data per for msat 39%comp are dto the 22%ofthe model that wastrainedonlyon the RT-1 data. Thisisa 17%per for mancedifference(almost 2 x). Additionally,RT-1 trainedon Kukabin-picking data and evaluated on the bin-picking tasks with the Everyday Robots (EDR) robot achieves 0% per for mance, confirming that it is difficult to transfer a behavior from another robot morphology. However, mixing the data from both robots allows RT-1 to infer the correct actions of the EDR 12 Preprint Figure 6: In Table 5,RT-1 istrained with data from tworoboticsplatforms and learnstogeneralize acrossthem. 17.5% 15.0% 12.5% Models Training Data Classroomeval Bin-pickingeval 10.0% RT-1 Kukabin-picking data+EDRdata 90(-2) 39(+17) 7.5% RT-1 EDRonly data 92 22 5.0% RT-1 Kukabin-pickingonly data 0 0 2.5% 0.0% 2.5% Bin-picking Eval Classroom Eval yln O RDE ot derapmo C eta R sseccu S EDR +Kuka Data +17% -2% Table 5: Experimental results for mixing data from two different robots. Incorporating Kuka bin- picking data from QT-Opt(Kalashnikovetal.,2018)in RT-1 minimallyimpacts the standardclass- roomevaluationper for mance and resultsinalmosta 2 ximprovementingeneralizationto the Bin- pickingevaluation(thatissimilarto the setupin the Kuka data)onthe Everyday Robotsmanipulator. Thisdemonstratesaneffectivetransferacrosstwodifferentrobotmorphologies. robot even when faced with the states observed by Kuka robots. This is achieved without explicit demonstrationsofbin-pickingon EDRrobot and bytakingadvantageofpastexperiencescollected by Kukarobots. Theseresultsindicate that RT-1\u2019sabsorptionpropertiesalsoinclude the abilityto acquire new skills through observing other robots\u2019 experiences and present an exciting avenue of futureworkwherewecombinemanymoremulti-robot data setstoenhance the robotcapabilities. 6.4 HOWDOVARIOUSMETHODSGENERALIZELONG-HORIZONROBOTICSCENARIOS? In the next set of experiments we evaluate whether our method generalizes enough to be used in long-horizonrealistickitchensettings. Toanswer this question,weexecute RT-1 andvarious base- lineswithin the Say Can(Ahnetal.,2022)frameworkintwodifferentrealkitchens. Since Say Can combines many low-level instructions to perform high-level instructions, the number of possible high-level instructions increases combinatorially with skills, so the skill-breadth of RT-1 can be fullyseen(formoredetailson the Say Canalgorithmpleasereferto Ahnetal.(2022)). Thesuccess rateoflong-horizon task salsodecreasesexponentially with thelengthof the task,sohighsuccess rates in manipulation skills are particularly important. Fur the rmore, as mobile manipulation tasks requirebothnavigation and manipulation,thepoliciesabilitytoberobustto base positioniscrucial. Moredetailisprovidedin Appendix D.3. Table 6 shows our results (on instructions in Appendix Table 12). Except for original Say Can, all methodsget 87%asplanningsuccessrate,and RT-1 performs the best,with 67%executionsuccess rate in Kitchen 1. Kitchen 2 constitutes a much more challenging generalization scene, since the Robot Classroom training scenes are modeled after Kitchen 1 (see the pictures of the kitchens in Fig.2). Dueto this generalizationdifficulty,Say Canwith Gatoisnotabletofinishanylonghorizon task, and",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 82,
      "paper_id": "rt1",
      "text": "original Say Can, all methodsget 87%asplanningsuccessrate,and RT-1 performs the best,with 67%executionsuccess rate in Kitchen 1. Kitchen 2 constitutes a much more challenging generalization scene, since the Robot Classroom training scenes are modeled after Kitchen 1 (see the pictures of the kitchens in Fig.2). Dueto this generalizationdifficulty,Say Canwith Gatoisnotabletofinishanylonghorizon task, and Say Canwith BC-Zisabletoachieve asuccessrateof 13%. Theoriginal Say Can paper didnotevaluateper for manceina new kitchen. Surprisingly,themanipulationper for mancedoesnot 13 Preprint seeavisibledrop from Kitchen 1 to Kitchen 2 for our method. Inthesupplementaryvideo,weshow that this enablesustooperateunseendrawersin Kitchen 2,andthatwe canuse Say Can-RT 1 toplan andexecuteultra-longhorizontasks,withasmanyas 50 steps. Say Can task sin Kitchen 1 Say Can task sin Kitchen 2 Planning Execution Planning Execution Original Say Can(Ahnetal.,2022)\u2217 73 47 - - Say Canw/Gato(Reedetal.,2022) 87 33 87 0 Say Canw/BC-Z(Jangetal.,2021) 87 53 87 13 Say Canw/RT-1(ours) 87 67 87 67 Table 6: Say Canstylelonghorizon task sin Kitchen 1 and Kitchen 2. (*Original Say Canevalusesa slightlydifferentpromptso the planningsuccessrateislower.) 6.5 HOWDOGENERALIZATIONMETRICSCHANGE WITH VARYINGAMOUNTSOF DATA QUANTITY AND DATADIVERSITY? While previous works have shown the scaling abilities of Trans for mer-based models (Lee et al., 2022 a;Reedetal.,2022;Jiangetal.,2022)with the numberof model parameters,inmanyrobotics works the model size is often not the primary bottleneck, and the maximum size is limited by the latency requirement for running such models on real robots. Instead, in this study we focus on ablating the influenceof data setsize and diversity,astheyplayanimportantrolein the traditionally data-limited robot learning field. Since data collection is particularly expensive for real robots, it is important to quantify what kind of data our models need to achieve a certain per for mance and generalization. Thus,ourlastquestionfocuseson the scalingpropertiesof RT-1 withdifferent data properties. Generalization Models %Tasks %Data Seen Tasks All Unseen Tasks Distractors Backgrounds Smaller Data RT-1(ours) 100 100 97 73 76 83 59 RT-1 100 51 71 50 52 39 59 RT-1 100 37 55 46 57 35 47 RT-1 100 22 59 29 14 31 41 Narrower Data RT-1(ours) 100 100 97 73 76 83 59 RT-1 75 97 86 54 67 42 53 Table 7: Various data ablations of RT-1 across seen tasks, generalization to unseen tasks, and ro- bustnesstodistractors and backgrounds. Datadiversityhasahigherimpacton the per for mance and generalizationth and ataquantity. In Table 7 we show the per for mance, generalization, and robustness of RT-1 as we decrease the dataset size (% data) and the dataset diversity (% tasks). To separate the axes of dataset size and diversity, we create smaller datasets with the same task diversity by removing data from the tasks with the largest data,capping the numberofexamplespertaskat 200(resultingin 51%ofthe data), 14 Preprint 100(37%ofthe data),and 50(22.5%ofthe data). Tocreateanarrow data set,weremove the tasks with the least data,thuskeeping 97%oftheoverall data butonly 75%ofthetasks. Aswedecrease dataset size, we see a general trend of decreasing per for mance and a steeper trend of decreasing generalization. Aswemake the datasetmorenarrow,weseemuchsteeperper for mancereductions, particularlyintermsofgeneralization. Infact,removing 25%ofthe task swhilekeeping 97%ofthe dataachievesanequivalentgeneralizationper for mancetoreducing the datasetsizebyasmuchas 49%. Ourkeytakeawayisthus that datadiversityismoreessentialth and ataquantity. 7 CONCLUSIONS, LIMITATIONS AND FUTURE WORK We presented Robotics Trans for mer 1, RT-1, a robot learning method that can effectively absorb largeamountsof data andscales with dataquantity and diversity. Wetrained RT-1 onalarge data set",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 83,
      "paper_id": "rt1",
      "text": "Infact,removing 25%ofthe task swhilekeeping 97%ofthe dataachievesanequivalentgeneralizationper for mancetoreducing the datasetsizebyasmuchas 49%. Ourkeytakeawayisthus that datadiversityismoreessentialth and ataquantity. 7 CONCLUSIONS, LIMITATIONS AND FUTURE WORK We presented Robotics Trans for mer 1, RT-1, a robot learning method that can effectively absorb largeamountsof data andscales with dataquantity and diversity. Wetrained RT-1 onalarge data set of demonstrations containing over 130 k episodes collected over the course of 17 months with 13 robots. Inourbroadsetofexperiments,wedemonstrated that ourmethod that can per for mover 700 instructionsat 97%successrate and effectivelygeneralizeto new tasks, objects and environments betterthanpreviouslypublished base lines. Wealsodemonstrated that RT-1 cansuccess full yabsorb heterogeneous data from simulationando the rrobotmorphologies with outsacrificingoriginal-tasks per for mance and whileimprovinggeneralizationto new scenarios.Lastly,weshowedhow this level ofper for mance and generalizationallowedustoexecuteverylong-horizon task sin the Say Can(Ahn etal.,2022)framework,withasmanyas 50 steps. While RT-1 presents a promising step towards large-scale robot learning with an data-absorbent model, it comes with a number of limitations. First, it is an imitation learning method, which inheritsthechallengesof that classofapproachessuchas the fact that itmaynotbeabletosurpass theper for manceof the demonstrators. Second, thegeneralizationto new instructionsislimitedto thecombinationsofpreviouslyseenconcepts and RT-1 isnotyetabletogeneralizetoacompletely newmotion that hasnot been seenbefore. Lastly, ourmethodispresentedonalargebutnotvery dexteroussetofmanipulationtasks. Weplantocontinueextending the setofinstructions that RT-1 enables and generalizestotoaddress this challenge. Asweexplorefuturedirections for thiswork,wehopeto scale the numberofrobotskillsfasterby developingmethods that allownon-expertstotrain the robotviadirected data collection and model prompting. While the current version of RT-1 is fairly robust especially to distractor objects, its robustness to backgrounds and environments could be further improved by greatly increasing the environmentdiversity. Wealsohopetoimprove the reactionspeeds and contextretentionof RT-1 throughscalableattention and memory. Toallow the researchcommunitytobuildontopof this work,wehave open-sourced the code for RT- 14,whichwehope will provideresearchers with avaluableres our ceforfutureresearch for scaling uprobotlearning. ACKNOWLEDGMENTS We would like to acknowledge Aleksandra Faust, Andy Christiansen, Chuyuan Fu, Daniel Kap- pler,David Rendleman,Eric Jang,Jessica Gomez,Jessica Lin,Jie Tan,Josh Weaver,Justin Boyd, Krzysztof Choromanski,Matthew Bennice,Mengyuan Yan,Mrinal Kalakrishnan,Nik Stewart,Paul Wohlhart, Peter Pastor, Pierre Sermanet, Wenlong Lu, Zhen Yu Song, Zhuo Xu, and the greater teamsat Roboticsat Google and Everyday Robots for the irfeedback and contributions. REFERENCES Michael Ahn,Anthony Brohan,Noah Brown,Yevgen Chebotar,Omar Cortes,Byron David,Chelsea Finn,Keerthana Gopalakrishnan,Karol Hausman,Alex Herzog,etal. Doas Ican,notas Isay: Groundinglanguageinroboticaf for dances. ar Xivpreprintar Xiv:2204.01691,2022. Daniel Cer,Yinfei Yang,Sheng-yi Kong,Nan Hua,Nicole Limtiaco,Rhomni St John,Noah Con- stant,Mario Guajardo-Cespedes,Steve Yuan,Chris Tar,etal. Universalsentenceencoder. ar Xiv preprintar Xiv:1803.11175,2018. 4 http://github.com/google-research/robotics_transformer 15 Preprint Lili Chen,Kevin Lu,Aravind Rajeswaran,Kimin Lee,Aditya Grover,Misha Laskin,Pieter Abbeel, Aravind Srinivas,and Igor Mordatch.Decisiontrans for mer:Rein for cementlearningviasequence modeling. Advancesinneuralin for mationprocessingsystems,34:15084\u201315097,2021. Michael Jae-Yoon Chung,Abram LFriesen,Dieter Fox,Andrew NMeltzoff,and Rajesh PNRao. Abayesi and evelopmentalapproachtoroboticgoal-basedimitationlearning. Plo Sone,10(11): e 0141965,2015. Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. In Conferenceon Robot Learning,2019. Marc Peter Deisenroth, Peter Englert, Jan Peters, and Dieter Fox. Multi-task policy search for robotics. In 2014 IEEEinternationalconferenceonrobotics and automation(ICRA),pp.3876\u2013 3881.IEEE,2014. Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learningmod- ularneuralnetworkpolicies for multi-task and multi-robottransfer. In 2017 IEEEinternational conferenceonrobotics and automation(ICRA),pp.2169\u20132176.IEEE,2017. Miroslav Dud\u00b4\u0131k,John Langford,and Lihong Li.Doublyrobustpolicyevaluation and learning.ar Xiv preprintar Xiv:1103.4601,2011. Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain data sets. ar Xivpreprintar Xiv:2109.13396,2021. Kuan Fang, Alexander Toshev,",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 84,
      "paper_id": "rt1",
      "text": "2017 IEEEinternational conferenceonrobotics and automation(ICRA),pp.2169\u20132176.IEEE,2017. Miroslav Dud\u00b4\u0131k,John Langford,and Lihong Li.Doublyrobustpolicyevaluation and learning.ar Xiv preprintar Xiv:1103.4601,2011. Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain data sets. ar Xivpreprintar Xiv:2109.13396,2021. Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory trans for mer for embodiedagentsinlong-horizontasks.In Proceedingsof the IEEE/CVFConferenceon Computer Vision and Pattern Recognition,pp.538\u2013547,2019. Roy Fox,Ron Berenstein,Ion Stoica,and Ken Goldberg. Multi-taskhierarchicalimitationlearning forhomeautomation. In 2019 IEEE 15 th International Conferenceon Automation Science and Engineering(CASE),pp.1\u20138.IEEE,2019. Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashch and Gandhi, and Lerrel Pinto. Robot learninginhomes:Improvinggeneralization and reducing data setbias. Advancesinneuralin for- mationprocessingsystems,31,2018. Agrim Gupta,Linxi Fan,Surya Ganguli,and Li Fei-Fei.Metamorph:Learninguniversalcontrollers withtrans for mers. ar Xivpreprintar Xiv:2203.11931,2022. Josiah PHanna,Peter Stone,and Scott Niekum. Bootstrapping with models: Confidenceintervals foroff-policyevaluation. In Thirty-First AAAIConferenceon Artificial Intelligence,2017. Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. Retina GAN: An object-aware approach to sim-to-real transfer, 2020. URL https://arxiv.org/abs/ 2011.03148. De-An Huang, Yu-Wei Chao, Chris Paxton, Xinke Deng, Li Fei-Fei, Juan Carlos Niebles, Ani- mesh Garg, and Dieter Fox. Motionreasoning for goal-basedimitationlearning. In 2020 IEEE International Conferenceon Robotics and Automation(ICRA),pp.4878\u20134884.IEEE,2020. Alexander Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, and Sergey Levine. Off-policyevaluationviaoff-policyclassification. Advancesin Neural Information Pro- cessing Systems,32,2019. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLBench: The robot learningbenchmark&learningenvironment. IEEERobotics and Automation Letters,5(2):3019\u2013 3026,2020. Eric Jang,Alex Irpan,Mohi Khansari,Daniel Kappler,Frederik Ebert,Corey Lynch,Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with roboticimitationlearning. In Confer- enceon Robot Learning,pp.991\u20131002.PMLR,2021. 16 Preprint Michael Janner,Qiyang Li,and Sergey Levine. Rein for cementlearningasonebigsequencemod- elingproblem. In ICML 2021 Workshopon Unsupervised Rein for cement Learning,2021. Yunfan Jiang,Agrim Gupta,Zichen Zhang,Guanzhi Wang,Yongqiang Dou,Yanjun Chen,Li Fei- Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodalprompts. ar Xivpreprintar Xiv:2210.03094,2022. Tom Jurgenson,Or Avner,Edward Groshev,and Aviv Tamar. Sub-goaltreesaframework for goal- basedrein for cementlearning.In International Conferenceon Machine Learning,pp.5020\u20135030. PMLR,2020. Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce- mentlearning for vision-basedroboticmanipulation. In Conferenceon Robot Learning,pp.651\u2013 673.PMLR,2018. Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic re- inforcementlearningat scale. ar Xivpreprintar Xiv:2104.08212,2021 a. Dmitry Kalashnikov, Jake Varley, Yevgen Chebotar, Ben Swanson, Rico Jonschkowski, Chelsea Finn,Sergey Levine,and Karol Hausman. MT-opt: Continuousmulti-taskroboticrein for cement learningat scale. ar Xiv,2021 b. Thomas Kollar,Stefanie Tellex,Deb Roy,and Nicholas Roy.Towardunderst and ingnaturallanguage directions. In 20105 th ACM/IEEEInternational Conferenceon Human-Robot Interaction(HRI), pp.259\u2013266.IEEE,2010. Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama,Ian Fischer,Eric Jang,Henryk Michalewski,etal. Multi-gamedecisiontransform- ers. ar Xivpreprintar Xiv:2205.15241,2022 a. Kuang-Huei Lee, Ted Xiao, Adrian Li, Paul Wohlhart, Ian Fischer, and Yao Lu. PI-QT-Opt: Pre- dictive information improves multi-task robotic rein for cement learning at scale. ar Xiv preprint ar Xiv:2210.08217,2022 b. Ian Lenz, Honglak Lee, and Ashutosh Saxena. Deep learning for detecting robotic grasps. The International Journalof Robotics Research,34(4-5):705\u2013724,2015. Corey Lynch and Pierre Sermanet. Languageconditionedimitationlearningoverunstructured data. ar Xivpreprintar Xiv:2005.07648,2020. Matt Mac Mahon,Brian Stankiewicz,and Benjamin Kuipers. Walk the talk: Connectinglanguage, knowledge,andactioninrouteinstructions. Def,2(6):4,2006.",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 85,
      "paper_id": "rt1",
      "text": "cement learning at scale. ar Xiv preprint ar Xiv:2210.08217,2022 b. Ian Lenz, Honglak Lee, and Ashutosh Saxena. Deep learning for detecting robotic grasps. The International Journalof Robotics Research,34(4-5):705\u2013724,2015. Corey Lynch and Pierre Sermanet. Languageconditionedimitationlearningoverunstructured data. ar Xivpreprintar Xiv:2005.07648,2020. Matt Mac Mahon,Brian Stankiewicz,and Benjamin Kuipers. Walk the talk: Connectinglanguage, knowledge,andactioninrouteinstructions. Def,2(6):4,2006. Hongyuan Mei,Mohit Bansal,and Matthew RWalter. Listen,attend,andwalk: Neuralmappingof navigationalinstructionstoactionsequences. In Thirtieth AAAIConferenceon Artificial Intelli- gence,2016. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language- conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning,pp.1303\u20131315.PMLR,2022. Niki Parmar,Ashish Vaswani,Jakob Uszkoreit,Lukasz Kaiser,Noam Shazeer,Alexander Ku,and Dustin Tran. Image trans for mer. In International conference on machine learning, pp. 4055\u2013 4064.PMLR,2018. Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic trans for mer for vision-and- language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pp.15942\u201315952,2021. Ethan Perez,Florian Strub,Harmde Vries,Vincent Dumoulin,and Aaron Courville. Film: Visual reasoning with a general conditioning layer. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. doi: 10.1609/aaai.v 32 i 1.11671. URL https://ojs.aaai. org/index.php/AAAI/article/view/11671. 17 Preprint Lerrel Pinto and Abhinav Gupta. Supersizingself-supervision:Learningtograsp from 50 ktries and 700 robothours. In 2016 IEEEinternationalconferenceonrobotics and automation(ICRA),pp. 3406\u20133413.IEEE,2016. Dean APomerleau. Alvinn: Anautonomousl and vehicleinaneuralnetwork. Advancesinneural informationprocessingsystems,1,1988. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp.8748\u20138763.PMLR,2021. Antonin Raffin,Ashley Hill,Rene\u00b4Traore\u00b4,Timo the\u00b4e Lesort,Natalia D\u00b4\u0131az-Rodr\u00b4\u0131guez,and David Fil- liat. Decouplingfeatureextraction from policylearning:assessingbenefitsofstaterepresentation learningingoal base drobotics. ar Xivpreprintar Xiv:1901.08651,2019. Aditya Ramesh,Mikhail Pavlov,Gabriel Goh,Scott Gray,Chelsea Voss,Alec Radford,Mark Chen, and Ilya Sutskever. Zero-shottext-to-imagegeneration. In International Conferenceon Machine Learning,pp.8821\u20138831.PMLR,2021. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. Ageneralistagent. ar Xivpreprintar Xiv:2205.06175,2022. Michael Ryoo, AJPiergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Token- learner:Adaptivespace-timetokenization for videos.Advancesin Neural Information Processing Systems,34:12786\u201312797,2021. Ashutosh Saxena, Justin Driemeyer, Justin Kearns, and Andrew Ng. Robotic grasping of novel objects. Advancesinneuralin for mationprocessingsystems,19,2006. Nur Muhammad Mahi Shafiullah,Zichen Jeff Cui,Ariuntuya Altanzaya,and Lerrel Pinto.Behavior trans for mers: Cloningkmodes with onestone. ar Xivpreprintar Xiv:2206.11251,2022. Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta. Multiple interactions made easy(mime): Large scale demonstrations data for imitation. In Conferenceonrobotlearning,pp. 906\u2013915.PMLR,2018. Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Proceedingsof the 5 th Conferenceon Robot Learning(Co RL),2021. Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task trans for mer for roboticmanipulation. ar Xivpreprintar Xiv:2209.05451,2022. Andrew Silva, Nina Moorman, William Silva, Zulfiqar Zaidi, Nakul Gopalan, and Matthew Gom- bolay.Lancon-learn:Learning with languagetoenablegeneralizationinmulti-taskmanipulation. IEEERobotics and Automation Letters,7(2):1635\u20131642,2021. Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, Murtaza Dalal, Sergey Levinev, Mohi Khansari, and Chelsea Finn. Scalable multi-task imitation learning with autonomous improve- ment. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 2167\u2013 2173.IEEE,2020. Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems,33:13139\u201313150,2020. Mingxing Tanand Quoc Le. Efficient Net: Rethinking model",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 86,
      "paper_id": "rt1",
      "text": "improve- ment. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 2167\u2013 2173.IEEE,2020. Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems,33:13139\u201313150,2020. Mingxing Tanand Quoc Le. Efficient Net: Rethinking model scaling for convolutionalneuralnet- works. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36 th In- ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 6105\u20136114. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr. press/v 97/tan 19 a.html. 18 Preprint Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller, and Nicholas Roy. Understandingnaturallanguagecommands for roboticnavigation and mobile manipulation. In Proceedingsof the AAAIConferenceon Artificial Intelligence,volume 25,pp. 1507\u20131514,2011. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa- tionprocessingsystems,30,2017. Ulrich Viereck,Andreas Pas,Kate Saenko,and Robert Platt. Learningavisuomotorcontroller for realworldroboticgraspingusingsimulateddepthimages. In Conferenceonrobotlearning,pp. 291\u2013300.PMLR,2017. Ted Xiao,Eric Jang,Dmitry Kalashnikov,Sergey Levine,Julian Ibarz,Karol Hausman,and Alexan- der Herzog. Thinkingwhilemoving:Deeprein for cementlearning with concurrentcontrol. ar Xiv preprintar Xiv:2004.06089,2020. Tianhe Yu,Deirdre Quillen,Zhanpeng He,Ryan Julian,Karol Hausman,Chelsea Finn,and Sergey Levine.Meta-world:Abenchmark and evaluation for multi-task and metarein for cementlearning. In Conferenceonrobotlearning,pp.1094\u20131100.PMLR,2020. Tianhao Zhang,Zoe Mc Carthy,Owen Jow,Dennis Lee,Xi Chen,Ken Goldberg,and Pieter Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEEInternational Conferenceon Robotics and Automation(ICRA),pp.5628\u20135635.IEEE, 2018. Yichi Zhang and Joyce Chai. Hierarchical task learning from language instructions with unified trans for mers and self-monitoring. ar Xivpreprintar Xiv:2106.03427,2021. 19 Preprint APPENDIX A AUTHOR CONTRIBUTIONS \u2022 Evaluations (ablations, designing procedures, implementations, and running abla- tions): Yevgen Chebotar,Keerthana Gopalakrishnan,Karol Hausman,Julian Ibarz,Brian Ichter, Alex Irpan, Isabel Leal, Kuang-Huei Lee, Yao Lu, Ofir Nachum, Kanishka Rao, Sumedh Sontakke,Austin Stone,Quan Vuong,Fei Xia,Ted Xiao,and Tianhe Yu. \u2022 Network Architecture (tokenizer, training, inference): Yevgen Chebotar, Keerthana Gopalakrishnan, Julian Ibarz, Alex Irpan, Kuang-Huei Lee, Yao Lu, Karl Pertsch, Kan- ishka Rao,Michael Ryoo,Sumedh Sontakke,Austin Stone,and Quan Vuong. \u2022 Developed Infrastructure(data,training,collect,simulation,evaluations,storage,and operations): Anthony Brohan,Keerthana Gopalakrishnan,Karol Hausman,Alex Herzog, Jasmine Hsu,Alex Irpan,Nikhil Joshi,Ryan Julian,Dmitry Kalashnikov,Yuheng Kuang, Isabel Leal,Yao Lu,Fei Xia,Ted Xiao,Peng Xu,Sichun Xu,and Tianhe Yu. \u2022 Leadership(managedoradvisedon the project): Chelsea Finn,Karol Hausman,Julian Ibarz,Sally Jesmonth,Sergey Levine,Yao Lu,Igor Mordatch,Carolina Parada,Kanishka Rao,Pannag Sanketi,Vincent Vanhoucke. \u2022 Paper (figures, vizualizations, writing): Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter,Sergey Levine,Ofir Nachum,Karl Pertsch,Kanishka Rao,Austin Stone,Fei Xia,and Ted Xiao. \u2022 Data collection and evaluations: Noah Brown, Justice Carbajal, Joseph Dabis, Tomas Jackson,Utsav Malla,Deeksha Manjunath,Jodily Peralta,Emily Perez,Jornell Quiambao, Grecia Salazar, Kevin Sayed, Jaspiar Singh, Clayton Tan, Huong Tran, Steve Vega, and Brianna Zitkovich. B MODEL CARD Wepresent the Model Card for RT-1 in Fig.7. C MODEL AND DATA C.1 MODELINFERENCE In addition to the inference speed requirement, we need to ensure that our system outputs actions at a consistent frequency, avoiding jitter. To accomplish this, we introduce a fixed-time waiting mechanism that waitsacertainamountoftime(280 ms,themaxobservedlatencyofallcomponents) after the state,thatwasusedtocompute the nextaction,has been captured,butbe for eapplying the action,similarlyto the proceduredescribedby Xiaoetal.(2020). C.2 DATACOLLECTIONat scale. Each of the robots autonomously approaches its station at the beginning of the episode and com- municates to the operator the instruction that they should demonstrate to the robot. To",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 87,
      "paper_id": "rt1",
      "text": "that waitsacertainamountoftime(280 ms,themaxobservedlatencyofallcomponents) after the state,thatwasusedtocompute the nextaction,has been captured,butbe for eapplying the action,similarlyto the proceduredescribedby Xiaoetal.(2020). C.2 DATACOLLECTIONat scale. Each of the robots autonomously approaches its station at the beginning of the episode and com- municates to the operator the instruction that they should demonstrate to the robot. To ensure a balanced dataset as well as randomization of the scene, we created a softw are module responsible for sampling the instructions to be demonstrated as well as the randomization of the background configuration. Each of the robots tells the demonstrator how to randomize the scene and which instructiontodemonstrate. Demonstrations are collected with direct line-of-sight between operator and robot using 2 virtual reality remotes. We map remote controls onto our policy action space to preserve consistency of thetransition-dynamics. 3 Dposition and rotationaldisplacementsof the remote are mappedto 6 d displacementsof the robottool. Thex,ypositionof the joystickismappedtoaturningangle and drivingdistanceof the mobile base. Wecompute and tracktrajectoriesto the targetposesthatwe obtain from the joystickcommands. 20 Preprint Model Card for RT-1(Robotics Trans for mer) Model Details \u2022 Developedbyresearchersat Roboticsat Google and Everyday Robots,2022,v 1. \u2022 Trans for mer-based model,builtupona Fi LM-conditioned Efficient Net(Tan&Le, 2019),a Token Learner(Ryooetal.,2021),anda Trans for mer(Vaswanietal.,2017). \u2022 Trainedwithimitationlearning with inputsofnaturallanguagetasks and images and outputrobotactions. Intended Use \u2022 Intendedtobeused for controllingan Everyday Robot for manipulationtasks. \u2022 Unclearsuitabilityasalearnedrepresentation for differentroboticembodiments, environments,orsignifi can tlyvarieddownstreamtasks. \u2022 Notsuitable for interaction with humans. Factors \u2022 Factorsincludevaryingbackgrounds,lighting,scenes,baseposition,andnovel naturallanguagetasks. Hardw are factorsincludecamera and robotembodiment. Metrics \u2022 Evaluationmetricsincludeseen task per for mance,unseen task per for mance, robustnesstobackgrounds and distractors,andper for manceinlong-horizon scenarios. Eachmeasuresthesuccessrateof the modelper for mingnaturallanguage specifiedtasks with randomizedobjectsandobjectlocations and varyingscenes. Training Data \u2022 Trainedon 130 ktele-operationdemonstrationsover 13 robots and 744 tasks. Skill Count Description Example Instruction Pick Object 130 Lifttheobjectoff the surface pickicedtea can Move Object Near Object 337 Movethefirstobjectnear the second movepepsi can nearrxbarblueberry Place Object Upright 8 Placeanelongatedobjectupright placewaterbottleupright Knock Object Over 8 Knockanelongatedobjectover knockredbull can over Open/Close Drawer 6 Openorcloseanyof the cabinetdrawers open the topdrawer Place Objectinto Receptacle 84 Placeanobjectinto are ceptacle placebrownchipbagintowhitebowl Pick Object from Receptacle 162 Pickanobjectup from alocation and then pickgreenjalapenochipbag from paper and Placeon the Counter placeiton the counter bowl and placeoncounter Additionaltasks 9 Skillstrained for realistic,longinstructions pullnapkinoutofdispenser Total 744 Evaluation Data \u2022 Evaluatedonreal-worldrandomizedscenes and over 3000 totalrolloutsin the environmentitwastrainedonaswellastwo new officekitchenenvironments. Quantitative Analyses \u2022 RT-1 showshigh-per for mance and robustness and can learn from heterogenous data. Ethical Considerations \u2022 Earlyresearch,modelhasnotyet been evaluated for suitabilitytouseoutsideofits currentresearchsetting. Caveats and Recommendations \u2022 While the current model coversonlyasmallportionofpossibleroboticmanipulation tasks,itpresents are cipe for scalableroboticlearning and anarchitecture that shows favorablegeneralization and dataabsorptionproperties. Figure 7: Model Card for RT-1. 21 Preprint C.3 MODELSELECTIONat scale Asrobotlearningsystemsbecomemorecapable and the numberofinstructionsthey can handlein- creases,evaluationof the semodelsbecomesdifficult(Kalashnikovetal.,2021 a;Jangetal.,2021). Thisisanimportantconsiderationnotonly for evaluatingdifferent model classes and datadistribu- tionsduring the developmentprocess,butalso for selecting the mostper for mant model checkpoints for a particular training run. While there have been a number of proposed solutions to this prob- lem (Dud\u00b4\u0131k et al., 2011; Irpan et al., 2019; Hanna et al., 2017), mostly known in the offline rein- forcementlearningliteratureas\u201coff-policyevaluation\u201d,itstillremainsan open researchchallenge toevaluatemulti-taskrobotlearningsystemsat scale. Inthiswork,weproposeleveragingsimulation for\u201crealtosim\u201dtransferasascalabletool that pro- videsanapproximateestimateof model per for manceduringtrainingacrossmanyrealtasks.Werun policiestrained from real data inasimulatortotest the fullrolloutper for mance. Note",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 88,
      "paper_id": "rt1",
      "text": "number of proposed solutions to this prob- lem (Dud\u00b4\u0131k et al., 2011; Irpan et al., 2019; Hanna et al., 2017), mostly known in the offline rein- forcementlearningliteratureas\u201coff-policyevaluation\u201d,itstillremainsan open researchchallenge toevaluatemulti-taskrobotlearningsystemsat scale. Inthiswork,weproposeleveragingsimulation for\u201crealtosim\u201dtransferasascalabletool that pro- videsanapproximateestimateof model per for manceduringtrainingacrossmanyrealtasks.Werun policiestrained from real data inasimulatortotest the fullrolloutper for mance. Note that allof our training data comes from the realworld(except the experimentin Section 6.3),and the simulatoris usedonly for modelselection. Toaccomplish this,weexp and the simulationenvironmentproposed by Leeetal.(2022 b)tosupport 551 ofthe task sdescribedin Section 5.2. Foreachof the setasks, we define a set of scene setup randomizations, robot pose randomizations, and success detection criteria. Tobridgethevisualdistributionshiftbetweentherealworld and the simulation, wetrain a Retina GAN (Ho et al., 2020) model that transforms simulated images into realistic looking im- ages. Then,wedeploypoliciestrainedonreal data directlyinto the sesimulationenvironmentsby applying Retina GANvisualtrans for mationsateachtimestep and measuringrolloutsimulated task successrates. While model strainedonlyonrealworld data per for mbetterintherealworldthan the ydoinsim- ulation,wefind that the simulationsuccessratesofhigh-per for mingrealworldpolicies are higher than the simulationsuccessratesoflow-per for mingrealworldpolicies.Ino the rwords,theordering of simulation policy success rates are informative for predicting the ordering of real world policy successrates. Wenote that inthisreal-to-simevaluationsetting, wehavealessstrictrequirement for simulation accuracy compared to sim-to-real settings; as long as simulation success rates are directionallycorrelated with realsuccessrates,we canacceptamoderateorevenhighgapbetween real and simulationsuccessrates. Wepresentexamplecameraimages from simulationaswellastheir Retina GAN-basedtransforma- tionsin Fig.8. Figure 8:Examplecameraimagesshowcasingrawsimulation,simulation with Retina GANapplied, and the realworld. 22 Preprint C.4 DATACOLLECTIONPROCESS Figure 9 shows the growthof data,numberoftasks,andthesuccessrateof the policyovertime.The numberoftasks/instructions that oursystemiscapableofgrowsovertimeasmore data iscollected. Thesameistrue with the per for manceofseentasks.Oneoftheimportantaspectsof the futurework isdeveloptechniques that allowustogrowthe data aswellas the robotsper for mance and general capabilitiesatafasterrate. Figure 9: Thegrowthof data,numberoftasks,andseeninstructionper for manceovertime. D EXPERIMENTS D.1 EVALUATIONDETAILS In Section 6.2, westudy the zero-shotgeneralizationcapabilitiesof RT-1 todifficultscenariosnot present in the training dataset. To fairly evaluate different ablations of RT-1 as well as baseline policies,wedesignst and ardizedevaluationprocedures that coverarangeofincrementaldifficulty levels. Seen tasks. We evaluate on 744 tasks present in the training dataset. The breakdown between 12 skillsisshownin Table 1. Forall\u201cSeen\u201devaluations, we use the sameclassroomsettingused for datacollectionasdescribedin Section 5.2. Foreachpolicy,wereportasinglerepresentativemetric thattakesaskill-weightedaverageacrossindividualskillevaluations. Unseentasks. Weevaluatepolicyper for manceon 53 tasks that are heldoutduringtraining. While the unseen instructions\u2019 specific combinations of skills and objects are not seen during training, othercombinationsofthesameskills and objects are presentin the trainingset. Weevaluatethese unseen task sinthesameenvironment and the samer and omizationprocedureas the Seentasks. A fulllistof the seunseen task sisshownin Table 8. Distractorrobustness. Wetestthreetasks(\u201cpickcoke can\u201d,\u201cplacecoke can upright\u201d,\u201cmovecoke canneargreenricechipbag\u201d)withincrementallymoredistractorobjectsaddedto the scene. The easysettingincludes 0,2,or 5 distractorobjects. Themediumsettingincludes 9 distractorobjects, but the coke can is never obscured. The hard setting includes 9 distractor objects, but the scene is more crowded and the coke can is partially occluded. Both the medium are hard setting are more difficult than scenarios in the training dataset, which contained between 0 and 4 distractors. Examplesof the sedifficultysettings and policyevaluationrollouts are shownin Figure 12. Background robustness. We test six tasks (\u201cpick coke can\u201d, \u201cmove blue chip bag near or- ange\u201d, \u201cknock redbull can over\u201d, \u201cpick green jalapeno chip bag\u201d, \u201cmove sponge near brown chip bag\u201d,\u201cplace redbull can upright\u201d) with incrementally more challenging backgrounds and counter textures. Intheeasysetting,weutilize the samebackgroundenvironments and countertexturesas thetraining data set. Inthemediumsetting,weutilize the samebackgroundenvironmentbutadda patternedtableclothtochange the countertexture.Inthehardsetting,weutilizeabr and newkitchen environment with anewcountertop;thischanges the countertexture,drawermaterial and color,and",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 89,
      "paper_id": "rt1",
      "text": "ange\u201d, \u201cknock redbull can over\u201d, \u201cpick green jalapeno chip bag\u201d, \u201cmove sponge near brown chip bag\u201d,\u201cplace redbull can upright\u201d) with incrementally more challenging backgrounds and counter textures. Intheeasysetting,weutilize the samebackgroundenvironments and countertexturesas thetraining data set. Inthemediumsetting,weutilize the samebackgroundenvironmentbutadda patternedtableclothtochange the countertexture.Inthehardsetting,weutilizeabr and newkitchen environment with anewcountertop;thischanges the countertexture,drawermaterial and color,and 23 Preprint backgroundvisuals. Examplesof the sedifficultysettings and policyevaluationrollouts are shown in Figure 10. Realistic instructions. To study how RT-1 performs in more realistic scenarios, we propose an evaluation setting in a real office kitchen that is a dramatic shift from the original training class- room environment. We propose a variety of skills that combine aspects of the previous zero-shot evaluations, including adding new distractors, including new backgrounds, and new combinations ofobjects with skills. Wereferto the easiestscenarioas L 1 generalization,whichintroducesa new countertopandlightingconditionbutkeepstheskills and objects the same. Next,L 2 generalization additionallyaddsnoveldistractorobjectssuchaskitchenjarcontainers. Finally,L 3 generalization addsnewobjectsor new locationssuchasnearasink. Whilesomeof the sedistributionshifts are tested in Section 6.2, these realistic instructions aim to test multiple dimensions simultaneously. Examplesof the seinstructions are presentedin Fig.11. Easy same background, same texture Medium same background, new texture Hard new background, new texture Figure 10: \u201cBackgrounds\u201d evaluations focus on testing the per for mance of RT-1 on settings with differenttabletextures and differentbackgrounds,suchasthosefoundinkitchensnevertrainedon. These visual differences are quite pronounced, which in the most challenging case entails a new kitchen with differentcountertexture,differentlightingconditions,differentcountermaterial,anda differentbackground. Figure 11:\u201cRealisticinstructions\u201devaluationsproposerealisticscenariosmultipledistributionshifts thatincrementallyincreaseindifficulty. L 1 generalizationintroducesa new realofficekitchen with newlightingconditions. L 2 generalizationadditionallyaddsunseendistractorobjects. Finally,L 3 generalizationincludesnewobjectsorobjectsin new locations,suchasnexttoasink. D.2 HETEROGENEOUS DATA Wealsoexplore the limitsof RT-1 forutilizinghighlyheterogeneous data.Wedemonstratehow RT- 1 canincorporateandlearn from vastlydifferent data sources and improve from such data with out 24 Preprint Instruction pickcoke can fromtopdrawer and placeoncounter pickgreen can fromtopdrawer and placeoncounter pickgreenricechipbag from middledrawer and placeoncounter pickredbull can fromtopdrawer and placeoncounter place 7 upcanintobottomdrawer placebrownchipbagintotopdrawer placegreen can intomiddledrawer move 7 upcannearredbull can moveappleneargreenricechipbag moveapplenearpaperbowl moveapplenearredbull can movebluechipbagnearblueplasticbottle movebluechipbagnearpepsi can movebluechipbagnearsponge movebrownchipbagnearapple movebrownchipbagneargreenricechipbag movebrownchipbagnearredbull can movecoke can neargreenjalapenochipbag movecoke can nearwaterbottle movegreen can near 7 upcan movegreen can nearapple movegreen can nearcoke can movegreenjalapenochipbagnearbluechipbag movegreenricechipbagnearorange movegreenricechipbagnearorange can movegreenricechipbagnearpaperbowl moveorange can nearbrownchipbag movepepsi can nearorange can moveredbull can nearcoke can moverxbarblueberrynearblueplasticbottle moverxbarblueberrynearorange can moverxbarchocolatenearpaperbowl moverxbarchocolatenearrxbarblueberry movespongenearapple movewaterbottlenear 7 upcan movewaterbottlenearsponge movewhitebowlnearorange can pickblueplasticbottle pickgreenricechipbag pickorange pickrxbarchocolate picksponge placepepsi can upright knockorange can over pickblueplasticbottle from paperbowl and placeoncounter pickbrownchipbag from whitebowl and placeoncounter pickgreen can frompaperbowl and placeoncounter pickgreenjalapenochipbag from whitebowl and placeoncounter pickorange can fromwhitebowl and placeoncounter pickredbull can fromwhitebowl and placeoncounter placeblueplasticbottleintopaperbowl placecoke can intopaperbowl placeorange can intopaperbowl Table 8: Listof Unseen Instructionsin Sec.6.2. Forthe\u201cUnseen Tasks\u201devaluation,weexcludea totalof 53 tasksduringtraining. Whiletheseexactinstructions were notpresentin the trainingset, theobjects and skillscontainedintheseinstructions were stillpresentin the trainingset. 25 Preprint Easy 2 - 5 distractors, no occlusion Medium 9 distractors, no occlusion Hard 9 distractors, occlusion Figure 12: \u201cDistractors\u201devaluationsfocusondiversifyinginitialsceneconfigurationswellbeyond thedistributionscontainedin the training data set,whichcontainbetween 2 and 4 distractorobjects. Inthemostchallengingscenarios, thesceneisextremelycluttered and containsocclusions for the objectsofinterest. sacrificingitsoriginal-tasksper for manceacross the varied task sinherentin this data. Tothisend, weconducttwoexperiments: (1)RT-1 trainedandtestedonbothreal data and simulation data and (2)RT-1 trainedacrosslarge data setsofdifferenttasks,originallycollectedbydifferentrobots. Absorbingsimulation data. Table 9 shows the abilityof RT-1, and base lines,toabsorbbothreal and simulation data. To test this, we take all of the real demonstration data but we also",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 90,
      "paper_id": "rt1",
      "text": "sacrificingitsoriginal-tasksper for manceacross the varied task sinherentin this data. Tothisend, weconducttwoexperiments: (1)RT-1 trainedandtestedonbothreal data and simulation data and (2)RT-1 trainedacrosslarge data setsofdifferenttasks,originallycollectedbydifferentrobots. Absorbingsimulation data. Table 9 shows the abilityof RT-1, and base lines,toabsorbbothreal and simulation data. To test this, we take all of the real demonstration data but we also provide additionalsimulation data thatincludesobjects that therobothasneverseenin the realworld. We addasetofsimobjects and onlyshow the monasubsetoftasks,specifically the pickingtasks,in simulation. To accomplish this, we run our real 2 sim method described in Sec. C.3 to bootstrap a simulation policy from the real world policy that is then trained with multi-task RL (Kalashnikov etal.,2021 a)withadditionalobjectsinsimulation. From this process, weextract 518 ksuccessful trajectories of picking new objects and mix them with the real data that was used in the previous experiments. Thegoalof this experimentistodemonstrate that byexp and ing the datasetofsimu- lationtrajectories,we can benefit RT-1\u2019sgeneralizationcapabilities with outsacrificing the original trainingper for mance\u2013adesiredpropertyofanabsorbent model. Toevaluate the propertiesof this model,wespecifydifferentgeneralizationscenarios:forseenskills withrealobjects the training data hasrealdataof that instruction(i.e.,per for manceonseentasks), forseenskills with simobjects the training data hassimdataof that instruction(e.g. \u201cpickupasim object\u201d,whichwaspresentinsim),and for unseenskills with simobjects the training data hassim dataof that objectbutthere are noexamplesoftheinstructiondescribing the skill with thatobject eitherinsimorinreal(e.g.,\u201cmoveasimobjecttoapple\u201d,eventhough the robothasonlypracticed inpicking that simobject and notmovingitnearo the robjects). Allevaluations are donein the real worldbuttolimit the numberofinstructionsevaluated,wefocusonpick and move-toskills. We find in Table 9 that for RT-1, we do not lose per for mance adding simulation data compared to the Real Only dataset. We do however, see a significant increase in per for mance (from 23% to 87%)onobjects and tasksseenonlyinsimulation, toapproximatelytheper for manceof the those in real, demonstrating an impressive degree of domain transfer. We also see a significant increase inper for manceonunseeninstructions from 7%to 33%;impressivegiven the objectinquestionhas never been seen in real and the instruction never seen at all. Overall, we find that RT-1 is able to efficiently\u201cspongeup\u201dnewdata,even from averydifferentdomain. 26 Preprint 60% 50% Real Objects Sim Objects(notseeninreal) 40% Seen Skill Seen Skill Unseen Skill Models Training Data w/Objects w/Objects w/Objects 30% RT-1 Real Only 92 23 7 20% RT-1 Real+Sim 90 87 33 10% 0% Sim-seen Objects Sim-seen Objects Real Tasks w/ Skills w/o Skills ot derapmo C eta R sseccu S ylno lae R Real +Sim Data +64% +26% -2% Table 9: Experimental results for incorporating simulation data in RT-1. Adding simulation data doesnotimpact the per for manceonrealobjects,whilesignifi can tlyimprovingrealper for manceon objects that wereonlyintroducedinsimulation. Absorbing data from different robots. To push the data absorption limits of RT-1, we conduct an additional set of experiments where we combine two data sources that originate from different robots: Kuka IIWA as well as the Everyday Robots mobile manipulators used in the experiments sofar. The Kuka data containsall the successfulexamplescollectedin QT-Opt(Kalashnikovetal., 2018),whichcorrespondsto 209 kepisodes,where the robotwasindiscriminatelygraspingobjects inabin(seeanexampleofa Kukaepisodein Table.10). Ourgoalin this experimentistoanalyze whether the per for manceon the RT-1 tasksdropswhenadding the additional data and,moreimpor- tantly,whe the rwe canobserveanytransfer from datacollectedbyadifferentrobotmorphology. Wewouldliketoemphasizethedifficultyof this settingbynoting the majordifferencesbetween the datasets. Notonly are therobots that collected the datadifferentinappearance and actionspace,but alsotheenvironment the yweredeployedinhasdifferentappearance and dynamics. Inaddition the QT-Opt data presentsacompletelydifferentactiondistribution\u2013itwascollectedbyan RLagentas opposedtohum and emonstrationspresentin our data set. Tomix the Kuka data toge the rwith the RT-1",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 91,
      "paper_id": "rt1",
      "text": "data and,moreimpor- tantly,whe the rwe canobserveanytransfer from datacollectedbyadifferentrobotmorphology. Wewouldliketoemphasizethedifficultyof this settingbynoting the majordifferencesbetween the datasets. Notonly are therobots that collected the datadifferentinappearance and actionspace,but alsotheenvironment the yweredeployedinhasdifferentappearance and dynamics. Inaddition the QT-Opt data presentsacompletelydifferentactiondistribution\u2013itwascollectedbyan RLagentas opposedtohum and emonstrationspresentin our data set. Tomix the Kuka data toge the rwith the RT-1 data,wefirsttransform the original Kuka 4-DOFaction spaceinto the sameactionspaceas RT-1,namelyweset the roll and pitchto 0,whilekeeping the yaw values that werepresentin the original Kuka data.Inaddition,wetransform the binarygripper-close comm and intoacontinuousgripper-closednesscomm and thatispresentin the RT-1 data. Wealso needtextinstructionscorrespondingto the taskper for med and since the Kuka data doesnotcontain thenameof the object that wasgrasped, werelabelall the datato the\u201cpickanything\u201dinstruction. With the semodifications,wemixboth data sets with the 2:1(RT-1 data: Kuka data)ratio and train RT-1 toobtain the final model. Totestwhether RT-1 caneffectivelyabsorb the setwoverydifferent data sets, weevaluate the per- formance on the original RT-1 tasks (in this case, we also focus on \u201cpick\u201d and \u201cmove to\u201d skills), which were fertoas the standard\u201cClassroomeval\u201d,aswellastheper for manceon the newlycon- structed tasks that reflect the bin-picking setup present in the Kuka data, which we refer to as the \u201cBin-pickingeval\u201d. Forthe Bin-pickingevaltobecloseto the original data set,weputin the same looking bin for the objects as well as modify the robot to be similar to the Kuka manipulators by adding extra wires and coloring the gripper gray. For all of the evaluations we use the Everyday Robotsrobot with the pickingcommands and evaluateit base don 72 graspingtrials. Theresults are presentedin Table 10. Weobserve that the model that mixes the RT-1 data and the Kuka data hasonlyaminimaldecreasein the originaltasks\u2019per for mance(i.e. Classroomeval),i.e. 2%. Even more importantly, in the Bin-picking eval, we observe that the model trained on multi- robot data per for msat 39%comp are dto the 22%ofthe model that wastrainedonlyon the RT-1 data. Thisisa 17%per for mancedifference(almost 2 x). Additionally,RT-1 trainedon Kukabin-picking data and evaluated on the bin-picking tasks with the Everyday Robots (EDR) robot achieves 0% per for mance, confirming that it is difficult to transfer a behavior from another robot morphology. However, mixing the data from both robots allows RT-1 to infer the correct actions of the EDR robot even when faced with the states observed by Kuka robots. This is achieved without explicit demonstrationsofbin-pickingon EDRrobot and bytakingadvantageofpastexperiencescollected by Kukarobots. Theseresultsindicate that RT-1\u2019sabsorptionpropertiesalsoinclude the abilityto 27 Preprint 17.5% 15.0% 12.5% Models Training Data Classroomeval Bin-pickingeval 10.0% RT-1 Kukabin-picking data+EDRdata 90 39 7.5% RT-1 EDRonly data 92 22 5.0% RT-1 Kukabin-pickingonly data 0 0 2.5% 0.0% 2.5% Bin-picking Eval Classroom Eval yln O RDE ot derapmo C eta R sseccu S EDR +Kuka Data +17% -2% Table 10: Experimental results for mixing data from two different robots. Incorporating Kuka bin-picking data from QT-Opt (Kalashnikov et al., 2018) in RT-1 minimally impacts the standard classroomevaluationper for mance and resultsinalmosta 2 ximprovementingeneralizationto the Bin-picking evaluation (that is similar to the setup in the Kuka data) on the Everyday Robots ma- nipulator. Thisdemonstratesaneffectivetransferacrosstwodifferentrobotmorphologies. acquire new skills through observing other robots\u2019 experiences and present an exciting avenue of futureworkwherewecombinemanymoremulti-robot data setstoenhance the robotcapabilities. D.3 LONG-HORIZONEVALUATIONDETAILS Inadditiontoshort-horizonindividualskillevaluationsshowninprevioussections,wealsoevaluate how RT-1 per for msinalong-horizonrealistickitchensetting that chainsmultiplemanipulation and navigation skills to accomplish natural language",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 92,
      "paper_id": "rt1",
      "text": "to the setup in the Kuka data) on the Everyday Robots ma- nipulator. Thisdemonstratesaneffectivetransferacrosstwodifferentrobotmorphologies. acquire new skills through observing other robots\u2019 experiences and present an exciting avenue of futureworkwherewecombinemanymoremulti-robot data setstoenhance the robotcapabilities. D.3 LONG-HORIZONEVALUATIONDETAILS Inadditiontoshort-horizonindividualskillevaluationsshowninprevioussections,wealsoevaluate how RT-1 per for msinalong-horizonrealistickitchensetting that chainsmultiplemanipulation and navigation skills to accomplish natural language instructions within the Say Can framework (Ahn etal.,2022). Alistoflong-horizoninstructionsused for the seevaluationsislistedin Table 12. Thesuccessrateoflong-horizon task sdecreasesexponentially with thelengthof the task,sohigh successratesinmanipulationskills are particularlyimportant. Fur the rmore,asmobilemanipulation tasks require both navigation and manipulation, the policies ability to be robust to base position iscrucial. Since Say Cancombinesmanylow-levelinstructionstoper for mhigh-levelinstructions, the number of possible high-level instructions increases combinatorially with instructions, so the skill-breadthof RT-1 can befullyseen. Say Can works by grounding language models in robotic affordances and it leverages few-shot prompting to break down a long horizon task expressed in natural language to a sequence of low level skills. An example of long horizon task would be \u201cBring me two different sodas\u201d, and one feasibleplanwouldbe\u201c1. findacoke,2. pickup the coke,3. bringittoyou,4. putdown the coke, 5. findapepsi,6. pickup the pepsi,7. bringittoyou,8. putdown the pepsi,9. done.\u201d Toobtain the affordancefunctionwe usevaluefunctionstrained with MT-OPT(Kalashnikovetal.,2021 a). Fora detaileddescriptionof Say Canalgorithmpleasereferto (Ahnetal.,2022). Since the focusof this paperisacquisitionofmanygeneralizableskills,wefocus our evaluationon onesubsetof task spresentedin Ahnetal.(2022).Itis the long-horizonfamilyoftasks,involving 15 instructions,eachinstructionrequiresanaverageof 9.6 stepstocomplete,andinvolvesanaverage of 2.4 manipulationskillsperinstruction. Afulllistof the instructions can befoundin Table 12. Wecomp are against 3 baselines.1)Say Canwith BC-Z,whichuses Say Canplanningalgorithm with BC-Z as manipulation policy, 2) Say Can with Gato, which uses Say Can planning algorithm with Gato as manipulation policy, 3) Originally reported Say Can results, which use Say Can planning algorithm with BC-Z,butsinceitusesaslightlydifferentprompt,theplanningsuccessrateislower. Wereimplemented 3)in 1)forafaircomparison. Asshownin Table 11,except for original Say Can,allmethodsget 87%asplanningsuccessrate,and RT-1 performs the best,with 67%executionsuccessratein Kitchen 1. Kitchen 2 constitutesamuch morechallenginggeneralizationscene,since the Robot Classroomtrainingscenes are modeledafter Kitchen 1 (see the pictures of the kitchens in Fig. 2). Due to this generalization difficulty, Say Can with Gato is not able to finish any long horizon task, and Say Can with BC-Z is able to achieve a success rate of 13%. The original Say Can paper did not evaluate per for mance in a new kitchen. Surprisingly,themanipulationper for mancedoesnotseeavisibledrop from Kitchen 1 to Kitchen 2 28 Preprint for our method. Inthesupplementaryvideo,weshow that thisenablesustooperateunseendrawers in Kitchen 2,andthatwe canuse Say Can-RT 1 toplan and executeultra-longhorizontasks,withas manyas 50 steps. Say Can task sin Kitchen 1 Say Can task sin Kitchen 2 Planning Execution Planning Execution Original Say Can(Ahnetal.,2022)\u2217 73 47 - - Say Canw/Gato(Reedetal.,2022) 87 33 87 0 Say Canw/BC-Z(Jangetal.,2021) 87 53 87 13 Say Canw/RT-1(ours) 87 67 87 67 Table 11: Say Canstylelonghorizon task sin Kitchen 1 and Kitchen 2. (*Original Say Canevaluses aslightlydifferentpromptso the planningsuccessrateislower.) D.4 MODELABLATIONS What are the important and practical decisions in the design of the model and how do they affectper for mance and generalization? To answer this question, we perform a set of ablations over different design decisions in RT-1. We aim to test a number of hypo the ses that will help us disambiguate where the benefits of our methodcome from. Possiblehypothesesabout the sourceofimprovementinclude: (i)thecapacity andexpressivenessof our model,whichweverifybyablating the modelsize,tryingo the rarchitec- tures(e.g.,byremoving the",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 93,
      "paper_id": "rt1",
      "text": "answer this question, we perform a set of ablations over different design decisions in RT-1. We aim to test a number of hypo the ses that will help us disambiguate where the benefits of our methodcome from. Possiblehypothesesabout the sourceofimprovementinclude: (i)thecapacity andexpressivenessof our model,whichweverifybyablating the modelsize,tryingo the rarchitec- tures(e.g.,byremoving the Trans for mercomponent);(ii)theparticularactionrepresentation,which makesiteasytorepresentcomplexmulti-modalactiondistributions,whichwetestbyswitchingto continuous(normallydistributed)actions,aswellasbyablating the auto-regressiveactionrepresen- tation;(iii)the Image Netpre-trainedinitializationof the components,whichwetestbyinitializing themodel\u2019sweightsr and omly;and(iv)accessto the shor this tory,whichwetestbyexcludingob- servationhistory.Moreconcretely,weablate our modelby(1)decreasing the modelsize(from 35 M to 21 Mparameters),(2)removing the Trans for merarchitecture(usingapre-trained Efficient Netin- stead),(3)usingacontinuousinsteadofdiscreteactionspace(usingan MSEloss and multivariate normaloutput), (4)auto-regressivelyconditioningonactions, (5)removing Image Netpre-training of the Fi LM Efficient Net, and (6) removing history (reducing the sequence of six images as input to a single image). For each ablation we comp are on the axes of per for mance on seen tasks, per- formanceonunseentasks,aswellasinferencespeedandrobustnesstodistractors and backgrounds (withamoredetaileddescriptionofeachcategoryin Section 6.1 and Appendix D.1). Table 13 shows the results of each ablation and the delta per for mance compared to the full RT-1. RT-1 achievesimpressiveper for manceontasks and newenvironments,andparticularlyoutperforms baselines on the most challenging robustness problems. We also find that each design decision is important,thoughatvaryinglevels. Wefirstevaluatea model that replaces the per-dimensiondis- cretizedactionrepresentationin our model with amorest and ardcontinuous Gaussi and istribution. We observe a significant decline in per for mance from this modification. The per-dimension dis- cretization allows our model to represent complex multi-modal distributions, while the Gaussian distributioncapturesonlyasinglemode. Theseresultssuggest that thisstandard and popularchoice ishighlysuboptimal with the morecomplex and diversedemonstration data usedby our system.Im- age Netpre-trainingisparticularlyimportant for modelgeneralization and robustness,decreasing the unseen task per for mancerateby 33%, asaresultof the large and diversevisualsof the Image Net dataset. Adding history has an impact primarily on generalization to distractors, while removing the Transformercomponenthasauni for mbutsmallnegativeimpactacross the seentasks, unseen tasks and distractors. Inordertokeep the Image Netpre-trainingwhilereducing the modelsize,we reduce the number of parameters only by 40% (from 31 M to 25 M). Resulting per for mance drops across training and generalization tasks but not as much as in other ablations. Finally, autoregres- sivelyconditioningonactions, asusedin(Reedetal.,2022;Chenetal.,2021;Leeetal.,2022 a), didnotbenefitper for mance and slowedinferencebymorethan 2 x. Asdescribedin Sec.5.1,inordertorunlarge Trans for mer model sonrealrobots,werequirea model thatsupportsfastinference for real-timeoperation. Note that inordertoachieve our targetcontrol rateof 3 Hz(describedin Sec.5.1),wealsoneedtoconsiderothers our cesoflatencyin the pipeline, such as the camera latency and communication overhead. However, these factors will be constant 29 Preprint Instruction Howwouldyouputanenergybar and waterbottleon the table Howwouldyoubringmealimesoda and abagofchips Canyouthrowaway the apple and bringmeacoke Howwouldyoubringmea 7 upcan and atea? Howwouldthrowawayalltheitemson the table? Howwouldyoumoveanmultigrainchipstothetable and anappleto the farcounter? Howwouldyoumove the limesoda,thesponge,andthewaterbottleto the table? Howwouldyoubringmetwosodas? Howwouldyoumovethreecokesto the trash can? Howwouldyouthrowawaytwocokes? Howwouldyoubringmetwodifferentsodas? Howwouldyoubringmeanapple,acoke,andwaterbottle? Ispilledmycokeon the table,howwouldyouthrowitaway and the nbringmesomething tohelpclean? Ijustworkedout,canyoubringmeadrink and asnacktorecover? Howwouldyoubringmeafruit,asoda,andabagofchips for lunch Table 12: Listof Say Caninstructionsevaluatedin Sec.6.4 Distractors Backgrounds Model Seen Tasks Unseen Tasks All Easy Medium Hard All Inference Time(ms) Gato(Reedetal.,2022) 65(-32) 52(-24) 43(-40) 71 44 29 35(-24) 129 BC-Z(Jangetal.,2021) 72(-25) 19(-57) 47(-36) 100 67 7 41(-18) 5.3 BC-ZXL 56(-41) 43(-33) 23(-60) 57 33 0 35(-24) 5.9 RT-1(ours) 97 76 83 100 100 64 59 15 RT-1 w/obig model 89(-8) 62(-14) 77(-6) 100 100 50 53(-6) 13.5 RT-1 w/opre-training 84(-13) 43(-33) 60(-23) 100 67 36 41(-18) 15 RT-1 w/continuousactions 68(-29) 43(-33) 37(-46) 71",
      "start_pos": 10164,
      "end_pos": 10676
    },
    {
      "chunk_id": 94,
      "paper_id": "rt1",
      "text": "100 67 7 41(-18) 5.3 BC-ZXL 56(-41) 43(-33) 23(-60) 57 33 0 35(-24) 5.9 RT-1(ours) 97 76 83 100 100 64 59 15 RT-1 w/obig model 89(-8) 62(-14) 77(-6) 100 100 50 53(-6) 13.5 RT-1 w/opre-training 84(-13) 43(-33) 60(-23) 100 67 36 41(-18) 15 RT-1 w/continuousactions 68(-29) 43(-33) 37(-46) 71 67 0 35(-24) 16 RT-1 w/auto-regressiveactions 85(-12) 71(-5) 67(-16) 100 78 43 65(+6) 36 RT-1 w/ohistory 82(-15) 62(-14) 50(-33) 71 89 14 59(+0) 15 RT-1 w/o Trans for mer 86(-13) 62(-14) 67(-16) 100 100 29 59(+0) 26 Table 13: Various model ablations of RT-1 across seen tasks, generalization to unseen tasks, and robustnesstodistractors and backgrounds. for all the models, and therefore we focus our evaluation on just the network inference time. The last column of Table 13 shows the inference speed of all the models. RT-1 is almost an order of magnitudefasterthan Gato with asimilarnumberofparameters,butitisalsoconsiderablyslower than a Res Net-based BC-Z. In terms of the different ablations of our model, we observe that the biggestslow-downiscausedbyincludingauto-regressiveactions(\u223c2 xslow-down),andsince this doesnotsignifi can tlyinfluence the per for mance,thefinalversionof RT-1 doesnotgenerateactions auto-regressively. 30 Preprint D.5 SUMMARY AND ANALYSIS In this section, we summarize some of our findings and propose intuition for RT-1\u2019s high perfor- mance,generalization,androbustness. First,Image Netpretraining(along with Universal Sentence Encoder language embedding) has a large impact particularly on unseen tasks. We observe that RT-1 inherits some of the knowledge that results from the generality and diversity of the datasets thesemodels were trainedon. Second,continuousactions have alargeimpactacrossallaspectsof per for mance. This has been previously observed and may be due to the ability to represent more complexactiondistributions\u2013theper-dimensiondiscretizationallows our model torepresentcom- plexmulti-modaldistributions,while the Gaussi and istributioncapturesonlyasinglemode. Third, given such expressive multi task models, data diversity has a larger impact than data size. Indeed, even datasets collected in simulated environments or from different robotic embodiments can be leveragedby RT-1,openingavenues for newregimesof data collection. Finally,RT-1 fuseslanguageinto the imagepipelineearlyvia Fi LMconditioning,comp are dtoe.g., Gato\u2019slatefusion. Thisenablesimagetokens that focusonlyonrelevantfeatures for the instruction at hand, which may be the cause of poor distractor per for mance for Gato. Figure 13 visualizes theattentionduringrolloutsof RT-1. Wesee that the attentionisfocusedonrelevantfeatures and particularlyoninteractionbetweenthegripper and the objectofinterest.Thebottleneckofattention layers such as these results in a compact representation which effectively ignores distractors and varyingbackgrounds. \u201cpick green jalapeno chip Layer 2, bag from middle Head 6 drawer and place on counter\u201d \u201cplace rxbar Layer 2, blueberry in Head 6 bottom drawer\u201d Layer 4, \u201copen middle Head 2 drawer\u201d Figure 13: Inthisfigureweshow the attentionmapof the RT-1 policy. Differentlayers and heads generallyfocusondifferentpartof the image. Mostcommonly,theyfocusonthepartsof the scene with the richest interaction affordances, such as graspable objets. For example, Layer 2 Head 6 focuses on the jalapeno chips and pepsi can in grasping tasks; and Layer 4 Head 2 focuses on the drawerindrawer open ingtasks. 31",
      "start_pos": 10626,
      "end_pos": 11076
    },
    {
      "chunk_id": 95,
      "paper_id": "RLGSBridge",
      "text": "RL-GSBridge: 3 D Gaussian Splatting Based Real 2 Sim 2 Real Method for Robotic Manipulation Learning Yuxuan Wu\u2217, Lei Pan\u2217, Wenhua Wu, Guangming Wang, Yanzi Miao, Fan Xu# and Hesheng Wang# Abstract\u2014Sim-to-Real refers to the process of transferring 1. Real 2 Sim by Soft Mesh Binding GS policieslearnedinsimulationto the realworld,whichiscrucial for achieving practical robotics applications. However, recent Sim 2 real methods either rely on a large amount of augmented data or large learning models, which is inefficient for specific tasks. In recent years, with the emergence of radiance field reconstruction methods, especially 3 D Gaussian splatting, it hasbecomepossibletoconstructrealisticreal-worldscenes.To Dynamics-based GS Editing Grasp Pick&place this end, we propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3 D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot sim- to-real transfer for vision-based deep rein for cement learning. Render img RL-GS We introduce a mesh-based 3 D GS method with soft binding Bridge constraints, enhancing the rendering quality of mesh models. Thenutilizinga GSeditingapproachtosynchronize the render- 2. Learn Policy at 3. Zero-shot Real-world ing with the physics simulator, RL-GSBridge could reflect the Simulator with Physic Robot Manipulation on visual interactions of the physical robot accurately. Through a Dynamics-based GS renderer Various Tasks series of sim-to-real experiments, including grasping and pick- Fig. 1. Pipeline of RL-GSBridge. (1) Real 2 Sim Environment Transfer. and-place tasks, we demonstrate that RL-GSBridge maintains Real-world scenarios is reconstructed through a novel soft mesh binding asatisfactorysuccessrateinreal-world task completionduring GS model. (2) Learn Policy at Simulator with GS Render. With physical sim-to-realtransfer.Fur the rmore,aseriesofrenderingmetrics dynamics-based GS editing, RL policies learn through realistic rendered andvisualizationresultsindicate that ourproposedmesh-based images in simulation. (3) Zero-shot Real-world Robot Manipulation. We 3 DGSreducesartifactsinunstructuredobjects,demonstrating directlyapply the policytoreal-worldtasks with outfine-tuning. more realistic rendering per for mance. I. INTRODUCTION reality, or to train a highly generalized large model to learn Learningroboticactionpoliciesinsimulation and transfer- knowledge for differenttasks.Thissignifi can tlyincreases the ring them to real-world represents an ideal robotic learning difficulty in training stage. strategy that balancesboththecost and safetyof the learning process.However,asignifi can tbottleneckis the reliabilityof To avoid additional training burden and achieve ideal sim-to-realtransfer,whichimpactsthepotentialof the entire Sim 2 Real per for mance on specific tasks, a novel framework framework towards substantial challenges. is needed. Recently, advances in radiance field-based recon- With the continuous development of the simulation-to- struction methods [5], [6], [7], [8] provide new directions reality(Sim 2 Real)field,extensiveworkisadvancingprogress for Sim 2 Real training. Based on a simple idea\u2014using radi- from multiple perspectives [1], [2], [3], [4]. However, most ance field reconstruction to create a visually realistic robot Sim 2 Real methods attempt to expand the distribution of trainingenvironment\u2014canweachievesatisfactory Sim 2 Real training data to cover various situations that may arise in per for mance? For this purpose, we design a Real 2 Sim 2 Real visualrein for cementlearningframework,RL-GSBridge,that Thisworkwassupportedinpartby the Shenzhen Science and Technology bridges the real-to-sim gap by 3 D Gaussian splatting, as Program under Grant KJZD 20230923114812027. (Corresponding Author: shown in Fig. 1. Utilizing 3 D Gaussian splatting and editing Fan Xuand Hesheng Wang) *Equalcontributions techniques, RL-GSBridge provide a \u2018virtual-reality\u2019 simula- #Co-correspondingauthors tion platform for policy learning. Y. Wu, W.",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 96,
      "paper_id": "RLGSBridge",
      "text": "and Technology bridges the real-to-sim gap by 3 D Gaussian splatting, as Program under Grant KJZD 20230923114812027. (Corresponding Author: shown in Fig. 1. Utilizing 3 D Gaussian splatting and editing Fan Xuand Hesheng Wang) *Equalcontributions techniques, RL-GSBridge provide a \u2018virtual-reality\u2019 simula- #Co-correspondingauthors tion platform for policy learning. Y. Wu, W. Wu, F. Xu, and H. Wang are with the Shenzhen Research Instituteof Shanghai Jiao Tong University,Shenzhen 518000,China. Forvision-basedrobottasks,itiscrucialtoavoidillusions Y. Wu, F. Xu, and H. Wang are also with the Department of Automa- caused by inconsistencies between visual perception and tion, Shanghai Jiao Tong University, Shanghai 200240, China. (e-mail: contact geometry. Thus, it is required to ensure an accurate furrygreen@sjtu.edu.cn;xufanlyra@sjtu.edu.cn;wanghesheng@sjtu.edu.cn) W. Wu is also with Mo E Key Lab of Artificial Intelligence, AI Institute, geometric representation of the model while achieving more Shanghai Jiao Tong University,Shanghai 200240,China. realisticrenderingresults.Ga Me S[9]hasdrawn our attention L. Pan and Y. Miao are with the School of Information and Control asitisamesh-based Gaussiansplatting(GS)method,which Engineering,China Universityof Mining and Technology,Xuzhou 221100, China. ensures that the optimizationof GSunitsisper for medwithin G.Wangis with the Departmentof Engineering,Universityof Cambridge, the geometric mesh model. However, it enforces Gaussians Cambridge CB 21 PZ,U.K.(e-mail:gw 462@cam.ac.uk) to be aligned with the mesh grid planes, which could be Code is available at https://github.com/IRMV-Manipulation-Group/RL- GSBridge considered as \u2018hard mesh binding\u2019, thereby limiting the 5202 be F 22 ]OR.sc[ 2 v 19202.9042:vi Xra flexibility of GS units. To address this, we propose a soft learning [20]. Meta-learning aims to teach robots the ability mesh binding method for Gaussian Splatting, which could to learn new tasks, whereas distillation learning trains a further enhance rendering quality while preserving editing student network using knowledge from expert networks. capabilities for both objects and the background. Ourapproachaligns with the firstcategoryofgap-bridging Physics-based dynamics simulation is also a crucial and methods, but with a unique twist. We use soft mesh binding challenging aspect of sim 2 real. To tackle this, an off-the- GS to create realistic simulation environments for robot shelf physics simulator is used to provide dynamic changing training.Typically,achievingsuchfidelityrequiresexpensive information. The GS editing process simultaneously updates 3 D scanning equipment or CAD expertise. In contrast, GS the scene, ensuring that the rendering results align with modelsvisuallyrealisticsimulationenvironmentsusingonly physical interaction processes. multi-view images captured with consumer-grade devices. Based on the designed simulation platform, we train B. Radiance Field in Robotics roboticmanipulationpoliciesbydeeprein for cementlearning methods. The model is trained on grasping and pick-and- Neural Radiance Fields (Ne RF) [5] is an implicit repre- place tasks across scenarios that include diverse textures, sentation technique for novel view syn the sis. It optimizes geometric shapes, and patterned desktop backgrounds. the parameters through multi-view images, and allows for Under the RL-GSBridge framework, the policy shows a the syn the sis of any target views using volumetric ren- minor variation in success rates during Sim 2 Real transfer. dering. Ne RF\u2019s representation has been applied to various This means that the policy maintains effective per for mance tasks, including Simultaneous Localization and Mapping on real-world tasks, reflecting a strong ability to generalize (SLAM) [7], [21], [22], scene reconstruction [23], scene from simulation to real-world environments.",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 97,
      "paper_id": "RLGSBridge",
      "text": "2 Real transfer. dering. Ne RF\u2019s representation has been applied to various This means that the policy maintains effective per for mance tasks, including Simultaneous Localization and Mapping on real-world tasks, reflecting a strong ability to generalize (SLAM) [7], [21], [22], scene reconstruction [23], scene from simulation to real-world environments. segmentation [24], navigation [25], and manipulation [26]. To summarize our contributions: Ne RF-RL [27] treats the novel view syn the sis task of Ne RF as a proxy task, where the learned encoding network \u2022 A Novel Sim 2 Real RL Framework: Leveraging the is directly used as feature input for rein for cement learning. high-fidelity rendering of 3 D GS and the convenience Y. Li et al. [28] uses Ne RF as a perceptual decoder for of modeling scenes with only consumer-grade cameras. the hidden states of a world model, training an additional \u2022 A Soft Mesh Binding GS Modeling Method: Propos- dynamicsestimationnetworktopredictfuturestatechanges. ing a soft mesh binding strategy to replace the hard Ne RF 2 Real [29] learns real-world contexts by converting mesh binding baseline, enhancing the flexibility and background meshes into a simulator, and trains robots to render quality. perform visual navigation, obstacle avoidance, and ball- \u2022 Physical Dynamics-Based GSEditing:Integratingdy- handling tasks. However, the training and rendering speed namicssignals from the simulatortoedit 3 DGSmodels, of Vanilla Ne RF has consistently been a bottleneck limiting reflecting realistic physical robotic interactions. its further deployment in practical applications. \u2022 Validation on Real Physical Robots: Testing the RL- 3 D GS [30] is an explicit radiance field method that GSBridge framework on physical robots through grasp- directly updates the attributes of each 3 D Gaussian com- ing and pick-and-place tasks in real-world scenarios ponent to optimize scene representation. 3 D GS employs with complex textures and geometries. a splatting technique. for rendering, and achieve extremely II. RELATEDWORK fast training efficiency through CUDA parallel technology. Moreover, compared to the implicit representation of Ne RF, A. Sim 2 Real Transfer in RL the explicit representation facilitates tracking dynamic scene RL constructs an interactive learning model in which an modeling and editing scene content. agent learns to maximize rewards through trial and error. Many robotics works also incorporate 3 D Gaussian tech- By incorporating deep learning, deep rein for cement learn- niques for perception and motion learning [31]. Mani Gaus- ing(DRL) enhances the framework\u2019s ability to tackle more sian [32] uses 3 D GS as visual and dynamic scene repre- complextasks[10].DRLhasshownimpressiveper for mance sentation for policy learning. Quach et al. [33] combine GS across various domains, including games [11], finance [12], with Liquid networks for real-world drone flight navigation autonomous driving [13], and robotics [14]. However, most tasks, training in simulation and then deploying the policy applications are restrictedtovirtualenvironmentsduetoreal- in the real world. world constraints related to safety, efficiency, and cost. Incontrastto Ne RF 2 Real[29],whichdirectlytextures the To improve the feasibility of deploying models in the real foreground objects in simulator, we model each foreground world, many researchers strive to bridge the gap between object with editable GS parameters. Also unlike the method simulation and reality[15].",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 98,
      "paper_id": "RLGSBridge",
      "text": "to safety, efficiency, and cost. Incontrastto Ne RF 2 Real[29],whichdirectlytextures the To improve the feasibility of deploying models in the real foreground objects in simulator, we model each foreground world, many researchers strive to bridge the gap between object with editable GS parameters. Also unlike the method simulation and reality[15]. These methods include domain designed by Quach et al. [33], which trains drone naviga- randomization [16] and domain adaptation [17], [18]. Do- tion policies for target-oriented tasks, our approach involves mainr and omizationinvolvesvarying task-relatedparameters robotic arm manipulation tasks with complex interactions. in the simulation to cover a broad range of real-world III. METHODS conditions, while domain adaptation focuses on extracting a unified feature space from both sources. Higher-level RL-GSBridgeaimstoharness the potentialofhigh-fidelity learning methods include metalearning [19] and distillation 3 DGS model sin Sim 2 Real for robotactiontrainingtasks.In this paper, we focus on manipulation tasks for robotic arms. Smoother Asshownin Fig.1,theoverallframeworkisdividedintotwo GS representation parts:Real 2 Simand Sim 2 Real.In Real 2 Simstage,wecollect real-world image data {I }, where I represent the image k k sequences of the k-th object or background in the scenarios. We will model both the geometry and appearance of the MVS scenetobuildasimulationenvironment for the manipulation task. In Sim 2 Real stage, we use visual perception and deep rein for cementlearningtotrainapolicynetwork,anddirectly 2 2 transfer the policy to the real world. 3 A. Real 2 Sim: Building simulator with soft mesh binding GS 1 1 Mesh model Soft binding constraint To obtain a realistic simulation environment, we use consumer-grade cameras to capture 2 D image data I of Fig. 2. Mesh-based GS Reconstruction with Soft Binding Constraints: k Releasing the hard constraints of Ga Me S [9] in the normal direction for desktop-level operating platforms. Our goal is to model a smoo the randmoreflexibleobjectsurfaces. geometricallyaccurate and texture-realisticsimulation model for trainingoperationaltasks.Morespecifically,we usemesh models {M k } to represent the accurate geometric informa- To address this, we propose a soft mesh binding method, tion, and Gaussian sets G k ={gi k } for high-quality texture. whichbuildsupon Ga Me S[9],butrelaxes the enforcedhard Below, we sequentially describe the steps to construct the mesh binding to a soft binding constraint. We introduce a Simulator with GS renderer. component along the normal direction into the vector of 1) Real-world Data Preparing: We use a monocular positions, specifically: cameraormobilephonetocapturea 1-2 minutevideoof the \u00b5i(cid:0) \u03b1i,\u03b1i,\u03b1i,\u03b1i(cid:1) =\u03b1iv +\u03b1iv +\u03b1iv +\u03b1iv , (2) targetobjecton the experimentalplatform.Weselectapprox- 1 2 3 n 1 1 2 2 3 3 n n imately 200 keyframes from the video and use COLMAP here, \u03b1i is a learnable weight parameter and v is the n n [34] to obtain the camera\u2019s internal and external parameters normal vector. \u03b1i is constrained within the range of [\u22121,1], n foreachframe.Imagesegmentationalgorithmisalsoneeded ensuring the association between each mesh model element for extracting the target object, and we use an off-the-shelf and Gaussian pairs. As shown in Fig. 2, our method allows segmenter,SAM-track[35],forefficientobjectsegmentation. the Gaussian units within the mesh to float within a certain To complete the GS model reconstruction for the simula- range along the normal vector. This flexibility in Gaus- tor,itisalsonecessarytopre-modelageometricallyaccurate",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 99,
      "paper_id": "RLGSBridge",
      "text": "the target object, and we use an off-the-shelf and Gaussian pairs. As shown in Fig. 2, our method allows segmenter,SAM-track[35],forefficientobjectsegmentation. the Gaussian units within the mesh to float within a certain To complete the GS model reconstruction for the simula- range along the normal vector. This flexibility in Gaus- tor,itisalsonecessarytopre-modelageometricallyaccurate sian unit optimization could bring a smoother distribution mesh model as a prior model for GS. Here we consider a of Gaussian units on the object\u2019s surface. Ultimately, the classic and stable open-source package open MVS to obtain algorithm not only ensures that the Gaussian model can the corresponding mesh model. still represent the accurate geometric structure according 2) Real 2 Sim model ingbysoftmeshbinding GS: With the to the mesh models, but also offers some tolerance and object mesh, we can define Gaussian units within triangular refinement space. Besides, the binds between the mesh grid faces as in Ga Me S [9], a method that binds Gaussians and Gaussians could even provide the possibility to handle onto the surface of meshes, and optimizes their properties non-rigid objects, as demonstrated in section IV-B.4. through multi-view consistency. Vanilla GS [30] optimizes 3) Physic Dynamics-Based GS Editing: After obtaining the attribute parameters of Gaussian units located at each the visual GS models {G } and geometry models {M } k k point cloud position, where \u03b8i = (\u00b5i,ri,si,\u03c3i,ci) denotes for real-world objects, we combine the dynamic simulation the position, rotation, scale, opacity, and color of the i- results from the simulator with real-time Gaussian model th Gaussian unit gi, respectively. To achieve controllable updates and render views to ensure that the visual represen- geometric editing effects, Ga Me S [9] constrains Gaussian tationfollows the entirephysicalchangeprocess.Wefirstuse units within the triangular mesh of the object surface, using RANSAC plane regression and manual alignment methods the mean vector as the convex combination of the mesh to align GS models with mesh models in the simulator, and vertices to establish the positional relationship between the set the initial position of the GS model. Gaussian unit and the three vertices of the triangular mesh: With the aligned initial model, we read the real-time pose changes of each object in the operational scene from the \u00b5i(cid:0) \u03b1 1 i,\u03b1 2 i,\u03b1 3 i(cid:1) =\u03b1 1 iv 1 +\u03b1 2 iv 2 +\u03b1 3 iv 3 , (1) simulator. For the k-th object with its GS model {G k }, We acquire the rotation quaternion q and the homogeneous here, v ,v ,v represents the triangular mesh vertex posi- k 1 2 3 trans for mation matric T in the world coordinate system. tions, \u03b1i,\u03b1i,\u03b1i are learnable positional weight parameters k 1 2 3 Given Gaussian parameters \u03b8i=(\u00b5i,ri,si,\u03c3i,ci), The edit- forg.However,thisapproachofen for cedmesh-GSbinding k k k k k k i ing process of each Gaussian gi in the model set G could would diminish the flexibility of 3 D Gaussians, limiting the k k be formulated as: optimization of Gaussian units when the mesh model is not accurate, introducing certain undesirable defects. \u00b5i =T \u00b5i, ri =q \u00d7ri. (3) k k k",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 100,
      "paper_id": "RLGSBridge",
      "text": "of each Gaussian gi in the model set G could would diminish the flexibility of 3 D Gaussians, limiting the k k be formulated as: optimization of Gaussian units when the mesh model is not accurate, introducing certain undesirable defects. \u00b5i =T \u00b5i, ri =q \u00d7ri. (3) k k k k k k Physical Simulator Simulated Eye-in-hand Image ... Physic Dynamics-Based GS Editing render 1 , 2 , rendered image , Viewpoint Conv M M L Base L Q P Con trol le r P Actor Critic Fig.3. Policytrainingpipelinein RL-GSBridge.Intheupperhalfof the figure,physicdynamics-based GSeditingreceives the trans for mationsignals of objects and synchronizes the states of GS models. In the lower half of the figure, an actor-critic RL network receives first-person perspective images renderedby GSmodelsasinput,tolearnavision-basedmanipulationpolicy. TABLEI here, (s ,a ,r ,s ) is a tuple belong to B, the y is t, t, t+1 t+1 t COMPARISONOFSUCCESSRATESBETWEENRL-GSBRIDGE AND calculated by target Q networks Q : RL-SIMINGRASPINGEXPERIMENTS,ALLCONDUCTINGUNDERFOAM \u03c6,j PAD(FP)BACKGROUND.BOLDINDICATESBETTERRESULTS.VALUES y t =r t +\u03b3(min Q \u03c6,j (s t+1 ,a t+1 )\u2212\u03b1log\u03c0 \u03b8 (a t+1 |s t+1 )). (5) j=1,2 INPAREN THE SESREPRESENTRELATIVECHANGEINSUCCESSRATE DURINGSIM 2 REALTRANSFER.(\u2193XX%)INDICATESADECREASE, In continuous action space, policy \u03c0 \u03b8 outputs actions of WHILE(\u2191XX%)INDICATESANINCREASE. Gaussian distribution. The loss for actor is defined as: 1 Loss = \u2211 Object Small cube Bear actor |B| (st,at,rt+1,st+1)\u2208B (6) Test scene Sim Real Sim Real (\u03b1log\u03c0 \u03b8 (a (cid:101)t |s t )\u2212min Q \u03c6,j (s t ,a (cid:101)t )), j=1,2 RL-sim 96.88 12.50 (\u219387%) 93.75 25.00 (\u219373%) where a is sampled using the reparameterization trick, i.e., (cid:101)t RL-GSBridge 96.88 96.88 87.50 100.00 (\u219114%) a = f (\u03b5;s), \u03b5 is Gaussian random noise. In practical t \u03b8 t t implementation, we set: f =tanh(\u00b5 (s)+\u03c3 (s)\u2299\u03b5). \u03b8 \u03b8 t \u03b8 t t Since vanilla SAC struggles to converge rapidly under sparse reward, to accelerate the learning process through As for local non-rigid trans for mations on the mesh grid, the automated guidance, we propose SACw B by referring to Gaussians could be updated according to Equation 2. After DDPGw B [38], and introduce a lightly designed baseline applying trans for mations to all object models, we concat the controllertoguide the policy.Thisapproachavoidscomplex Gaussian sets to acquire the Gaussian model of the whole reward designs while eliminating ineffective action spaces. scene G . Then we use rasterization to render G , scene scene In SACw B, the agent executes actions from the baseline obtaining the synchronized edited rendering view. controller with probability \u03bb and selects the best option between the baselinecontroller\u2019sand the actor\u2019soutputs with probability 1\u2212\u03bb, the objective is typically defined as: B. Sim 2 Real: Train in simulation with physic dynamics- y =r +\u03b3max((min Q (s ,a )\u2212\u03b1log\u03c0 (a |s )), based GS renderer and zero-shot transfer to reality t t \u03c6,j t+1 t+1 \u03b8 t+1 t+1 j=1,2 (min Q (s ,\u00b5 (s ))\u2212\u03b1log\u03c0 (\u00b5 (s )|s ))). We use Pybullet [36] as the simulation training platform, j=1,2 \u03c6,j t+1 b t+1 \u03b8 b t+1 t+1 (7) and employ SAC (Soft Actor-Critic) [37] algorithm for For the supervision of the actor network, we introduce an policy learning, due to",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 101,
      "paper_id": "RLGSBridge",
      "text": "Q (s ,\u00b5 (s ))\u2212\u03b1log\u03c0 (\u00b5 (s )|s ))). We use Pybullet [36] as the simulation training platform, j=1,2 \u03c6,j t+1 b t+1 \u03b8 b t+1 t+1 (7) and employ SAC (Soft Actor-Critic) [37] algorithm for For the supervision of the actor network, we introduce an policy learning, due to its mature development in RL and additionalbehaviorcloneloss L in Equation 6 tosupervise widespread application in robotics. The SAC is an offline bc the mean values of the action distribution through base policy algorithm belonging to maximum entropy RL, which controller actions, with the probability \u03bb: consistsoftwocriticnetworks,Q (s,a)and Q (s,a),and \u03c6,1 \u03c6,2 one actor network, \u03c0 \u03b8 (s). For the sampled set B from the L bc =\u2225\u00b5 \u03b8 (s)\u2212\u00b5 b (s)\u22252, (8) replay buffer, the update loss of critic is defined as: and with the probability 1\u2212\u03bb: (cid:13) (cid:13)2 Loss critic = |B 1 | \u2211 (st,at,rt+1,st+1)\u2208B (y t \u2212Q \u03c6,j (s t, a t ))2, (4) L bc = (cid:13) (cid:13) (cid:13) \u00b5 \u03b8 (s)\u2212\u00b5|argmax( j m =1 in ,2 Q \u03c6,j (s,\u00b5 \u03b8 (s))) (cid:13) (cid:13) (cid:13) , (9) TABLEII SIM 2 REALRESULTS FOR GRASPING TASK INVARIOUSMANIPULATIONSCENARIOS.CONTENTSINP ARE NTHESESAFTER THE OBJECTNAMES REPRESENTDIFFERENTBACKGROUNDS(BG),WHEREFPDENOTESFOAMPAD,ANDTCDENOTESTABLECLOTH. Object(Bg) Cake(FP) Banana(FP) Small cube(TC) Cake(TC) Banana(TC) Bear(TC) Testscene Sim Real Sim Real Sim Real Sim Real Sim Real Sim Real Successrate(%) 100.00 100.00 100.00 93.75(\u21936%) 96.88 87.50(\u219310%) 100.00 93.75(\u21936%) 100.00 96.88(\u21933%) 87.50 75.00(\u219314%) Simulation Real World (Ours) Real World (RL-sim) Camera View Robot Arm Behavi our Fig.4. Comparisonofsim-to-realbehaviorconsistencybetween RL-GSBridge and RL-sim. TABLEIII The Intel Real Sense D 435 i camera fixed on the robot arm THESIM 2 REALRESULT FOR THE PICK AND PLACE TASK. captures the RGB images from the first-person perspective. 2) Tasks: As shown in Fig. 1, we design two types of Object Cake&Plate tasks from the robot\u2019s first-person perspective: grasping and Testscene Sim Real pick-and-place operations. Successrate(%) 68.75 71.87(\u21914%) For grasping, the robot grasps a target object and lifts it up.Theinitialpositionsof the objectsarer and omizedwithin a 30 \u00d7 30 cm section. We use Small cube, Cake, Banana, and Bear as objects. As for the operation platform, we use here, \u03bb is the decay factor, which gradually approaches 0 as a foam pad with and without a tablecloth as two different training progresses. backgrounds. Success is considered when the object is lifted The whole training pipeline in the simulator with physic 10 cm above the table. dynamics-based GS renderer is shown in Fig. 3. The RL Forpick-and-placetasks,theroboticarmpickup the cake algorithm provides executable actions to the physics sim- model and place it on a plate. The position of the cake is ulator, which will return state information for actor-critic set similarly as described in the grasping task. The plate learning. The GS renderer simultaneously renders the realis- is placed in a fixed position on the foam pad. Success is tic images by physic dynamics-based GS rendering, serving considered when the cake is placed on the plate. as observation input to the actor network. We rely solely on 3) Evaluation Setup: For each task, we conduct both the thefirst-personperspectiveimage and proprioceptivestateof simulation and the real world experiments.",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 102,
      "paper_id": "RLGSBridge",
      "text": "pad. Success is tic images by physic dynamics-based GS rendering, serving considered when the cake is placed on the plate. as observation input to the actor network. We rely solely on 3) Evaluation Setup: For each task, we conduct both the thefirst-personperspectiveimage and proprioceptivestateof simulation and the real world experiments. During testing, the robotic arm, considering several advantages as described we divide the 30 \u00d7 30 cm section into four quadrants. in [39]. To enhance sim 2 real robustness, we add random For each task, we test the policy across a fixed number of noise to the rendered images of the GS model and randomly positions in each quadrant to calculate the success rate. alter certain image attribute parameters during training for 4) Baseline: To demonstrate that our method effectively environment challenges. reduces Sim 2 Real gap, we conduct a baseline experiment After policy training in the simulator, we directly deploy called RL-sim: using the same learning method but training the actor network onto the real robot arm, with the real- directly on images rendered from mesh models shaded in world eye-in-hand camera observations of the environment the Py Bullet simulator. We select two representative and serving as visual input. The trained policy subsequently easily shaded objects: the Small cube and Bear. Grasping outputsthepositionof the end-effector and the gripperstates experiments for RL-sim are conducted on a foam pad. as executable actions. B. Experiment Results IV. EXPERIMENTS 1) Grasping: In Table I,wecomp are the graspingper for- mance of RL strategies trained with the baseline (RL-sim) A. Experiment Setup and RL-GSBridge in both simulation and real environments. 1) Robot Platform: We use a KUKA iiwa robot arm For both the Small cube with simple geometry and textures paired with a Robotiq 2 F-140 gripper as the platform. and the Bear with complex geometry and textures, RL-sim TABLEIV OURSOFTMESHBINDINGMETHODCOMP ARE DWITHGAMES[9]ONMULTIPLE FOR EGROUNDOBJECTS AND BACKGROUNDRENDERING METRICS.SSIMIS SCALE STRUCTURALSIMILARITYINDEX.PSNRISPEAKSIGNAL-TO-NOISERATIO.LPIPSISLEARNEDPERCEPTUALIMAGE PATCHSIMILARITY.BGREFERSTO THE BACKGROUND,WHILECONTENTSINP ARE NTHESESEXPLAIN THE DETAILEDDIFFERENCES. Banana Bear Cake Small cube Bg (Foam Pad) Bg (Tablecloth) Bg (Plate) Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S SSIM\u2191 0.989 0.894 0.964 0.956 0.975 0.964 0.989 0.989 0.944 0.939 0.781 0.776 0.776 0.756 PSNR\u2191 35.46 26.53 29.82 28.18 33.38 30.73 37.18 36.64 28.40 27.32 22.88 21.86 21.86 20.80 LPIPS\u2193 0.026 0.049 0.034 0.040 0.066 0.079 0.012 0.012 0.144 0.149 0.248 0.308 0.308 0.311 Banana Cake Bg (tablecloth) Bg (plate) Before Editing After Editing GT Ga Me S Fig. 6. The editing capability on non-rigid objects of our soft binding constraint GSmodelingmethod. Ours Fig.5. Oursoftbindingconstraintreconstructionmethodcomp are dwith simulator and in real scenarios. With the same environment, Ga Me S[9]ontwo for egroundobjects and twobackgrounds. RL-GSBridgeexhibitsbehaviorhighlyconsistent with simu- lationtestsduringmanipulation,whereas RL-sim with out the GS model shows significant differences. Notably, blue lights shows a significant success rate drop when transferring to in the camera view of Fig. 4 is caused by the gripper indi- real world (an average decrease of 80%) due to visual dis- cator light. The image augmentation during policy",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 103,
      "paper_id": "RLGSBridge",
      "text": "out the GS model shows significant differences. Notably, blue lights shows a significant success rate drop when transferring to in the camera view of Fig. 4 is caused by the gripper indi- real world (an average decrease of 80%) due to visual dis- cator light. The image augmentation during policy training crepancies. In contrast, RL-GSBridge demonstrates a minor would mitigate this hue effect, ensuring the consistency of variation in success rates, maintaining high per for mance as policy behavior. Meanwhile, RL-GSBridge sitll ensure the in simulation. Notably, in the Bear grasping scenario, the consistency of texture details between simulated and real- success rate in the real world has increased by 14.28%. This world images. maybeattributedto the suboptimalsimulationof Bear\u2019ssoft 4) GS Rendering results of different Mesh Binding ap- material and non-structured shape in the simulation. proach: In Table IV,wecomp are the per for manceof Ga Me S Table II shows that RL-GSBridge experiences an average [9] and our soft mesh binding GS model under various drop of 6.6% of the success rate in sim-to-real transfer scenarios and achieve the SOTA per for mance. Fur the rmore, across various complex test scenarios, including diverse Fig. 5 shows that our method obtains fewer artifacts and objects and desktop backgrounds. This demonstrates that more detailed texture rendering. As a supplement, in Fig. the integration of the physic dynamics-based GS rendering 6, our soft binding constraint GS modeling method achieves effectively bridges the perception gap between simulation consistent results in editing a non-rigid toy bear. Unfortu- and real environments, maintaining stable strategy transfer nately, due to the limitations in simulating soft objects in across a range of scenarios. Additionally, we observe a Py Bullet,wedonotdemonstratecomprehensiveexperiments noticeable decrease in success rates for the Bear in the on deformable objects manipulation. However, this can be Tablecloth background scenario, both in simulation and real explored as a future direction. environments.Theprimaryreason for thisdropis the choice of a brown tablecloth, which has similar texture features to V. CONCLUSIONS the brown toy bear, causing difficulty in visual perception. 2) Pick-and-Place: As shown in Table III, we test RL- We propose RL-GSBridge, a real-to-sim-to-real frame- GSBridge\u2019s Sim 2 Real per for mance in pick-and-place task work for robotic rein for cement learning. As an attempt where a cake is placed onto a plate. The results indicate a to apply the recently successful radiance field reconstruc- 4.54% increase of success rate in real environments, mainly tion methods to construct a realistic robotic simulator, RL- due to the differences in physical contacts between simula- GSBridge has shown promising sim-to-real success rates tion and reality. In simulation, even minor excess contacts in desktop-level tasks. This motivates us to explore future during the placementprocess are consideredas task failures. directions, such as investigating the simulation of realistic In contrast, some contacts in real environments that do not lighting [40], and integrating RL-GSBridge with advanced affect the task can betolerated,leadingtobetterper for mance large-scalepolicymodels[41]andperceptionlearningmeth- since the task is ultimately completed successfully. ods [42], [43], [44].We hope RL-GSBridge will enc our age 3) Comparison of Sim&Real Behavior Consistency: In moreattemptstoapplyradiancefieldreconstructionmethods Fig. 4, we",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 104,
      "paper_id": "RLGSBridge",
      "text": "contacts in real environments that do not lighting [40], and integrating RL-GSBridge with advanced affect the task can betolerated,leadingtobetterper for mance large-scalepolicymodels[41]andperceptionlearningmeth- since the task is ultimately completed successfully. ods [42], [43], [44].We hope RL-GSBridge will enc our age 3) Comparison of Sim&Real Behavior Consistency: In moreattemptstoapplyradiancefieldreconstructionmethods Fig. 4, we comp are the behavior of the robotic arm in the in robotics. REFERENCES IEEE/CVFConferenceon Computer Vision and Pattern Recognition, 2024,pp.21167\u201321177. [1] W. Zhao, J. P. Queralta, L. Qingqing, and T. Westerlund, \u201cTowards [22] S.Zhu,R.Qin,G.Wang,J.Liu,and H.Wang,\u201cSemgauss-slam:Dense closing the sim-to-realgapincollaborativemulti-robotdeepreinforce- semantic gaussian splatting slam,\u201d ar Xiv preprint ar Xiv:2403.07494, mentlearning,\u201din 20205 th Internationalconferenceonrobotics and 2024. automationengineering(ICRAE). IEEE,2020,pp.7\u201312. [23] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, \u201cMonosdf: [2] F. Muratore, C. Eilers, M. Gienger, and J. Peters, \u201cBayesian Exploringmonoculargeometriccues for neuralimplicitsurfacerecon- domain randomization for sim-to-real transfer,\u201d ar Xiv preprint struction,\u201dNeur IPS,pp.25018\u201325032,2022. ar Xiv:2003.02471,2020. [24] S.Zhi,T.Laidlow,S.Leutenegger,and A.J.Davison,\u201cIn-placescene [3] K. Arndt, M. Hazara, A. Ghadirzadeh, and V. Kyrki, \u201cMeta rein- labelling and underst and ing with implicit scene representation,\u201d in forcementlearning for sim-to-realdomainadaptation,\u201din 2020 IEEE ICCV,2021,pp.15838\u201315847. internationalconferenceonrobotics and automation(ICRA). IEEE, [25] M.Adamkiewicz,T.Chen,A.Caccavale,R.Gardner,P.Culbertson, 2020,pp.2725\u20132731. J.Bohg,and M.Schwager,\u201cVision-onlyrobotnavigationinaneural [4] R. Traore\u00b4, H. Caselles-Dupre\u00b4, T. Lesort, T. Sun, N. D\u00b4\u0131az-Rodr\u00b4\u0131guez, radianceworld,\u201dRA-L,pp.4606\u20134613,2022. and D. Filliat, \u201cContinual rein for cement learning deployed in real- [26] Q.Dai,Y.Zhu,Y.Geng,C.Ruan,J.Zhang,and H.Wang,\u201cGraspnerf: life using policy distillation and sim 2 real transfer,\u201d ar Xiv preprint multiview-based 6-dof grasp detection for transparent and specular ar Xiv:1906.04452,2019. objectsusinggeneralizablenerf,\u201din ICRA,2023,pp.1757\u20131763. [5] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor- [27] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint, \u201cRein- thi, and R. Ng, \u201cNerf: Representing scenes as neural radiance fields forcementlearning with neuralradiancefields,\u201dNeur IPS,2022. forviewsyn the sis,\u201din ECCV,2020. [28] Y.Li,S.Li,V.Sitzmann,P.Agrawal,and A.Torralba,\u201c3 dneuralscene [6] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, \u201cNerf++: representations for visuomotorcontrol,\u201din Co RL,2022,pp.112\u2013123. Analyzing and improving neural radiance fields,\u201d ar Xiv preprint [29] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori, ar Xiv:2010.07492,2020. T. Haarnoja, B. Moran, S. Bohez, F. Sadeghi, B. Vujatovic, et al., [7] Z.Zhu,S.Peng,V.Larsson,W.Xu,H.Bao,Z.Cui,M.R.Oswald, \u201cNerf 2 real: Sim 2 real transfer of vision-guided bipedal motion skills and M. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for usingneuralradiancefields,\u201din ICRA,2023,pp.9362\u20139369. slam,\u201din CVPR,2022,pp.12786\u201312796. [30] B.Kerbl,G.Kopanas,T.Leimkuehler,and G.Drettakis,\u201c3 dgaussian [8] S.Zhu,G.Wang,H.Blum,J.Liu,L.Song,M.Pollefeys,and H.Wang, splatting for real-timeradiancefieldrendering,\u201dACMTrans.Graph., \u201cSni-slam:Semanticneuralimplicitslam,\u201dCVPR,2024. vol.42,no.4,2023. [9] J.Waczyn\u00b4ska,P.Borycki,S.Tadeja,J.Tabor,and P.Spurek,\u201cGames: [31] S. Zhu, G. Wang, D. Kong, and H. Wang, \u201c3 d gaussian splatting in Mesh-based adapting and modification of gaussian splatting,\u201d ar Xiv robotics:Asurvey,\u201dar Xivpreprintar Xiv:2410.12262,2024. preprintar Xiv:2402.01459,2024. [32] G.Lu,S.Zhang,Z.Wang,C.Liu,J.Lu,and Y.Tang,\u201cManigaussian: [10] V.Franc\u00b8ois-Lavet,P.Henderson,R.Islam,M.G.Bellem are,J.Pineau, Dynamicgaussiansplatting for multi-taskroboticmanipulation,\u201dar Xiv etal.,\u201cAnintroductiontodeeprein for cementlearning,\u201dFoundations preprintar Xiv:2403.08321,2024. and Trends\u00ae in Machine Learning, vol. 11, no. 3-4, pp. 219\u2013354, [33] A.Quach,M.Chahine,A.Amini,R.Hasani,and D.Rus,\u201cGaussian 2018. splattingtorealworldflightnavigationtransfer with liquidnetworks,\u201d [11] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van ar Xivpreprintar Xiv:2406.15149,2024. Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, [34] J.L.Scho\u00a8nberger and J.-M.Frahm,\u201cStructure-from-motionrevisited,\u201d M. Lanctot, et al., \u201cMastering the game of go with deep neural in Conferenceon Computer Vision and Pattern Recognition(CVPR), networks and tree search,\u201d nature, vol. 529, no. 7587, pp. 484\u2013489, 2016. 2016. [35] Y. Cheng, L. Li, Y. Xu, X. Li, Z. Yang, W. Wang, and Y. Yang, [12] Y. Deng, F. Bao, Y. Kong, Z. Ren,",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 105,
      "paper_id": "RLGSBridge",
      "text": "of go with deep neural in Conferenceon Computer Vision and Pattern Recognition(CVPR), networks and tree search,\u201d nature, vol. 529, no. 7587, pp. 484\u2013489, 2016. 2016. [35] Y. Cheng, L. Li, Y. Xu, X. Li, Z. Yang, W. Wang, and Y. Yang, [12] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, \u201cDeep direct \u201cSegment and trackanything,\u201dar Xivpreprintar Xiv:2305.06558,2023. reinforcementlearning for financialsignalrepresentation and trading,\u201d [36] E. Coumans and Y. Bai, \u201cPybullet, a python module for physics IEEE transactions on neural networks and learning systems, vol. 28, simulation for games,robotics and machinelearning,\u201d2016. no.3,pp.653\u2013664,2016. [37] T.Haarnoja,A.Zhou,P.Abbeel,and S.Levine,\u201cSoftactor-critic:Off- [13] X. Pan, Y. You, Z. Wang, and C. Lu, \u201cVirtual to real rein for cement policymaximumentropydeeprein for cementlearning with astochastic learning for autonomous driving,\u201d ar Xiv preprint ar Xiv:1704.03952, actor,\u201din Internationalconferenceonmachinelearning. PMLR,2018, 2017. pp.1861\u20131870. [38] G.Wang,M.Xin,W.Wu,Z.Liu,and H.Wang,\u201cLearningoflong- [14] L.Pinto,M.Andrychowicz,P.Welinder,W.Zaremba,and P.Abbeel, horizonsparse-rewardroboticmanipulatortasks with basecontrollers,\u201d \u201cAsymmetric actor critic for image-based robot learning,\u201d ar Xiv IEEETransactionson Neural Networks and Learning Systems,vol.35, preprintar Xiv:1710.06542,2017. no.3,pp.4072\u20134081,2022. [15] Y.Liu,W.Chen,Y.Bai,J.Luo,X.Song,K.Jiang,Z.Li,G.Zhao, [39] K. Hsu, M. J. Kim, R. Rafailov, J. Wu, and C. Finn, \u201cVision-based J.Lin,G.Li,etal.,\u201cAligningcyberspace with physicalworld:Acom- manipulators need to also see from their hands,\u201d in International prehensivesurveyonembodiedai,\u201dar Xivpreprintar Xiv:2407.06886, Conferenceon Learning Representations,2021. 2024. [40] J.Gao,C.Gu,Y.Lin,Z.Li,H.Zhu,X.Cao,L.Zhang,and Y.Yao, [16] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,and P.Abbeel, \u201cRelightable 3 d gaussians: Realistic point cloud relighting with brdf \u201cDomain randomization for transferring deep neural networks from decomposition and raytracing,\u201din European Conferenceon Computer simulation to the real world,\u201d in 2017 IEEE/RSJ international Vision. Springer,2024,pp.73\u201389. conference on intelligent robots and systems (IROS). IEEE, 2017, [41] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King, \u201cA survey pp.23\u201330. on vision-language-action models for embodied ai,\u201d ar Xiv preprint [17] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr- ar Xiv:2405.14093,2024. ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al., \u201cUsing [42] R.Mendonca,S.Bahl,and D.Pathak,\u201cStructuredworldmodels from simulation and domain adaptation to improve efficiency of deep humanvideos,\u201dar Xivpreprintar Xiv:2308.10901,2023. roboticgrasping,\u201din 2018 IEEEinternationalconferenceonrobotics [43] X. Fang, D. Liu, P. Zhou, and G. Nan, \u201cYou can ground earlier andautomation(ICRA). IEEE,2018,pp.4243\u20134250. than see: An effective and efficient pipeline for temporal sentence [18] X. Fang, A. Easwaran, B. Genest, and P. N. Suganthan, \u201cYour data groundingincompressedvideos,\u201din CVPR,2023. is not perfect: Towards cross-domain out-of-distribution detection in [44] X. Fang, Z. Xiong, W. Fang, X. Qu, C. Chen, J. Dong, K. Tang, class-imbalanced data,\u201dESWA,2024. P.Zhou,Y.Cheng,and D.Liu,\u201cRethinkingweakly-supervisedvideo [19] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, temporalgrounding from agameperspective,\u201din ECCV,2025. R.Munos,C.Blundell,D.Kumaran,and M.Botvinick,\u201cLearningto rein for cementlearn,\u201dar Xivpreprintar Xiv:1611.05763,2016. [20] A.A.Rusu,S.G.Colmenarejo,C.Gulcehre,G.Desjardins,J.Kirk- patrick,R.Pascanu,V.Mnih,K.Kavukcuoglu,and R.Hadsell,\u201cPolicy distillation,\u201dar Xivpreprintar Xiv:1511.06295,2015. [21] S.Zhu,G.Wang,H.Blum,J.Liu,L.Song,M.Pollefeys,and H.Wang, \u201cSni-slam: Semantic neural implicit slam,\u201d in Proceedings of the",
      "start_pos": 4620,
      "end_pos": 5045
    },
    {
      "chunk_id": 106,
      "paper_id": "rialto",
      "text": "Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation Marcel Torne 1 Anthony Simeonov 1,4 Zechu Li 1,3,4 April Chan 1,4 Tao Chen 1,4 Abhishek Gupta 2\u2217 Pulkit Agrawal 1,4\u2217 1 Massachusets Institute of Technology 2 University of Washington 3 TU Darmstadt 4 Improbable AI Lab Abstract\u2014Imitation learning methods need significant human data across a massive range of scenes since content creation supervision to learn policies robust to changes in object poses, can be challenging in simulation and data collection can be physical disturbances, and visual distractors. Rein for cement challenging for the real world, (2) a widely general, robust learning, on the other hand, can explore the environment policy may be overly conservative, lowering its per for mance autonomously to learn robust behaviors but may require im- practical amounts of unsafe real-world data collection. To learn on the specific target domains encountered on deployment. per for mant, robust policies without the burden of unsafe real- Alternatively, we suggest that to maximally benefit a specific world data collectionorextensivehumansupervision,wepropose user, it is more critical that the robot achieves high success Rial To, a system for robustifying real-world imitation learning in their particular home environment, showing robustness policies via rein for cement learning in \u201cdigital twin\u201d simulation to various local disturbances and distractors that might be environmentsconstructedon the fly from smallamountsofreal- world data. To enable this real-to-sim-to-real pipeline, Rial To encountered in this setting. With this in mind, our goal is to proposes an easy-to-use interface for quickly scanning and develop a robot learning technique that requires minimal hu- constructingdigitaltwinsofreal-worldenvironments.Wealsoin- man effort to syn the size visuomotor manipulation controllers troduceanovel\u201cinversedistillation\u201dprocedure for bringingreal- that are extremely robust for task per for mance in deployment world demonstrations into simulated environments for efficient environments. The question becomes - how do we acquire fine-tuning, with minimal human intervention and engineering required.Weevaluate Rial Toacrossavarietyofroboticmanipu- these robust controllers without requiring prohibitive amounts lationproblemsin the realworld,suchasrobustlystackingdishes of effort for data collection or simulation engineering? on a rack, placing books on a shelf, and six other tasks. Rial To A potential technique for data-driven learning of robotic increases (over 67%) in policy robustness without requiring control policies is to adopt the paradigm of imitation learning extensive human data collection. Project website and code at (IL), learning from expert demonstration data [58, 56, 25]. https://real-to-sim-to-real.github.io/Rial To/. However, controllers learned via imitation learning tend to I. INTRODUCTION exhibit limited robustness unless a large number of demon- strations are collected. Fur the rmore, imitation learning does Imagine a robot that can de-clutter kitchens by putting notlearntorecover from mistakesorout-of-distributiondistur- dishesonadishrack.Considerall the environmentalvariations bancesunlesssuchbehaviors were intentionallydemonstrated. that might be encountered: different configurations of plates This makes direct imitation learning algorithms unsuitable for or changes in rack positions, a plate unexpectedly slipping in widespread, robust deployment in real-world scenarios. the gripper during transit, and visual distractions, including The alternative paradigm of rein for cement learning (RL) clutter and lighting changes. For the robot to be effective, allows robots to train on self-collected data, reducing the it must robustly solve the task across the various scene and burden",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 107,
      "paper_id": "rialto",
      "text": "real-world scenarios. the gripper during transit, and visual distractions, including The alternative paradigm of rein for cement learning (RL) clutter and lighting changes. For the robot to be effective, allows robots to train on self-collected data, reducing the it must robustly solve the task across the various scene and burden on humans for extensive data collection [31] and object perturbations, without being brittle to transient scene to discover robust recovery behaviors beyond a set of pre- disturbances.Ourdesiderataisaframework that makesiteasy collected demonstrations (e.g., re-grasping when an object is for humans to program the robot to achieve a task robustly dropped, re-aligning when an object moves in the gripper, under the sevariationsordisturbances.Tobeascalablechoice adjusting to external perturbations, etc. \u2014 see examples in for deployment, the framework should not make task-specific Fig. 1). However, directly per for ming RL in the real world is assumptions and must seamlessly apply to many tasks. prohibitively slow, often results in unsafe data collection, and To design these types of robust robot controllers, one could is challenging due to problems like resets and reward specifi- attempt to train policies across a massive range of scenes and cation[75].Therefore,currently,it\u2019simpracticalinmanycases with highly variable objects [12, 21]. This is hard-pressed to employ RL for learning robust control policies directly in to provide a scalable solution to robotic learning for two therealworld.Simulation,ontheo the rhand,offers the ability reasons - (1) it is challenging to actually collect or syn the size to collect significant amounts of data broadly, cheaply, safely, *Equaladvising and withprivileged information [12,34, 61,40, 1].However, 4202 vo N 42 ]OR.sc[ 3 v 94930.3042:vi Xra 1 Real-to-sim transfer of the scene 2 Real-to-sim transfer of policies Point cloud-based Policy 3 D reconstruction through API Articulated USD Imitation (Universal Scene Descriptor) learning 3 Simulation fine-tuning RL fine-tuning with sparse Simulation rollout rewards and demos Privileged information demos in sim Sim-to-real transfer Robust to disturbances and distractors in the real world 4 New distractor Robot base Perturb closing Policy succeeds objects moved toaster opening the toaster Fig. 1. Rial To system overview. 1) Transfer the real-world scene to the simulator through an easy-to-use API (see Section III-B). 2) Transfer a policy learned from real-worlddemonstrationstocollectasetofdemonstrations with privilegedin for mationinsimulation.Wenote this stepisoptional,and Rial To is compatible with skipping this step and providing demonstrations in simulation (see Section IV-C 2) 3) Use the collected set of demonstrations to bias explorationin the RLfine-tuning with sparserewardsofastate-basedpolicy(see Section III-C)4)Per for mteacher-studentdistillation and deploy the policy intherealworldobtainingrobustbehaviors(see Section III-D). manually constructing geometrically, visually, and physically engineering, we leverage a set of real-world demonstrations realistic simulation environments for problems like robotic thatbootstrapefficientfine-tuning with rein for cementlearning. manipulation in the home can be time and labor-intensive, These real-world demonstrations help narrow the sim-to-real making it an impractical alternative at scale. gap and increase the per for mance of our policies, as shown in To safely and efficiently learn robust manipulation behav- Section IV-B.However,transferringreal-worlddemonstrations iors, our key insight is to train RL controllers on quickly into simulation is non-trivial because we do not have access constructed simulation scenes. By leveraging a video from to the Lagrangian state of",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 108,
      "paper_id": "rialto",
      "text": "of our policies, as shown in To safely and efficiently learn robust manipulation behav- Section IV-B.However,transferringreal-worlddemonstrations iors, our key insight is to train RL controllers on quickly into simulation is non-trivial because we do not have access constructed simulation scenes. By leveraging a video from to the Lagrangian state of theenvironment (e.g. object poses). the target deployment domain, we can obtain scenes complete We therefore propose a new \u201cinverse-distillation\u201d technique with accurate geometry and articulation that reflect the ap- thatenablestransferringreal-worlddemosinto the simulation. pearance and kinematicsof the realworld.These\u201cin-domain\u201d After using RL in constructed simulation environments to simulation environments can serve as a sandbox to safely robustify the real-world imitation learning policies, the fine- and quickly learn robust policies across various disturbances tuned policies can be transferred back to the real world with and distractors, without requiring expensive exploration in signifi can tlyimprovedsuccessrates and robustnesstotest-time the real world. We show how imitation learning policies disturbances. trained with small numbers of real-world demonstrations can Overall, our pipeline simultaneously improves the effec- be robustified via large-scale RL fine-tuning in simulation tiveness of both rein for cement and imitation learning. RL in on these constructed simulation environments, using minimal simulationhelpsmakeimitationlearningpoliciesdeployment- amounts of human effort in terms of environment design ready without requiring prohibitive amounts of unsafe, inter- and reward engineering. To remove the burden of reward active data collection in the real world. At the same time, bootstrapping from real-world demonstration data via inverse improve the per for mance of models originally trained with distillation makes the exploration problem tractable for RL imitation learning. RL has exploded in its capacity for fine- fine-tuning in simulation. This minimizes the amount of task- tuning LLMs [49] and image generation models [4], learning specific engineering required by algorithm designers such as rewards from human feedback [14]. In robotics, prior work designingthedenserewardsormanuallydesigning the scenes. has explored techniques such as offline RL [70, 47, 33], Concretely, we propose Rial To, a system for robustifying learning world models [42, 18], and online fine-tuning in the real-worldimitationlearningpolicies with outrequiringsignifi- real world [2, 3, 22, 70]. Expert demonstrations have also canthumaneffort,byconstructingrealisticsimulationanalogs been used to bootstrap exploration and policy learning with for real-world environments on the fly and using these for RL [28, 27, 54, 74]. We similarly combine imitation and RL robust policy learning. Our contributions include: to guide exploration in sparse reward settings. However, our \u2022 A simple policy learning pipeline that syn the sizes con- pipeline showcases how demonstrations additionally benefit trollers to perform diverse manipulation tasks in the RL by biasing policies toward physically plausible solutions real world that (i) reduces human effort in constructing that compensate for imperfect physics simulation. environments and specifying rewards, (ii) produces ro- Sim-to-real policy transfer: RL in simulation has been bust policies that transfer to real-world, cluttered scenes, used to syn the size impressive control policies in a variety showing robustness to disturbances and distractors, (iii) of domains such as locomotion [40, 34, 32], dexterous in- requires minimal amounts of expensive and unsafe data handmanipulation[11,12,1,23],anddroneflight[61].Many collection in the real world. simulation-based RL methods leverage some form",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 109,
      "paper_id": "rialto",
      "text": "cluttered scenes, used to syn the size impressive control policies in a variety showing robustness to disturbances and distractors, (iii) of domains such as locomotion [40, 34, 32], dexterous in- requires minimal amounts of expensive and unsafe data handmanipulation[11,12,1,23],anddroneflight[61].Many collection in the real world. simulation-based RL methods leverage some form of domain \u2022 A novel algorithm for transferring demonstrations from randomization [65, 51], system identification [26, 63], or the real world to the reconstructed simulation to boot- improved simulator visuals [55, 24] to reduce the simulation- strap efficient rein for cement learning from the low-level to-reality(sim-to-real)domaingap.Priorworkhasalsoshown Lagrangian state for policy fine-tuning. We show that the benefit of \u201cteacher-student\u201d distillation [12, 32, 60, 9], this real-to-sim transfer of human demonstrations both whereinprivileged\u201cteacher\u201dpolicieslearnedquickly with RL improves efficiency and biases policies toward realistic are distilled into \u201cstudent\u201d policies that operate on sensor behavior in simulation which effectively transfers back observations. To acquire transferable controllers, we similarly to the real world. leverage GPU-acceleratedsimulation,teacher-studenttraining, \u2022 An intuitive graphical interface for quickly scanning anddomainr and omizationacrossparallelenvironments.How- and constructing digital twins of real-world scenes with ever, we address the more challenging scenario of household articulation, separated objects, and accurate geometries. manipulation, which is characterized by richer visual scenes, \u2022 We present extensive experimental evaluation showing and minimize the necessary engineering effort by relying on that Rial To produces reactive policies that solve several sparse rewards. We also simplify sim-to-real by training on manipulation tasks in real-world scenes under physical digital twin assets and co-training with real data [67]. disturbances and visual distractions. Across eight diverse Real-to-sim transfer of scenes: Designing realistic sim- tasks, our pipeline provides an improvement of 67% ulation environments has been studied from the perspective over baselines in average success rate across scenarios of syn the sizing digital assets that reflect real objects. Prior withvaryingobjectposes,visualdistractors,andphysical work has used tools from 3 D reconstruction [29] and inverse perturbations. graphics [10] for creating digital twins, and such real-to-sim pipelines have been used for both rigid and deformable [62] II. RELATEDWORK objects. These approaches are all compatible with our system Learning Visuomotor Control from Demonstrations: and could be used to automate real-to-sim scene transfer and Behavior cloning (BC) of expert trajectories can effectively reduce human effort. Our work similarly leverages advance- acquirerobotcontrolpolicies that operatein the realworld[19, ments in 3 D vision [64] for reconstructing object geometry, 13, 72, 6, 20, 39]. While several works have used BC but we also introduce an easy-to-use GUI for building a to learn per for mant policies from small to moderately-sized URDF/USD with accuratearticulations.Fur the rmore,our GUI datasets [13, 72, 39], per for mance tends to drop when the could be used to improve the aforementioned methods by policy must generalize to variations in scene layouts and making it easier to collect a large dataset of human-annotated appearance. Techniques for improving BC often require much articulated scenes. The accuracy of the simulator could be larger-scale data collection[6,57],raisingscalabilityconcerns. improved further combining our GUI with the latest system Other techniques support generalization with intermediate identification research [41, 35]. representations [20] and leverage generative models to",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 110,
      "paper_id": "rialto",
      "text": "large dataset of human-annotated appearance. Techniques for improving BC often require much articulated scenes. The accuracy of the simulator could be larger-scale data collection[6,57],raisingscalabilityconcerns. improved further combining our GUI with the latest system Other techniques support generalization with intermediate identification research [41, 35]. representations [20] and leverage generative models to add Real-to-sim-to-real transfer: Prior work has used visual distractors [71, 38]. These can improve robustness to Ne RF [43] and other 3 D reconstruction techniques to create visual distractors but do not address physical or dynamic realistic scene representations for improving manipulation disturbances, as these require producing actions not present [73], navigation [16, 8] and locomotion [7]. These works, in the data. however, only use the visual component of the syn the tic Fine-tuning Imitation with RL and Improving RL with scene and do not involve any physical interaction with a Demonstrations: Rein for cement learning has been used to reconstructed geometry. As a result, these systems cannot adjust to environmental changes beyond visual distractions. we have verified the approach with a variety of scanning apps For instance, different grasp poses may require different (e.g., Polycam [52] and ARCode [15]) and 3 D reconstruction placements, and a policy cannot discover these novel pipelines [64, 45], each of which convert a set of multi-view behaviors without physically interacting with the environment 2 Dimages(oravideo)intoatextured 3 Dmesh.Therawmesh during training. A limited number of works have learned denoted G, is typically exported as a single globally-unified policies that interact with the reconstructed environments, geometry,whichisunsuitable for directpolicylearning.Scene but they either simplify the reconstructed shapes [37] or are objects are not separated and the kinematics of objects with limited to simple grasp motions [68]. internal joints are not reflected. Physical parameters like mass and friction are also required and unspecified. We therefore III. RIALTO:AREAL-TO-SIM-TO-REALSYSTEM FOR further process the raw mesh G into a set of separate bod- ROBUSTROBOTICMANIPULATION ies/links {G }M with kinematic relations K and physical i i=1 A. System Overview parameters P. Our goal is to obtain a control policy that maps real-world While there are various automated techniques for automat- sensory observations to robot actions. We only assume access ically segmenting and adding articulations to meshes [29], to a small set of demonstrations (\u223c 15) containing (obser- in this work, we take a simple human-centric approach. We vation, action) trajectories collected by an expert, although in offer a simple graphical interface for humans to quickly principle Rial Tocanalsobeusedtorobustifylarge,expressive separate meshes and add articulations (see Fig. 2). Our GUI pretrained model saswell.Ourapproachrobustifiesreal-world allows users to upload their own meshes and drag/drop, imitationlearningpoliciesusingsimulation-based RLtomake reposition, and reorient them in the global scene. Users can learned controllers robust to disturbances and distractors not then separate meshes and add joints between different mesh presentin the demos.Theproposedpipeline,Rial To,achieves elements, allowing objects like drawers, fridges, and cabinets this with four main steps (Fig 1): to be scanned and processed. Importantly, our interface is 1) We construct geometrically, visually, and kinematically lightweight, intuitive, and requires minimal domain-specific accurate simulation environments from real-world image knowledge. We conducted a study (Section VI) evaluating capture.",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 111,
      "paper_id": "rialto",
      "text": "objects like drawers, fridges, and cabinets this with four main steps (Fig 1): to be scanned and processed. Importantly, our interface is 1) We construct geometrically, visually, and kinematically lightweight, intuitive, and requires minimal domain-specific accurate simulation environments from real-world image knowledge. We conducted a study (Section VI) evaluating capture. We leverage 3 D reconstruction tools and develop six non-expert users\u2019 experiences with the GUI and found an easy-to-use graphical interface for adding articulations they could scan complex scenes and populate them with a and physical properties. couple of articulated objects in under 15 minutes of active 2) We obtain a set of successful trajectories containing priv- interaction time. Examples of real-world environments with ileged information (such as Lagrangian state, e.g. object their corresponding digital twins are shown in Fig 4 and and joint poses) in simulation. We propose an \u201cinverse Appendix Fig. 16. distillation\u201d algorithm to transfer a policy learned from The next question is \u2014how do we infer the physics real-worlddemonstrationstocreatea data setoftrajectories parameters that faithfully replicate the real world? While (i.e., demos) in the simulation environment. accuratelyidentifyingphysicalparametersispossible,this can 3) The syn the sized simulation demos bootstrap efficient fine- bechallenging with outconsiderableinteraction[5,69].While tuning with RL in simulation using an easy-to-design adapting to dynamics variations is an important direction for sparse reward function and low-dimensional state space, future work, in this system we set a single default value for with added randomization to make the policy robust to massandfrictionuni for mlyacrossobjects and compensate for environmental variations. thesim-to-realgaptoactualreal-worldvaluesbyconstraining 4) The learned policy is transferred to reality by distilling a the learned policy to be close to a small number of real-world state-based simulation policy into a policy operating from demonstrations as discussed in Section III-C. raw sensor observations available in the real world [9, 12]. This procedure produces a scene S = {{G i }M i=1 ,K,P} During distillation, we also co-trained with the original represented in a USD/URDF file that references the separated real-world demonstrations to capitalize on the combined meshes and their respective geometric (G i }M i=1 ), kinematics benefits of simulation-based robustification and in-domain (K) and physical parameters (P). This environment can sub- real-world data. sequently be used for large-scale policy robustification in The following sections describe each component in detail, simulation. along with a full system overview in Fig 1. C. Robustifying Real-World Imitation Learning Policies in B. Real-to-Sim Transfer for Scalable Scene Generation Simulation The first step of Rial To is to construct geometrically, Given the simulation environment generated in Section visually,andkinematicallyrealisticsimulatedscenes for policy III-B, the next step in Rial To involves learning a robust training. This requires (i) generating accurate textured 3 D policy in simulation that can solve desired tasks from a geometry from real-world images and (ii) specifying articu- wide variety of configurations and environmental conditions. lations and physical parameters. For geometry reconstruction, While this can be done by training policies from scratch in we use existing off-the-shelf 3-D reconstruction techniques. simulation,thisisoftenaprohibitivelyslowprocess,requiring Our pipeline is agnostic to the particular method used, and considerable manual engineering. Instead, we will adopt a 3 D reconstruction",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 112,
      "paper_id": "rialto",
      "text": "and environmental conditions. lations and physical parameters. For geometry reconstruction, While this can be done by training policies from scratch in we use existing off-the-shelf 3-D reconstruction techniques. simulation,thisisoftenaprohibitivelyslowprocess,requiring Our pipeline is agnostic to the particular method used, and considerable manual engineering. Instead, we will adopt a 3 D reconstruction (Ne RFStudio, ARCode, Scene reconstruction GUI Articulated USD Polycam) Upload/Scale/Move scene Upload more objects Cut mesh Add joint Fig. 2. Overview of the real-to-sim pipeline for transfering scenes to the simulator. The first stage consists of scanning the environment, using off-the- shelf tools such as Ne RFStudio, ARCode, or Polycam. Each has its strengths and weaknesses and should be used appropriately (see Appendix XII for recommendations). The second stage consists of uploading the reconstructed scene into Rial To\u2019s GUI where the user can cut the mesh, specify joints, and organize the sceneasdesired.Oncecomplete,thescene can bedownloadedasa USDasset,which can bedirectlyimportedinto the simulator. {a 1 , . e . 1 . , x 1 , e a i : : e a e c ti p o o n s e (e e pose) i a T , e T , x T } x i : object poses desab-noisi V ycilop {a, e, o } D t t t real o t e t ee pose e t 3 D Conv MLP Succeeded PPO with Failed BC loss a t\u02bc \u2229 Learning from 1 2 Simulation transfer 3 RL fine-tuning the real world Real-world demos o t point Vision-based cloud Si R m o u l l l a o t u e t d e t ee policy Sim Sc u e la n t e ed point Scene pose object x ee poses t pose cloud Collect fully simulated trajectory State-based MLP policy Train with Privileged information Supervised Learning demos a t\u02bc Fig. 3. Inverse distillation & RL fine-tuning. We introduce a novel procedure for going from point cloud-based policies trained from real-world demonstrations D real to a robust privileged state-based policy in simulation. 1) Train a vision-based policy with supervised learning on D real 2) Rollout thevision-basedpolicyon the simulationrenderedpointclouds and collectasetof 15 privilegeddemonstrations with objectposes,D sim 3)Trainarobust state-basedpolicy with RLandasparsereward,addinga BClossfitting D sim tobiasexploration and setaprioronreal-world-transferablepolicies. fine-tuning-based approach, using rein for cement learning in information demonstrations can then be used to instantiate an simulationtofine-tuneapolicyinitialized from asmallnumber efficient RL-based fine-tuning procedure (Section III-C 2) in of expert demonstrations collected in the real world. Since simulation to massively improve policy robustness. training RL directly from visual observations is challenging, we would ideally like to fine tune simulation policies that 1) Inverse-distillation from Real-to-Sim for Privileged Pol- are based on a privileged Lagrangian state. However, real- icy Transfer: We assume a human provides a small worlddemonstrationsdonot have accessto the low-levelstate number of demonstrations in the real world D real = informationin the environment.Toenable the bootstrappingof {(oi,ai),...,(oi ,ai )}N ,wheretrajectoriescontainobser- 1 1 H H i=1 RLfinetuninginsimulation from aprivilegedstateusingreal- vations o (3 D point clouds) and actions a (delta end-effector world demonstrations, we introduce a novel \u201cinverse distilla- pose). Considering that simulation-based RL fine-tuning is far tion\u201d(Section",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 113,
      "paper_id": "rialto",
      "text": "real world D real = informationin the environment.Toenable the bootstrappingof {(oi,ai),...,(oi ,ai )}N ,wheretrajectoriescontainobser- 1 1 H H i=1 RLfinetuninginsimulation from aprivilegedstateusingreal- vations o (3 D point clouds) and actions a (delta end-effector world demonstrations, we introduce a novel \u201cinverse distilla- pose). Considering that simulation-based RL fine-tuning is far tion\u201d(Section III-C 1)procedure that isabletotakereal-world moreefficient and per for mantwhenoperating from acompact demonstrations with only raw sensor observations and actions staterepresentation[32,11](see Section V-C)andwewishto and transfer them to simulation demonstrations, complete use real-world human demonstrations to avoid the difficulties with low-level privileged state information. These privileged with running RL from scratch (see Section V-B), we want to transfer our observation-action demonstrations from the real world to simulation in a way that allows for subsequent RL In addition to mitigating issues associated with explo- fine-tuning in simulation from compact state-based represen- ration [46, 54], leveraging the additional imitation learning tations. This presents a challenge because we do not have term in the objective helps bias the policy toward physically an explicit state estimation system that provides a Lagrangian plausible, safe solutions that improve transfer of behaviors to state for the collected demonstrations in the real world. We reality.During this process,we cantrain the policy for robust- insteadintroduceaprocedure,called\u201cinverse-distillation\u201d,for ness by randomizing initial robot/object/goal poses. Appendix converting our real-world set of demonstrations into a set of VIII contains complete details of our training procedure. The trajectories in simulation that are paired with privileged low- result is a robust policy \u03c0\u2217 (a|s) operating from Lagrangian sim level state information. state that is successful from a wide variety of configurations Given the demonstrations D , we can naturally train a and environmental conditions. real policy \u03c0 (a|o) on this dataset via imitation learning. \u201cIn- real D. Teacher-Student Distillation with Co-Training on Real- verse distillation\u201d involves executing this perception-based World Data for Sim-to-Real Transfer learned policy \u03c0 (a|o) in simulation, based on simu- real lated sensor observations o, to collect a dataset D sim = In previous sections, we described a method for efficiently {(oi,ai,si)...,(oi ,ai ,si )}M of successful trajectories learning a robust policy \u03c0\u2217 (a|s) in simulation using privi- 1 1 1 H H H i=1 sim which contain privileged state information si. The key insight leged state information. However, in the real world, this priv- t here is that while we do not have access to the Lagrangian ileged information is unavailable. Policy deployment requires state in the real-world demonstrations when a learned real- operating directly from sensory observations (such as point world imitation policy is executed from perceptual inputs in clouds) in the environment. To achieve this, we build on simulation, low-level privileged Lagrangian state information the framework of teacher-student distillation (with interactive cannaturallybecollected from the simulationaswellsince the DAgger labeling)[57, 12] where the privileged information pairing between perceptual observations and Lagrangian state policy \u03c0\u2217 (a|s) serves as a teacher and the perceptual policy sim is known apriori in simulation. Since the goal is to improve \u03c0\u2217 (a|o) is the student. Since there is inevitable domain shift real beyond the real-world imitation policy \u03c0 real (a|o), we can",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 114,
      "paper_id": "rialto",
      "text": "perceptual observations and Lagrangian state policy \u03c0\u2217 (a|s) serves as a teacher and the perceptual policy sim is known apriori in simulation. Since the goal is to improve \u03c0\u2217 (a|o) is the student. Since there is inevitable domain shift real beyond the real-world imitation policy \u03c0 real (a|o), we can then between simulation and real domains, this training procedure perform RL fine-tuning, incorporating the privileged demon- can be further augmented by co-training the distillation ob- stration dataset D sim into the training process, as discussed in jective with a mix of the original real-world demonstration the following subsection. data D and simulation data drawn from \u03c0\u2217 (a|s) (via the real sim 2) Rein for cement Learning Fine-tuning in Simulation: DAgger objective [12]). This results in the following co- Given the privileged information dataset D , and the con- training objective for teacher-student policy learning: sim structed simulation environment the goal is to learn a robust policy \u03c0\u2217 (a|s) using rein for cement learning. There are two max\u03b1 (cid:88) \u03c0 \u03b8 (\u03c0 teacher (s i )|o i ) key chal s l i e m nges in doing so in a scalable way: (1) resolving \u03b8 (si,oi,ai)\u223c\u03c4\u03c0\u03b8 (cid:80) ac \u03c0 \u03b8 (a c |o i ) (2) exploration challenges with minimal reward engineering, and (2) ensuring the policy learns behaviors that will transfer +\u03b2 (cid:88) (cid:80) \u03c0 \u03b8 (a i |o i ) \u03c0 (a |o ) to the real world. \u2018 We find that both challenges can be (oi,ai)\u2208Dreal ac \u03b8 c i addressedbyasimpledemonstrationaugmentedrein for cement Here the first term corresponds to DAgger training in learning procedure [60, 46, 54], using the Lagrangian state- simulation, while the second term co-trains on real-world based dataset D . To avoid reward engineering, we define a sim expert data. This allows the policy to take advantage of simple reward function that detects if the scene is in a desired small amounts of high-quality real-world data to bridge the goalstate(detailedsparserewardfunctionsusedineachtaskin perceptual gap between simulation and real-world scenes and Appendix VIII).Webuildon the proximalpolicyoptimization improve generalization compared to only using the data from [59] algorithm with the addition of an imitation learning loss simulation. We empirically demonstrate (Section III-D) that asfollows(where A\u02c6 istheestimatorof the advantagefunction t thissignifi can tlyincreasestheresultingsuccessratein the real at step t [59], and V is the learned value function): \u03d5 world.Onapracticalnote,werefer the readerto Appendix IX for additional details on the student-teacher training scheme max\u03b1 (cid:88) min( \u03c0 \u03b8 (a t |s t ) A\u02c6 , thatenablesittobesuccessfulin the proposedproblemsetting. \u03b8,\u03d5 (st,at,rt)\u2208\u03c4\u03c0\u03b8old \u03c0 \u03b8old(at|st) t IV. EXPERIMENTALEVALUATION clip( \u03c0 \u03b8 (a t |s t ) ,1\u2212\u03f5,1+\u03f5)A\u02c6 ) Our experiments are designed to answer the following \u03c0 t questions about Rial To: (a) Does Rial To provide real-world +\u03b2 \u03b8 (cid:88) old(at|st) (V (s )\u2212Vtarg)2 (1) policiesrobusttovariationsinconfigurations,appearance,and \u03d5 t t disturbances? (b) Does co-training policies with real-world (st,V t targ)\u2208\u03c4\u03c0\u03b8old +\u03b3 (cid:88) \u03c0 \u03b8 (a i |s i ) For the sakeof this work,wewillassume that the optimalactions for the (cid:80) \u03c0 (a |s ) student and teachercoincide,andthere are noin for mationga the ringspecific (si,ai)\u2208Dsim ac \u03b8 c i",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 115,
      "paper_id": "rialto",
      "text": "t t disturbances? (b) Does co-training policies with real-world (st,V t targ)\u2208\u03c4\u03c0\u03b8old +\u03b3 (cid:88) \u03c0 \u03b8 (a i |s i ) For the sakeof this work,wewillassume that the optimalactions for the (cid:80) \u03c0 (a |s ) student and teachercoincide,andthere are noin for mationga the ringspecific (si,ai)\u2208Dsim ac \u03b8 c i challengesinducedbypartialobservability[60] Open toaster Plate on rack Book on shelf Mug on shelf Open Drawer Open cabinet htiw derettul C ksa T lanigir O noitalumi S secnaburtsid noitazimodna R Fig.4. Wedepict the six task susedtoevaluate Rial To.Fromtoptobottom,wefirstshowtheoriginalenvironmentwherewecollect the demonstrations, second the simulatedenvironment,third the environmentwherewedo our finalevaluationcontainingclutter and disturbances,andfourth the taskr and omization overvieweachshaded are acorrespondstoanapproximationofhowmuchr and omizationeachobject/robot have. databenefitreal-worldevaluationper for mance?(c)Isthereal- robot base when possible. to-sim transfer of scenes and policies necessary for training We conduct our experiments on a Franka Panda arm with efficiency and the resulting per for mance? (d) Does Rial To the default parallel jaw gripper, using 6 Do F Cartesian end scale up to more in-the-wild scenes? effector position control. For perceptual inputs, we obtain To answer these questions, we evaluate Rial To in eight 3 D point cloud observations from a single calibrated depth differenttasks,shownin Figure 4 and 8.Theseinclude 6-Do F camera. More details on the hardw are setup can be found in grasping and reorientation of free objects (book on a shelf, Appendix X. All of the results in the real world are evaluated plate on a rack, mug on a shelf) and 6-Do F grasping and using the best policy obtained for each method, we report interacting with articulated objects (drawer and cabinet) on a the average across at least 10 rollouts and the bootstrapped tabletop and opening a toaster, plate on a rack, putting a cup standard deviation. Videos of highlights and evaluation runs in the trash in more uncontrolled scenes. More details on the are available in the website. tasks such as their sparse reward functions and randomization Throughout the next sections, we will evaluate Rial To setups are presented in Appendix VIII. For each task, we against the following set of baselines and ablations: 1) Im- consider three different disturbance levels in increasing order itation learning from 15 and 50 demos (Section IV-A); 2) No of difficulty (see Appendix VIII for more details): co-training on real-world data (Section IV-B); 3) Co-training 1) Randomizing object poses: at the beginning of each on demonstrations in simulation (Section IV-B); 4) Rial To episode we randomize the object and/or robot poses. from simulation demos (Section IV-C 2); 5) Learning from an 2) Adding visual distractors: at the beginning of each untargeted set of simulated assets (Section IV-C 1); 6) Rial To episode we also add visual distractors in a cluttered way. without distractors (Section V-A); 7) Rial To without demos 3) Applying physical disturbances: we apply physical dis- (Section V-B) turbances throughout the episode rollout. We change A. Rial To Learns Robust Policies via Real-to-Sim-to-Real the pose of the object being manipulated or the target location where the object needs to be placed, close the In this section, we aim to underst and whether Rial To",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 116,
      "paper_id": "rialto",
      "text": "physical dis- (Section V-B) turbances throughout the episode rollout. We change A. Rial To Learns Robust Policies via Real-to-Sim-to-Real the pose of the object being manipulated or the target location where the object needs to be placed, close the In this section, we aim to underst and whether Rial To drawer/toaster/cabinet being manipulated, and move the can solve complex tasks, showing robustness to variations in 50 100 50 100 50 100 50 100 50 100 50 100 90 90 90 100 90 85 30 10 10 10 40 50 90 70 60 70 90 80 20 0 0 0 50 0 90 60 50 80 90 80 20 10 0 0 0 0 Success Rate eso P srotcartsi D secnabrutsi D noitazimodnar Kitchen toaster Book on shelf Plate on rack Mug on shelf Open drawer Open cabinet Rial To Imitation learning Fig.5. Comparisonof Rial Toagainstimitationlearningboth from 15 demonstrations.Rial Toprovidesrobustpoliciesacrosstasks and levelsofdistractions whileimitationlearningseverelysufferswhenaddingdistractors and disturbances. configurations, disturbances, and distractors. We comp are our Onlyr and omization Distractors Disturbances approachofreal-to-sim-to-real RLfine-tuningagainstapolicy BC(15 demos) 10\u00b19% 0\u00b10% 0\u00b10% trainedonlyonreal-worlddemosviast and ardimitationlearn- BC(50 demos) 40\u00b115% 30\u00b116% 20\u00b113% ing (BC). We report the results of running Rial To\u2019s pipeline Rial To(15 demos) 90\u00b19% 70\u00b114% 60\u00b116% starting from 15 demoscollecteddirectlyinsimulationandco- TABLEI training with 15 real-world demos during the teacher-student RIALTO AND IMITATIONLEARNINGONPLACINGABOOKON THE SHELF. distillation.In Section IV-C 2 weshowacomparisonofrunning Rial To uniquely on real or sim demos. morethan the humanef for tfor Rial Toforwhichwecollect 15 The results in Figure 5 show Rial To maintains high perfor- demos,in 30 minutes,andbuild the environmentin 15 minutes mance across configuration levels, achieving on average 91% of active time (see Section VI). Although more data improves success across tasks for randomizing object poses, 77% with theper for manceofdirectimitationlearning from 10%to 40%, distractors, and 75% with disturbances. On the other hand, 0% to 30%, and 0% to 20% for the three different levels of the presence of distractors and disturbances severely reduces robustness, the results in Table I show that Rial To achieves the per for mance of pure imitation learning. For instance, approximately 2.5 times higher success rate than pure BC, when only randomizing the object poses, the BC baseline despiteusinglessthanonethird the numberofdemonstrations achieves an average of 25% success rate across tasks. Under and taking less than half of the human supervision\u2019s time. more challenging conditions, the BC baseline drops to 11% and 5% overall per for mance on average for distractors and B. Impact of Co-Training with Real-World Data disturbances, respectively. Next, we assess the benefits offered by co-training with Figure 1, 10 and the videos in the website qualitatively real-world demonstrations during teacher-student distillation, show how the resulting policies are robust to many kinds of rather than just purely training policies in simulation. We environment perturbations, including moving the robot, mov- consider the book on shelf, plate on rack, mug on shelf, and ing the manipulated object and target positions, and adding open drawer tasks (the two first being two of the harder visual distractors that cause occlusion and distribution shift. tasks with lower overall per for mance). The results in Fig-",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 117,
      "paper_id": "rialto",
      "text": "the book on shelf, plate on rack, mug on shelf, and ing the manipulated object and target positions, and adding open drawer tasks (the two first being two of the harder visual distractors that cause occlusion and distribution shift. tasks with lower overall per for mance). The results in Fig- The policy rollouts also demonstrate error recovery capabil- ure 6 illustrate that co-training the policy with 15 real-world ities, correcting the robot\u2019s behavior in closed loop when, demonstrations signifi can tly increases real-world per for mance e.g., objects are misaligned or a grasp must be reattempted. on some tasks(3.5 x and 2 x success rate increase for book on This highlights that Rial To provides robustness that does not shelf and plate on rack with disturbances, when comparing emerge by purely learning from demonstrations. co-training on real-world demos against co-training with sim We also comp are Rial To against behavior cloning with 50 demos) while keeping the same per for mance on tasks that demonstrations to show that the problem is not simply one already have asmallsim-to-realgap.Qualitatively,weobserve of slightly more data. Collecting the 50 demonstrations takes theco-trainedpolicyismoreconservative and safertoexecute. a total time of 1 hour and 45 minutes, which is signifi can tly For instance, the policy without co-training usually comes eso P srotcartsi D secnabrutsi D noitazimodnar 50 100 50 100 50 100 Plate on rack Book on shelf Open drawer 80 70 100 90 90 90 50 50 100 50 60 90 70 70 80 60 70 90 30 30 90 20 50 90 60 70 80 50 60 90 30 30 80 20 40 90 real demos and sim demos and sim demos and only sim demos real co-training real co-training sim co-training Fig. 6. Comparison between running Rial To on sim vs real data. The per for manceon the methodsdoingco-training with real-worlddemosishigher thanusingonlysimulateddemosornoreal-worldco-training,ontheharder tasks(plateonrack and bookonshelf),andmatches the per for mancein the easiertasks(opendrawer and mugonshelf).Fur the rmore,starting from real- worldorsimulateddemosdoesequallywell. very close to the plate or the book, occasionally causing it to fall. The policy with co-training data, however, leaves more space between the hand and the book before grasping, which is closer to the demonstrated behavior. The observation that sim co-training performs signifi can tly worse than real- world co-training, indicates that co-training with real-world demonstrations is helping in reducing the sim-to-real gap for both the visual distribution shift between simulated and real point clouds and the sim-to-real dynamics gap. C. Is Real-to-Sim Transfer Necessary? 100 50 Simulated Real world Objaverse Target etar sseccus reward nep O 1) Real-to-Sim Transfer of Scenes: Instead of reconstruct- ing assets from the target environment, one could train a policy on a diverse set of syn the tic assets and hope the model generalizes to the real-world target scene [12, 21, 66]. While thishasshownpromisingresults for object-levelmanipulation, such as in-hand reorientation [12], it is still an active area of work for scene-level manipulation and rearrangement [21]. Moreover, such methods require significant effort in creating a dataset of scenes and objects that enables the learned policies to generalize. Acquiring a controller that can act",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 118,
      "paper_id": "rialto",
      "text": "While thishasshownpromisingresults for object-levelmanipulation, such as in-hand reorientation [12], it is still an active area of work for scene-level manipulation and rearrangement [21]. Moreover, such methods require significant effort in creating a dataset of scenes and objects that enables the learned policies to generalize. Acquiring a controller that can act in many scenes is also a more challenging learning problem, requiringlongerwallclocktime,morecompute,andadditional engineering effort to train a per for mant policy on a larger and more diverse training set. To probe the benefits of Rial To over such a sim-only train- ing pipeline, we compared the per for mance against a policy trained using only syn the tic assets. Using an amount of time effort roughly comparable to what is required from a single user following our real-to-sim approach (see Section VI), we collected a set of 4 drawers from the Objaverse dataset (see Figure 7). Although this is small compared to the growing size of 3 D object datasets, we found it non-trivial to transfer articulated objects into simulation-ready USDs and we leave it as future work. Given these manually constructed diverse simulation scenes, we then trained a multi-task policy using Rial To from 20 demonstrations to open the 4 drawers. See Appendix IX-C for the minor modifications to incorporate multi-task policy learning to Rial To. As shown in Figure 7, when evaluating the real target drawer, the policy trained on multiple drawers only achieves a 10% success rate, much lower than the 90% obtained by the policy trained on the target drawer in simulation. This leads us to conclude that to train a generalist agent, considerably more data and effort are needed as compared to the relatively simple real-to-sim procedure we describe for test time specialization. Moreover, this suggests that for Objaverse Drawers Target per for mance on particular deployment environments, targeted generation of simulation environments via real-to-simulation pipelines may be more effective than indiscriminate, diverse procedural scene generation. 2) Real-to-simtransferofpolicies: Weadditionallywantto underst and the impact of transferring policies from real-world Trained on target (Rial To) Trained on Objaverse demonstrations in comparison to running the pipeline starting withdemoscollecteddirectlyinsimulation.Thishelpsanalyze whether instead of collecting demos both in simulation and in the real world (for the co-training) we can simply collect demos in the real world and do all the training with those. Figure 6 shows the real-world per for mance of policies trained using Rial To when starting the RL fine-tuning step 17 81 90 10 usingreal-worlddemonstrationsasexplainedin III-C 1 against using demonstrations provided directly in simulation. We observe that the per for mance for both cases is very close. Fig.7. Comparisonbetweentraining with Rial Toon the reconstructionof the These results show that Rial To successfully learns policies targetdraweragainsttrainingonasetoff our drawers from Objaverse[17].We with demonstrations from either source of supervision as long observe, that Rial To on the real-to-sim asset does signifi can tly better (90% vs 10%) when testing in the real world on the target drawer compared to aswekeepco-training the policies with real-worlddatain the trainingon the setofr and omizeddrawers. teacher-student distillation step. Firstly, this indicates that we Pose Randomization Distractors Rial Towithoutdistractortraining 60\u00b115%",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 119,
      "paper_id": "rialto",
      "text": "To on the real-to-sim asset does signifi can tly better (90% vs 10%) when testing in the real world on the target drawer compared to aswekeepco-training the policies with real-worlddatain the trainingon the setofr and omizeddrawers. teacher-student distillation step. Firstly, this indicates that we Pose Randomization Distractors Rial Towithoutdistractortraining 60\u00b115% 30\u00b115% Rial Towithdistractortraining 100\u00b10% 70\u00b115% TABLEII REAL-WORLDPER FOR MANCEOFPOLICIESTRAINED WITH AND WITHOUT DISTRACTORSON THE TASKOFPLACINGAMUGONASHELF. do not need to collect both demos in sim and real, but we can run Rial To uniquely from the demos in the real world. Fur the rmore, this flexibility is a strength of our pipeline, as the ease of acquiring different sources of supervision may varyacrossdeploymentscenarios\u2013i.e.,onecouldusepolicies pretrained from large-scalereal-world data orobtain data from a simulation-based crowds our cing platform. D. Scaling Rial To to In-the-Wild Environments Open toaster Cup in trash Dish in rack Cup in trash 100 50 90 30 90 30 50 0 etar sseccu S robust policy that succeeds even in visual clutter. We analyze how this affects the final robustness of the learned policy. For the sake of analysis, we consider the per for mance on the mug on the shelf task. The small size of the mug and its resemblance in shape and size to other daily objects make the visual component of this task particularly challenging when other objects are also present. Our findings in Table II show that adding distractors during training increases the success rate from 30%to 70%whentesting the policyinenvironments with distractors. We also observe a per for mance improvement insetups with nodistractorssuggesting that suchtrainingalso supports better sim-to-real policy transfer. B. Comparison to RL from Scratch We hypo the size two key advantages of incorporating demonstrations in the fine tuning process: (1) aiding explo- ration,and(2)biasing the policytowardbehaviors that transfer welltoreality.Resultsin Table IIIshow that training from PPO from scratch fails (0% success) in three out of five tasks and much poorer per for mance in the other two tasks. On tasks with non-zero success, we observed that the policy exploits simulator inaccuracies and learns behaviors that are unlikely to transfer to reality. (see Appendix Fig. 15). For example, the PPO policy opens the toaster by pushing on the bottom of the toaster, leveraging the slight misplacement of the joint on the toaster. Such behaviors are unsafe and would not transfer to reality, underlining the importance of using demonstrations during policy robustification. C. RL from Vision Rial To\u2019s\u201cinversedistillation\u201dproceduretoacompactstate- spaceaddssomemethodologicaloverheadto the systemwhen compared to the possibility of doing RL fine-tuning directly on visual observations. However, as reported in Appendix Fig. 14, on the task of drawer opening, RL from compact states achieves a 96% success rate after 12 hours of wall- clock time, while RL from vision only achieves a 1% success Rial To Imitation learning rate after 35 hours. Hence, inverse distilling to state space is necessarybecausetraining RLfromvision with sparserewards Fig.8. Wetest Rial Toonuncontrolledandin-the-wildscenes,andweseewe is prohibitively slow, motivating the methodology outlined in cancontinuetosolveavarietyof task smorerobustlythanimitationlearning Section III-C 1. techniques. Inthissection,wescaleup Rial Totomoreuncontrolled and VI. USERSTUDY in-the-wild environments. We test Rial To on three different We analyzed the usability of Rial",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 120,
      "paper_id": "rialto",
      "text": "inverse distilling to state space is necessarybecausetraining RLfromvision with sparserewards Fig.8. Wetest Rial Toonuncontrolledandin-the-wildscenes,andweseewe is prohibitively slow, motivating the methodology outlined in cancontinuetosolveavarietyof task smorerobustlythanimitationlearning Section III-C 1. techniques. Inthissection,wescaleup Rial Totomoreuncontrolled and VI. USERSTUDY in-the-wild environments. We test Rial To on three different We analyzed the usability of Rial To\u2019s pipeline for bringing tasks:open the microwaveinakitchen(alsoshownin Section real-world scenes to simulation. We ran a user study over IV-A), put a cup in the trash, and bring the plate from 6 people, User 6 being an expert who used the GUI before the sink to the dishrack. We observe that Rial To scales and Users 1-5 never did any work on simulators before. Each up to these more diverse scenes and continues to perform participantwastasked with creatinganarticulatedsceneusing signifi can tlybetterthanst and ardimitationlearningtechniques. the provided GUI. More precisely, their task was to: 1) scan In particular, Rial To brings on average a 57% improvement a big scene, 2) cut one object, 3) scan and upload a smaller upon standard imitation learning, see Fig 8. object, and 4) add a joint to the scene. From Figure 9, we found that the average total time to create a scene was 25 V. FUR THE RANALYSIS AND ABLATIONS minutes and 12 seconds of which only 14 minutes and 40 A. Training with Distractors seconds were active work. We also observed that the expert When per for ming teacher-student distillation we per for med user accomplished the task faster than the rest, and twice as randomization with additionalvisualdistractorstotrainamore fast as the slowest user. This indicates that with practice, our Open Bookon Plateon Mugon Open toaster shelf rack shelf drawer RLfromscratch with 0 demos 62\u00b12% 0\u00b10% 2\u00b10% 0\u00b10% 0\u00b10% RLfine-tuning from 15 realdemos 91\u00b11% 90\u00b11% 81\u00b12% 81\u00b12% 96\u00b11% RLfine-tuning from 15 simdemos 96\u00b11% 89\u00b11% 82\u00b12% 82\u00b12% 95\u00b11% TABLEIII COMPARISONOFTRAININGRLFROMSCRATCHAGAINSTRL FROM REAL AND SIMDEMOS.RLFROMSIM AND REALDEMOSSEEMTOBEEQUIVALENT INMOSTCASES,BUTRL FROM SCRATCHB ARE LYSOLVES THE TASK. GUI allows users to become faster at generating scenes. We days of wall-clock time end-to-end to train a policy for each conclude that doing the real-to-simtransferof the scenesusing task, this time bottleneck makes continual learning infeasible the proposed GUI seems to be an intuitive process that is and underst and ing how to obtain policies faster with minimal neither time nor labor-intensive when compared to collecting human supervision would be valuable. We expect with more many demonstrations in the real world. We provide more efficient techniques for learning with point clouds and better details about the study in Appendix XIII. parallelization, this procedure can be sped up signifi can tly. Conclusion: This work presents Rial To, a system for acquiring policies that are robust to environmental varia- User 1 19:46 tions and disturbances on real-world deployment. Our system User 2 16:30 achieves robustness through the complementary strengths of real-world imitation learning and large-scale RL on digital User 3 16:07 twin simulations constructed on the fly. Our results show that User 4 12:27 byimporting 3-Dreconstructionsofrealscenesintosimulation and collecting a small amount of demonstration data, non- User 5 12:17 expertusers can programmanipulationcontrollers that succeed User 6 10:54 under challenging",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 121,
      "paper_id": "rialto",
      "text": "strengths of real-world imitation learning and large-scale RL on digital User 3 16:07 twin simulations constructed on the fly. Our results show that User 4 12:27 byimporting 3-Dreconstructionsofrealscenesintosimulation and collecting a small amount of demonstration data, non- User 5 12:17 expertusers can programmanipulationcontrollers that succeed User 6 10:54 under challenging conditions with minimal human effort, showing enhanced levels of robustness and generalization. 0 5 10 15 20 25 30 35 40 Minutes Scan Processing Scan Time Joint Time & Uploading Time ACKNOWLEDGMENTS 2 nd Scan Time Cut Time 10:54 Total Active Time The authors would like to thank the Improbable AI Lab Fig.9. 3 Dreconstruction GUI\u2019suserstudybreakdowntimes.Onaverageit andthe WEIRDLabmembers for the irvaluablefeedback and takes 14 minutes and 40 secondsofactivetimeor 25 minutes and 12 seconds supportindeveloping this project.Inparticular,wewouldlike oftotaltimetocreateascenethrough our proposedpipeline. to acknowledge Antonia Bronars and Jacob Berg for helpful suggestions on improving the clarity of the manuscript, and VII. LIMITATIONS AND CONCLUSION Marius Memmel for providing valuable insights on learning frompointcloudsintheearlystagesof the project.Thiswork Limitations: While our useof 3 Dpointcloudsinsteadof was partly supported by the Sony Research Award, the US RGB enables easier sim-to-real transfer, we require accurate Government, and Hyundai Motor Company. depth sensors that can struggle to detect thin, transparent, and reflective objects. Future work may investigate applying Rial Tototrainpolicies that operateon RGBimagesor RGBD, Author Contributions as our framework makes no fundamental assumptions that Marcel Torne conceived the overall project goals, investi- prevent using different sensor modalities. We are also limited gatedhowtoobtainreal-to-simtransferofscenes and policies, to training policies for tasks that can be easily simulated and and robustly do sim-to-real transfer of policies, wrote all the for real-world objects that can be turned into digital assets. code for the policy learning pipeline and Rial To\u2019s GUI for Currently, this is primarily limited to articulated rigid bodies, real-to-sim transfer of scenes, ran simulation and real-world but advancements in simulating and representing deformables experiments, wrote the paper, and was the primary author of should allow our approach to be applied to more challeng- the paper. ing objects. Even though we show Rial To works on fast Anthony Simeonov helped with setting up the robot hard- controllers, these are still relatively slow to minimize the ware, made technical suggestions on learning policies from sim-to-real gap in dynamics, thereafter there is potential to point clouds, helped with the task of placing the plate on the investigate tasks for which faster controllers are needed. In rack, and actively helped with writing the paper. this work, we consider relatively quasistatic problems, where Zechu Li assisted in the early stage of conceiving the exact identification of physics parameters is not necessary project and helped develop Rial To\u2019s GUI for the real-to-sim for the constructed simulation. This will become important transfer of the scenes. as more complex environments are encountered. Finally, as April Chan led the user study experiments to analyze we explain in Section XIV, Rial To currently takes around 2 Rial To\u2019s GUI. Tao Chenprovidedvaluableinsights and recommendations via simulation and generative modeling. In Towards on sim-to-real transfer. Generalist Robots: Learning Paradigms for Scalable Abhishek Gupta was involved in conceiving",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 122,
      "paper_id": "rialto",
      "text": "as April Chan led the user study experiments to analyze we explain in Section XIV, Rial To currently takes around 2 Rial To\u2019s GUI. Tao Chenprovidedvaluableinsights and recommendations via simulation and generative modeling. In Towards on sim-to-real transfer. Generalist Robots: Learning Paradigms for Scalable Abhishek Gupta was involved in conceiving the goals Skill Acquisition@ Co RL 2023, 2023. of the project, assisted with finding the scope of the paper, [11] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for suggested baselines and ablations, played an active role in general in-hand object re-orientation. In Conference on writing the paper and co-advised the project. Robot Learning, pages 297\u2013307. PMLR, 2022. Pulkit Agrawal suggested the idea of doing real-to-sim [12] Tao Chen,Megha Tippur,Siyang Wu,Vikash Kumar,Ed- transfer of scenes, was involved in conceiving the goals of ward Adelson, and Pulkit Agrawal. Visual dexterity: In- the project, suggested baselines and ablations, helped edit the hand reorientation of novel and complex object shapes. paper, and co-advised the project. Science Robotics, 8(84):eadc 9244, 2023. [13] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric REFERENCES Cousineau, Benjamin Burchfiel, and Shuran Song. Dif- [1] Open AI: Marcin Andrychowicz, Bowen Baker, Maciek fusion policy: Visuomotor policy learning via action Chociej, Rafal Jozefowicz, Bob Mc Grew, Jakub Pa- diffusion. ar Xiv preprint ar Xiv:2303.04137, 2023. chocki, Arthur Petron, Matthias Plappert, Glenn Powell, [14] Paul FChristiano,Jan Leike,Tom Brown,Miljan Martic, Alex Ray, et al. Learning dexterous in-hand manipula- Shane Legg, and Dario Amodei. Deep rein for cement tion. The International Journalof Robotics Research,39 learning from human preferences. Advances in neural (1):3\u201320, 2020. information processing systems, 30, 2017. [2] Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey [15] AR Code. Ar code. https://ar-code.com/, 2022. Levine. Efficient online rein for cement learning with [16] Matt Deitke, Rose Hendrix, Ali Farhadi, Kiana Ehsani, offline data. ar Xiv preprint ar Xiv:2302.02948, 2023. and Aniruddha Kembhavi. Phone 2 proc: Bringing robust [3] Max Balsells,Marcel Torne,Zihan Wang,Samedh Desai, robots into our chaotic world. In Proceedings of the Pulkit Agrawal, and Abhishek Gupta. Autonomous IEEE/CVF Conference on Computer Vision and Pattern roboticrein for cementlearning with asynchronoushuman Recognition, pages 9665\u20139675, 2023. feedback. ar Xiv preprint ar Xiv:2310.20608, 2023. [17] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca [4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Weihs, Oscar Michel, Eli Vander Bilt, Ludwig Schmidt, and Sergey Levine. Training diffusion models with re- Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. inforcement learning. ar Xiv preprint ar Xiv:2305.13301, Objaverse: A universe of annotated 3 d objects. In 2023. Proceedings of the IEEE/CVF Conference on Computer [5] Jeannette Bohg, Karol Hausman, Bharath Sankaran, Vision and Pattern Recognition, pages 13142\u201313153, Oliver Brock, Danica Kragic, Stefan Schaal, and Gau- 2023. rav S Sukhatme. Interactive perception: Leveraging [18] Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chan- action in perception and perception in action. IEEE dramouli Rajagopalan, and Xiaolong Wang. fine tuning Transactions on Robotics, 33(6):1273\u20131291, 2017. offline world models in the real world. ar Xiv preprint [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- ar Xiv:2310.16029, 2023. gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana [19] Pete Florence, Corey",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 123,
      "paper_id": "rialto",
      "text": "perception and perception in action. IEEE dramouli Rajagopalan, and Xiaolong Wang. fine tuning Transactions on Robotics, 33(6):1273\u20131291, 2017. offline world models in the real world. ar Xiv preprint [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- ar Xiv:2310.16029, 2023. gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana [19] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Hsu, et al. Rt-1: Robotics trans for mer for real-world Johnny Lee, Igor Mordatch, and Jonathan Tompson. control at scale. ar Xiv preprint ar Xiv:2212.06817, 2022. Implicit behavioral cloning. In Conference on Robot [7] Arunkumar Byravan,Jan Humplik,Leonard Hasenclever, Learning, pages 158\u2013168. PMLR, 2022. Arthur Brussee, Francesco Nori, Tuomas Haarnoja, Ben [20] Peter Florence, Lucas Manuelli, and Russ Tedrake. Self- Moran, Steven Bohez, Fereshteh Sadeghi, Bojan Vuja- supervised correspondence in visuomotor policy learn- tovic,etal. Nerf 2 real:Sim 2 realtransferofvision-guided ing. IEEE Robotics and Automation Letters, 5(2):492\u2013 bipedal motion skills using neural radiance fields. In 499, 2019. 2023 IEEE International Conference on Robotics and [21] Ran Gong, Jiangyong Huang, Yizhou Zhao, Haoran Automation (ICRA), pages 9362\u20139369. IEEE, 2023. Geng, Xiaofeng Gao, Qingyang Wu, Wensi Ai, Zi- [8] Matthew Chang, Theophile Gervet, Mukul Khanna, Sri- heng Zhou, Demetri Terzopoulos, Song-Chun Zhu, et al. ram Yenamandra,Dhruv Shah,So Yeon Min,Kavit Shah, Arnold: A benchmark for language-grounded task learn- Chris Paxton, Saurabh Gupta, Dhruv Batra, et al. Goat: ing with continuous states in realistic 3 d scenes. ar Xiv Gotoanything. ar Xivpreprintar Xiv:2311.06430,2023. preprint ar Xiv:2304.04321, 2023. [9] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp [22] Abhishek Gupta,Justin Yu,Tony ZZhao,Vikash Kumar, Kra\u00a8henbu\u00a8hl. Learning by cheating. In Conference on Aaron Rovinsky,Kelvin Xu,Thomas Devlin,and Sergey Robot Learning, pages 66\u201375. PMLR, 2020. Levine. Reset-free rein for cement learning via multi- [10] Qiuyu Chen, Marius Memmel, Alex Fang, Aaron Wals- tasklearning:Learningdexterousmanipulationbehaviors man, Dieter Fox, and Abhishek Gupta. Urd for mer: without human intervention. In 2021 IEEE International Constructinginteractiverealisticscenes from realimages Conference on Robotics and Automation (ICRA), pages 6664\u20136671. IEEE, 2021. Laskey, and Ken Goldberg. Planar robot casting with [23] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, real 2 sim 2 realself-supervisedlearning,2022. URLhttps: Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys //arxiv.org/abs/2111.04814. Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, [36] Yixin Lin, Austin S. Wang, Giovanni Sutanto, Ak- Balakumar Sundaralingam, et al. Dextreme: Transfer shara Rai, and Franziska Meier. Polymetis. https: of agile in-hand manipulation from simulation to reality. //facebookresearch.github.io/fairo/polymetis/, 2021. In 2023 IEEE International Conference on Robotics and [37] Naijun Liu, Yinghao Cai, Tao Lu, Rui Wang, and Shuo Automation (ICRA), pages 5977\u20135984. IEEE, 2023. Wang. Real\u2013sim\u2013real transfer for real-world robot con- [24] Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi trol policy learning with deep rein for cement learning. Khansari, and Yunfei Bai. Retinagan: An object-aware Applied Sciences, 10(5):1555, 2020. approach to sim-to-real transfer. In 2021 IEEE Interna- [38] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens, tional Conference on Robotics and Automation (ICRA), Shuran Song, Aravind Rajeswaran, and Vikash Ku- pages 10920\u201310926. IEEE, 2021. mar. Cacti: A framework for scalable multi-task [25] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, multi-scene visual imitation learning. ar Xiv",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 124,
      "paper_id": "rialto",
      "text": "In 2021 IEEE Interna- [38] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens, tional Conference on Robotics and Automation (ICRA), Shuran Song, Aravind Rajeswaran, and Vikash Ku- pages 10920\u201310926. IEEE, 2021. mar. Cacti: A framework for scalable multi-task [25] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, multi-scene visual imitation learning. ar Xiv preprint and Chrisina Jayne. Imitation learning: A survey of ar Xiv:2212.05711, 2022. learning methods. ACM Computing Surveys (CSUR), 50 [39] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush (2):1\u201335, 2017. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, [26] Jemin Hwangbo,Joonho Lee,Alexey Dosovitskiy,Dario Silvio Savarese, Yuke Zhu, and Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n. Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco What matters in learning from offline human demon- Hutter. Learning agile and dynamic motor skills for strations for robot manipulation. ar Xiv preprint legged robots. Science Robotics, 4(26):eaau 5872, 2019. ar Xiv:2108.03298, 2021. [27] Stephen James and Andrew J Davison. Q-attention: [40] Gabriel BMargolis,Ge Yang,Kartik Paigwar,Tao Chen, Enabling efficient learning for vision-based robotic ma- and Pulkit Agrawal. Rapidlocomotionviarein for cement nipulation. IEEE Robotics and Automation Letters, 7(2): learning. ar Xiv preprint ar Xiv:2205.02824, 2022. 1612\u20131619, 2022. [41] Marius Memmel, Andrew Wagenmaker, Chuning Zhu, [28] Stephen James, Kentaro Wada, Tristan Laidlow, and Patrick Yin, Dieter Fox, and Abhishek Gupta. Asid: Andrew J Davison. Coarse-to-fine q-attention: Efficient Active exploration for system identification in robotic learning for visual robotic manipulation via discretisa- manipulation. ar Xiv preprint ar Xiv:2404.12308, 2024. tion. In Proceedings of the IEEE/CVF Conference on [42] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Computer Vision and Pattern Recognition,pages 13739\u2013 Structured world models from human videos. ar Xiv 13748, 2022. preprint ar Xiv:2308.10901, 2023. [29] Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu. Ditto: [43] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Building digital twins of articulated objects from inter- Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. action. In Proceedings of the IEEE/CVF Conference on Nerf: Representing scenes as neural radiance fields for Computer Vision and Pattern Recognition, pages 5616\u2013 view syn the sis. Communications of the ACM, 65(1):99\u2013 5626, 2022. 106, 2021. [30] Michael Kazhdan,Matthew Bolitho,and Hugues Hoppe. [44] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Poisson surface reconstruction. In Proceedings of the Nikita Rudin,David Hoeller,Jia Lin Yuan,Ritvik Singh, fourth Eurographicssymposiumon Geometryprocessing, Yunrong Guo, Hammad Mazhar, et al. Orbit: A unified volume 7, 2006. simulationframework for interactiverobotlearningenvi- [31] Jens Kober,JAndrew Bagnell,and Jan Peters.Reinforce- ronments. IEEE Robotics and Automation Letters, 2023. ment learning in robotics: A survey. The International [45] Thomas Mu\u00a8ller, Alex Evans, Christoph Schied, and Journal of Robotics Research, 32(11):1238\u20131274, 2013. Alexander Keller. Instant neural graphics primitives [32] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra with a multiresolution hash encoding. ACM Trans. Malik. Rma: Rapid motor adaptation for legged robots. Graph., 41(4):102:1\u2013102:15, July 2022. doi: 10. ar Xiv preprint ar Xiv:2107.04034, 2021. 1145/3528223.3530127. URL https://doi.org/10.1145/ [33] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey 3528223.3530127. Levine. Conservativeq-learningforofflinerein for cement [46] Ashvin Nair, Bob Mc Grew, Marcin Andrychowicz, Wo- learning. Advances in Neural Information Processing jciech Zaremba, and Pieter Abbeel. Overcoming ex- Systems, 33:1179\u20131191, 2020. ploration in rein for cement learning with demonstrations. [34] Joonho Lee,",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 125,
      "paper_id": "rialto",
      "text": "[33] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey 3528223.3530127. Levine. Conservativeq-learningforofflinerein for cement [46] Ashvin Nair, Bob Mc Grew, Marcin Andrychowicz, Wo- learning. Advances in Neural Information Processing jciech Zaremba, and Pieter Abbeel. Overcoming ex- Systems, 33:1179\u20131191, 2020. ploration in rein for cement learning with demonstrations. [34] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, In 2018 IEEE international conference on robotics and Vladlen Koltun,and Marco Hutter.Learningquadrupedal automation (ICRA), pages 6292\u20136299. IEEE, 2018. locomotion over challenging terrain. Science robotics, 5 [47] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and (47):eabc 5986, 2020. Sergey Levine. Awac: Accelerating online reinforce- [35] Vincent Lim, Huang Huang, Lawrence Yunliang Chen, ment learning with offline datasets. ar Xiv preprint Jonathan Wang,Jeffrey Ichnowski,Daniel Seita,Michael ar Xiv:2006.09359, 2020. [48] NVIDIA. Nvidia isaac-sim. Machine Learning, pages 31077\u201331093. PMLR, 2023. https://developer.nvidia.com/isaac-sim, May 2022. [61] Yunlong Song, Angel Romero, Matthias Mu\u00a8ller, Vladlen [49] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Koltun, and Davide Scaramuzza. Reaching the limit Carroll Wainwright, Pamela Mishkin, Chong Zhang, in autonomous racing: Optimal control versus reinforce- Sandhini Agarwal, Katarina Slama, Alex Ray, et al. ment learning. Science Robotics, 8(82):eadg 1462, 2023. Training language models to follow instructions with [62] Priya Sund are san, Rika Antonova, and Jeannette Bohgl. human feedback. Advances in Neural Information Pro- Diffcloud: Real-to-sim from point clouds with differen- cessing Systems, 35:27730\u201327744, 2022. tiablesimulation and renderingofde for mableobjects. In [50] Songyou Peng, Michael Niemeyer, Lars Mescheder, 2022 IEEE/RSJ International Conference on Intelligent Marc Pollefeys, and Andreas Geiger. Convolutional Robots and Systems (IROS), pages 10828\u201310835. IEEE, occupancy networks. In Computer Vision\u2013ECCV 2020: 2022. 16 th European Conference,Glasgow,UK,August 23\u201328, [63] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, 2020,Proceedings,Part III 16,pages 523\u2013540.Springer, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent 2020. Vanhoucke. Sim-to-real: Learning agile locomotion for [51] Xue Bin Peng, Marcin Andrychowicz, Wojciech quadruped robots. ar Xiv preprint ar Xiv:1804.10332, Zaremba, and Pieter Abbeel. Sim-to-real transfer of 2018. robotic control with dynamics randomization. In 2018 [64] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, IEEE international conference on robotics and automa- Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake tion (ICRA), pages 3803\u20133810. IEEE, 2018. Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfs- [52] Polycam. Polycam. https://poly.cam, 2020. tudio: A modular framework for neural radiance field [53] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi development. In ACM SIGGRAPH 2023 Conference Kanervisto, Maximilian Ernestus, and Noah Dormann. Proceedings, pages 1\u201312, 2023. Stable-baselines 3: Reliable rein for cement learning im- [65] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, plementations. Journal of Machine Learning Research, Wojciech Zaremba, and Pieter Abbeel. Domain ran- 22(268):1\u20138, 2021. URL http://jmlr.org/papers/v 22/ domization for transferring deep neural networks from 20-1364.html. simulation to the real world. In 2017 IEEE/RSJ in- [54] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, ternational conference on intelligent robots and systems Giulia Vezzani, John Schulman, Emanuel Todorov, and (IROS), pages 23\u201330. IEEE, 2017. Sergey Levine. Learning complex dexterous manipula- [66] Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shrid- tion with deep rein for cement learning and demonstra- har,Chen Bao,Yuzhe Qin,Bailin Wang,Huazhe Xu,and tions. ar Xiv preprint ar Xiv:1709.10087, 2017. Xiaolong Wang. Gensim: Generating",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 126,
      "paper_id": "rialto",
      "text": "John Schulman, Emanuel Todorov, and (IROS), pages 23\u201330. IEEE, 2017. Sergey Levine. Learning complex dexterous manipula- [66] Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shrid- tion with deep rein for cement learning and demonstra- har,Chen Bao,Yuzhe Qin,Bailin Wang,Huazhe Xu,and tions. ar Xiv preprint ar Xiv:1709.10087, 2017. Xiaolong Wang. Gensim: Generating robotic simulation [55] Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, tasks via large language models. In The Twelfth Interna- Julian Ibarz,and Mohi Khansari.Rl-cyclegan:Reinforce- tional Conference on Learning Representations, 2023. ment learning aware simulation-to-real. In Proceedings [67] Lirui Wang,Jialiang Zhao,Yilun Du,Edward HAdelson, of the IEEE/CVF Conference on Computer Vision and and Russ Tedrake. Poco: Policy composition from Pattern Recognition, pages 11157\u201311166, 2020. and for heterogeneous robot learning. ar Xiv preprint [56] Nathan Ratliff, J Andrew Bagnell, and Siddhartha S ar Xiv:2402.02511, 2024. Srinivasa. Imitation learning for locomotion and manip- [68] Luobin Wang,Runlin Guo,Quan Vuong,Yuzhe Qin,Hao ulation. In 20077 th IEEE-RASInternational Conference Su, and Henrik Christensen. A real 2 sim 2 real method for on Humanoid Robots, pages 392\u2013397. IEEE, 2007. robustobjectgrasping with neuralsurfacereconstruction. [57] Ste\u00b4phane Ross, Geoffrey Gordon, and Drew Bagnell. A In 2023 IEEE 19 th International Conferenceon Automa- reduction of imitation learning and structured prediction tion Science and Engineering (CASE), pages 1\u20138. IEEE, to no-regret online learning. In Proceedings of the 2023. fourteenth international conference on artificial intelli- [69] Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B Tenen- gence and statistics, pages 627\u2013635. JMLR Workshop baum, and Shuran Song. Densephysnet: Learning dense and Conference Proceedings, 2011. physical object representations via multi-step dynamic [58] Stefan Schaal, Auke Ijspeert, and Aude Billard. Compu- interactions. ar Xiv preprint ar Xiv:1906.03853, 2019. tationalapproachestomotorlearningbyimitation.Philo- [70] Jingyun Yang, Max Sobol Mark, Brandon Vu, Archit sophical Transactions of the Royal Society of London. Sharma, Jeannette Bohg, and Chelsea Finn. Robot fine- Series B:Biological Sciences,358(1431):537\u2013547,2003. tuning made easy: Pre-training rewards and policies for [59] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec autonomous real-world rein for cement learning. ar Xiv Radford,and Oleg Klimov. Proximalpolicyoptimization preprint ar Xiv:2310.15145, 2023. algorithms. ar Xiv preprint ar Xiv:1707.06347, 2017. [71] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, [60] Idan Shenfeld,Zhang-Wei Hong,Aviv Tamar,and Pulkit Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Agrawal. Tgrl: An algorithm for teacher guided re- Jodilyn Peralta,Brian Ichter,etal. Scalingrobotlearning inforcement learning. In International Conference on with semantically imagined experience. ar Xiv preprint ar Xiv:2302.11550, 2023. [72] Tony ZZhao,Vikash Kumar,Sergey Levine,and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardw are. ar Xiv preprint ar Xiv:2304.13705, 2023. [73] Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, and Chelsea Finn. Nerf in the palm of your hand: Correctiveaugmentation for roboticsvianovel-viewsyn- thesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 17907\u2013 17917, 2023. [74] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar. Dexterous ma- nipulation with deep rein for cement learning: Efficient, general, and low-cost. In 2019 International Conference on Robotics and Automation (ICRA), pages 3651\u20133657. IEEE, 2019. [75] Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients of real-world robotic re- inforcement",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 127,
      "paper_id": "rialto",
      "text": "ma- nipulation with deep rein for cement learning: Efficient, general, and low-cost. In 2019 International Conference on Robotics and Automation (ICRA), pages 3651\u20133657. IEEE, 2019. [75] Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients of real-world robotic re- inforcement learning. ar Xiv preprint ar Xiv:2004.12570, 2020. Next, we provide additional details of our work. More \u2022 Cupintrash:success=||cup site\u2212trash site|| 2 <0.07 concretely: && condition(gripper open) \u2022 Task Details VIII: provides more details on the tasks A. Simulation details used to evaluate Rial To and the baselines. For simulating each one of the tasks, we use the latest sim- \u2022 Implementation Details IX: provides more detailed in- ulator from NVIDIA, Isaac Sim [48]. Fur the rmore, to develop formation on the exact hyperparameters such as network our code we were inspired by the Orbit code base [44], one of architectures, point cloud processing, and dataset sizes the first publicly available codebases that run Rein for cement using in Rial To. Learning and Robot Learning algorithms on Isaac Sim. \u2022 Further Analysis XI: we provide further details on Regarding the simulation parameters of the environments, Rial To, more concretely on running RL from vision, RL as mentioned in the text, we set default values in our GUI from scratch and on the sim-to-real gap. and these are the same that are used across the environments. \u2022 Hardw are Setup X: Details on the robot hardw are and In more detail, we use convex decomposition with 64 hull cameras used for the experiments. vertices and 32 convex hulls as the collision mesh for all \u2022 GUI for Real-to-Sim Transfer of Scenes XII: We objects. These values could vary in some environments, but provide further details on the GUI that we proposed wehavefound the yareingeneralagooddefaultvalue.There together with advice on which scanning methods to use is one exception, the dish on the rack task, where the rack for each scenario. needs to be simulated very precisely, in that case, we used \u2022 GUI User Study XIII: We explain how we ran the User SDF mesh decomposition with 256 resolution which returns Study together with visualizations of the scanned scenes. high-fidelity collision meshes. Note that all these options can \u2022 Compute Res our ces XIV:Wegivedetailson the compute be changed from our GUI. Regarding the physics parameters, used to run the experiments. we set the dynamic and static frictions of the objects to be VIII. TASKDETAILS 0.5, the joint frictions to be 0.1, and the mass of the objects to be 0.41 kg. Note thatin many ofthe tasks, wealso leverage In this section of the appendix, we describe additional setting fixed joints on the objects, to make sure these won\u2019t details about each task. Across tasks, the state space consists move, for example, on the shelf or kitchen. of a concatenation of all of the poses of the objects present in the scenes together with the states of the joints and the state IX. IMPLEMENTATIONDETAILS of the robot. The action space consists of a discretized end- A. Network",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 128,
      "paper_id": "rialto",
      "text": "space consists move, for example, on the shelf or kitchen. of a concatenation of all of the poses of the objects present in the scenes together with the states of the joints and the state IX. IMPLEMENTATIONDETAILS of the robot. The action space consists of a discretized end- A. Network architectures effectordeltaposeofdimension 14.Moreconcretely,wehave 1) State-based policy: As described in Section III-C 2, we 6 actions for the delta position, which moves \u00b10.03 meters fine-tune a state-based policy with privileged information in in each axis, 6 more actions for rotating \u00b10.2 radians in each the simulator. This policy is a simple Multi-Layer Perceptron axis, and 2 final actions for opening and closing the gripper. (MLP)withtwolayersofsize 256 each.Thistakesasinput the Asweexplainin Section III-B,wedefineasuccessfunction privileged state from the simulator and outputs a Categorical that will be used for selecting successful trajectories in the distribution of size 14 encoding the probabilities for sampling inverse distillation procedure and as a sparse reward in the each discrete end-effector action. For our PPO with BC loss RL fine-tuning phase. Next, we specify which are the success implementation, we build on top of the Stable Baselines 3 functions for each of the tasks: repository [53]. The network for the value function shares the \u2022 Kitchen Toaster: success= first layer with the actor. See Table VI for more details. toaster joint>0.65 && condition(gripper open) 2) Point cloud policy: For both the inverse distillation pro- \u2022 Open Drawer: success= cedure(Section III-C 1)and the lastteacher-studentdistillation drawer joint>0.1 && condition(gripper open) steps (Section III-D) we train a policy that takes as input \u2022 Open Cabinet: success= the point cloud observation together with the state of the cabinet joint>0.1 && condition(gripper open) robot (end-effector pose and state) and outputs a Categorical \u2022 Plate on the rack: success= distribution of size 14 encoding the probabilities for each ||plate site \u2212 rack site|| < 0.2 && rack y axis \u00b7 action. The network architecture consists of an encoder of the 2 plate z axis>0.9 && condition(gripper open) pointclouds that mapstoanembeddingofsize 128.Then this \u2022 Book on shelf: success= embeddingisconcatenatedtothestateof the robot(size 9)and ||book site\u2212shelf site|| <0.12 ispassedthroughan MLPofsize 256,256.Regarding the point 2 && condition(gripper open) cloud encoder, we use the same volumetric 3 D point cloud \u2022 Mug on shelf: success= encoderproposedin Convolutional Occupancy Networks[50], ||mug site \u2212 shelf site|| < 0.12 && mug z axis \u00b7 consisting of a local point net followed by a 3 D U-Net which 2 shelf z axis>0.95 && condition(gripper open) outputsadensevoxelgridoffeatures.Thesefeatures are then \u2022 Plate on the rack in the kitchen: success= pooled with both a max pooling layer and an average pooling ||plate site \u2212 rack site|| < 0.2 && rack y axis \u00b7 layer and the resulting two vectors are concatenated to obtain 2 plate z axis>0.9 && condition(gripper open) the final point cloud encoding of size 128. move the mug move the robot around back down close back the cabinet perturbe the book pose after pick move the mug close back the drawer back down Fig.10. Overviewof the disturbances that Rial Toisrobusttoin the differenttasks that weevaluatediton. Task USDName Episode",
      "start_pos": 10164,
      "end_pos": 10676
    },
    {
      "chunk_id": 129,
      "paper_id": "rialto",
      "text": "the final point cloud encoding of size 128. move the mug move the robot around back down close back the cabinet perturbe the book pose after pick move the mug close back the drawer back down Fig.10. Overviewof the disturbances that Rial Toisrobusttoin the differenttasks that weevaluatediton. Task USDName Episode Randomized Position Position Orientation Orientation length Parameters Object Ids Min(x,y,z) Max(x,y,z) Min(z-axis) Max(z-axis) Kitchentoaster kitchentoaster 3.usd 130 [267] [0.3,-0.2,- [0.7,0.1,0.2] [-0.1] [0.1] 0.2] Plateonrack dishinrackv 3.usd 150 [278, [-0.4,- [0,0.25,0] [-0.52,0] [0.52,0] [270,287]] 0.035,0] Mugonshelf mug and shelf 2.usd 150 [267,263] [[-0.3,0,0], [[0.25,0.3,0.07], [-0.52,-0.54] [0.52,0.54] [-0.1,0.25,0]] [0.4,0.4,0]] Bookonshelf booknshelve.usd 130 [277, [[-0.25,- [[0.15,0.28,0], [-0.52,0] [0.52,0] [268,272]] 0.12,0], [0.15,0.15,0]] [-0.15,- 0.05,0]] Opencabinet cabinet.usd 90 [268] [-0.5,- [0,0.3,-0.1] [-0.52] [0.52] 0.1,0.1] Opendrawer drawerbiggerhandle.usd 80 [268] [-0.26,-0.07,- [0.16,0.27,0] -0.5 0.5 0.05] Cupintrash cupntrash.usd 90 [263,266] [[[-0.2,-0.3, [[[0.2,0.1, [0,0] [0,0] -0.2],[-0.2,- 0.2], 0.12,0]]] [0.2,0.2,0]]] Plateonrack from kitchen dishsinklab.usd 110 [[263,278, [[[-0.25,-0.1, [[[0.1,0.2, [0,-0.3,0] [0,0.3,0] 270]] -0.1], 0.1], [-0.1,0.05,0], [0.1,0.15,0], [-0.2,0,0]]] [0,0,0]]] TABLEIV SPECIFICPARAMETERS FOR EACHONEOF THE TASKS. B. Teacher-student distillation mix 15000 trajectories rendering full point clouds (where all faces of the objects are visible, which is obtained through Given the state-based policy \u03c0 (a|s) learned in the sim- sim directly sampling points from the mesh, as proposed in [12]), ulator, we wish to distill it into a policy \u03c0\u2217 (a|o) that takes sim 5000 trajectories rendered from a camera viewpoint that is the point cloud observation and outputs the action. We take approximately the same position as the camera in the real thest and ardteacher-studentdistillationapproach[32,12].The world, a set of 2000 trajectories also generated from the same first step consists of doing imitation learning on a set of camera viewpoint in sim but adding distractor objects (see trajectories given by the expert policy \u03c0 (a|s) rollout. This sim Figure 12), finally, we mix the 15 real-world trajectories. The set of trajectories needs to be care fully designed to build an four different splits in the dataset are sampled equally, with implicit curriculum so that we can learn the student policy 1/4 probability each. successfully. When designing this dataset of trajectories, we Task Position(x,y,z) Rotation(quat) Crop Min Crop Max Size Parameters Camera Camera Camera Camera Image Kitchentoaster [0.0,-0.37,0.68] [0.82,0.34,-0.20,-0.41] [-0.8,-0.8,-0.8] [0.8,0.8,0.8] (640,480) Plateonrack [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480) Mugonshelf [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480) Bookonshelf [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480) Opencabinet [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480) Opendrawer [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480) Cupintrash [0.0,-0.37,0.68] [0.82,0.34,-0.20,-0.41] [-1,-1,-1] [1,1,1] (640,480) Plateonrack from kitchen [0.0,-0.37,0.68] [0.82,0.34,-0.20,-0.41] [-0.8,-0.8,-0.8] [0.8,0.8,0.8] (640,480) TABLEV CAMERAPARAMETERS FOR EACH TASK. MLPlayers PPOn steps PPObatchsize PPOBCbatchsize PPOBCweight Gradient Clipping 256,256 episodelength 31257 32 0.1 5 TABLEVI STATE-BASEDPOLICYTRAININGPARAMETERS.THERESTOFTHEPARAMETERS ARE THE DEFAULTASDESCRIBEDINSTABLE BASE LINES 3[53]. Simulated Scenes Mug on shelf Kitchen - toaster Dish track Fig.12. Distractorobjectsusedtogetarobustpolicytovisualdistractors intheteacher-studentdistillationstep III-D. D. Imitation Learning Baseline Cabinet Book on shelf Drawer For the imitation learning baseline, we collect 15 (unless otherwise specified) real-world demonstrations using a key- Fig. 11. Overview of the scenes generated using our GUI and used for evaluating Rial To. board interface. We preprocess the point clouds in the same manner as for the teacher-student distillation training (see Section",
      "start_pos": 10626,
      "end_pos": 11138
    },
    {
      "chunk_id": 130,
      "paper_id": "rialto",
      "text": "the imitation learning baseline, we collect 15 (unless otherwise specified) real-world demonstrations using a key- Fig. 11. Overview of the scenes generated using our GUI and used for evaluating Rial To. board interface. We preprocess the point clouds in the same manner as for the teacher-student distillation training (see Section IX-B. We complete the point cloud sampling points After this firstdistillationstep,weper for mastepof DAgger from the arm mesh leveraging the joints from the real robot. [57], where we roll out the policy \u03c0\u2217 (a|o) and relabel the We also add the same randomization: jitter, dropout, and sim actions with \u03c0 (a|s). In this second and last step, we mix translation. sim the DAgger data set with the trajectories with distractorsinsim 1) Imitation learning with new assets: We implemented an and the real-world trajectories and sample trajectories. Again additional base linewhereweaddedpointcloudssampled from each dataset is sampled equally with 1/3 probability each. different object meshes (see Figure 12) into the real-world Finally,thedetails for generatingandr and omizing the point point cloud to make the policy more robust to distractors. clouds are available in Table VII and were largely inspired However, no improvement in the robustness of this baseline by [12]. The parameters for training the point cloud-based was found as seen in Figure IX. We hypo the size that this is network are available in VIII. the case because the added meshes into the point cloud do not bring any occlusions which is one of the main challenges when adding distractors in point clouds. C. Simulated Assets Baseline Details To implement the baseline with multiple simulation assets X. HARDW ARE SETUP we had to incorporate two modifications for enabling the Our experiments are run on two different Panda Franka multi-task policy learning: 1) at each episode we select a arms. One is, the Panda Franka arm 2, which is mounted drawer randomly from the set of drawers 2) we expand the on a fixed table, we run the book on the shelf, mug on the observationspaceof the state-basedpolicytoinclude the index shelf, dish on the rack, open the cabinet, and open the drawer of the drawer selected to open. there. Then we also ran part of our experiments, on a Panda Totalpcd Sample Arm Dropout Jitter Jitter Sample Object Pcd Pcd Grid points Points(#) ratio ratio noise Meshes Points Normalization Scale Size 6000 3000 [0.1,0.3] 0.3 N(0,0.01) 1000 [0,0,0] 0.625(toaster) 32 x 32 x 32 (toaster) 1(others) [0.35,0,0.4] (others) TABLEVII POINTCLOUDGENERATIONANDR AND OMIZATIONPARAMETERS. MLPlayers lr Optimizer Batch Size Nbfullpcdtraj Nbsimulatedpcd Nbsimulatedpcd Nbrealtraj traj traj(distractors) 256,256 0.0003 Adam W 32-64 15000 5000 1000 15 TABLEVIII POINTCLOUDTEACHER-STUDENTDISTILLATIONPARAMETERS. Pose Distractors Disturbances used for compact state policies. Rendering point clouds in randomization simulation is also approximately 10 x slower than running the IL 40\u00b115% 50\u00b117% 10\u00b19% pipeline without any rendering. When adding these factors ILwithdistractors 50\u00b117% 20\u00b113% 10\u00b19% up, RL from vision becomes much slower and practically TABLEIX infeasible given our setup with sparse rewards. COMPARISONOF THE PLAINIMITATIONLEARNING BASE LINE(IL) AGAINSTADDING NEW DISTRACTORS(ILWITHDISTRACTORS)ONTHE TASKOF OPEN ING THE DRAWER.NOIMPROVEMENTISOBSERVED. 1 R D e 4 a 3 l 5 sense Franka Research",
      "start_pos": 11088,
      "end_pos": 11600
    },
    {
      "chunk_id": 131,
      "paper_id": "rialto",
      "text": "When adding these factors ILwithdistractors 50\u00b117% 20\u00b113% 10\u00b19% up, RL from vision becomes much slower and practically TABLEIX infeasible given our setup with sparse rewards. COMPARISONOF THE PLAINIMITATIONLEARNING BASE LINE(IL) AGAINSTADDING NEW DISTRACTORS(ILWITHDISTRACTORS)ONTHE TASKOF OPEN ING THE DRAWER.NOIMPROVEMENTISOBSERVED. 1 R D e 4 a 3 l 5 sense Franka Research 3 R D e 4 a 5 l 5 sense 0.8 0.6 Franka Emika Panda 0.4 0.2 0 Mobile table Fixed table 0 5 10 15 20 25 30 35 Wall Time (hours) Fig. 13. Overview of the hardw are setup used for evaluating Rial To. left: used for the kitchentoaster task,right:used for thebookon the shelf,mug ontheshelf,dishon the rack,opencabinet,and open drawertasks. Frankaarm 3,mountedonamobiletable,moreconcretely,the open toaster in the kitchen was the task run on this arm. The communication between the higher and lower level controller of the arm is done through Polymetis [36]. We mount one calibrated camera per setup to extract the depth maps that will be passed to our vision policies. More concretely we use the Intel depth Realsense camera D 455 on the first setup and the Intel depth Realsense camera D 435 on the second setup. See Figure 13 for more details on the robot setup. XI. FUR THE RANALYSIS A. RL from vision Part of the inefficiency of running RL from vision comes from the increased memory required to compute the policy loss for vision-based RL \u2013 on the same GPU, the batch size for vision-based policies is 100 x smaller than the batch size oita R sseccu S 96% RL from states RL from vision 1% 12 Fig. 14. Wall clock time comparison of running PPO from vision against fromcompactstates. B. RL from Scratch In Figure 15, we qualitatively observe the phenomena that we mention in III-C 2, where the policy trained from scratch, without demos, exploits the model\u2019s inaccuracies. In this specific case, we observe that the policy leverages the slightly incorrectlyplacedjointto open the microwaveinanunnatural way that wouldn\u2019t transfer to the real world. C. RL from different amounts of real-world data In this section, we analyze further how many real-world demonstrations are needed to successfully fine-tune policies with RL in simulation. We start with 0,5,10,15 real-world demonstrations and inverse-distill the policy by collecting 15 simtrajectories from thisreal-worldtrainedpolicy.Weobserve intable Xthat for the task ofplacingabookon the shelf,there is a step function where the PPO has a 0% success rate until 15 demos are used.Thereasonis that with lessthan 15 demos Open Mug drawer onshelf Imitationlearning 40\u00b117% 10\u00b19% Rial To 90\u00b19% 100\u00b10% Rial Tomulti task 90\u00b19% 80\u00b115% TABLEXII time COMPARISONOFTRAININGRIALTOONMULTIPLE TASK SAGAINST SINGLE-TASKRIALTO.NOIMPROVEMENTISOBSERVED. Fig.15. Visualizationofarolloutof the finalpolicylearned with RLwithout demos and achievinga 62%accuracyon open ing the toasterinsimulation.We observe the resulting policy that learns without demos exploits the model\u2019s 1) Train separate state-based single-task policies per task inaccuracies,thereafterit will nottransferto the realworld. 2) Collect trajectories from each one of the tasks with the state-based policies Bookon Open 3) Distill these trajectories into a single multi-task policy shelf drawer conditioned with the task-id RLfine-tuning from 0 realdemos 0\u00b10% 0\u00b10% 4) Run multiple iterations of DAgger on each task sequen- RLfine-tuning from 5",
      "start_pos": 11550,
      "end_pos": 12062
    },
    {
      "chunk_id": 132,
      "paper_id": "rialto",
      "text": "realworld. 2) Collect trajectories from each one of the tasks with the state-based policies Bookon Open 3) Distill these trajectories into a single multi-task policy shelf drawer conditioned with the task-id RLfine-tuning from 0 realdemos 0\u00b10% 0\u00b10% 4) Run multiple iterations of DAgger on each task sequen- RLfine-tuning from 5 realdemos 0\u00b10% 89\u00b11% RLfine-tuning from 10 realdemos 0\u00b10% 96\u00b11% tially to obtain a final multi-task policy RLfine-tuning from 15 realdemos 90\u00b12% 96\u00b11% We evaluate this policy in the real world on two of the TABLEX tasks and observein Table XII that inopening the drawer,the COMPARISONOFTRAININGRL FROM DIFFERENTAMOUNTSOF per for mance of multi-task Rial To matches single-task (90% REAL-WORLDDEMOS. success). However, the per for mance slightly decreases on the mug on the shelf task (from 100% on single-task to 80% on thereal-worldpolicydoesnottransferto the simulationhence multi-task). Never the less, the per for mance is still above the no sim demos can be collected during the inverse distillation imitation learning baseline (40% for the drawer and 10% for procedure. Thereafter the RL fine-tuned policy starts from the mug on the shelf). We did not tune any hyperparameters, scratchwhenusing<15 real-worlddemos.Ontheo the rside, andwekept the samenetworksizethatwe used for the Rial To for the easier task of opening a drawer, we observe this step experiments. We should be able to bring the per for mance of functionearlier,whereat>5 demoswe cando RLfine-tuning the mug on the shelf task to match the single-task policy with from demos and obtain successful policies. some hyperparameter tuning. We showed that Rial To can be easily adapted to train multi- D. Mixing Rial To with syn the tic data task policies. We hypo the size that we need to train in more Werun Rial Tocombiningthe data from thesyn the ticassets environments to obtain multi-task generalization. experiment (see Figure 7) together with the simulated target F. Sim-to-real gap environment data and study whether we get any per for mance gain by combining these two sources of data on the task of We analyze and propose an explanation for the observed opening the drawer. We observe in Table XI that there is no sim-to-realgapin Table XIII,whereweshow the per for mance clear improvement when combining the simulated assets with of the final point cloud-based policy in both simulation and the target asset. One reason could be that more syn the tic data therealworld.Weobserve that ingeneral,thesim-to-realgap is needed to observe an increase in per for mance. The other doesnotseemtobepresent.Insomecasessuchas for the mug hypo the sis is that learning only on the target environment onshelf task,weobserve that the per for manceinsimulationis (Rial To) is enough and the 10% left to reach 100% success worsethantheper for mancein the realworld.Themainreason rate in the real world comes from the sim-to-real gap. for this disparityis that wewanttomake the simulationharder than the real-world environment to make sure that we will be E. Rial To Multi-Task able to recover a good robust policy in the real world. We propose a multi-task version of Rial To. We train multi- XII. GUI FOR REAL-TO-SIMTRANSFEROFSCENES task Rial Toon the tasksof open ingadrawer,puttingamugon the shelf, cup in the trash, and",
      "start_pos": 12012,
      "end_pos": 12524
    },
    {
      "chunk_id": 133,
      "paper_id": "rialto",
      "text": "make sure that we will be E. Rial To Multi-Task able to recover a good robust policy in the real world. We propose a multi-task version of Rial To. We train multi- XII. GUI FOR REAL-TO-SIMTRANSFEROFSCENES task Rial Toon the tasksof open ingadrawer,puttingamugon the shelf, cup in the trash, and dish on the rack environments. In the main text and video, we provide an overview of Theproposedmulti-task Rial Toprocedureis the following: the features and capabilities of our GUI. Additional valuable features include the ability to populate the scene with assets from object datasets such as Objaverse [17]. This allows for Pose Distractors randomizingsurroundingclutter and supportingpolicytraining randomization that generalizes to distractor objects (see Section V-A). Rial To 90\u00b19% 90\u00b19% 1) 3 D reconstruction softw are used: We mainly used 3 Rial To+syn the ticassets 90\u00b19% 80\u00b113% different methods/apps for obtaining the 3 D meshes from videos: TABLEXI COMPARISONOFUSINGRIALTO WITH ADDEDSYN THE TICASSETS 1) Polycam [52] is used to scan larger scenes, such as AGAINSTST AND ARDRIALTOONTHE TASK OFOPENING THE DRAWERIN the kitchen. Polycam makes effective use of the built-in THEREALWORLD.NOIMPROVEMENTISOBSERVED. Kitchen Bookon Plateon Mugon Open Open toaster shelf rack shelf drawer cabinet Per for manceinsimulation 90\u00b14% 84\u00b15% 80\u00b16% 72\u00b16% 95\u00b13% 92\u00b14% Per for mancein the realworld 90\u00b19% 90\u00b19% 90\u00b19% 100\u00b10% 90\u00b19% 85\u00b18% TABLEXIII COMPARISONOFPER FOR MANCEINSIMULATION(TOP)AND THE REALWORLD(BOTTOM). i Phone depth sensor which helps extract realistic surface User Study Scanned Scenes geometry for large uniform flat surface (e.g., a kitchen counter). However, we find it struggles with fine-grained details. Polycam outputs a GLTF file, which we convert directly to a USD for loading into Isaac Sim using an online conversion tool. 2) AR Code [15] is used to extract high-quality meshes for single objects that can be viewed by images covering thefull 360 degreessurrounding the object(e.g.,cabinet, mug,microwave,drawer).While ARCodeleadstomore Phone cabin Printer room Table desk accurate geometry than Polycam for singulated objects, Bathroom 1 Kitchen Bathroom 2 we still find it struggles on objects with very thin parts. AR Code directly outputs a USD file that can be loaded into Isaac Sim. 3) Ne RFStudio [64] is used to capture objects that re- quire signifi can tly more detail to represent the geometry faithfully. For example, AR Code failed to capture the thin metal structures on the dish rack, whereas Ne RFs are capable of representing these challenging geometric parts. We use the default \u201cnerfacto\u201d model and training parameters. This method trains a relatively small model Added Joint Added Object Cut Object on a single desktop GPU in about 10 minutes. After training converges, we use the Ne RFStudio tools for Fig. 16. Overview of the scenes assembled by the Users during the user extractinga 3 Dpointcloud and obtainingatexturedmesh study,see Section VI. with Poisson Surface Reconstruction[30].Thisoutputsan OBJfile,whichweconvertintoa USDbyfirstconverting from OBJ to GLTF, and then converting from GLTF into the GLB file into the provided GUI, and the time required to USD(withbothfileconversionsper for med with anonline complete these steps was recorded as \u201cScan Processing and conversion tool). Uploading Time.\u201d Because the uploaded mesh was created using ones can, allobjects inthe scene are connected, andthe XIII.",
      "start_pos": 12474,
      "end_pos": 12986
    },
    {
      "chunk_id": 134,
      "paper_id": "rialto",
      "text": "converting from GLTF into the GLB file into the provided GUI, and the time required to USD(withbothfileconversionsper for med with anonline complete these steps was recorded as \u201cScan Processing and conversion tool). Uploading Time.\u201d Because the uploaded mesh was created using ones can, allobjects inthe scene are connected, andthe XIII. GUIUSERSTUDY userisunabletomoveasingleitem with outshifting the entire To test the functionality and versatility of the real-to-sim background.Thus,inordertocreateamorerealisticscene,the generationpipeline,weranauserstudyoversixpeople,where participant was asked to use the GUI to cut an object out of each participant was tasked with creating an articulated scene the scene, allowing this item to be manipulated independently using the provided GUI. Every individual was given the same of the background. The time it took for the user to cut this set of instructions that would guide them through the process object from the original mesh was regarded as \u201cCut Time.\u201d ofconstructingausable and accuratescene.Atthestartofeach In an attempt to further the realistic nature of this scene, the trial,theparticipantwasinstructedtodownload Polycam[52], participantwas the ninstructedtospecifyjointparameters and which uses a mobile device\u2019s Li DAR to generate 3 D models. create a fixed joint that would allow an object in the scene The user then selected a location and captured their scene by to rotate about a specific point. For instance, a fixed joint at taking a sequence of images. The time required to complete a door would allow the door to rotate about its hinge and this step was recorded as \u201cScan Time.\u201d Once the images were generate an accurate simulation of door movement. The time captured,Polycamneededtoprocess the picturestotransform required to create a fixed joint in the scene was recorded as the scene into a three-dimensional mesh. Once the mesh had \u201cJoint Time.\u201d Lastly, to demonstrate the full capabilities of been generated, the participant was then instructed to upload the GUI, the participant was asked to add another object to thearticulated USDtoacomputer and convert this fileinto the their current scene. They were instructed to download another GLB for mat(requiredby our GUI).Finally,theuseruploaded 3 D scanning application, AR Code [15], which was used to Scan Process+ Cut Joint 2 nd Scan Process+ Totaltime Totalactive Upload 1 st Scan Upload 2 nd time (idle) Scan(idle) User 1 2:25 5:41 4:15 4:56 8:10 10:45 36:12 19:46 User 2 6:30 12:57 3:32 3:51 2:37 4:19 33:46 16:30 User 3 3:52 5:52 4:35 4:14 3:26 4:15 26:14 16:07 User 4 2:34 2:06 2:48 1:41 5:14 4:33 19:06 12:27 User 5 1:32 2:33 4:43 1:28 4:34 3:50 18:40 12:17 User 6 2:30 3:52 2:08 1:17 4:59 2:26 17:12 10:54 TABLEXIV DETAILEDTIMESPENTBYEACHUSERIN THE USERSTUDY,SEESECTIONVI. create the three-dimensional mesh of the additional object. processed, uploaded, and converted more quickly. However, The time required to generate this mesh was recorded as their speed did reduce the quality of their backgrounds, since \u201cScan Time (2).\u201d Then the participant again converted their thedetailsinboths can sarenotaspreciseas the others.Thus, mesh to GLB format and uploaded this file to the same GUI. it seems User 3 completed the tasks quickly with the most Once uploaded, the object was placed in a realistic position accurate scan. within the scene, and the time elapsed",
      "start_pos": 12936,
      "end_pos": 13448
    },
    {
      "chunk_id": 135,
      "paper_id": "rialto",
      "text": "again converted their thedetailsinboths can sarenotaspreciseas the others.Thus, mesh to GLB format and uploaded this file to the same GUI. it seems User 3 completed the tasks quickly with the most Once uploaded, the object was placed in a realistic position accurate scan. within the scene, and the time elapsed during this step was User 6 had previous experience with the real-to-sim addedto the\u201cScan Processing and Uploading Time\u201dcategory. pipeline, so they were able to use this expertise to quickly Through this user study, we found that it took an average complete the tasks. The only abnormality with User 6\u2019s trial of 14.67 active minutes (excluding the \u201cScan Processing and was their longer Scan Time for object 2. They had trouble Uploading Time\u201dcategory)tocreateascene that includedone with the \u201cAR code\u201d app during this trial, resulting in a longer cutobject,onefixedjoint,andoneadditionalobject.However, Scan Time (2). it is important to note that User 6 had previous experience A. Scaling laws of the Rial To GUI using this GUI,whileallo the rusershadnoexperience.Thus, if we disregard theresults of User 6, we find the average time to create a scene to be 15.42 active minutes, which is not total active time=t scanscene a significant difference. As a result, the real-to-sim transfer +t \u00b7N using the provided GUI seems to be an intuitive process that scanobject objects (3) is neither time nor labor-intensive. +t cutobject \u00b7N cutobjects User 1 took the longest time to complete this series of +t addjoint \u00b7N joints tasks mostly due to their extensive upload period. Because We derive a relation to express the total active time needed User 1 scanned their environment for a lengthy period, their to create a scene with respect to the number of joints and articulated USDfilewaslargerthanallo the rusers.Asaresult, objects there are in the scene. The total active time to create it took longer for them to upload their file to a computer and a scene increases linearly in complexity with the number of convert this fileto GLB for mat.Theabnormalsizeof User 1\u2019s objects and joints present in the scene, as seen in Relation 3. file coupled with their difficulty operating the file conversion We define N as the number of scanned objects that we objects website led to a lengthy Scan Processing and Upload Time, want to add, N as the number of objects that we want cutobjects which led to the slowest overall per for mance. to extract from the scanned scene, N as the number of joints User 2 was the onlyuserwhowassentinstructionsdigitally joints the scene has. Taking the average times from our user and completed the tasks remotely. An individual experienced study (see Table XIV) we find t =4:50, t = scanobject scanscene with the real-to-sim pipeline was present for all other trials. 3 : 14, t = 2 : 54, t = 3 : 40. Note that these addjoint cutobject Thus,thismay have contributedto User 2\u2019slongercompletion valuesareon the conservativesidesinceonlyoneuserwasan time,astheirquestionshadtobeans were dremotely.However, expert,and with increasedexpertise,thesecoefficientsbecome User 2 did not have trouble with any particular section of the smaller. pipelinebutra the rtookalongertimetocompleteeachsection. User 3\u2019s experience with the real-to-sim pipeline went smoothly,asthere were noobviousdifficultieswhiles",
      "start_pos": 13398,
      "end_pos": 13910
    },
    {
      "chunk_id": 136,
      "paper_id": "rialto",
      "text": "= 3 : 40. Note that these addjoint cutobject Thus,thismay have contributedto User 2\u2019slongercompletion valuesareon the conservativesidesinceonlyoneuserwasan time,astheirquestionshadtobeans were dremotely.However, expert,and with increasedexpertise,thesecoefficientsbecome User 2 did not have trouble with any particular section of the smaller. pipelinebutra the rtookalongertimetocompleteeachsection. User 3\u2019s experience with the real-to-sim pipeline went smoothly,asthere were noobviousdifficultieswhiles can ning, uploading, or using the GUI. They followed the instructions XIV. COMPUTERES OUR CES quickly and precisely, resulting in a better completion time We run all of our experiments on an NVIDIA Ge Force than Users 1 and 2. RTX 2080 or an NVIDIA Ge Force RTX 3090. The first step Users 4 and 5 completed all tasks in the pipeline more of learning a vision policy from the real-world demos and quickly than User 3 because the background they chose was collecting a set of 15 demonstrations in simulation takes an smaller with fewer details. Thus, they were able to scan their average of 7 hours. The next step of RL fine-tuning from scenes faster, generating a smaller file that was able to be demonstrationstakesonaverage 20 hourstoconverge.Finally, the teacher-student distillation step takes 24 hours between collecting the trajectories, distilling into the vision policy, and running the last step of DAgger. This adds up to a total of 2 days and 3 hoursonaveragetotrainapolicy for agiven task.",
      "start_pos": 13860,
      "end_pos": 14077
    },
    {
      "chunk_id": 137,
      "paper_id": "ego4d",
      "text": "Ego 4 D: Around the World in 3,000 Hours of Egocentric Video Kristen Grauman 1,2,Andrew Westbury 1,Eugene Byrne\u22171,Zachary Chavis\u22173,Antonino Furnari\u22174, Rohit Girdhar\u22171,Jackson Hamburger\u22171,Hao Jiang\u22175,Miao Liu\u22176,Xingyu Liu\u22177,Miguel Martin\u22171, Tushar Nagarajan\u22171,2,Ilija Radosavovic\u22178,Santhosh Kumar Ramakrishnan\u22171,2,Fiona Ryan\u22176, Jayant Sharma\u22173,Michael Wray\u22179,Mengmeng Xu\u221710,Eric Zhongcong Xu\u221711,Chen Zhao\u221710, Siddhant Bansal 17,Dhruv Batra 1,Vincent Cartillier 1,6,Sean Crane 7,Tien Do 3,Morrie Doulaty 13, Akshay Erapalli 13,Christoph Feichtenhofer 1,Adriano Fragomeni 9,Qichen Fu 7, Abrham Gebreselasie 12,Cristina Gonza\u00b4lez 14,James Hillis 5,Xuhua Huang 7,Yifei Huang 15, Wenqi Jia 6,Weslie Khoo 16,Ja\u00b4chym Kola\u00b4\u02c7r 13,Satwik Kottur 13,Anurag Kumar 5,Federico Landini 13, Chao Li 5,Yanghao Li 1,Zhenqiang Li 15,Karttikeya Mangalam 1,8,Raghava Modhugu 17, Jonathan Munro 9,Tullie Murrell 1,Takumi Nishiyasu 15,Will Price 9,Paola Ruiz Puentes 14, Merey Ramazanova 10,Leda Sari 5,Kiran Somasundaram 5,Audrey Sou the rland 6,Yusuke Sugano 15, Ruijie Tao 11,Minh Vo 5,Yuchen Wang 16,Xindi Wu 7,Takuma Yagi 15,Ziwei Zhao 16,Yunyi Zhu 11, Pablo Arbela\u00b4ez\u202014,David Crandall\u202016,Dima Damen\u20209,Giovanni Maria Farinella\u20204, Christian Fuegen\u202013,Bernard Ghanem\u202010,Vamsi Krishna Ithapu\u20205,C.V.Jawahar\u202017,Hanbyul Joo\u20201, Kris Kitani\u20207,Haizhou Li\u202011,Richard Newcombe\u20205,Aude Oliva\u202018,Hyun Soo Park\u20203, James M.Rehg\u20206,Yoichi Sato\u202015,Jianbo Shi\u202019,Mike Zheng Shou\u202011,Antonio Torralba\u202018, Lorenzo Torresani\u20201,20,Mingfei Yan\u20205,Jitendra Malik 1,8 1 Facebook AIResearch(FAIR),2 Universityof Texasat Austin,3 Universityof Minnesota,4 Universityof Catania, 5 Facebook Reality Labs,6 Georgia Tech,7 Carnegie Mellon University,8 UCBerkeley,9 Universityof Bristol, 10 King Abdullah Universityof Science and Technology,11 National Universityof Singapore, 12 Carnegie Mellon University Africa,13 Facebook,14 Universidaddelos Andes,15 Universityof Tokyo,16 Indiana University, 17 International Instituteof Information Technology,Hyderabad,18 MIT,19 Universityof Pennsylvania,20 Dartmouth Abstract episodicmemory),present(analyzingh and-objectmanipu- lation,audio-visualconversation,andsocialinteractions), andfuture(forecastingactivities). Bypubliclysharing this Weintroduce Ego 4 D,amassive-scaleegocentricvideo massiveannotated data set and benchmarksuite,weaimto dataset and benchmarksuite. Itoffers 3,670 hoursofdaily- push the frontieroffirst-personperception. Projectpage: lifeactivityvideospanninghundredsofscenarios(house- https://ego 4 d-data.org/ hold, outdoor, workplace, leisure, etc.) captured by 931 uniquecamerawe are rsfrom 74 worldwidelocations and 9 differentcountries. Theapproachtocollectionisdesigned toupholdrigorousprivacyandethicsst and ards,withcon- 1.Introduction sentingparticipants and robustde-identificationprocedures whererelevant. Ego 4 Ddramaticallyexpands the volumeof Today\u2019scomputervisionsystemsexcelatnamingobjects diverse egocentric video footage publicly available to the andactivitiesin Internetphotosorvideoclips. Theirtremen- researchcommunity. Portionsof the video are accompanied dousprogressover the lastdecadehas been fueledbymajor byaudio, 3 Dmeshesof the environment, eyegaze, stereo, dataset and benchmark efforts, which provide the annota- and/orsynchronizedvideos from multipleegocentriccam- tionsneededtotrain and evaluatealgorithmsonwell-defined erasat the sameevent. Fur the rmore,wepresentahostof tasks[49,60,61,92,108,143]. newbenchmarkchallengescenteredaroundunderst and ing While this progressisexciting,current data sets and mod- thefirst-personvisualexperiencein the past(queryingan elsrepresentonlyalimiteddefinitionofvisualperception. 1 2202 ra M 11 ]VC.sc[ 3 v 85070.0112:vi Xra 1 2 3 4 Doing laundry Baking Geographic diversity Multi-perspective IMU L R Shopping Sports Reading Human locomotion Stereo vision Gardening Sewing / Knitting 3 D Pets Playing games Social interaction Video + 3 D scans Figure 1.Ego 4 Disamassive-scaleegocentricvideo data setofdailylifeactivityspanning 74 locationsworldwide.Hereweseeasnapshotof the data set(5%oftheclips,randomlysampled)highlightingitsdiversityingeographiclocation,activities,andmodalities.The data includes socialvideoswhereparticipantsconsentedtoremainunblurred.Seehttps://ego 4 d-data.org/fig 1.html for interactivefigure. First,today\u2019sinfluential Internet data setscapturebrief,iso- theygoaboutdailyactivitiesin the home,workplace,leisure, latedmomentsintime from athird-person\u201cspectactor\u201dview. social settings, and commuting. Based on self-identified However,inbothrobotics and augmentedreality,theinput characteristics, the camera wearers are of varying back- isalong,fluidvideostream from the first-personor\u201cego- grounds,occupations,gender,andages\u2014notsolelygraduate centric\u201d point of view\u2014where we see the world through students! Thevideo\u2019srichgeographicdiversitysupports the theeyesofanagentactivelyengaged with itsenvironment. inclusionofobjects,activities,andpeoplefrequentlyabsent Second,whereas Internetphotos are intentionallycaptured fromexisting data sets. Sinceeachparticipantworeacamera byahumanphotographer,images from analways-onwear- for 1 to 10 hoursatattime,the data setofferslong-formvideo able egocentric camera lack this active curation. Finally, content that displays the fullarcofaperson\u2019scomplexinter- first-personperceptionrequiresapersistent 3 Dunderst and- actions with the environment,objects,ando the rpeople. In ingof the camerawearer\u2019sphysicalsurroundings,andmust additionto RGBvideo,portionsof the datasetalsoprovide interpretobjects and actionsinahumancontext\u2014attentive",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 138,
      "paper_id": "ego4d",
      "text": "are intentionallycaptured fromexisting data sets. Sinceeachparticipantworeacamera byahumanphotographer,images from analways-onwear- for 1 to 10 hoursatattime,the data setofferslong-formvideo able egocentric camera lack this active curation. Finally, content that displays the fullarcofaperson\u2019scomplexinter- first-personperceptionrequiresapersistent 3 Dunderst and- actions with the environment,objects,ando the rpeople. In ingof the camerawearer\u2019sphysicalsurroundings,andmust additionto RGBvideo,portionsof the datasetalsoprovide interpretobjects and actionsinahumancontext\u2014attentive audio,3 Dmeshes,gaze,stereo,and/orsynchronizedmulti- tohuman-objectinteractions and high-levelsocialbehaviors. camera views that allow seeing one event from multiple perspectives. Our data setdrawsinspiration from priorego- Motivated by these critical contrasts, we present the centricvideo data efforts[43,44,129,138,179,201,205,210], Ego 4 D dataset and benchmark suite. Ego 4 D aims to cat- butmakessignifi can tadvancesintermsof scale,diversity, alyze the nexteraofresearchinfirst-personvisualpercep- andrealism. tion. Ego is for egocentric, and 4 D is for 3 D spatial plus temporalin for mation. Equallyimportanttohaving the right data isto have the Ourfirstcontributionis the dataset: amassiveego-video rightresearchproblems. Oursecondcontributionisasuite collectionofunprecedented scale and diversity that captures offivebenchmark task sspanning the essentialcomponents dailylifeactivityaround the world. See Figure 1. Itconsists ofegocentricperception\u2014indexingpastexperiences,ana- of 3,670 hoursofvideocollectedby 931 uniqueparticipants lyzingpresentinteractions,andanticipatingfutureactivity. from 74 worldwidelocationsin 9 differentcountries. The Toenableresearchon the sefronts,weprovidemillionsof vastmajorityof the footageisunscripted and\u201cinthewild\u201d, rich annotations that resulted from over 250,000 hours of representingthenaturalinteractionsof the camerawe are rsas annotatoreffort and range from temporal,spatial,andseman- 2 tic labels, to dense textual narrations of activities, natural languagequeries,andspeechtranscriptions. Ego 4 Dis the culminationofanintensivetwo-yeareffort by Facebook and 13 universitiesaround the worldwhocame together for the commongoalofspurring new researchin egocentricperception. Wearekickstarting that workwitha formalbenchmarkchallengetobeheldin June 2022. Inthe comingyears,webelieve our contribution can catalyze new researchnotonlyinvision,butalsorobotics,augmentedreal- ity,3 Dsensing,multimodallearning,speech,andlanguage. These directions will stem not only from the benchmark taskswepropose,butalsoalternativeones that the commu- Figure 2.Ego 4 Dcamerawe are rdemographics\u2014age,gender,coun- nity will developleveraging our massive,publiclyavailable triesofresidence,andoccupations(self-reported).Fontsizereflects relativefrequencyof the occupation. dataset. 2.Related Work graduatestudentsascamerawearers[43,44,66,129,129,138, Large-scalethird-person data sets Inthelastdecade,an- 168,179,194,210],Ego 4 Dcamerawearers are ofamuch notated data sets have bothpresented new problemsincom- wider demographic, as detailed below. Aside from daily puter vision and ensured their solid evaluation. Existing lifeactivity,priorego data setsfocusonconversation[170], collectionslike Kinetics[108],AVA[92],UCF[207],Ac- inter-personinteractions[66,168,194,231],placelocaliza- tivity Net [61], How To 100 M [157], Image Net [49], and tion[183,208],multimodalsensor data[124,166,204],hu- COCO[143]focusonthird-person Webdata,which have man hands [16,134] human-object interaction [106,184], thebenefit and biasofahumanphotographer. Incontrast, andobjecttracking[56]. Ego 4 Disfirst-person. Passivelycapturedwearablecamera Ego 4 Disanorderofmagnitudelargerthantoday\u2019slargest video entails unusual viewpoints, motion blur, and lacks egocentric data setsbothintermsofh our sofvideo(3,670 temporal curation. Notably, pre-training egocentric video hoursvs.100 in[43])anduniquecamerawearers(931 peo- models with third-person data[70,221,224,239]suffers from ple vs. 71 in [201]); it spans hundreds of environments thesizeabledomainmismatch[139,201]. (ratherthanoneordozens,asinexistingcollections);and itsvideocomes from 74 worldwidelocations and 9 coun- Egocentricvideounderst and ing Egocentricvideooffers tries(vs.justoneorafewcities). The Ego 4 Dannotations a host of interesting challenges, such as human-object in- are also of unprecedented scale and depth, with millions teractions[26,46,163],activityrecognition[110,139,243], ofannotationssupportingmultiplecomplextasks. Assuch, anticipation[4,75,86,144,205],videosummarization[48, Ego 4 Drepresentsastepchangein data setscale and diversity. 129,131,147,148,232],detectinghands[16,134],parsing We believe both factors are paramount to pursue the next socialinteractions[66,168,231],andinferring the camera generationofperception for embodied AI. wearer\u2019s body pose [107]. Our dataset can facilitate new workinall the seareas and more,and our proposedbench- 3.Ego 4 DDataset marks(andannotations the reof)widen the tasksresearchers canconsidermoving for ward. Wedeferdiscussionofhow Nextweoverview the dataset,whichwe are makingpub- priorworkrelatesto our benchmark task sto Sec.5. liclyavailableunderan Ego 4 Dlicense. Egocentric video datasets Multiple egocentric datasets 3.1.Collectionstrategy and camerawearers have been developedover the lastdecade. Mostrelevantto ourwork are thosecontainingunscripteddailylifeactivity, Notonlydowewishtoamassanego-videocollection that whichincludes EPIC-Kitchens[43,44],UTEgo[129,210], issubstantialin scale,butwealsowanttoensureitsdiversity Activities of Daily",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 139,
      "paper_id": "ego4d",
      "text": "the reof)widen the tasksresearchers canconsidermoving for ward. Wedeferdiscussionofhow Nextweoverview the dataset,whichwe are makingpub- priorworkrelatesto our benchmark task sto Sec.5. liclyavailableunderan Ego 4 Dlicense. Egocentric video datasets Multiple egocentric datasets 3.1.Collectionstrategy and camerawearers have been developedover the lastdecade. Mostrelevantto ourwork are thosecontainingunscripteddailylifeactivity, Notonlydowewishtoamassanego-videocollection that whichincludes EPIC-Kitchens[43,44],UTEgo[129,210], issubstantialin scale,butwealsowanttoensureitsdiversity Activities of Daily Living (ADL) [179], and the Disney ofpeople,places,objects,andactivities. Fur the rmore,for dataset[66]. Thepracticeofgivingcamerastoparticipants realism,weareinterestedinunscriptedfootagecapturedby totakeoutof the lab,firstexploredin[66,129,179],inspires peoplewearingacamera for longperiodsoftime. our approach. Others are (semi-)scripted, where camera To this end, we devised a distributed approach to data wearers are instructed to perform a certain activity, as in collection. The Ego 4 D project consists of 14 teams from Charades-Ego[201]and EGTEA[138]. Whereastoday\u2019s universities and labs in 9 countries and 5 continents (see largestego data setsfocussolelyonkitchens[44,44,124,138], mapin Figure 1). Eachteamrecruitedparticipantstoweara Ego 4 Dspanshundredsofenvironmentsbothindoors and out- camera for 1 to 10 hoursatatime,foratotalof 931 unique doors. Fur the rmore,whileexisting data setsrelylargelyon camerawearers and 3,670 hoursofvideoin this first data set 3 Carpenter > 7 hrsof videos Crafting> 12 hrsof videos Bike Mechanic> 5.5 hrsof videos Figure 3. Scenarios in Ego 4 D. Outer circle shows the 14 most commonscenarios(70%ofthe data).Wordleshowsscenariosin theremaining 30%.Innercircleiscolorcodedby the contributing partner(seemapcolorlegendin Fig 1). Figure 4. Somevideos(bottom)havecoupled 3 Dmeshes(top) from Matterport 3 Dscanners,allowingonetorelate the dynamic videoto the static 3 Denvironment(middle). release(Ego 4 D-3 K).Participantsin 74 totalcitieswerere- cruitedbywordofmouth,ads,andpostingsoncommunity bulletinboards.Someteamsrecruitedparticipants with occu- underst and ing [108]. In this way, we capture unscripted pations that haveinterestingvisualcontexts,suchasbakers, activitywhilebeingmindfulof the scenarios\u2019coverage. carpenters,landscapers,ormechanics. The exception is for certain multi-person scenarios, Both the geographic spread of our team as well as our where,inordertoensuresufficient data for the audio-visual approachtorecruitingparticipants were criticaltoarriveat andsocialbenchmarks, weaskedparticipantsatfivesites adiversedemographiccomposition,asshownin Figure 2.1 whohadconsentedtosh are the irconversationaudioandun- Participantscoverawidevarietyofoccupations,spanmany blurredfacestotakepartinsocialactivities,suchasplaying agebrackets,with 96 ofthemover 50 yearsold,and 45% games. We leverage this portion of Ego 4 D for the audio- arefemale. Twoparticipantsidentifiedasnon-binary,and visual and socialinteractionbenchmarks(Sec.5.3 and 5.4). twopreferrednottosayagender. Figure 3 shows the widedistributionofscenarioscaptured inour data set. Note that with ineachgivenscenario the reare 3.2.Scenarioscomposing the dataset typicallydozensofactionstakingplace,e.g.,thecarpentry scenario includes hammering, drilling, moving wood, etc. What activities belong in an egocentric video dataset? Overall,the 931 camerawe are rsbestow our data setwitha Ourresearchismotivatedbyproblemsinrobotics and aug- glimpseofdailylifeactivityaround the world. mentedreality, wherevisionsystems will encounterdaily lifescenarios. Hence,weconsultedasurvey from the U.S. 3.3.Cameras and modalities Bureauof Labor Statistics 2 thatcaptureshowpeoplespend thebulkoftheirtimein the home(e.g.,cleaning,cooking, To avoid models overfitting to a single capture device, yardwork),leisure(e.g.,crafting,games,attendingaparty), sevendifferenthead-mountedcameras were deployedacross transportation (e.g., biking, car), errands (e.g., shopping, the data set: Go Pro,Vuzix Blade,Pupil Labs,ZShades,OR- walkingdog,gettingcarfixed),andin the workplace(e.g, DRO EP 6, i Vue Rincon 1080, and Weeview. They offer talking with colleagues,makingcoffee). tradeoffs in the modalities available (RGB, stereo, gaze), Tomaximizecoverageofsuchscenarios,ourapproachis fieldofview, andbatterylife. Thefieldofview and cam- acompromisebetweendirectingcamerawearers and giving era mounting are particularly influential: while a Go Pro noguidanceatall: (1)werecruitedparticipantswhosecol- mounted on the head pointing down offers a high resolu- lectivedailylifeactivitywouldnaturallyencompassaspread tionviewof the handsmanipulatingobjects(Fig.5,right), ofthescenarios(asselectedfreelyby the participant),and a heads-up camera like the Vuzix shares the vantage of a (2)weaskedparticipantstowear the cameraatlength(at person\u2019seyes,but will missinteractionscloseto the body leastaslongasthebatterylifeof the device)sothat the activ- (Fig.5,left). itywouldunfoldnaturallyinalongercontext. Atypicalraw Inadditiontovideo,portionsof Ego 4 Dofferseveralother videoclipin our data setlasts 8 minutes\u2014signifi can tlylonger datamodalities:3 Dscans,audio,gaze 3,stereo,multiplesyn- than the 10 secondclipsoftenstudiedinthird-personvideo chronized wearable cameras, and textual narrations. See Table 1. Each can support new",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 140,
      "paper_id": "ego4d",
      "text": "of a (2)weaskedparticipantstowear the cameraatlength(at person\u2019seyes,but will missinteractionscloseto the body leastaslongasthebatterylifeof the device)sothat the activ- (Fig.5,left). itywouldunfoldnaturallyinalongercontext. Atypicalraw Inadditiontovideo,portionsof Ego 4 Dofferseveralother videoclipin our data setlasts 8 minutes\u2014signifi can tlylonger datamodalities:3 Dscans,audio,gaze 3,stereo,multiplesyn- than the 10 secondclipsoftenstudiedinthird-personvideo chronized wearable cameras, and textual narrations. See Table 1. Each can support new research challenges. For 1 for 64%ofallparticipants;missingdemographics are duetoprotocols example, having Matterport 3 D scans of the environment orparticipantsoptingoutofansweringspecificquestions. 2 https://www.bls.gov/news.release/atus.nr 0.htm 3 Eyetrackers were deployedby Indiana U.and Georgia Techonly. 4 Modality: RGBvideo Textnarrations Features Audio Faces 3 Dscans Stereo Gaze IMU Multi-cam #hours: 3,670 3,670 3,670 2,535 612 491 80 45 836 224 Table 1. Modalitiesofdatain Ego 4 Dand the iramounts. \u201cNarrations\u201daredense,timestampeddescriptionsofcamerawe are ractivity (cf. Sec.4).\u201c3 Dscans\u201daremeshes from Matterport 3 Dscanners for the full environmentinwhich the videowascaptured.\u201cFaces\u201drefersto videowhereparticipantsconsentedtoremainunblurred.\u201cMulti-cam\u201dreferstosynchronizedvideocapturedat the sameeventbymultiple camerawearers.\u201cFeatures\u201dreferstoprecomputed Slow Fast[70]videofeatures.Gazecollectedonlyby Indiana U.and Georgia Tech. coupled with ego-videoclips(Figure 4)offersauniqueop- portunity for underst and ingdynamicactivitiesinapersistent 3 Dcontext, asweexploitin the Episodic Memorybench- mark(see Sec.5.1). Multiplesynchronizedegocentricvideo streams allow accounting for the first and second-person viewinsocialinteractions. Audioallowsanalysisofconver- sationandacousticscenes and events. Figure 5.Examplenarrations.\u201cC\u201dreferstocamerawearer. 3.4.Privacy and ethics From the onset,privacyandethicsst and ards were critical will be at least subtle ways in which the language-based tothis data collectioneffort. Eachpartnerwasresponsible narrations are biasedtowards the irlocalwordchoices. fordevelopingapolicy. Whilespecificsvarypersite,this generallyentails: 3.6.Datasetaccessibility \u2022 Comply with own institutional research policy, e.g., At 3,670 hours of video, we are mindful that Ego 4 D\u2019s independentethicscommitteereviewwhererelevant scale can be an obstacle for accessibility for some re- searchers,dependingon the irstorage and computeres our ces. \u2022 Obtainin for medconsentofcamerawearers,whocan Tomitigate this,wehavetakenseveralmeasures. First,we askquestions and withdrawatanytime,and are freeto provide precomputed action features (Slow Fast 8 x 8 with review and redact the irownvideo Res Net 101 backbonepretrained for Kinetics 400)with the \u2022 Respect rights of others in private spaces, and avoid dataset,anoptionalstartingpoint for anydownstreamwork. captureofsensitive are asoractivities Second,onlyportionsofthe data constitute the formalchal- \u2022 Follow de-identification requirements for personally lengetrain/testsets for eachbenchmark\u2014notall 3,670 hours identifiablein for mation(PII) (see Appendix E).As Ego 4 Dannotationsincrease,wewill createst and ardizedmini-sets. Finally,weprovide the option Inshort,thesest and ardstypicallyrequire that the videobe todownloadonly the datatargetinganindividualbenchmark capturedinacontrolledenvironmentwithin for medconsent ormodalityofinterest. by all participants, or else in public spaces where faces andother PII are blurred. Appendix Kdiscussespotential 4.Narrationsof Camera Wearer Activity negativesocietalimpact. Before any other annotation occurs, we pass all video 3.5.Possibles our cesofbias throughanarrationprocedure. Inspiredby the pause-and- While Ego 4 D pushes the envelope on massive every- talknarrator[44],annotators are askedtowatcha 5 minute dayvideo from geographically and demographicallydiverse clipofvideo,summarizeit with afewsentences,andthen sources, we are aware of a few biases in our dataset. 74 re-watch,pausingrepeatedlytowriteasentenceabouteach locationsisstillalongway from completecoverageof the thing the camera wearer does. We record the timestamps globe. Inaddition,thecamerawearers are generallylocated and the associatedfree-formsentences. See Figure 5. Each inurbanorcollegetownareas. The COVID-19 pandemic video receives two independent narrations from different ledtoamplefootageinstay-at-homescenariossuchascook- annotators. Thenarrations are temporallydense: onaverage ing,cleaning,crafts,etc.andmorelimitedopportunitiesto wereceived 13.2 sentencesperminuteofvideo,foratotalof collectvideoatmajorsocialpublicevents. Inaddition,since 3.85 Msentences. Intotal the narrationsdescribe the Ego 4 D batterylifeprohibitsdaylongfilming,thevideos\u2014though videousing 1,772 uniqueverbs(activities)and 4,336 unique unscripted\u2014tendtocontainmoreactiveportionsofapartic- nouns(objects). See Appendix Dfordetails. ipant\u2019sday. Finally,Ego 4 Dannotations are donebycrowd- The narrations allow us to (1) perform text mining for sourcedworkersintwositesin Africa. Thismeans that there data-driventaxonomyconstruction for actions and objects, 5 Figure 6. The Ego 4 Dbenchmarksuitecentersaround the first-personvisualexperience\u2014fromremembering the",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 141,
      "paper_id": "ego4d",
      "text": "Ego 4 D batterylifeprohibitsdaylongfilming,thevideos\u2014though videousing 1,772 uniqueverbs(activities)and 4,336 unique unscripted\u2014tendtocontainmoreactiveportionsofapartic- nouns(objects). See Appendix Dfordetails. ipant\u2019sday. Finally,Ego 4 Dannotations are donebycrowd- The narrations allow us to (1) perform text mining for sourcedworkersintwositesin Africa. Thismeans that there data-driventaxonomyconstruction for actions and objects, 5 Figure 6. The Ego 4 Dbenchmarksuitecentersaround the first-personvisualexperience\u2014fromremembering the past,toanalyzing the present,toanticipating the future. (2)sortthevideosbytheircontenttomap the mtorelevant benchmarks,and(3)identifytemporalwindowswherecer- tainannotationsshould beseeded. Beyond the seuses,the narrations are themselvesacontributionof the dataset,po- tentiallyvaluable for researchonvideo with weaklyaligned naturallanguage.Toourknowledge,oursis the largestrepos- itoryofalignedlanguage and video(e.g.,How To 100 M[157], anexisting Internetrepository with narrations,containsnoisy spokennarrations that onlysometimescommentontheac- tivitiestakingplace). Figure 7.Episodic Memory\u2019sthreequerytypes 5.Ego 4 DBenchmark Suite First-personvisionhas the potentialtotrans for mmany to France?\u201d), to be distinguished from semantic memory applications in augmented reality and robotics. However, (\u201cwhat\u2019sthecapitalof France?\u201d). Anaugmentedrealityas- compared to mainstream video underst and ing, egocentric sistant that processes the egocentricvideostreamcouldgive perceptionrequires new fundamentalresearchtoaccount for ussuper-humanmemoryifitcouldappropriatelyindex our long-formvideo,attentioncues,person-objectinteractions, visualexperience and answerqueries. multi-sensory data,and the lackofmanualtemporalcuration inherenttoapassivelyworncamera. Taskdefinition Givenanegocentricvideo and aquery,the Inspiredbyall the sefactors,weproposeasuiteofchal- Ego 4 D Episodic Memory task requires localizing where lengingbenchmarktasks. Thefivebenchmarkstackle the the answer can be seen within the user\u2019s past video. We past,present,andfutureoffirst-personvideo. See Figure 6. consider three query types. (1) Natural language queries Thefollowingsectionsintroduceeach task and itsannota- (NLQ),inwhich the queryisexpressedintext(e.g.,\u201cWhat tions. Thefirst data setreleasehasannotations for 48-1,000 did I put in the drawer?\u201d), and the output response is the hoursof data perbenchmark,ontopof the 3,670 hoursof temporalwindowwhere the answerisvisibleordeducible. data that isnarrated. The Appendicesdescribehowwesam- (2)Visualqueries(VQ),inwhich the queryisastaticimage pledvideosperbenchmarktomaximizerelevanceto the task of an object, and the output response localizes the object whilemaintaininggeographicdiversity. thelasttimeitwasseenin the video,bothtemporally and Wedeveloped base line model sdrawingonstate-of-the- spatially. Thespatialresponseisa 2 Dboundingboxon the artcomponents from the literatureinordertotestdriveall object, and optionally a 3 D displacement vector from the Ego 4 Dbenchmarks. The Appendixpresents the baseline currentcamerapositionto the object\u2019s 3 Dboundingbox.VQ models and quantitativeresults. Wearerunninga for mal captureshowausermightteach the systemanobject with Ego 4 Dcompetitionin June 2022 inviting the researchcom- animageexample, thenlaterask for itslocation(\u201cWhere munitytoimproveon the sebaselines. isthis[pictureofmykeys]?\u201d). (3)Momentsqueries(MQ), in which the query is the name of a high-level activity or 5.1.Episodic Memory \u201cmoment\u201d,and the responseconsistsofalltemporalwindows where the activity occurs (e.g., \u201cWhen did I read to my Motivation Egocentric video from a wearable camera children?\u201d). See Figure 7. records the who/what/when/whereofanindividual\u2019sdaily lifeexperience. Thismakesitideal for what Tulvingcalled Annotations Forlanguagequeries,wedevisedasetof 13 episodic memory [213]: specific first-person experiences template questions meant to span things a user might ask (\u201cwhat did I eat and who did I sit by on my first flight to augment their memory, such as \u201cwhat is the state of 6 object X?\u201d,e.g.,\u201cdid Ileave the window open?\u201d.Annotators express the queriesinfree-formnaturallanguage,andalso provide the slot filling (e.g., X = window). For moments, weestablishe data xonomyof 110 activitiesina data-driven, pre-condition PNR post-condition semi-automaticmannerbymining the narrationsummaries. State-change:Plantremoved from ground Momentscapturehigh-levelactivitiesin the camerawearer\u2019s day,e.g.,setting the tableisamoment,whereaspickupis anactionin our Forecastingbenchmark(Sec.5.5). For NLQ and VQ, we ask annotators to generate lan- guage/visual queries and couple them with the \u201cresponse pre-condition PNR post-condition track\u201dinthevideo. For MQ,weprovide the taxonomyof State-change:Woodsmoothed labels and askannotatorstolabelclipswi the ach and every temporalsegmentcontainingamomentinstance. Intotal, Figure 8.Hands and Objects:Exampleobjectstatechangesdefined wehave 74 Ktotalqueriesspanning 800 hoursofvideo. bypre-condition,PNR,andpost-conditionframes. \u223c Evaluationmetrics and baselines For NLQ,we usetop-k recall at a certain temporal intersection over union (t Io",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 142,
      "paper_id": "ego4d",
      "text": "couple them with the \u201cresponse pre-condition PNR post-condition track\u201dinthevideo. For MQ,weprovide the taxonomyof State-change:Woodsmoothed labels and askannotatorstolabelclipswi the ach and every temporalsegmentcontainingamomentinstance. Intotal, Figure 8.Hands and Objects:Exampleobjectstatechangesdefined wehave 74 Ktotalqueriesspanning 800 hoursofvideo. bypre-condition,PNR,andpost-conditionframes. \u223c Evaluationmetrics and baselines For NLQ,we usetop-k recall at a certain temporal intersection over union (t Io U) manactionsbetter,aswellastotrainrobotstolearn from threshold. MQ adopts a popular metric used in temporal hum and emonstrationsinvideo. actiondetection:m APatmultiplet Io Uthresholds,aswellas Taskdefinitions Weinterpretanobjectstatechangetoin- top-kxrecall. VQadoptstemporal and spatio-temporallocal- cludevariousphysicalchanges,includingchangesinsize, izationmetricsaswellastimelinessmetrics that enc our age shape,composition,andtexture. Objectstatechanges can be speedysearches. Appendix Fpresents the baselinemodels viewedalongtemporal,spatial and semanticaxes,leadingto wedeveloped and reportsresults. thesethreetasks: (1)Point-of-no-returntemporallocaliza- Relation to existing tasks Episodic Memory has some tion: givenashortvideoclipofastatechange,thegoalisto foundationsinexistingvisionproblems,butalsoadds new estimatethekeyframe that contains the point-of-no-return challenges. All three queries call for spatial reasoning in (PNR)(thetimeatwhichastatechangebegins);(2)State astaticenvironmentcoupled with dynamicvideoofaper- changeobjectdetection: giventhreetemporalframes(pre, son who moves and changes things; current work largely post,PNR),thegoalistoregress the boundingboxof the treats these two elements separately. The timeliness met- objectundergoingastatechange; (3)Objectstatechange ricsenc our ageworkonintelligentcontextualsearch. While classification: givenashortvideoclip,thegoalistoclassify currentliteratureonlanguage+visionfocusesoncaptioning whe the ranobjectstatechangehastakenplaceornot. and question answering for isolated instances of Internet Annotations Weselect the datatoannotate base donactivi- data[12,35,119,228],NLQismotivatedbyqueriesabout ties that are likelytoinvolveh and-objectinteractions(e.g., thecamerawearer\u2019sownvisualexperience and operatesover knitting,carpentry,baking,etc.). Westartbylabelingeach long-termobservations. VQupgradesobjectinstancerecog- narratedh and-objectinteraction. Foreach, welabelthree nition [23,85,126,155] to deal with video (frequent Fo V momentsintime(pre,PNR,post)and the boundingboxes changes, objects entering/exiting the view) and to reason for the hands,tools,andobjectsineachof the threeframes. aboutobjectsin the contextofa 3 Denvironment. Finally, Wealsoannotate the statechangetypes(remove,burn,etc., MQcan beseenasactivitydetection[141,229,237]butfor see Fig.8),actionverbs,andnouns for the objects. theactivitiesof the camerawearer. Evaluation metrics and baselines Object state change 5.2.Hands and Objects temporallocalizationisevaluatedusingabsolutetemporal errormeasuredinseconds. Objectstatechangeclassifica- Motivation While Episodic Memory aims to make past tion is evaluated by classification accuracy. State change video queryable, our next benchmark aims to underst and objectdetectionisevaluatedbyaverageprecision(AP).Ap- the camera wearer\u2019s present activity\u2014in terms of inter- pendix Gdetails the annotations and presents base line model actions with objects and other people. Specifically, the results for the three Hands and Objectstasks. Hands and Objectsbenchmark captures how the camera wearer changes the state of an object by using or manip- Relation to existing tasks Limited prior work considers ulatingit\u2014whichwecallanobjectstatechange. Though objectstatechangeinphotos[102,164]orvideo[8,68,242]; cutting a piece of lumber in half can be achieved through Ego 4 Disthefirstvideobenchmarkdedicatedto the taskof manymethods(e.g.,varioustools,force,speed,grasps,end- underst and ingobjectstatechanges. The task issimilarto effectors),allshould berecognizedas the samestatechange. actionrecognition(e.g.,[100,110,139,221,243])becausein Thisgeneralizationability will enableustounderstandhu- somecasesaspecificaction can correspondtoaspecificstate 7 rate (DER) [11] and word error rate (WER) [114] for di- arization and transcription,respectively. Wepresent AVD baselinemodels and resultsin Appendix H. Relation to existing tasks The past few years have seen audiostudiedincomputervisiontasks[245]foractionclas- sification[110,226],objectcategorization[125,234],source localization and tracking[14,197,212]andembodiednavi- gation[33]. Meanwhile,visualin for mationisincreasingly usedinhistoricallyaudio-only task slikespeechtranscrip- tion,voicerecognition,audiospatialization[5,80,104,161], speakerdiarization[10,83],ands our ceseparation[57,78,82]. Datasetslike Vox Celeb[39],AVASpeech[31],AVAactive Figure 9. Audio-Visual and Socialbenchmarkannotations speaker[192],AVDIAR[83],and Easy Com[53]support this research. However,these data sets are mainlynon-egocentric. change. However,asinglestatechange(e.g.,cutting)can Unlike Ego 4 D,theydonotcapturenaturalconversational alsobeobservedinmanyforms(variousobject-tool-action characteristics involving a variety of noisy backgrounds, combinations). Itis our hope that the proposedbenchmarks overlapping,interruptingandun-intelligiblespeech,environ- will lead to the development of more explicit models of mentvariation,movingcamerawearers,andspeakersfacing objectstatechange,whileavoidingapproaches that simply away from the camerawearer. overfittoactionorobjectobservations. 5.4.Social Interactions 5.3.Audio-Visual Diarization Motivation Anegocentricvideoprovidesauniquelens for studyingsocialinteractionsbecauseitcapturesutterances Motivation Ournexttwo task saimtounderst and the cam- and nonverbal cues [115] from each participant\u2019s unique erawearer\u2019spresentinteractions with people. Peoplecom- view and enablesembodiedapproachestosocialunderst and- municateusingspokenlanguage,making the",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 143,
      "paper_id": "ego4d",
      "text": "to the development of more explicit models of mentvariation,movingcamerawearers,andspeakersfacing objectstatechange,whileavoidingapproaches that simply away from the camerawearer. overfittoactionorobjectobservations. 5.4.Social Interactions 5.3.Audio-Visual Diarization Motivation Anegocentricvideoprovidesauniquelens for studyingsocialinteractionsbecauseitcapturesutterances Motivation Ournexttwo task saimtounderst and the cam- and nonverbal cues [115] from each participant\u2019s unique erawearer\u2019spresentinteractions with people. Peoplecom- view and enablesembodiedapproachestosocialunderst and- municateusingspokenlanguage,making the captureofcon- ing. Progressinegocentricsocialunderst and ingcouldlead versationalcontentinbusinessmeetings and socialsettings tomorecapablevirtualassistants and socialrobots. Compu- aproblemofgreatscientifi can dpracticalinterest. While tational model sofsocialinteractions can alsoprovide new diarizationhas been ast and ardproblemin the speechrecog- tools for diagnosing and treatingdisordersofsocialization nition community, Ego 4 D brings in two new aspects (1) andcommunicationsuchasautism[188],andcouldsupport simultaneouscaptureofvideo and audio(2)theegocentric novelprosthetictechnologies for the hearing-impaired. perspectiveofaparticipantin the conversation. Taskdefinition While the Ego 4 Ddataset can supportsuch Task definition and annotations The Audio-Visual Di- along-termresearchagenda,ourinitial Socialbenchmark arization(AVD)benchmarkiscomposedoff our tasks(see focusesonmultimodalunderst and ingofconversationalin- Figure 9): teractionsviaattention and speech. Specifically,wefocuson \u2022 Localization and trackingof the participants(i.e.,candi- identifyingcommunicativeacts that are directedtowards the datespeakers)inthevisualfieldofview(Fo V).Abound- camera-wearer,asdistinguished from thosedirectedtoother ingboxisannotatedaroundeachparticipant\u2018sface. socialpartners: (1)Lookingatme(LAM):givenavideoin \u2022 Activespeakerdetectionwhereeachtrackedspeakerisas- which the facesofsocialpartners have beenlocalized and signedananonymouslabel,including the camerawearer identified,classifywhe the reachvisiblefaceislookingat the whoneverappearsin the visual Fo V. camerawearer;and(2)Talkingtome(TTM):givenavideo \u2022 Diarization of each speaker\u2019s speech activity, where and audio segment with the same tracked faces, classify we provide the time segments corresponding to each whethereachvisiblefaceistalkingto the camerawearer. speaker\u2019svoiceactivityin the clip. \u2022 Transcriptionofeachspeaker\u2019sspeechcontent(only En- Annotations Socialannotationsbuildonthose from AVdi- glishspeakers are considered for thisversion). arization(Sec.5.3). Given(1)faceboundingboxeslabeled withparticipant IDs and trackedacrossframes,and(2)asso- Evaluationmetrics and baselines we usest and ardizedob- ciatedactivespeakerannotations that identifyineachframe ject tracking (MOT) metrics [18,19] to evaluate speaker whether the socialpartnerswhosefacesarevisible are speak- localization and trackingin the visual Fo V.Speakerdetec- ing,annotatorsprovide the groundtruthlabels for LAMand tion with anonymouslabelsisevaluatedusing the speaker TTMasabinarylabel for eachfaceineachframe.For LAM, error rate, which measures the proportion of wrongly as- annotatorslabel the timesegment(start and endtime)ofa signedlabels. Weadopt the wellstudieddiarizationerror visiblepersonwhentheindividualislookingat the camera 8 wearer. For TTM,we use the vocalactivityannotation from take AVD, then identify the time segment when the speech is doughin 0.8 s take directedat the camerawearer. See Figure 9. doughin 0.8 s Evaluation metrics and baselines We use mean average Locomotion Movements Hands Movements Short-Term Anticipation precision(m AP)and Top-1 accuracytoquantify the classifi- prediction:kneaddough putdough packspice pourspice cationperformance for bothtasks. Unlike AVD,wemeasure precisionateveryframe. Appendix Iprovidesdetails and Input video Long-Term Anticipation presents Social base linemodels and results. Relationtoexistingtasks Comp are dto[67],Ego 4 Dcon- Figure 10.The Forecastingbenchmarkaimstopredictfutureloco- tainssubstantiallymoreparticipants,hoursofrecording,and motion,movementofhands,nextobjectinteractions,andsequences offutureactions. varietyofsensors and socialcontexts.The LAM task ismost closelyrelatedtopriorworkoneyecontactdetectioninego- video[36,159],butaddressesmorediverse and challenging Evaluationmetrics and baselines Weevaluatefutureloco- scenarios. Mutualgazeestimation[54,150\u2013152,172,176] motionmovementandh and movementpredictionusing L 2 andgazefollowing[37,65,111,186]arealsorelevant. The distance. Short-termobjectinteractionanticipationiseval- TTM task isrelatedtoaudio-visualspeakerdetection[7,193] uatedusinga Top-5 mean Average Precisionmetricwhich andmeetingunderst and ing[21,132,154]. discounts the Top-4 falsenegativepredictions.Long-termac- 5.5.Forecasting tionanticipationisevaluatedusingeditdistance. Appendix J details the tasks,annotations,baselinemodels,andresults. Motivation Havingaddressed the past and presentof the Relation to existing tasks Predicting future events from camera wearer\u2019s visual experience, our last benchmark egocentric vision has increasing interest [191]. Previous moves on to anticipating the future. Forecasting move- workconsidersfuturelocalization[113,120,174,230],ac- ments and interactionsrequirescomprehending the camera tionanticipation[76,77,86,118,127,219],nextactiveobject wearer\u2019sintention. Ithasimmediateapplicationsin ARand prediction[20,74],futureeventprediction[149,167],andfu- human-robotinteraction,suchasanticipativelyturningon tureframeprediction[145,146,153,215,218,227]. Whereas appliancesormovingobjects for the human\u2019sconvenience. pastworkreliesondifferentbenchmarks and taskdefinitions, Thescientificmotivation can beseenbyanalogy with lan- we propose a unified benchmark to assess progress in the guage model ssuchas GPT-3[24],whichimplicitlycapture field. knowledgeneededbymanyo the rtasks. Ratherthanpredict thenextword,visual for ecastingmodels the dynamicsofan 6.Conclusion agentactingin",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 144,
      "paper_id": "ego4d",
      "text": "and interactionsrequirescomprehending the camera tionanticipation[76,77,86,118,127,219],nextactiveobject wearer\u2019sintention. Ithasimmediateapplicationsin ARand prediction[20,74],futureeventprediction[149,167],andfu- human-robotinteraction,suchasanticipativelyturningon tureframeprediction[145,146,153,215,218,227]. Whereas appliancesormovingobjects for the human\u2019sconvenience. pastworkreliesondifferentbenchmarks and taskdefinitions, Thescientificmotivation can beseenbyanalogy with lan- we propose a unified benchmark to assess progress in the guage model ssuchas GPT-3[24],whichimplicitlycapture field. knowledgeneededbymanyo the rtasks. Ratherthanpredict thenextword,visual for ecastingmodels the dynamicsofan 6.Conclusion agentactingin the physicalworld. Taskdefinition The Forecastingbenchmarkincludesf our Ego 4 Disafirst-of-its-kind data set and benchmarksuite tasks (Fig. 10): (1) Locomotion prediction: predict a set aimed at advancing multimodal perception of egocentric of possiblefuture ground plane trajectoriesof the camera video. Comp are dtoexistingwork,our data setisordersof wearer. (2) Hand movement prediction: predict the hand magnitudelargerin scale and diversity. Thedata will allow positionsof the camerawe are rinfutureframes. (3)Short- AItolearn from dailylifeexperiencesaround the world\u2014 termobjectinteractionanticipation: detectasetofpossible seeingwhatwesee and hearingwhatwehear\u2014while our futureinteractedobjectsinthemostrecentframeof the clip. benchmark suite provides solid footing for innovations in Toeachobject,assignaverbindicating the possiblefuture videounderst and ingthat are critical for augmentedreality, interactionanda\u201ctimetocontact\u201destimateofwhen the inter- robotics,andmanyo the rdomains. Welook for wardto the actionisgoingtobegin. (4)Long-termactionanticipation: research that willbuildon Ego 4 Din the yearsahead. predict the camerawearer\u2019sfuturesequenceofactions. Contributionstatement Annotations Using the narrations, we identify the occur- renceofeachobjectinteraction,assigningaverb and atarget Projectled and initiatedby Kristen Grauman. Program objectclass. Theverb and nountaxonomies are seeded from management and operationsledby Andrew Westbury. Scien- thenarrations and the nhand-refined. Foreachaction, we tificadvisingby Jitendra Malik. Authors with stars(\u2217)were identifyacontactframe and apre-conditionframeinwhich keydriversofimplementation,collection,and/orannotation we annotate bounding boxes around active objects. The developmentthroughout the project. Authors with daggers sameobjectsaswellash and sareannotatedinthreeframes (\u2020)arefaculty PIs and workinggroupleadsin the project. preceding the pre-conditionframeby 0.5 s,1 sand 1.5 s. We Thebenchmarksbroughttoge the rmanyresearchers from all obtain ground truth ego-trajectories of the camera wearer institutionsincludingcross-institution base lineevaluations. usingstructure from motion. Appendices Fthrough Jdetail the contributionsofindividual 9 authors for the variousbenchmarks. Thevideocollectedby Facebook Reality Labsused Vuzix Blade\u00aeSmart Glasses andwasdoneinaclosedenvironmentin Facebook\u2019sbuild- ingsbypaidparticipantswhosignedconsentstosh are their data. Allo the rvideocollection and participantrecruitment wasmanagedby the universitypartners. Appendix Apro- videsdetailsabout the datacollectiondonepersiteandac- knowledges the primarycontributors. Theannotationeffort wasledby Facebook AI. Acknowledgements Wegrate full yacknowledge the followingcolleagues for valuablediscussions and supportof our project: Aaron Ad- cock, Andrew Allen, Behrouz Behmardi, Serge Belongie, Antoine Bordes, Mark Broyles, Xiao Chu, Samuel Clapp, Irene D\u2019Ambra,Peter Dodds,Jacob Donley,Ruohan Gao, Tal Hassner,Ethan Henderson,Jiabo Hu,Guillaume Jean- neret,Sanjana Krishnan,Devansh Kukreja,Tsung-Yi Lin, Bobby Otillar, Manohar Paluri, Maja Pantic, Lucas Pinto, Vivek Roy,Jerome Pesenti,Joelle Pineau,Luca Sbordone, Rajan Subramanian,Helen Sun,Mary Williamson,and Bill Wu. Wealsoacknowledge Jacob Chalk for settingup the Ego 4 DAWSbackend and Prasanna Sridhar for developing the Ego 4 Dwebsite. Thankyouto the Common Visual Data Foundation(CVDF)forhosting the Ego 4 Ddataset. Theuniversitiesacknowledge the usageofcommercial softw are forde-identificationofvideo. brighter.aiwasused forredactingvideosbysomeof the universities. Personal data from the Universityof Bristolwasprotectedby Prim- loc\u2019s Secure Redactsoftw are suite. UNICT is supported by MIUR AIM - Attrazione e Mobilita Internazionale Linea 1 - AIM 1893589 - CUP E 64118002540007. Bristolissupportedby UKRIEngineer- ingand Physical Sciences Research Council(EPSRC)Doc- toral Training Program(DTP),EPSRCFellowship UMPIRE (EP/T 004991/1). KAUSTissupportedby the KAUSTOf- ficeof Sponsored Researchthrough the Visual Computing Center(VCC)funding. National Universityof Singaporeis supportedby Mike Shou\u2019s Start-Up Grant. Georgia Techis supportedinpartby NSFaward 2033413 and NIHaward R 01 MH 114999. 10 totheparticipantsindifferentpartsof the country. Videos weresh are dbackei the rinexternalharddisksorover the Appendix cloudstorage. Eachvideowasmanuallyinspected for any sensitivecontentbe for esharing. Primarycontributors:Raghava Modhugu-datacollection Table of Contents pipeline,designof the setup and workflow.",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 145,
      "paper_id": "ego4d",
      "text": "Computing Center(VCC)funding. National Universityof Singaporeis supportedby Mike Shou\u2019s Start-Up Grant. Georgia Techis supportedinpartby NSFaward 2033413 and NIHaward R 01 MH 114999. 10 totheparticipantsindifferentpartsof the country. Videos weresh are dbackei the rinexternalharddisksorover the Appendix cloudstorage. Eachvideowasmanuallyinspected for any sensitivecontentbe for esharing. Primarycontributors:Raghava Modhugu-datacollection Table of Contents pipeline,designof the setup and workflow. Siddhant Bansal - IRB application, consent forms and de-identification. C. Appendices 11 V.Jawahar -leadcontributor for data collection. Wealso A .Data Collection . . . . . . . . . . . . 11 acknowledge the contributionsof Aradhana Vinod(coordi- B .De-identification Process . . . . . . . 16 nation and communication),Ram Sharma(local data man- C .Demographics. . . . . . . . . . . . . 18 agement and verification),and Varun Bhargavan(systems D .Narrations . . . . . . . . . . . . . . . 20 andres our ces). E .Benchmark Data Splits . . . . . . . . 24 Universityof Tokyo,Japan: Werecruited 81 Japanesepar- F .Episodic Memory Benchmark . . . . 25 ticipants(41 male,40 female)livingaround Tokyo,Japan G .Hands and Objects Benchmark . . . . 44 throughatemporaryemploymentagency. Theparticipant\u2019s H .Audio-Visual Diarization Benchmark. 51 gender and age(from the 20 sto 60 s)werebalancedtocollect I .Social Interaction Benchmark. . . . . 63 diversebehaviorpatterns. Wefocusedontwosingle-actor J .Forecasting Benchmark . . . . . . . . 67 activities: cooking(40 participants,90 hours)andh and craft K .Societal Impact . . . . . . . . . . . . 82 (41 participants,51 hours). Inthecookingscenario,partici- pants were askedtorecordunscriptedvideosofcookingat theirhomes. Intheh and craftscenario,participantsvisited A.Data Collection our laboratory and per for med various handcraft activities (e.g.,origami,woodworking,plastic model,cutoutpicture). Thissectionoverviews the collectionprocedures and sce- Wecollected data using Go Pro HERO 7 Blackcamera for nariospersite. cooking and Weeview SID 3 Dstereocameraforh and craft. International Institute of Information Technology Our data collectionprotocolwasreviewed and approvedby (IIIT), Hyderabad, India: At IIIT, Hyderabad, we fol- Universityof Tokyoethicalreviewboard. lowedaprotocolofdistributed data collection with acen- Primarycontributors: Yoichi Sato\u2013leadcoordinator for tralizedteamdoingcoordination and verification. Wefirst datacollection,Takuma Yagi and Takumi Nishiyasu\u2013con- identifiedlocalcoordinatorsindifferentpartsof the country tributedtoparticipantrecruiting,protocoldesign,datacollec- andexplained the datacollectionplans,goals and process. tion and inspection,and IRBsubmission,Yifei Huang and Theythenhelpedincollectingdatain the irownlocalregions Zhenqiang Li\u2013contributedto data inspection and transfer, fromnaturalsettingswithin for medparticipants.Participants Yusuke Sugano\u2013contributedtoselectingvideorecording wererecruitedlocallyconsidering the rangeofactivities,and scenarios,protocoldesign and IRBsubmission. also the guidelines and restrictionsof COVID-19. Thecen- tralteamcouldnottraveltoall the selocations for training University of Bristol, UK: Participants were recruited thecoordinatorsorcollecting the data. Weshippedmultiple throughadvertsonsocialmedia and universityinternalcom- camerasto the localcoordinators and remotelyguidedthem munication channels. These participants then spread the ondatacollectionfollowing the COVIDprotocols. Thecol- wordto the iracquaintances and someparticipantsjoined the lected data and consent forms were then shipped back to projectthroughword-of-mouthrecommendationsofprevi- theuniversity,wheremanualverification,de-identification ousparticipants. Datawascollectedbetween Janand Dec (whereverapplicable),andsharing with the consortiumtook 2020,from 82 participants.With the pandemictakingoverin place. March,theprojectshiftedtoonlineoperationwherecameras At IIITHyderabad,werecorded 660.5 hoursof data with wereposted,andtrainingtookplaceover Zoommeetings. the help of 138 subjects. The videos were collected in 5 Participantsfirstexpressedinterestbysendinganemail and differentstatesin India,geographicallywellapart. Wecover they were provided with anin for mationsheet. Thiswasfol- 36 differentscenarios,suchasmakingbricksusinghands, lowedbyapreliminary Zoommeeting with are searcherto knitting, making egg cartons, and hairstyling. The age of briefparticipantsabout the procedure,answeranyquestions subjects ranged from 18-84 years with 10 distinct profes- andagreeon the scenariostoberecorded. sionalbackgrounds(teachers,students,farmers,blacksmiths, Wesetalimitto the totalnumberofminutesperscenario, homemakers,etc.). Outofall",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 146,
      "paper_id": "ego4d",
      "text": "Participantsfirstexpressedinterestbysendinganemail and differentstatesin India,geographicallywellapart. Wecover they were provided with anin for mationsheet. Thiswasfol- 36 differentscenarios,suchasmakingbricksusinghands, lowedbyapreliminary Zoommeeting with are searcherto knitting, making egg cartons, and hairstyling. The age of briefparticipantsabout the procedure,answeranyquestions subjects ranged from 18-84 years with 10 distinct profes- andagreeon the scenariostoberecorded. sionalbackgrounds(teachers,students,farmers,blacksmiths, Wesetalimitto the totalnumberofminutesperscenario, homemakers,etc.). Outofall the subjects,94 weremales, to increase diversity of recordings. For example, driving and 44 were females. We use Go Pro Hero 6 and Go Pro cannotbelongerthan 30 minuteswhilecooking can beup Hero 7 forrecording the videos. The Go Pro\u2019swereshipped to 1.5 hours. Each participant was instructed to record a 11 minimumof 2 hoursacross 4 scenarios. Importantly,partic- wascomprisedoffriendsorfamilymemberswhok new each ipants were enc our agedtocollectactivities the ynaturally otherpriortoparticipatingin the study. Participants were do. For example if one regularly cycles or practices mu- requiredtobeaged 18-64,tonotbeconsideredhighrisk for sic,they were askedtorecord the sescenarios. Additionally, COVID-19,andtobeabletoplaysocialdeductiongamesin pairedscenarios(peoplecookingtoge the rorplayinggames) English. Ourstudyprotocolwasreviewed and approvedby wereenc our aged and multiple(2-3)cameras were posted for the Georgia Tech Institutional Review Board(IRB).Intotal, participantssharingahousehold. Allparticipantssigneda approximately 43 hoursofegocentricvideo were collected consentformbe for eacamerawaspostedto the irresidence. from 19 participants(perparticipantdisclosure-10 male, Cameras were postedto 9 UKcitiesin Engl and,Wales and 7 female,1 non-binary,1 notreported). Participantshada Scotl and includingoneparticipantin the Isleof North Uist. meanageof 31.6 years with 7 participantsaged 20-29 years, Uponreceiptof the camera,asecond Zoommeetingwas 10 participants aged 30-39 years, and 2 participants aged scheduledtotraintheparticipanton the equipment and detail 40-49 years. how footage is reviewed and uploaded. Participants were Participantsworeanegocentrichead-worncamera and given 2 weekstorecord,withanadditionalweekofexten- on-earbinauralmicrophones. Someparticipantswore the sionuponrequest. Oncerecordingiscompleted,footageis ORDROEP 6 camerawhileo the rswore the Pupil Invisible uploadedby the participant and reviewed for goodlighting, cameras. Theaudiowasrecordedusinga Tascam DR-22 WL correctsetting and viewpoint. Participants were reimbursed and Sound Professionals MS-EHB-2 Ear-hookbinauralmi- fortheirparticipationin the project. crophones. A third-person video was also captured via a Scenariosrecordedin the UKcovered: commuting(driv- Logitech C 930 e Webcam. Participantswore the provided ing,walking,cycling,taking the bus,hiking,jogging),en- recordingdeviceswhileeating,drinking,andplayingsocial tertainment(cardgames,boardgames,videogames,lego, deductiongamessuchas One Night Ultimate Werewolf and reading,practisingamusicalinstrument,listeningtomusic, The Resistance: Avalonin the irownhome. Thisat-home watching TV),jobs(labwork,carpentry),sports(football, game-nightsettingelicitedawiderangeofspontaneous and basketball,climbing,golf,yoga,workouts)andhome-based naturalistic social behaviors and interactions. In addition, daily activities (cooking, cleaning, laundry, painting, car- eating and drinkingbehaviors were captured from both the ing for pets,tidying,watering the plants),DIY(fixing,gar- egocentri can dthird-personcameras. dening,woodwork)andcrafts(col our ing,crafting,crochet, Inadditiontoparticipatingin the recordedsession,partic- drawing, knitting, sewing). Footage was captured using ipantscompletedasurvey that captured the irdemographic Go Pro Hero-7,Hero-8 and Vuzix. information. All data wasscreened and censoredbystudy Footagewas the nreviewedbyresearcherstoidentifyany personneltoremoveanyidentifyingin for mationincluding PII.36%ofallvideosrequiredde-identification. we used visiblepersonalin for mationon the irphonescreensor the Primloc\u2019s Secure Redactsoftw are suite,withintegratedtools exteriorof the home. Participantsalsohad the opportunity anduserinterfaces for manualtracking and adjustingdetec- toreview the videos and requestadditionalcensoring. tions. Redactedrecordings were reviewedmanually, then Primarycontributors: Fiona Ryan-leadcoordinator for encoded and uploadedto the AWSbucket. Duringencoding, datacollection,includingsynchronization,de-identification, IMUmeta data wasseparatelyextracted. Integratedaudio and ingestion; Audrey Sou the rland - lead coordinator for andvideousingnative 50 fpsrecordings are available. IRBdevelopment and recruiting;Miao Liu-contributedto Intotal,262 hours were recordedby 82 participants. On datacollection and ingestion;James M.Rehg-contributed average,eachparticipantrecorded 3.0 hours(\u03c3 =0.7 hours) toprotocoldesign and datacollection. The data ispublishedunder General Data Protection Regula- Indiana University,Bloomington,IN,USA: Participants tion(GDPR)compliance. in the Bloomington, Indiana, USA area were recruited Primary contributors: Michael Wray - data collection, throughadvertisementsonsocialmedia,onlineclassifieds consent forms and information sheets; Jonathan Munro - boards,andemaillists. Wealsousedsnowballsamplingby datacollection and",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 147,
      "paper_id": "ego4d",
      "text": "and ingestion;James M.Rehg-contributed average,eachparticipantrecorded 3.0 hours(\u03c3 =0.7 hours) toprotocoldesign and datacollection. The data ispublishedunder General Data Protection Regula- Indiana University,Bloomington,IN,USA: Participants tion(GDPR)compliance. in the Bloomington, Indiana, USA area were recruited Primary contributors: Michael Wray - data collection, throughadvertisementsonsocialmedia,onlineclassifieds consent forms and information sheets; Jonathan Munro - boards,andemaillists. Wealsousedsnowballsamplingby datacollection and ethicsapplication;Adriano Fragomeni- askingparticipantstosh are ourads with the irfriends. Were- datacollectionandde-identificationoversight;Will Price- cruitedparticipantswho were willingtoper for minteractive dataingestion,encoding and meta data;Dima Damen-sce- smallgroupactivitiessuchasplayingsports,playingboard narios,procedures,datacollectionoversight and participant orcardgames,playingmusicalinstruments,assemblingpuz- communication. Weacknowledge the effortsof Christianne zles, etc. The health of participants and study personnel Ferneeinmanuallyreviewingall data. wassafeguardedbycollectingdataei the routdoors(where Georgia Tech, Atlanta, GA, USA: Participant groups people can moresafelyinteract with outwearingmasks),or from the Atlanta,Georgia,USAmetro are awererecruited indoorsinthehomesof the participants. Inei the rcase,we viaonlineposts and advertisementsonsitessuchas Face- initially required that all participants in a social group be book, Reddit, and Instagram. Each group of participants partofthesamehouseholdtominimize the riskofspreading 12 diseasebetweenhouseholds,butlaterweallowedgroupsof Primarycontributors: David Crandall-leadcoordinator peoplewho were com for tableinteracting with oneanother for data collection;Yuchen Wang-contributedtoprotocol (e.g., because the yarevaccinated for COVID-19). Group design, participant recruiting, and data collection; Weslie sizesranged from 1 to 6 people,withgroupsof 2 or 3 being Khoo - developed multi-camera synchronization and de- themostcommon. identificationpipelines. We collected data with four different devices: z Shade Universityof Minnesota,Twin Cities,MN,USA: Partic- 1080 p camera glasses, i Vue Rincon 1080 camera glasses, ipants in the Minneapolis and St. Paul, Minnesota, USA ORDRO EP-6, and Pupil Labs Invisible camera and gaze area were recruitedthroughadvertisementsonsocialmedia trackingglasses. we usedmultipledevicesbecauseeachhas anduniversitybulletinssuchas Facebook AD,Craiglist,and variousadvantages and disadvantages; z Shadehasalarge Redhat. A total of approximately 313 hours of data was horizontalfieldofview,forexample,whilei Vuehasanad- collected from 45 participants (22 males and 23 females). justableverticalfieldofview,ORDROsitsby the earandis Agegroupsinclude 5 teenagers,20 peoplein the irtwenties, mountedonaheadb and whichworkswell for peoplewear- 11 people in their thirties, 8 people in their forties, and 1 ingprescriptionglasses,and Invisibleoffersgazetracking personin the irfifties. Werecruitedparticipantsasmultiple but is very expensive. We asked as many participants as groups and enc our aged the mtoengageinunstructurednat- possiblein the grouptowearcameras. Weprimarilyused uralsocialinteractions. Suchinteractionsincludedplaying ourtwo Pupil Labs Invisibleswheneverpossible,because cardgames, talkingin the kitchenwhilecooking, playing oftheireaseofuse and abilitytocollectgaze data,butwe basketball,andbuildingatentatacampsite. Inallcases, alsoused the ORDROEP-6 when the rewerelargergroups werequired that allparticipantsinasocialgroupbepartof orwhenparticipantsworeprescriptionglasses. thesamehouseholdtominimize the COVID-19 risk. Group Our protocol was reviewed and approved by the Indi- sizesranged from 1 to 6 people,withgroupsof 2 or 3 being ana University Institutional Review Board(IRB).Wefirst themostcommon. conductedanonlinemeeting with potentialparticipantsto Wecollected data with thez Shade 1080 pcameraglasses describe the study,explaintheuseof the cameras,agreeon that have alargefieldofview. Ourprotocolwasreviewed anactivity for the mtoperform,andanswer the irquestions. andapprovedby the Universityof Minnesota Institutional We ask participants to try to limit capture of potentially Review Board (IRB). We first conducted an online meet- privacy-sensitivecontentbychoosingaplace with intheir ing with potentialparticipantstodescribe the study,explain home that didnot have personallyidentifiablein for mation, the use of the cameras, agree on an activity for them to byavoidingrecordingpeopleo the rthanthoseparticipating perform, and answer their questions. We then arranged a in the study, and by avoiding saying last names or other time for them to receive the cameras and provided them sensitiveaudio. withapostage-paidbox for camer are turn. Afewdayslater, Wethenarrangeatimetomeetthem,typicallyoutside participants shipped the cameras to our designated return their home or in an outdoor public place. We",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 148,
      "paper_id": "ego4d",
      "text": "We then arranged a in the study, and by avoiding saying last names or other time for them to receive the cameras and provided them sensitiveaudio. withapostage-paidbox for camer are turn. Afewdayslater, Wethenarrangeatimetomeetthem,typicallyoutside participants shipped the cameras to our designated return their home or in an outdoor public place. We set up the address. Wedownloaded the dataaftersanitizingcameras cameras,helptheparticipantsput the mon,give the mour and equipment. After the data capture was complete, we contact information in case they have any problems, and visuallyinspectedeverysecondofvideoinordertoexclude thenweleavewhiletheyperform the activity. Wethenre- anyprivacy-sensitivein for mation(e.g. licenseplates,smart turn after about one hour to pick up the cameras. Within phonescreens,andcreditcardnumbers),andtoassess the a few days, we send each participant a copy of the video durationofnon-socialactivities. Forincidentalparticipants takenby the ircamera,andaskthemtoreview the footage (i.e. byst and ers)appearingin data collectedby the camera andidentifyanyprivacy-sensitivecontent(videooraudio) wearerinpublicsettings(e.g.,shopping,concert,atapark, that the ywouldprefertobeblurredorremoved. Wemanu- etc.),datacollectionconsistsonlyofrecordingpubliclyob- allyeditoutanysuchcontent(using Adobe Premiere Pro). servablebehavior with nomanipulationordirectinteraction Wealsoreviewallvideo for facesofnon-participants and with the participants, and this university\u2019s IRB allows an personally-identifyingin for mationsuchashousenumbers assumedwaiverofconsent for thoseparticipants. or license plates, and blurred these accordingly. We use Primarycontributors: Hyun Soo Park-leadcoordinator Pupil Labssoftw are tosynchronizeeyegaze with the video for data collection;Jayant Sharma-contributedtoparticipant foreachparticipant,and the nused Adobe Premiere Proto recruiting, data collection, IRB submission, analysis, and temporallysynchronizevideoacrossdifferentparticipants dataingestion. usingaudiotrackcomparison. Intotal,approximately 103 hoursofvideo were collected National University of Singapore, Singapore: Partici- from 66 participants(42 female,23 male,1 non-binary;for pants were recruited from Singaporethroughadvertisements age, 46 were 20-29 years old, 14 were 30-39 years old, 1 on social media, via flyers and surveys, as well as from was 40-49,2 were 50-59,1 was 60-69,and 2 were 70-79). sourcingby the projectcoordinator. Residentsof Singapore 13 aged 21 to 70 whocouldwearacamerawhileparticipating 44 twenties, 3 thirties, 2 forties, 6 fifties, and 1 sixties). in social sessions were eligible for inclusion in our study. Our data collectionfocusesmainlyonsimultaneousvideo During the recordingsession,theparticipants were required recording in groups of camera wearers within a common toattendsocialeventssuchasfamilyga the rings,exercising setting. Thus,these data captureasinglescene and social with a trainer, hairdressing, getting manicure, attending a interactions from differentpointsofview. Weincludeboth session for teachingassistants,attendingagroupmeeting, outdoor and indoorscenariosin Colombia. Outdoorscenar- etc. Thedevicesused for datacollection were Go Pro Hero 8, iosinclude Bogota\u00b4 and Cartagena\u2019shistorical and colonial Go Pro Hero 9,and ARglasses. Go Procameras have binau- centers,asurbansettings,anda Natural National Park and ralmicrophoneswhile the ARglasses can onlyrecordmono astream,asruralsettings. Indoorlocationsincludeprofes- audio. Intotal,51 hoursofvideos were collected from 40 sionalactivitiessuchaslaboratoryworkers and hairstylers. participants(25 males and 15 females). Agegroupsinclude Fur the rmore, we include sports events such as salsa and 31 twenties,5 thirties,3 fifties,and 1 sixties. urbandancerehearsals and rockclimbing. Primarycontributors: Mike Zheng Shou-leadcoordina- Primarycontributors: Cristina Gonza\u00b4lezand Paola Ruiz tor for datacollection;Eric Zhongcong Xu-contributedto Puentes. datacollection;Ruijie Tao-contributedto data collection. Carnegie Mellon University, Pittsburgh, PA, USA and Facebook Reality Labs (FRL), Redmond, WA, USA: Kigali,Rwanda: Carnegie Mellon University(CMU)Pitts- Participants were recruited from the Seattle area through burghga the redalargeportionofits data from skilledwork- a FRL-hiredvendorcompany. Intotal,there were 400 hours ers such as carpenters, construction workers, landscapers, collected from 206 unique participantsin 6 scenes staged mechanics,arborists,painters,andartists. Thisportionof in FRL\u2019sresearchlabsin 2019. Theethnicgroupsinclude the data setdoesnotincludeanygraduatestudents with the 50.8% Caucasian, 28.2% Afri can, 11.9% Asian and 9% explicitgoalofcapturingadiverserangeofreal-worldoccu- Hispanic. The staged environments include four types of pationalactivities. Over 500 hoursofvideo were",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 149,
      "paper_id": "ego4d",
      "text": "hours ers such as carpenters, construction workers, landscapers, collected from 206 unique participantsin 6 scenes staged mechanics,arborists,painters,andartists. Thisportionof in FRL\u2019sresearchlabsin 2019. Theethnicgroupsinclude the data setdoesnotincludeanygraduatestudents with the 50.8% Caucasian, 28.2% Afri can, 11.9% Asian and 9% explicitgoalofcapturingadiverserangeofreal-worldoccu- Hispanic. The staged environments include four types of pationalactivities. Over 500 hoursofvideo were captured apartments, a clothing store, and a grocery store. During inthe Pittsburgharea. The data wasmostlyrecordedusing therecordingsessions,theparticipants were askedtowear a Go Pro camera and a small portion was collected using Vuzixglassestogothrough the followingeverydayscenarios Wee View,awearablestereocamera. asnaturallyaspossible: groceryshopping,buyingclothes, Carnegie Mellon University Africa gathered data from watching TV,playingvideogames,listeningtomusic,danc- hobbyistcraftspeople and dailyworkersworkingin Kigali, ing, weightlifting, stretching, readingemail, payingbills, Rwanda. Anef for twasmadetocollect data mostrepresen- onlinegaming,cooking,talkingwitho the rpeople,meetings, tativeofhowtasks are carriedoutin Rwanda(suchasdoing whiteboarding,andvideocalling. Theemails and bills were laundrymanuallyasopposedto with awashingmachine). always mock data, not personal emails or bills of the par- Over 150 hours of video were captured, and a portion of ticipants. Thevideocallstookplacebetweenparticipants thosehours are availablein the currentrelease. Allof the only. datawascollectedusinga Go Procamera. Three out of four apartments have corresponding 3 D Primarycontributors: Kris Kitani-projectcoordinator scans. we use the state-of-the-artdensereconstructionsys- forboth CMUPittsburgh and CMUAfricavideocollection. tem[209]toobtain the 3 Dphoto-realisticreconstructionof Sean Crane-leadcoordinatorof CMUPittsburgh data col- thoseapartments. Volumetricrepresentations are obtained lection (over 500 hours), main lead of CMU IRB review. from a customized capture rig and dense 3 D meshes are Abrham Gebreselasie-leadcoordinatorof CMUAfrica data extracted by the Marching Cubes algorithm with textures. collection. Qichen Fuand Xindi Wu-developmentofvideo We further annotate the dense meshes by labeling object de-identification pipeline, manual video de-identification categoriesover the meshpolygons;35 objectcategoriesplus annotationof CMUPittsburgh data. Vivek Roy-mainarchi- abackgroundclasslabel are usedinannotation. tectureof the licensesigningwebserver,coordinating with Primarycontributors: Mingfei Yan,Richard Newcombe, America Web Developers. Kiran Somasundaram,Chao Li. Universityof Catania,Italy: Morethan 359 hoursofvideo Universidad de los Andes, Colombia: We gather 302.5 have been recorded from 57 different subjects recruited hoursacross 20 scenarios from 77 uniqueparticipants. We through word of mouth, starting from family members, recordvideosusing Go Pro Hero 9 camerasbetween July and friendsandacquaintancesofstudents and facultymembers August 2021. Werecruitvolunteerparticipants from within oftheresearchgroup. Videos are relatedto 25 scenarios. We the Uni and escommunity and the irfamilies and friends. The chose the participantstocoverawidevarietyofprofessional ethnicgroupsinclude 89.9%Hispanic,1.4%Afri can,and backgrounds(24 backgroundsincludingcarpenters,bakers, 5.8%Caucasian.Thegenderdistributionfollows 41.6%male employees,housewives,artists,andstudents)andages(sub- and 58.4%female with agesranging from 18 to 65(6 teens, jects were aged from 20 to 77,withanaverageageof 36.42). 14 Baker > 9.5 hrsof videos Carpenter > 7 hrsof videos Crafting> 12 hrsof videos Bike Mechanic> 5.5 hrsof videos Bike Mechanic> 17.5 hrsof videos Scooter Mechanic> 9.5 hrsof videos Car Mechanic> 3.5 hrsof videos Figure 11.Matterport 3 Dscans(top)relatedtosevendifferentlocationscoupled with somevideos(bottom). 21 oftheparticipants were female,while the remaining 36 inour Facebookadvertisementsorpostersincampusrestau- weremale. Femaleparticipantscollectedabout 137 hours rants and supermarkets. Each candidate participant was ofvideo,whereasmalescollected 222 hoursofvideo. The requiredtoregisterthroughanonlineform,whichcontained averagenumberofh our sofvideosacquiredbyeachpartic- anintroductionto and requirementsof the recording task, ipantis 6 h:18 m:23 s,withaminimumnumberofh our sof andcollectedhis/herbasicdemographicin for mation. The 06 m:34 s,andamaximumnumberofh our sof 15 h:40 m:42 s. participants\u2019 ages range from 22 to 53. They come from Toprep are participantstorecordvideos,wedemonstrated 20 different countries, and about half are females. Many tothemtheoperationsof the camera and howtowearit. We participants were graduatestudents and researchers,while providedexamplesofvalidrecording and invalidrecordings othershadvariouskindsofoccupationssuchaschefs,facil- beforetheystarted the acquisitionsession. Therecording itymanagers,andteachers. procedurewasdescribedinadocumentleftto the partici- Inordertoprep are theparticipants",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 150,
      "paper_id": "ego4d",
      "text": "m:42 s. participants\u2019 ages range from 22 to 53. They come from Toprep are participantstorecordvideos,wedemonstrated 20 different countries, and about half are females. Many tothemtheoperationsof the camera and howtowearit. We participants were graduatestudents and researchers,while providedexamplesofvalidrecording and invalidrecordings othershadvariouskindsofoccupationssuchaschefs,facil- beforetheystarted the acquisitionsession. Therecording itymanagers,andteachers. procedurewasdescribedinadocumentleftto the partici- Inordertoprep are theparticipants for the recordingpro- pantstohelpthemremember the deviceusage and howto cess,theteamdescribedindocuments and demonstratedto per for magoodacquisition. Acquisitionofvideoshas been themtheoperationsof the camera. Theteamalsoprovided per for medusingdifferent model sof Go Procameras(Go Pro examples of what constitute valid and invalid recordings 4, Go Pro 7, Go Pro 8, and Go Pro Hero Max), which were before the ystarted. Eachparticipantwasprovideda Go Pro handedoverto the participantswhotypicallyacquiredtheir mountablecamera with 2 batteriesanda 512/256 GBSD videosautonomouslyoveraperiodofafewdaysorweeks. card. Each participant needed to choose at least 2 differ- 3 Dscans for 7 locations using the Matterport 3 Dscanner entactivities from ourscenariolist and record 1-10 hours have been alsocollected(Figure 11). ofvideowithin 2 days. Theuniversityteamwentthrough Primarycontributors: Giovanni Maria Farinella and An- therecordingsaftertheparticipantsreturned the camerato tonino Furnari-scenarios,procedures,datacollectionover- checktheirqualityaswellastomakesure the videosmeet sight, data formatting, encoding, meta data and ingestion. theuniversity\u2019s IRBrequirements. Irene D\u2019Ambra-datacollection,consentforms and informa- Primarycontributors: Chen Zhao,Merey Ramazanova, tionsheets,manualdat are view,de-identificationoversight. Mengmeng Xu,and Bernard Ghanem. King Abdullah University of Science and Technology (KAUST), Saudi Arabia: A total of 453 hours of videos have been collected from 66 uniqueparticipantsin 80 differ- entscenarios with Go Pro Hero 7. All the participants were KAUSTcommunitymembers,who are fromvariouscoun- tries and havevariousoccupations.Allrecordingstookplace inthe KAUSTuniversitycompound,whichis 3600 hectares inarea with diversifiedfacilities(e.g.,sportscourts,super- markets, a 9-hole golf course, and 2 beaches) and scenes (e.g.,buildings,gardens,theredsea,and the desert). There- fore,theteamwasabletocollectvideosofvariousscenarios suchassnorkeling,golfing,cycling,anddriving. The participants were recruited from multiple sources, such as friends and families, individuals referred to us by earlierparticipants,aswellaspeoplewho were interested 15 B.De-identification Process The dataset has two types of video. The first includes videos recorded indoors where informed consent for cap- turingidentitiesisexplicitlycollected from allparticipants inthescene,includingfaces and voice. Onlyvideoof this type is used in our Audio-Visual Diarization and Social Interaction benchmark studies. All 400 hours of data col- lectedby Facebook Reality Labsfallsin that category. The second category, which forms the majority of our videos, requiresde-identificationasconsent for capturingidentities isnotgiven\u2014includingfootagecapturedoutdoorsinpublic spaces.4 Onlyvideocollectedby the universitiesfallsinto thissecondcategory. See Appendix Afordetailsabout the per-sitecollectionapproaches. Figure 12. CMU\u2019sde-identificationpipeline B.1 De-identificationoverview bemanuallyidentified and blurredper-frame. For this part Allvideosin the secondcategory were manuallyscreened ofourde-identificationprocess,we usedbothcommercial to address any de-identification needs, and are further di- toolswithin the above-mentionedcommercialsoftw are and videdintotwogroups. Group 1: videos that donotcontain opens our cesoftw are,including Computer Vision Annota- anypersonallyidentifiablein for mation(PII).5 Thisiswhen tion Tool(CVAT)8,Anonymal 9 and Siam Mask 10. thevideoisrecordedindoors with onepersonwearing the Timecosts. Therelativetimecosts with respectto the orig- camera per for ming tasks such as cleaning or knitting for inalvideolengthvariedsignifi can tlyfor the differentscenar- example,andno PIIispresentin the video. Thesevideos ios. Videoscapturedoutdoorscouldtake 10 xthelengthof didnotrequirede-identification. Group 2: videoswhere PII thevideotoc are fullyredact. is captured. These include indoor settings with multiple participants present, PII captured accidentally such as an addressonanenvelopeor are flectionof the wearer\u2019sfaceon B.2 Samplepipeline amirrororasurface,aswellasvideosrecordedoutdoorsina publicspacewherebyst and ersorcarsappearin the footage. Whilepartnersfollowedvaryingpipelines,weofferasam- Videosin Group 2 weremarkedforde-identification,deploy- plepipelinetoshowcase the processfollowedby Carnegie ingadvancedvideoredactionsoftw are,opens our cetools, Mellon University that uses brighter.ai as the commercial andh our sofhumanreviewstoredactvisible PIIs.University softw are. Thissamplepipelineshowcases the combination partnersundertookthisde-identificationef for tfor",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 151,
      "paper_id": "ego4d",
      "text": "accidentally such as an addressonanenvelopeor are flectionof the wearer\u2019sfaceon B.2 Samplepipeline amirrororasurface,aswellasvideosrecordedoutdoorsina publicspacewherebyst and ersorcarsappearin the footage. Whilepartnersfollowedvaryingpipelines,weofferasam- Videosin Group 2 weremarkedforde-identification,deploy- plepipelinetoshowcase the processfollowedby Carnegie ingadvancedvideoredactionsoftw are,opens our cetools, Mellon University that uses brighter.ai as the commercial andh our sofhumanreviewstoredactvisible PIIs.University softw are. Thissamplepipelineshowcases the combination partnersundertookthisde-identificationef for tfor the irown ofautomatedprocesses and humanlabor with relativespeeds data. Wesummarize the approachbelow. ofthesesteps. Videos marked for redaction were processed through This semi-automatic de-identification process was per- de-identificationsoftw are thatremovesspecificidentifiers formedinf our sequentialstages(Figure 12): (1)automatic at scale. We used two commercial softwares: brighter.ai 6 face and licenseplatedetection,(2)falsepositiveremoval, and Primloc\u2019s Secure Redact 7 thatenableddetectingfaces (3)negativedetectionh and ling,and(4)imageblurring. and number plates automatically. We care fully reviewed Sensitiveobjectdetection Given the collectedvideos(raw all outputs from automated blurring, identifying both in- data),areviewers can sthroughvideos and marksthosecon- stancesoffalsepositives(blurring that mistakenlyoccurred tainingsensitiveobjectssuchashumanfaces,licenseplates, on non-privacy related items) or false negatives (inaccu- creditcards,etc.Thende-identificationsoftw are(brighter.ai) rate or insufficient automated blurring of faces and num- wasusedtoautomaticallydetectsensitivein for mation. ber plates). Additionally, other PII data such as written names/addresses,phonescreens/passwordsortattooshadto False positive removal To improve the quality of the de- tection,falsepositives were removed. Reviewersmanually 4 Theexceptionis data from Universityof Minnesota,whose IRBper- scanned through the bounding boxes detected by the de- mittedrecordingofincidentalparticipantsinpublicspaceshavingnoma- nipulationordirectinteraction with studypersonnel. identificationsoftw are,andrejectedthoseboundingboxes 5 we use the abbreviation PIItocapture data protectedundervarious whichdidnotcontainsensitivein for mation. dataprotectionregimesincluding the General Data Protection Regulation (GDPR)where the term\u201cpersonal data\u201disused. 8 https://github.com/openvinotoolkit/cvat 6 http://brighter.ai 9 https://github.com/ezelikman/anonymal 7 http://secureredact.co.uk 10 https://github.com/foolwood/Siam Mask 16 Falsenegativecorrection Additionally,reviewersstudied everyvideotosearch for falsenegatives and manuallyan- notated the musingaboundingbox. Tomake the process more efficient, an online object tracking algorithm [222] wasusedtogenerateboundingboxproposalsacrossframes. Reviewers verified that all tracked bounding boxes were correct. Imageblurring Onceallof the detections were modified and corrected, a robust blurring process was used to de- identifyimageregionsdefinedby the boundingboxes. Timecosts Therelativetimecosts with respectto the orig- inal video length for each step are shown in Figure 12. Though this number depends greatly on the scenario cap- turedin the video,roughlyspeakingtode-identify 500 hours ofvideo data,ittook 780 hoursofmanuallabor. Review 1 of 500 hoursofvideorequired 250 hoursofwork,removal of false positive over 115 hours of video took 115 hours ofwork, Review 2 of 115 videostook 115 hoursofwork, correctingfalsenegativesin 35 hoursofvideosrequired 50 hoursofwork,and Review 3 of 500 hoursofvideotook 250 hoursofwork(250+115+115+50+250=780 hrs). 17 C.Demographics US 39 China 10 Wefur the rprovideself-decl are din for mationonethnic India 10 groups and/orcountryofbirthby the participants. Wereport Bangladesh 2 theseseparatelyperstate/countrydueto the differencesin Vietnam 2 granularityofethnicgroupings.Allparticipants are residents Georgia,USA,Residents 100%ofparticipants that reside in the country specified per paragraph. This data is not in Georgia,USA,self-reported the irethnicgroupmember- available for participants from Minnesota,US. shipasfollows: United Kingdom Residents Reportingdemographicswas White/Caucasian 16 optional and thus 63%ofparticipants(52/82)thatresidein Black/Afri can Ameri can 1 the United Kingdomself-reported the irethnicgroupmem- Asian/Indian&White/Caucasian 1 bershipasfollows: Other/Taiwanese 1 White\u2014English,Welsh,Scottish,Northern Irishor British 35 Japan Residents 100%ofparticipants that residein Japan White\u2014Anyother Whitebackground 12 self-reported the irethnicgroupmembershipasfollows: Mixed\u2014White and Asian 1 Asian(Japanese) 81 Mixed\u2014Anyother Mixedor Multipleethnicbackground 2 Arab 1 Kingdomof Saudi Arabia Residents 100%ofparticipants Prefernottosay 1 that reside in KSA self-reported their country of birth as follows: Italy Residents 100% of participants that reside in Italy China 12 self-reported the ircountryofbirthasfollows: Russia 9 Italy 53 Colombia 8 Germany 1 Mexico 5 Russia 1 Kazakhstan 4 Portugal",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 152,
      "paper_id": "ego4d",
      "text": "2 Arab 1 Kingdomof Saudi Arabia Residents 100%ofparticipants Prefernottosay 1 that reside in KSA self-reported their country of birth as follows: Italy Residents 100% of participants that reside in Italy China 12 self-reported the ircountryofbirthasfollows: Russia 9 Italy 53 Colombia 8 Germany 1 Mexico 5 Russia 1 Kazakhstan 4 Portugal 1 India 4 Poland 1 US 4 Saudi Arabia 3 India Residents 100%ofparticipants that residein India Kyrgyzstan 2 self-reported the irethnicgroupmembershipasfollows: New Zeal and 2 Eastern India 10 Greece 2 Northern India 15 Ukraine 2 Southern India 108 Italy 2 Western India 5 Lebanon 1 Jordan 1 Egypt 1 Pennsylvania,USA,Residents 100%ofparticipants that Kashmir 1 residein Pennsylvania,USA,self-reported the irethnicgroup Portugal 1 membershipasfollows: South Afri can 1 White 42 Thail and 1 Asian 4 Mixed\u2014White and Black Afri can 2 Singapore Residents 100% of participants that reside Black,Afri can,Caribbean 1 in Singapore self-reported their nationalities as follows: Chinese 26 Washington,US,Residents 100%ofparticipantsthatre- Singaporean 12 sidein Washington,USA,self-reported the irethnicgroup Indian 1 membershipasfollows: Malayan 1 Caucasian 101 Colombia Residents 90% of participants that reside in Blackor Afri can Ameri can 58 Colombia self-reported their ethnic group membership as Ameri can Indian(Native Ameri can) 24 follows: Hispanic 19 Hispanic/Latin 62 Indian(South Asian) 4 White/Caucasian 4 Black,Africanor Caribbean 1 Indiana,US,Residents 95%ofparticipants that residein Mixed-Whitean Afri can 1 Indiana,US,self-reported the ircountryofbirthasfollows: Prefernottosay 1 18 Rwanda Residents 100% of participants that reside in Rwandaself-reported the irethnicgroupmembershipasfol- lows: Black,Africanor Caribbean 14 19 D.Narrations D.2 Narrationanalysis Thegoalof the narrationsistoobtainadensetemporally- Wepresentsomestatisticson the collectednarrations. Al- aligned textual description of what happens in the video, together, we collected 3.85 M sentences across the 3,670 particularlyintermsof the activities and objectinteractions hours of video. Figure 15 (left) shows the distribution of bythecamerawearer. The Ego 4 Dnarration data isitselfa frequencyofnarrationsacrossallvideosin the dataset. De- newresource for learningaboutlanguagegroundedinvisual pendingon the activitiesdepicted,videos are annotatedat perception. Inaddition,asdescribedin the mainpaper,we varyingfrequencies. Forexample,avideoofapersonwatch- leverage the narrationsasa for mof\u201cpre-annotation\u201dtoindex ing television is sparsely annotated as very few activities thevideosbysemanticterms. Specifically,thenarrations are occur (0.17 sentences/minute), while a video of a person usedtoconstructaction and objecttaxonomiestosupport harvesting crops, per for ming repetitive actions is densely variousbenchmarks,toidentifyvideos that are relevantto annotated(63.6 sentences/minute). Onaverage,therearean eachbenchmark,andtoselectregionswithin the videos that 13.2 sentencesperminuteofvideo. requireannotation. Figure 15 (middle and right) show the distribution of Thissectionoverviewshowweinstructedannotatorsto lengthof the collectednarrations. Theindividualtimepoint narrate the videos,andhowwetrans for mednarrationtext narrations are short, highlightasingleactionorobjectin- intotaxonomiesofobjects and actions. teraction,and have anaverageof 7.4 words. Thoughshort, thesenarrationscoveravarietyofactivitiesrangingfromob- D.1 Narrationinstructions and content jectinteractions,tooluse,camerawe are rmotions,activities ofo the rpeopleetc. Incontrast,thesummarynarrations are We divide the dataset into clips of (max) 5 minutes long longer(onaverage,16.8 words)anddescribeactivitiesata whenacquiringnarrations.Each 5-minuteclipis the npassed higherlevel. Table 2 showsafewtextexamplesofeachtype totwodifferentannotators,tocollecttwoindependentsets ofnarrationinadditionto the visualexamplesin Figure 14. of narrations for every video clip in the dataset for better Finally, we study the diversity of the video dataset by coverage and toaccount for narrationerrors.11 Narrators are lookingatthefrequencyofoccurrenceofwordsin the narra- instructedtowatch the 5 minutevideoclipfirst, andthen tionscollected for videosofeachscenariotype. Figure 16 askedtoprovideashort 1-3 sentence\u201csummary\u201dnarration showswordcloudsdepictingobjects that prominentlyfea- fortheentireclip that correspondsto the overallactivity and tureinacrossvariousscenarios. Thewordcloudshighlight settingof the videoclip(e.g.,\u201cthepersondoeslaundryin characteristicobjectsperscenario(e.g.,bowl,spoon,plate thewashingmachine\u201d). Thesesummaries are marked with in \u201cCooking\u201d videos; card, dice, pawn in \u201cPlaying board thetag\u201c#summary\u201dinthereleasednarrations. games\u201dvideos)whilealsohintingatcommonobjectsacross Following this first screening, which is critical for the allscenarios(e.g.,hands,paper,phones). Thediversityin overallunderst and",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 153,
      "paper_id": "ego4d",
      "text": "videosofeachscenariotype. Figure 16 askedtoprovideashort 1-3 sentence\u201csummary\u201dnarration showswordcloudsdepictingobjects that prominentlyfea- fortheentireclip that correspondsto the overallactivity and tureinacrossvariousscenarios. Thewordcloudshighlight settingof the videoclip(e.g.,\u201cthepersondoeslaundryin characteristicobjectsperscenario(e.g.,bowl,spoon,plate thewashingmachine\u201d). Thesesummaries are marked with in \u201cCooking\u201d videos; card, dice, pawn in \u201cPlaying board thetag\u201c#summary\u201dinthereleasednarrations. games\u201dvideos)whilealsohintingatcommonobjectsacross Following this first screening, which is critical for the allscenarios(e.g.,hands,paper,phones). Thediversityin overallunderst and ingof the clip, thedensenarrations are narrationscollectedhighlights the diversityofvideocontent collectedasfollows.Annotatorsre-watch the clip,pause and capturedin the dataset. markthetimepointwhensomethinghappensin the video, thenenterashortnaturallanguagedescriptionof the ongoing D.3 Action and objecttaxonomy actionorinteraction,beforeresumingwatching the video. Narrators are provided the followingprompt:\u201cPretendas Intotal the rawnarrationsdescribe the Ego 4 Dvideousing youwatch this video that you are alsotalkingtoafriendon 1,772 uniqueverbs and 4,336 uniquenouns. Thedistribution thephone,andyouneedtodescribetoy our friendeverything of the most frequently occurring verbs and nouns can be thatishappeningin the video. Yourfriend can notsee the seenin Figure 17. video.\u201dThispromptisintendedtoelicitdetaileddescriptions Following ideas from [44], we leverage the narrations thatprovideaplay-by-playof the action. See Figure 13 for datatoconstructataxonomyover the actions and objects anillustrationof the narrationtoolinterface. Eachnarration thatappearin the video,asfollows. we useapart-of-speech thuscorrespondstoasingle,atomicactionorobjectinter- (POS)tagger and dependencyparsertoidentifyverbs and action that the camerawe are rperforms(e.g.,\u201c#Copens the nouns from each narrated action. We use an ensemble of washing-machine\u201dor\u201c#Cpicksup the detergent\u201d,where the parsermodels from the Spacy[98]toolkittodo this. Given tag#Cdenotes the camerawearer). Importantly,ournarra- a natural language narration, we first identify verbs using tionsalsocaptureinteractionsbetween the camera-wearer their POStag. Thenusing the dependencytree,weidentify and others in the scene, denoted by other letter tags, e.g. all direct objects of the verb. To ensure verbs and nouns #X(e.g. \u201c#Cchecksmobilewhile#Xdrives the car\u201d,\u201c#C areaccuratelyparsed, weadoptseveralheuristics: Parsed passesacardto#Y\u201d).See Figure 14 fornarrationexamples. verbs are splitintomultiplesenses(e.g.,\u201cturn\u201dissplitinto 11 Wesimplykeepbothindependentnarrations; they are notmerged \u201cturn-on\u201d,\u201cturn-off\u201dand\u201cturn-over\u201d);compoundnouns are because the ydonotserveasgroundtruth for anybenchmark. decomposed into a root noun coupled with a modifier to 20 Figure 13. Narrationtoolinterface. Narratorsmarkatimepointwheresomethinghappensin the video(bottombar),andenteratext descriptionof the activity(leftsidebar). Objectinteraction Contextobjects Multi-personactions Manipulationactions #ccflips the paper #cctapsahandon the floor #oamanxmoves the legs. #cccutsaleaf from the plant with hislefth and. #ccliftsthet-shirt #ccholds the wheel with hislefth and. #oamanysitsonachair #ccpullshish and off the chesspiece #ccdrops the plate #ccputsthebrushin the colours. #oawomanxsteps for ward. #ccholdstheknittingneedle with theo the rhand #ccholds the pieceofcloth #ccplacesplastic model skiton the table #oapersonxhits the cricketball #ccopens the screwdrivercontainer with hishands #ccfixeson the modelcraft #ccarrangesthedoughson the tray #oamanythrows the balltowardsmanx #cctouchesthepieceofwood with the hand Camerawe are rmotion Summarynarrations #ccraiseshands cwasinaroom,fixedawood model kit.#summary #ccstands ctightenedthemotorontheheadofthehoeof the lawnmower.ccutgrassesonthefield with the lawnmower.#summary #ccst and supfrom the stairs cwasinakitchen,hecutsausagesintopieces with aknife,mixedthesausages and cooked the mwithapan.#summary #ccwalksaroundakitchen cwasin the house and shestudied#summary #ccsitsup cstudiedinaroom.cwentthroughamobilephone and amobiletabletwhilereadingin the room.#summary Table 2.Textexamplesofnarrations.Thecollectednarrationsdescribediverseaspectsofhumanactivity.Summarynarrationscapture highleveldescriptionsofactivitiesina 5 minuteclip.See Figure 14 forvisualexamples. ensure the nountaxonomyisunambiguous(e.g.,modifier D.4 Narrations for annotationprioritization \u201cegg\u201dandrootnoun\u201cshell\u201din\u201ceggshell\u201d);collectivenouns aremappedto the irmainentity(e.g,. \u201cpieceofcheese\u201d \u2192 \u201ccheese\u201d). Finally,wemanuallycluster the verbs and nouns toavoidredundancyin the taxonomy(e.g.,\u201ccut\u201d,\u201cchop\u201d, Allvideosin Ego 4 Darenarrated,andsubsetsof the mare \u201cslice\u201dareallmappedto the verbcluster\u201ccut\u201d). manuallylabeled for eachbenchmark. Ratherthanr and omly labelinstances for agivenbenchmark,weaimtotargetthose that are mostrelevantto the task. Forexample,videoslikely tocontainmulti-personconversation are mostinteresting for the AVDiarizationbenchmark,whereasvideos with ample Theresultingtaxonomyconsistsofasetof 115 verbs( ) hand-objectinteraction are mostinteresting for Hands and V andasetof 478 nouns( ). Figure 39 shows the distribution Objects. Tothatend,we use the narrations and summaries N ofverbs and nounsinasetofvideo data annotated with the asatooltoautomaticallyprioritizecertainvideostolabel taxonomy. See Section J.2 fordetailsonhow the taxonomy perbenchmark. Thebenchmarkappendicesbelowprovide isusedinthecontextof the benchmarktasks. details. 21 Figure 14.Examplenarrationsatkeyframesofvideo.#Crefersto the camera-wearer.Thelastrowshowsnarrations that includeother people that",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 154,
      "paper_id": "ego4d",
      "text": "Hands and V andasetof 478 nouns( ). Figure 39 shows the distribution Objects. Tothatend,we use the narrations and summaries N ofverbs and nounsinasetofvideo data annotated with the asatooltoautomaticallyprioritizecertainvideostolabel taxonomy. See Section J.2 fordetailsonhow the taxonomy perbenchmark. Thebenchmarkappendicesbelowprovide isusedinthecontextof the benchmarktasks. details. 21 Figure 14.Examplenarrationsatkeyframesofvideo.#Crefersto the camera-wearer.Thelastrowshowsnarrations that includeother people that participateinactivities with the camera-wearer(denotedbyo the rlettertags,e.g.,#O,#X). D.5 Contributionsstatement Tushar Nagaraj and eveloped the taxonomy,helpeddevelop narrationinstructions,andper for med the narrationanalysis presentedin the paper.Kristen Graum and evelopednarration instructions,helpedcoordinatepilots and annotationwork, andcontributedtotaxonomy for mation. Michael Wrayco- developed the taxonomy. 22 1400 1200 1000 800 600 400 200 0 0 20 40 60 80 # narrations / minute tnuoc Narrations per minute 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 0 10 20 30 40 50 # words tnuoc 1 e 6 Narration length distribution 2000 1750 1500 1250 1000 750 500 250 0 0 10 20 30 40 50 60 70 # words tnuoc Summary narration length distribution Figure 15.Collectednarrationstatistics.Left:Distributionoffrequencyofnarrationscollected.Middle and right:Thedistributionof lengthof the collectednarrations and summaries. Summaries are naturallylonger,anddescribeactivitiesatahigherlevelcomp are dto individualactionnarrations.Seetext for discussion. Figure 16.Distributionofobjectsinnarrationsofvideos from eightcommonscenarios.Thevarietyofobjectscoveredacrossscenarios showcasesthediversityofactivitiesin the videocollected. kcip tup pord ecalp dloh evom evomer ekat tsujda hcuot nepo tuc llup tfil ruop worht epiw hsup ssap xif yrrac nrut naelc esolc hsaw yalp sserp tih egnarra dlof rits bur poocs tniap pid ekahs esnir tresni esu no_nrut nruter esiar daerps tcelloc etarepo revoc klaw hcterts kcehc barg ylppa llor evig kcap gnah nethgit pilf erusaem ekam xim refsnart pots hcatta daer pat gard gnirb ngila etator rewol tfihs dda nethgiarts yarps eit hctarcs tink tniop tae ezeeuqs eunitnoc nehtooms hctaw wes burcs etarapes tif rehtag raew tcennoc tcepsni eparcs leep dnah nesool egnahcxe tser dlofnu llird pohc tes kram nioj kool peews nam erats raet ward hctiws egnahc evird kcits kaerb eraperp ffo_nrut elknirps esaeler mirt wercs yal yrt enimaxe parw wohs knird tel tsud peek elffuhs edir llorcs kooc tnuoc wolb evael hcated od wercsnu noitisop ecils yrd hsurb no_hctiws gniws wardhtiw dneb trats hctef bmilc 105 104 103 secnatsni # dnah drac repap doow htolc elttob reniatnoc lwob hguod hsurb enohp noops etalp dil retaw eceip efink draob revoc puc koob gab daerht top doof kcits nap yart wercs enihcam xob nosrep cirbaf eriw tniap llab ehtolc tlob noino tekcub lewot epat tun elbac nolyn taem nac knalp emag tnalp epip ssalg elbategev nep latem rood rossics ruolf trid lian gel lios nit kcitspohc tfarc reppep ledom egnops elzzup epor regnif hcnerw aremac otatop nug evael ssarg loot eulg trihs nray eldeen relur rennaps lairetam kcirb eht reward llaw gge draobdrac rewom elbat reilp ebut nip remmah lio notrac elit llird yalc eohs daerb revirdwercs nottub tehcas kcap ecid licnep kcolb hsid evolg redloh revird teksab hcnarb rettuc meti rewolf tekcap enots paos erusaem alutaps guj eveis llor otamot pac rebmit dnas lewort torrac dor leehw raj pat rellor rab krof tam rac edalb roolf egap tnemec tiurf egabbac rebmucuc 104 Figure 17.Narrationverb/noundistribution.Distributionofautomaticallyextractedverbs(top)andnouns(bottom)fromnarrations.Top 150 mostfrequentlyoccurringofeachisshown for clarity. 23 Numhours Numclips Avgcliplength EMVQ-2 D 432.9 5,831 6.1 min EMVQ-3 D 13 159 4.9 min EMMoments 328.7 2,522 7.9 min",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 155,
      "paper_id": "ego4d",
      "text": "otamot pac rebmit dnas lewort torrac dor leehw raj pat rellor rab krof tam rac edalb roolf egap tnemec tiurf egabbac rebmucuc 104 Figure 17.Narrationverb/noundistribution.Distributionofautomaticallyextractedverbs(top)andnouns(bottom)fromnarrations.Top 150 mostfrequentlyoccurringofeachisshown for clarity. 23 Numhours Numclips Avgcliplength EMVQ-2 D 432.9 5,831 6.1 min EMVQ-3 D 13 159 4.9 min EMMoments 328.7 2,522 7.9 min EMNLQ 227.1 1,659 8.2 min Hands+Obj. 196.2 88,585 8.0 sec Forecasting 110.5 1,498 4.4 min AVD 47.7 572 5 min Social 47.7 572 5 min Table 3.Amountofannotated data for eachbenchmark.EMrefers to Episodic Memory and AVDrefersto Audio-Visual Diarization. All 3,670 hoursofvideo have narrations and features. E.Benchmark Data Splits Foreachbenchmark task,certainportionsof the Ego 4 D videorepository are labeled. Table 3 shows the breakdown of theamount ofdata annotated foreach. Note that there are 764 totalh our sofvideorelevantto the AVDand Social tasks(i.e.,haveaudio,conversation,andunblurredfaces), including the annotatedsetof 47.7 hoursabove. Forother benchmarks,therelevancehasasofterdependencyon the specificvideocontent(e.g.,amemoryquery can applytoany ofthe 3,670 hours). Thefollowingappendices will explain howwesampled data tobeannotated for eachbenchmark. For the public Ego 4 Dbenchmarkchallenge,weensure that the splits are consistent with inafamilyofrelatedtasks. Forinstance,allthe Forecasting and Hands+Objectstasks share the samesplits and ensuretrainingvideosinonedonot occurasvalidationvideosinanother. Similarly,the Episodic Memory task ssh are the samesplits. However,itisharder toensure this acrossverydifferenttasks, since the videos selected for annotations are different. Forexample,the So- cialbenchmarkconsidersmulti-personinteractionswhich maynot have manyh and-objectinteractions;hence the set ofvideoslabeled for Social and Hands+Objects have little overlap and the train/val/testsplits are naturallydifferent. Sinceweplantousethetestset for the publicchallenge, weare with holdingall the testannotations and makingthem accessible only through a submission server. We are also withholdingthenarrations that overlap with anyof the test sets. 24 F.Episodic Memory Benchmark andnotfactualqueries,i.e.,queries that requireanexternal knowledge base toanswer. Thissectiondetails the Episodic Memorybenchmark task NLQisachallengingmultimodal task requiringvisual definitions,annotations,baselinemodels,andresults. and linguistic underst and ing and reasoning. Consider the query \u201cWhat did I pick up before leaving the party?\u201d In F.1 Formal task definitions ordertofulfill this request,thesystemneedsto: (a)break down and underst and the languagequeryasasearchforan As presented in the main paper, there are three kinds of object(what)withwhich the userinteracted(pickup)before Episodic Memory queries\u2014visual, natural language, and anevent(leaving the party),(b)gothrough the egocentric moments\u2014eachofwhichrequireslocalizing the responsein video and identify the desiredeventof\u201cleaving the party\u201d, thevideo. Their for maldefinitions are asfollows. (c)visuallysearch for theobject with which the userinter- Visualqueries(VQ) This task aimstoqueryanegocentric acted prior to this event. This example demonstrates the video base donastaticimagecropofanobject. Specifically, complexity of NLQfrom bothvisual(recognizingevents, it asks the question \u2018Where was object X last seen in the objects,places,etc.)andlinguistic(breakingdownreason- video?\u2019,where Xisasingle\u2018canonical\u2019imagecropinwhich ing,underst and ingrelations,etc.)perspective. Inaddition, theobjectisclearlyvisible and human-identifiable. Apo- thediversesetofquerieswithin NLQ,whilefacilitatinga tential use case for visual queries is where a user teaches flexiblesearch and retrievalthroughanintuitiveinterfaceof thesystema new objectbyshowingaphoto(\u201cthesearemy language,alsoincreasesthecomplexityof the task. keys\u201d) and then later queries for it among past video. By Concretely, NLQ is formulated as follows: Given an enablingvisualqueries,asopposedtocategoricalqueries, egocentricvideo andanaturallanguagequery ,thegoal V Q thisisa for mofopen-worldobjectlocalization. isagaintoidentifya\u2018responsetrack\u2019r,such that the answer Weformulate the problemasfollows.Givenanegocentric to can be deduced from r. The response track should Q video ,aqueryobjectospecifiedviaastaticvisualcrop beasetoftemporallycontiguousframeswithin . Given V V v, and a query frame q, the goal is to identify when the the episodic nature of our task, r should be sufficient to objectowaslastseeninthevideobefore the queryframeq. answer ,without the additionalneed for oranyexternal Q V Theresponseisspecifiedasa\u2018responsetrack\u2019rwhichisa knowledgebases. temporallycontiguoussetofboundingboxessurrounding Moments queries (MQ) This task aims to query an ego- theobjectoineachframe:",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 156,
      "paper_id": "ego4d",
      "text": "V v, and a query frame q, the goal is to identify when the the episodic nature of our task, r should be sufficient to objectowaslastseeninthevideobefore the queryframeq. answer ,without the additionalneed for oranyexternal Q V Theresponseisspecifiedasa\u2018responsetrack\u2019rwhichisa knowledgebases. temporallycontiguoussetofboundingboxessurrounding Moments queries (MQ) This task aims to query an ego- theobjectoineachframe: centricvideo base donacategoryofactions. Specifically,it poses the followingrequest\u2018Retrieveall the moments that I r = r ,r , ,r ,r , (1) s s+1 e\u22121 e { \u00b7\u00b7\u00b7 } do Xin the video.\u2019,where\u2018X\u2019comes from apre-definedtax- onomyofactioncategories,suchas\u2018interact with someone\u2019 wheresistheframewhere the objecto(atleastpartially) or\u2018usephone\u2019. Comp are dto the naturallanguagequeries, enters the camera-wearer\u2019sfieldofview,eis the framewhere themomentqueriesfocusondaily-lifeactionsoractivities. theobjectexits the camera-wearer\u2019sfieldofview,andr isa i Onemomentquery can correspondtomultipleresponsein- boundingbox(x,y,w,h)inframei. Iftheobjectappears stances(temporalwindows)inthevideo. This task provides multipletimesin the video,theresponseonlyrefersto the theuserafast and convenientwaytoretrievemultipleaction \u2018mostrecentoccurrence\u2019oftheobjectin the past, i.e., the momentsatatime,where the userdoesnotneedtocomeup responsetrackwhichminimizesq r withq >r . e e \u2212 withasentencetodescribewha the/shewants,butinstead Whena 3 Dscanof the environmentassociated with the candirectlychooseamong the pre-definedcategories. videoisavailable,theresponseadditionallyincludesa 3 D Themomentqueries task isrelatedto the taskoftemporal displacementvector\u2206d = (\u2206x,\u2206y,\u2206z)between the 3 D actiondetection[141,229,237],whichaimstoidentify and locationwhere the querywasmade(i.e.,atqueryframeq), localizeallinstancesofallactioncategories that takeplace andthe 3 Dlocationintheenvironmentwhere the objectwas inavideo. Bothtasks have alistofactioncategoriespre- lastseen(i.e.,attheendof the responsetrackr ). e defined, andbothaimtopredictmultipleactioninstances Naturallanguagequeries(NLQ) Themotivationbehind with their temporal boundaries. The difference is that 1) the NLQ task istoenablesearchingthroughanegocentric our moment queries task is a retrieval task where action videousinganaturallanguagequery. Thesystemresponds categories are providedasqueries,meaningitdoesnotneed toaquerybyprovidingatemporalwindowlocalizedin the to produce instances of categories that are not among the video,fromwhichtheanswerto the query can bededuced. queries; and 2)ourmomentstaxonomyisspecifictofirst- Thesequeries can berelatedtoobjects,places,people,and personactivity. Weaim for moments that are activitiesat activities that appe are dintheepisodicmemoryof the user. amediumlevelofgranularity\u2014coarserthan the actionsin Note that we only consider episodic queries, i.e., queries Forecasting,and finerthan the\u201cscenario\u201dlabelsshownin that can beanswered/deduced from the egocentricvideos, Figure 3 ofthemainpaper. 25 Navigationverbs for entropy-basedvideoselection Toselectvideos base don the seconsiderations, we use atwo-stepprocess. First,wefilteroutvideos base don the appear ascend bend bring carry catch climb close come descend dig dispose associated\u2018scenario\u2018labels(see Figure 3)thatprovidehigh- drag dribble drop enter fall fetch levelin for mationabout the content and activitiesinvideos find fly gather get give grab (e.g.,cooking,cleaning,golfing,etc.). Wemanuallypreview hang jog jump kick lean leave randomly sampled videos from each scenario to identify lift lower move navigate open propel interesting scenarios such as cooking, indoor navigation, raise return ride rise run shut farmer,cleaning,andgroceryshopping. Wethensortvideos steer step turn vaccum walk withineachscenario base donascoringfunctionusing the Table 4.Weprioritizevideostoannotate for visualqueriesbased narrations for the video. Specifically,weextract the listof ontheentropyof the senavigation-relatedverbsin the narrations. verbsin the narrations(along with the irfrequencies). We then measure the entropy of the distribution of manually curated navigation verbs (See Tab. 4). The video is more The MQtaskisalsorelatedtotemporallanguageground- likelytoallowchallengingvisualqueriesifitsnavigation inginvideos[236],whichaimstoretrieveasegment from entropy is higher. For videos with near-zero entropy, we a video, as queried by a natural language sentence. Both observe that the camera-wearerisusuallystayingstaticin tasks have aquery and aimtopredictcorrespondingtempo- asinglelocation with outanymovement. Finally,alimited ral segments. The difference is that MQ uses pre-defined numberof 3 Dscans were available for the 3 Dlocalization querycategoriesra the rthannaturallanguagesentences,and task. Videos associated with these scans were prioritized, onequery can correspondtomultipleinstancesra the rthana regardlessof the irnavigationentropy,insupportof the 3 D uniqueone. responseversionof the VQtask. Weformulate the problemasfollows. Givenanegocen- Naturallanguagequeries For NLQweapplysimilarsam- tric video , and a query action",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 157,
      "paper_id": "ego4d",
      "text": "pre-defined numberof 3 Dscans were available for the 3 Dlocalization querycategoriesra the rthannaturallanguagesentences,and task. Videos associated with these scans were prioritized, onequery can correspondtomultipleinstancesra the rthana regardlessof the irnavigationentropy,insupportof the 3 D uniqueone. responseversionof the VQtask. Weformulate the problemasfollows. Givenanegocen- Naturallanguagequeries For NLQweapplysimilarsam- tric video , and a query action category c, the goal is to V pling criteria as above for VQ, but augment it to avoid retrievealltheinstancesof this actioncategoryin the video, repetitiveactions(e.g.,sewingwhilesittingon the couch). assuming that the query is made at the end of the video. First,wemanuallyselectamenablescenarios(see Figure 3). Theresponseisasetofactioninstancesof the categoryc N Amongthose,weprioritizeclips with highentropycomputed \u03a6 = \u03c6 =(t ,t ,s ) , where n is the number c { n n,s n,e n }n=1 overnavigationaltermsasabove. Finally,weprioritizenon- of instances for this category, t and t are start time n,s n,e repetitiveactionsbycomputingtheratioof the numberof andendtimeof the nth instancerespectively,ands isits n unique verbs in a clip\u2019s narration vs. the total number of predictionconfidence. verbsin that samenarration\u2014higherisbetter. Momentsqueries Toselectclips for momentsqueries,we F.2 Selectingclips for annotation compute the overlapofverbs/nouns with the momentstax- Forallbenchmarkswesamplevideoclipstoannotatebased onomy. Wecalculateasimil are ntropy-basedscore and sort on criteria for geographic diversity and scenario diversity. videosaccordingto this score. Inaddition,werestrictvideos For Episodic Memoryweimposeadditionalsamplingcrite- toafixedsetofcategoriespresentin our taxonomytoavoid riameanttohighlight data mostinteresting for the task,as labelingvideos that donotcontainrelevantactivities. follows. F.3 Annotation Visual queries Video clips to annotate for visual queries (VQ)areselected base don the frequencyofobjectoccur- Nextwedescribe the annotationprocedures and outputs for rences and amount of navigation in the video. To have Episodic Memory. interestingvisualqueriesinavideo,theremust beseveral Visualqueries Forannotatingvisualqueries,wefirstsam- \u2018interesting\u2019 objects that can be queried about. An object plecontiguousclipsofvaryinglengths(5 mins,8 mins,and is\u2018interesting\u2019inthecontextofvisualqueriesif the reisa 16 mins)from the setofinterestingvideos. Theannotators sufficientlyhighseparationinspace and timebetweenany are instructed to create and annotate 3 visual queries for twooccurrencesof the object. Thistypicallyhappenswhen each clip. A visual query consists of the query frame q, thecamera-wearervisitsthelocationnear the objectbriefly, thevisualcropv ofthequeryobjecto, theresponsetrack and then navigates elsewhere before revisiting the object r = r ,r , ,r ,r , and a textual name for the s s+1 e\u22121 e again. Forexample,considerapersonwhofinishescleaning { \u00b7\u00b7\u00b7 } object(eg. cup,hammer,broomstick,etc). Theannotators a living room, visits the kitchen for some period of time per for med the followingstepstoannotateagivenclip: beforerevisiting the livingroomagain. Mostobjectsin the livingroom are interestingtoqueryaboutwhen the person 1. Identifythreeinterestingqueryobjectsin the clip. An isin the kitchen. objectisinterestingifitoccursinatleasttwodifferent 26 partsof the video. In order to validate an annotation we collect two 3 D bounding boxes per query from two different annotators. 2. For a given object, enter a textual name. While our Leveragingthetwoboxeswecompute the followingvalida- current task queries with the imagecrop,not the name, tionmetrics: thisannotation will allowfuturevariants that doquery for the objectbyname. c c 1 2 2 d = (cid:107) \u2212 (cid:107) (3) norm m 3. Selectoneoftheobjectoccurrencesin the video and diag V mark a visual crop v = (x v ,y v ,w v ,h v ). The visual V = global , (4) norm cropmust beagoodrepresentativeviewof the object, V union anditmust have goodlighting,large-enoughsize,and wherec andc arethecentroidsof the twoboxes,m is mustnotbeblurred. 1 2 diag theaveragediagonallengthof the twoboxes,V isthe global 4. Mark a different occurrence of the object as the re- volumeof the 3 Dconvexhullof the twoboxes,and V union sponse track r = r",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 158,
      "paper_id": "ego4d",
      "text": "(4) norm cropmust beagoodrepresentativeviewof the object, V union anditmust have goodlighting,large-enoughsize,and wherec andc arethecentroidsof the twoboxes,m is mustnotbeblurred. 1 2 diag theaveragediagonallengthof the twoboxes,V isthe global 4. Mark a different occurrence of the object as the re- volumeof the 3 Dconvexhullof the twoboxes,and V union sponse track r = r , ,r . The response track is the volume of the union of the two boxes. These met- s e { \u00b7\u00b7\u00b7 } starts from the frame when the object is first visible ricsmeasuretheagreementlevelbetwen the twoannotators. andendswhentheobjectleaves the field-of-view. The When the twoannotations are perfectlyaligned,themetrics responsetrackmustalsobecontiguousintime and the areequaltod norm =0 and V norm =1.0. Theassumption boundingboxesmustaccuratelymark the position and isthatifthetwoannotatorsagreeon the position,scale,and sizeof the object. orientationoftheboundingbox the nitislikelytobecorrect. Ifthetwoannotations are far from eacho the rwe will discard 5. Thequeryframeq issampledsometimeafterthere- the query. There are a couple of reasons that can explain sponsetrackr. Theobjectomustnotappearanywhere suchcase: (1)oneannotatormislabeled the query,(2)the between the response track r and the query frame q, queryishardtoannotate. Somequeriesrequireasignificant sothat the groundtru this well-defined and unique for amount of hallucination to retrieve the object location in \u201cwhendid Ilastsee...?\u201d. thes can whichclearlyleadstosubjectiveannotations. We empiricallydefinedtwothresholdsof 1.5 over d and For each annotation, we apply automated and manual norm 15 over V to filter out poor annotations. Any query qualitycheckstoensurecorrectness. Incase the qualityfalls norm thathaseitheroneofthetwometricsabove the thresholdof belowacertainthreshold,theclipisreannotated. acceptanceisrejected. For visual queries associated with 3 D scans, we also collect 3 Dannotationsin the formof 3 Dboundingboxes Naturallanguagequeries Tocollect NLQannotations,we capturingwhere the objectwaslastseen. Wethenusethose sample contiguous clips of length 8 minutes and 20 min- boundingboxestoestablish the groundtruthdisplacement utes. Theannotators are instructedtowatch the seclips and vector from thequeryframeto the object,whichis the target generatenaturallanguagequeries,focusedonretrievingin- of the task. Each annotation a is collected in the scan q formationaboutobjects,places,andpeoplein the egocentric coordinatesystems: videoclips. Toreducethecognitiveoverloadon the anno- T =[R t ], (2) tators,andfocus the iref for tsonmemory-relevantqueries, s s s | wealsoprovidealistof 13 querytemplates(see Table 5), whereq 1,..., , thetotalnumberofqueries,and correspondingtoqueriesausermigh task toaugmenttheir where T s \u2208 {R 4 isth Q e } tra Q nsformationmatrixof the bounding memory. Note that these templates are provided only to \u2208 box.R s andt s are the correspondingrotation and translation guide the irchoiceofquery,anddoesnotlimit the linguistic forannotationa q . variabilitysince the annotators are instructedtoparaphrase Theannotationprocedureisdefinedasfollows: Aquery thetemplate with outcopying the masis. consistsofavideoclip,avisualcrop,and are sponsetrack. To elaborate, the annotators per for med the following Foreachquery,thegoalistoretrieveinthes can thelocation steps: of the object defined in the video. Once the location is found,wedrawa 3 Dboundingboxat this position with the 1. Watch the entirevideoclip inordertounderst and the V appropriate scale and orientation. Itisimportanttonote that high-levelcontext(optionallyin 2 fast-forward), \u00d7 3 Dscans and videos have beenrecordedatdifferenttimes. Therefore,itislikely that anobjectatacertainlocationin 2. Pickaquerytemplate from the availablelist and para- thevideo will notbepresentat that samelocationin the 3 D phrase/reword the query to obtain , e.g., template Q scan. Insuchcases, weask the annotatortohallucinatea \u2018Wherewasobject Xbefore/afterevent Y?\u2019can bepara- 3 Dboundingboxin the 3 Dscanatthepositionof the target phrasedas\u2018Wherewas the bluebucketpriortomydog objectdefinedin the video. exiting the livingroom?\u2019 27 Category Template resultofmomentarygazeshift are stillconsideredtobe contiguous. Whereisobject Xbefore/afterevent Y? Whereisobject X? \u2022 Foragivenquery,ifthere are multiplenon-contiguous Whatdid Iputin X? temporalwindows(separatedbymorethan 3 seconds) Howmany X\u2019s?(quantityquestion) asindependentlyvalidanswers,weinstruct the anno- Objects What Xdid IY? tatorstoeitherdiscard the",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 159,
      "paper_id": "ego4d",
      "text": "bepara- 3 Dboundingboxin the 3 Dscanatthepositionof the target phrasedas\u2018Wherewas the bluebucketpriortomydog objectdefinedin the video. exiting the livingroom?\u2019 27 Category Template resultofmomentarygazeshift are stillconsideredtobe contiguous. Whereisobject Xbefore/afterevent Y? Whereisobject X? \u2022 Foragivenquery,ifthere are multiplenon-contiguous Whatdid Iputin X? temporalwindows(separatedbymorethan 3 seconds) Howmany X\u2019s?(quantityquestion) asindependentlyvalidanswers,weinstruct the anno- Objects What Xdid IY? tatorstoeitherdiscard the query and createadifferent Inwhatlocationdid Iseeobject X? one,oraddmoredetailsto the wordingtomakeitmore What Xis Y? specific. Similarly,queries that requiremultipletem- Stateofanobject poralwindows(separatedbymorethan 3 seconds)to Whereismyobject X? deduce the answer are alsodisallowed. Forexample, Place Wheredid Iput X? \u2018Howmanyshirtsdid Ipackinmysuitcase?\u201disinvalid Whodid Iinteract with when Ididactivity X? ifpackinghappensacrossmultipletemporalwindows, People Whodid Italktoinlocation X? separatedbymorethan 3 seconds(e.g.,theuserpauses Whendid Iinteractwithperson with role X? tomakecoffee,and the nreturnstopacking). Table 5. The NLQtemplatescaptureadiversesetofqueries that \u2022 We enc our age diversity by instructing that the query humans can asktoaugment the irmemory and recollectobjects, responsesnotbeconcentratedatonepartof the video places,andpeoplein the ireverydayexperience. clip,oraroundfewobjects/places/people. Inaddition, wealsodisallow the queryresponsewindowtobemore than 50%ofthetotalcliplength. 3. Findthetemporalwindowwhere the responseto the natural language query can be deduced visually, and \u2022 Finally,queries that requirereasoning and knowledge annotateitasr. on top of visual evidence are invalid. For instance, \u2018Whatcountry\u2018sflagwashangingon the wall?\u201disin- During our data collection,wealsorequested the annota- validwhile\u2018Wherewas the flag that washangingon torstomark the slotvalues and correspondingverbs,forthe thewall?\u201disvalid.Similarly,queries that guessthemo- selectedlanguagequerytemplates. Whilewedonotuse this tivationorintentionsoftheuserorpeoplein the video information for ourtask, itmaybeusefulforo the rfuture clip are also not allowed. As an example, \u2018Why did research. thepersonatthedoorleaveapackageon the porch?\u2019 The desiderata for the collected queries are as follows. isdisallowedwhile\u2018Whatdid the personleaveon the Theyshould: (a)reflect the underlyingmotivationofaug- porch?\u2019 isaccepted. mentinghumanmemory,(b)berich and diverseintermsof language and the objects,places,people,andevents,and,(c) After the annotation process, we apply both automatic bechallengingenough for anintelligentsystembutnottoo and manual quality checks, including the diversity of lan- complicatedorconvolutedtoreduce the naturalnessof the guagequeries and temporalwindowlocations,toscore the queries. Forinstance,thoughaquerylike\u2018Whatwasplaying annotations. Iftheoverallqualityscoreisbelowathreshold, onthetelevisionwhen Iwasfoldingmyseventh T-shirtafter theclipisre-annotated. my dog exited the room?\u2019 is challenging from a learning Momentsqueries Toannotatemomentsqueries,wesam- perspective,itisnotnatural from anapplicationst and point. plecontiguousclipsof 8 minutes from the setofinteresting Inordertoensure the abovequalities for NLQ,weenforce moments videos. The annotators are instructed to mark thefollowingconstraints: instancesofactivities with atemporalwindow and the activ- \u2022 Allparaphrasedlanguagequeriesmust beinpasttense, ity\u2019sname from afixedtaxonomyofactivities.Wehaveeach and must be posed as questions asked at the end of instancelabeledbythreeindependentannotators. Byassum- theentirevideoclip. Thisresembles the real-lifesce- ingeachannotatorisreliable,wetake the unionofmoments narioofqueryingaboutepisodicmemory(past)ofthe acrossannotatorstoensurecompletenessofannotations. user,andresolvesambiguitywhen the rearemultiple Thetaxonomywascreatedsemi-automatically from the occurrencesofanobjectto the the lastrelevantone. narrations. Specifically,we use the summarynarrationscol- lected for five-minuteclipsegments,astheycapturehigher- \u2022 Toaccount for momentaryshiftsofview for the egocen- levelevents and activities that are suitable for the moments tricvideo,weallowsmallinterruptions(<3 seconds) retrieval task. Thisisincontrastto the verb-nountaxonomy between the trulyrelevantframes for agivenquery. In thatiss our ced from individualnarrations for eachatomic otherwords,frameswhere the object/person/placeof action, which are used in the Forecasting and Hands and interestgoesoutofview for lessthan 3 secondsasa Objectsbenchmarks(see Appendices Gand J). 28 Thetaxonomywascreatedasfollows. First,eachsum- Split Train Val Test mary narration was encoded into a feature vector using a #videohours 262(19) 87(5) 84(9) pre-trained BERT[51]language model,and the nconcate- #clips 3.6 k(164) 1.2 k(44) 1.1 k(69) nated with thewordembeddings for the mainverb and noun #queries 13.6 k(604) 4.5 k(164) 4.4 k(264) extracted from the summary. Thesesummaries were then clustered into groups, and then labels were manually as- Table 6. Visualqueries data setstatistics. Thenumbersin the signed to groups based on the coherent activities they de- paranthesescorrespondto the subsetof",
      "start_pos": 10164,
      "end_pos": 10676
    },
    {
      "chunk_id": 160,
      "paper_id": "ego4d",
      "text": "with thewordembeddings for the mainverb and noun #queries 13.6 k(604) 4.5 k(164) 4.4 k(264) extracted from the summary. Thesesummaries were then clustered into groups, and then labels were manually as- Table 6. Visualqueries data setstatistics. Thenumbersin the signed to groups based on the coherent activities they de- paranthesescorrespondto the subsetof data used for 3 Dlocaliza- scribed. tion,wherewefocusonvideos for whichwe have Matterport 3 D Note that thisprocesswasdoneindependently for aset scans. ofscenarios that weselected base donhowfrequentlythey occurin the dataset,thediversityofactivities the yrepresent, Split Train Val Test andhowlikely the ycontainhigh-level,event-likeactivities. For example videos that primary involve a single activity #videohours 136 45 46 like\u201cdriving\u201darenotinterestingcategoriesin this context, #clips 1.0 k 0.3 k 0.3 k whereas\u201chouseholdcleaning\u201dcontainsseveraldifferentac- #queries 11.3 k 3.9 k 4.0 k tivities that aresh are dacrosso the rindoortasks,makingitan appropriatescenario. Intotal,weselectvideos from 5 sce- Table 7.NLQ data setstatisticsacross the train/val/testsplits. nariostocreate our momentstaxonomy: Cooking,Cleaning, Shopping,Handyman,Farmer/Gardener. Eachannotationis throughout the image,withveryfewboundingboxesanno- inthe for matof(starttime,endtime,label). tatedat the top 10%oftheimage(see Figure 22,right). Our analyses indicate that there may be a potential bias in the F.4 Data Analysis firsttwomeasures,while the boundingboxespositions are largelyunbiased. Wenowoverviewthestatisticsof the annotationsperquery Forthe 3 Dlocalization task,weannotateasubsetof 1,043 type. visualqueries with 3 Dannotations. Thesecompriseof 13 Visualqueries The VQannotationsconsistofsamples from videoh our sassociated with 4 scans from the Universityof a diverse set of scenarios and universities (see Figure 20 Catania(UNICT). and 21). In total, 433 hours of videos are annotated with Naturallanguagequeries Asoutlinedin Table 7,the NLQ 22,602 visualqueries. Thesevideos are sampled from 10 annotations are from 227 hours of video, with a total of universities and consistof 54 scenarios. Thestatisticsover 19.2 K queries spanning the selected 13 query templates. thetrain/val/testsplits are providedin Table 6. Weensured Theassociatedvideoclipscome from 10 differentuniversi- that the splitscontainadisjointsetofvideos. Tolook for ties with atotalof 34 scenarios(withatleast 1 hourofvideo possiblebiasesin the data,weplot the distributionoverthree annotated). Similartoother task swithin the episodicmem- measures. ory,weensure that the train/val/testsplits(60%,20%,20%) 1)Querytoresponseseparationis the temporaldistance contain a disjoint set of video clips. We further analyze (in frames) between the query frame and the end of the the data through: (a) Distribution over template queries, responsetrack. Thismeasureshowfarbackintimeanalgo- shownin Figure 24. Thechallenging\u2018Whereisobject Xbe- rithmneedstosearchinordertofind the queryobject. fore/afterevent Y?\u2019isthemostpopulartemplate with around 2)Responsetracksizemeasures the temporallengthof the 3 K queries,with are asonabledistributionovero the rtem- responsetrack. plates. Overall, thequeriesin NLQhave 8.3 2.1 words 3)Responsebboxpositionis the spatialstart and end(x,y) \u00b1 inthem. (b)Distributionof the responsewindowleng this coordinates for eachboundingboxin the responsetrack. We shownin Figure 25. Typically,thewindows are 9.3 21.5 normalize the coordinates by the image width and height \u00b1 secondslong. toaccount for varyingimagesizesin the data. Eachpixel Mostresponsewindowsarequiteshortcomp are dto the within the boundingboxcontributestoanimageheatmap fullvideoclip,making the taskachallenging\u201cneedlein the that shows the frequency of each pixel belonging to a re- haystack\u201dsearchproblem. (c)Distributionofquerywords sponsetrackboundingbox. is shown in Figure 19. The branching off evidences the The analyses are shown in Figure 22. The query to re- richness and diversityof the queriesin NLQ. sponseseparationdistances are fairlyspreadbetween 1 to 200 frames with a mode of 30 frames (see Figure 22, Moments queries For MQ, similar to other tasks in \u223c left). Theresponsetracksizes are welldistributedbetween episodicmemory, wemaintainaratioof 6:2:2 among the 1 to 40 frames with amodeof 8 frames(see Figure 22, train/val/test splits, which contains disjoint sets of video \u223c center).",
      "start_pos": 10626,
      "end_pos": 11138
    },
    {
      "chunk_id": 161,
      "paper_id": "ego4d",
      "text": "200 frames with a mode of 30 frames (see Figure 22, Moments queries For MQ, similar to other tasks in \u223c left). Theresponsetracksizes are welldistributedbetween episodicmemory, wemaintainaratioof 6:2:2 among the 1 to 40 frames with amodeof 8 frames(see Figure 22, train/val/test splits, which contains disjoint sets of video \u223c center). Theboundingboxes are near-uni for mlydistributed clips. Tomakesure the reareenoughsamplesineachcate- 29 enoemos htiw tcaretni / esrevnoc enohp esu ...o smeti gnihtolc hguorht esworb ...ippohs / enizagam / koob a daer ...eti doof )tuo ekat ro( yawa tup rorrim eht ni sehtolc ta kool gnikooc elihw doof xim / rits ...elbategev a ecils / pohc / tuc\" sriats pu klaw / sriats nwod klaw ...ni hsart tup / hsart yawa worht ...o ro ecafrus rehto epiw / naelc hguod tuo-llor / epahs / daenk ...awekab / slisnetu / sehsid hsaw smeti rehto ezinagro / egnarra ...oof ro seirecorg hguorht esworb ...eoh a htiw lios eht llit ro gid egareveb knird ...nah no / tesolc ni sehtolc gnah meti rehto xif sdnah hsaw lian xif ot nug-lian / remmah esu trac gnippohs ni smeti ecalp dnuorg morf sdeew evomer pohs / tekramrepus a retne moorb htiw roolf peews / naelc retupmoc / potpal a esu emag drac ro emag draob yalp ...reniatnoc / elttob / top a llif noisivelet hctaw ...r no smeti rehto hguorht esworb .../ sehsid )tuo ekat ro( yawa tup ... htiw oediv drocer / otohp ekat steehs / sehtolc dlof etalp a otno doof evres sloot rehto htiw ssarg mirt / tuc ... roolf / doow / llaw otni llird ... .g.e( tnempiuqe ytefas no tup\" rellor / hsurb tniap gnisu tniap ...wob a ni stneidergni xim / rits sehcnarb ro segdeh mirt ...tni seirecorg / smeti doof kcap reddal a nwod / pu bmilc ... a ta enil / eueuq eht ni dnats elbategev ro tiurf a leep ... smeti gnihtolc raew / tuo-yrt\" sporc / stnalp / lios retaw loot gnisu meti rehto tuc renrub evots eht thgil / no-nrut ...o erit a ecalper / evomer / xif pohs / tekramrepus a tixe ...nehctik ro elbat a epiw / naelc retnuoc gnillib ta yap yrd ot sehtolc gnah ...a gnisu tneidergni / doof hgiew ...eidergni )tuo ekat ro( yawa tup sexob / sgab otni smeti rehto kcap ...enihcam gnihsaw a daolnu / daol meti doof / tiurf / elbategev hsaw lamina / tep htiw yalp ro tcaretni esicrexe emos od loot gnisu seceip doow tuc / pohc hguod tuc hcnarb eert tuc emag oediv a yalp rewomnwal a htiw ssarg mirt / tuc ecafrus / llaw retsalp kcans a tae ... srewolf / stnalp / sdees tnalp gniriw xif naelc ot renaelc muucav a esu ...rg no sevael yrd ekar / tcelloc ...r no seirossecca hguorht esworb ...gnisu .ge( lios / dnuorg level\" hguod yrf ...s htiw stnalp / sehcnarb pu eit ...s / repapdnas gnisu doow htooms gniyap erofeb yenom tnuoc ecnailppa nehctik epiw / naelc dnah yb lios eht llit ro gid ...arepo / mta morf yenom wardhtiw ... a ro dnuorg eht otni lios kcap rac",
      "start_pos": 11088,
      "end_pos": 11600
    },
    {
      "chunk_id": 162,
      "paper_id": "ego4d",
      "text": "lios / dnuorg level\" hguod yrf ...s htiw stnalp / sehcnarb pu eit ...s / repapdnas gnisu doow htooms gniyap erofeb yenom tnuoc ecnailppa nehctik epiw / naelc dnah yb lios eht llit ro gid ...arepo / mta morf yenom wardhtiw ... a ro dnuorg eht otni lios kcap rac fo enigne / tennob xif steehs ro sehtolc nori ....e( seirossecca raew / tuo-yrt\" ...c ni sehtolc ezinagro / egnarra ...c / stiurf / selbategev tsevrah ... epat gnisu meti nedoow erusaem smeti gnihtolc owt erapmoc ...oitcurtsnoc dnuora tfihs / evom sloot llams egnarra / tfihs / evom egdirf ni smeti ezinagro / egnarra gnibmulp / epip xif ... draobdrac / repap / daerht tuc ...rcnoc / tnemec ylppa ro eraperp ...ffoc a esu / aet ro eeffoc ekam ...swollip egnarra / deb eht ekam\" meti cillatem lio / epiw / naelc koob / repap a ni seton etirw ...tnempiuqe llams riaper / naelc\" ...s htiw .g.e( egakcap a nepo tuc elcihev a evird ...m / nep / licnep htiw meti kram riahc / hcuoc no swollip egnarra meti doof rehto yrf ...i meti doof rehto niard / esnir ekab ot nevo eht otni doof tup meti rehto eltnamsid nevo eht morf doof evomer epav / etteragic / ragic ekoms gnikooc elihw doof etsat 103 102 secnatsni # Figure 18.Distributionofmomentslabels.Thefigureshows the numberofinstancespercategoryacross 5 scenarios and 300 hoursof data. All 110 categories are shown,sortedbyfrequency. Thedistributionislongtailed,with the smallestclassescontainingatleast 50 instances.Note that the seareonly the Moments for Episodic Memory with temporalwindowannotationsin the currentrelease;Ego 4 Dhas manyo the rscenarios and activitiesnotreflectedin this distribution. Split Train Val Test Total Videohours 194.9 68.5 62.9 326.4 #Videoclips 1,486 521 481 2,488 #Instances 13.6 k 4.3 k 4.3 k 22.2 k Table 8.MQdatasetstatisticsacross the train/val/testsplits. clip. Theaveragedurationeachinstanceis 45.2 seconds. (b) Thedistributionofdifferentcategoriesisshownin Fig 18. Wenotice that thisisalong-taileddistribution,somecate- gories(e.g.,\u2018usephone\u2019,\u2018converse/interact with someone\u2019) withover 1000 instances and somecategories with lessthan 100 instances. Eachcategoryhas 205 instancesonaverage. (c)Thedistributionofinstancenumbersinavideoclipis shown in Fig 27. The majority of video clips have 1-20 momentinstances,whereasveryfew can haveasmanyas over 80 instances. Figure 19.Distributionofquerywordsin NLQ. F.5 Evaluationmeasures Next we detail the evaluation metrics for all three query gory,weonlykeepcategories that haveatleast 50 instances types. from the annotations and haveinstancesinalltrain/val/test splits. Visualqueries Wedefine the followinglocalizationmetrics Consequently,the MQdatasethas 110 categories,spans forthe 2 Dlocalization task with top-1 retrieval. atotal 326.4 hoursofvideos,2,488 videoclips and 22.2 kac- Temporal AP (t AP) measures how closely the temporal tioninstances. Wesummarizethestatisticsacross the three extent of the prediction matches with the ground-truth re- splits in Table 8. We further explore the data through the sponsetrack. Itiscalculatedas the average-precisionof the followingaspects. (a)Thedistributionofactiondurationis predictedresponsetrack\u2019stemporalextent,andis base don shownin Fig 26. we cansee that mostmoments have very the Activity Netm APmetric[61]. Weevaluatethet APat 4 short duration. The majority of moments last less than 1 differentt Io Uthresholds 0.25,0.50,0.75,0.95 ,aswellas { } minute,and 22.4%actions have durationlessthan 3 seconds. theiraveragevalue. Note that the reisalsoapeak(2.6%instances)atthelargest Spatio-temporal AP (st AP) measures how closely the durationbin,wheretheactionsalmostcover the wholevideo spatio-temporalextentofthepredictionmatches the ground- 30 Figure 20.Distributionoverscenarios for visualqueries.The data setcontainsalong-tailofscenarios.Theplottitleindicates the number ofscenarios and thetotalvideoh our sincludedin the dataset. rithmsearching for the queryobject. Itiscalculatedas n s Eff=1 (5) \u2212 N where n is the",
      "start_pos": 11550,
      "end_pos": 12062
    },
    {
      "chunk_id": 163,
      "paper_id": "ego4d",
      "text": "theiraveragevalue. Note that the reisalsoapeak(2.6%instances)atthelargest Spatio-temporal AP (st AP) measures how closely the durationbin,wheretheactionsalmostcover the wholevideo spatio-temporalextentofthepredictionmatches the ground- 30 Figure 20.Distributionoverscenarios for visualqueries.The data setcontainsalong-tailofscenarios.Theplottitleindicates the number ofscenarios and thetotalvideoh our sincludedin the dataset. rithmsearching for the queryobject. Itiscalculatedas n s Eff=1 (5) \u2212 N where n is the number of video frames previewed by an algorithmtopredict the responsetrack,and N isthetotal numberofframesinthevideobefore the querywasmade (i.e., the search window). An algorithm that accesses ev- eryframeinthesearchwindowbe for elocalizing the query objectgets 0.0 searchefficiency. This\u201ctimeliness\u201dmetric isdesignedtoenc our ageresearchonmethodsper for ming intelligentcontextual-search. Weevaluateper for manceon the 3 DVQlocalization task using the rootmeansqu are error(RMSE)and the angular Figure 21.Distributionoveruniversities for visualqueries.The errormetrics: datasetcontainsannotationscorrespondingtovideos from 10 uni- versities.Theplottitleindicates the numberofuniversities and the RMSE= t s t\u02c6 s 2 (6) (cid:107) \u2212 (cid:107) totalvideoh our sincludedin the dataset. v T v\u02c6 Q Q angular error=acos( . ) (7) v v\u02c6 Q 2 Q 2 (cid:107) (cid:107) (cid:107) (cid:107) where t and t\u02c6 are the ground-truth and predicted object truthresponsetrack. Itiscalculatedas the average-precision s s positioninthes can coordinatesystem. v andv\u02c6 arethe ofthepredictedspatial-tube,andis base don the video-AP Q Q ground-truth and predicted 3 Ddisplacementvectorin the metric from[88]. Weevaluatethest APat 4 differentst Io U queryframe Qcoordinatesystem. Wealsodefineasuccess thresholds 0.25,0.50,0.75,0.95 ,aswellas the iraverage { } metricleveraging the twoannotationsperquery: value. Success (Succ) measures whether the prediction has any succ= c t\u02c6 <6 ( c c +\u03b4) (8) overlap with the groundtru that all. Itiscalculatedas the (cid:107) m \u2212 s (cid:107) 2 \u00d7 (cid:107) 1 \u2212 2 (cid:107) 2 percentageofsampleswhere the predictedresponsetrack With c 1 and c 2 the centroids of the two bounding box hasatleast 0.05 spatio-temporal Io Uwith the groundtruth. annotations, c the mid-centroid between c 1 and c 2 and Recovery% (rec%) measures how much of the ground- m \u03b4 =exp\u2212mdiag,withm diag theaveragediagonallengthof truthresponsetrackisaccuratelyrecoveredby the prediction. thetwoboxes. Itiscalculatedas the%offramesin the responsetrackwhere the predicted bounding box has at least 0.5 Io U with the Naturallanguagequeries Evaluation for NLQissimilar groundtruth. Thisismotivatedby the trackingrobustness toexistingvideo-languagegroundingproblems. Following metric from the VOTchallenge[121]. priorwork[236],we userecall@k,Io U=m,whereweselect Searchefficiency(s Eff)measurestheefficiencyof the algo- k = 1,5 and m = 0.3,0.5 . This metric computes { } { } 31 Distribution of query to response Distribution of response track lengths Distribution of response bboxpositions separation distances Figure 22.Visualqueriesbiasanalysis.Weanalyze the full VQdataset for potentialbiases.Left:Theplotshows the distributionofquery toresponseseparationdistancesin the VQdataset.Whilethemodeof the distributionis\u223c30 frames,we cansee that separationdistances arefairlyspreadbetween 1 to 200 frames.Center:Theplotshows the distributionofresponsetracksizesin the VQdataset.While the modeof the distributionis\u223c8 frames,we cansee that the responsetracksizes are welldistributedbetween 1 to 40 frames.Right:The heatmapshows the normalizedfrequencyofeachpixelbelongingto are sponsetrackboundingbox.Theboundingboxesnear-uni for mly distributedacrossmostof the image. gnikoo C yrdnual / gninael C snoc ot detaler sboj cinahcem ra C retnepra C reka B cinahcem retooc S w( noitagiva N roodn I dni gnippohs yrecor G ppohs rehto ,sehtol C teerts no gnikla W ylimaf htiw gnikla T eeffoc gnika M gnita E cinahcem eki B ni gnihtemos gnixi F c/pu-kcap/putes pma C step htiw gniyal P namydna H semag draob gniyal P emoh ta tuo gnikro W ep / god eht gnikla W htimskcal B s aetklim ni gnikro W es/gnittink/gnitfar C lcni( laicos roodtu O eki B aor",
      "start_pos": 12012,
      "end_pos": 12524
    },
    {
      "chunk_id": 164,
      "paper_id": "ego4d",
      "text": "M gnita E cinahcem eki B ni gnihtemos gnixi F c/pu-kcap/putes pma C step htiw gniyal P namydna H semag draob gniyal P emoh ta tuo gnikro W ep / god eht gnikla W htimskcal B s aetklim ni gnikro W es/gnittink/gnitfar C lcni( laicos roodtu O eki B aor ,gnitummoc - ra C lacisum a gnicitcar P ksed ta gnikro W siybboh( scinortcel E eugaelloc ot gnikla T ediv / semag gniyal P l/enohp( neercs a n O e - myg eht ot gnio G eneigyh ylia D gniggoj / gnilcy C su B ti gnikam( ba L reka M sdneirf htiw gnikla T krap eht ot gnio G cisum ot gninetsi L gninedra G 80 60 40 20 0 Scenarios sruoh oediv # Total: scenarios=43, hours=358.93 Figure 23. Distributionoverscenarios for the NLQannotations,indicatingalongtailoverscenarios. Note that the scenariolabels are approximate and asinglevideo can containmultiplescenariolabels.For this plot,weequallydividethetimeacrossall the labelledscenarios. thepercentageoftimesatleastoneof the topk predicted temporalactiondetection data sets,suchas Activity Net[61], candidates have anintersection-over-union(Io U)ofatleast themean AP(m AP)overallcategoriesiscomputedgivena m. Note that weleantowardslowerthresholdvalues(m)as t Io Uthreshold. Multiplet Io Uthresholds are adopted,and theaveragelengthof the window( 10 s)ismuchsmaller theaveragem APoverall the set Io Uthresholdsiscomputed. \u223c thanthatof the videoclip(500 s),about 2%ofthecliplength. Formomentqueries,weevaluatem APat 5 differentt Io U thresholds 0.1,0.2,0.3,0.4,0.5 ,aswellas the iraverage { } Moments queries Considering that the moment queries value. taskisrelatedto the tasksoftemporalactiondetection[61, Recall@kx, t Io U=m, is a metric adapted from the metric 141,229,237] and video grounding [236], we adapt their recall@k, t Io U=m, used for NLQ. The metric recall@k, respectivemetricstomomentqueries. t Io U=mmeasuresthepercentageof the querysentences that Average Precision(AP)isacommonlyadoptedmetricin haveatleastonepredictionwi that Io Ulargerthan the thresh- temporalactiondetection. Itmeasureshowclosely the tem- oldmin the top-kresults. Inourmomentqueriescase,since poralextentofthepredictionsmatches the ground-truthac- wemight have morethanoneinstancecorrespondingtoa tioninstances for eachactioncategory[61,141,229,237]in querymomentcategory,weneedtomeasure the percentage termsofbothprecision and recall. Thetemporalintersection ofall the correctlypredictedinstances that haveatleastone overunion(t Io U)betweenaprediction and aground-truth predictionwi that Io Ulargerthanthethresholdmin the top- actioninstanceisusedtomeasure the irdistance. Ifthet Io U k resultsof this instance. Considering that predictions are is higher than a threshold, the prediction is considered as usuallymade base donacategorynotaspecificinstance,we true positive; otherwise, false positive. In representative modifythemetrictobe the followingrecall@kx,t Io U=m, 32 X tcejbo si ereh W ?Y tneve retfa/erofeb ?X tup I did ereh W ?X tcejbo si ereh W ?X ni tup I did tah W ?s X ynam wo H )noitseuq ytitnauq( ?Y I did X tah W I did noitacol tahw n I ? X tcejbo ees ?Y si X tah W tcejbo na fo etat S tcaretni I did oh W did I nehw htiw ?X ytivitca ?X tcejbo ym si ereh W ni ot klat I did oh W ?X noitacol ro ot klat I did neh W nosrep htiw tcaretni ?X elor htiw 3000 2500 2000 1500 1000 500 0 Templates sruoh oediv # Figure 24.Distributionofqueriesover the correspondingtemplates acrossobjects,place,andpeoplecategories(Tab.5). Seetext for moredetails. 2000 1500 1000 500 0 0.0 10.0 20.0 30.0 40.0 50.0 60.0+ window length (seconds) seireuq # Figure 25.Distributionofresponsewindowlength for NLQ.For thesakeofbrevity,we use the lastbintorepresentallwindows longerthanaminute.Seetext for moredetails. 6000 5000 4000 3000 2000 1000 0 0 100 200 300 400 500 Action duration (seconds) secnatsni # 250 200 150 100 50 0 0 10",
      "start_pos": 12474,
      "end_pos": 12986
    },
    {
      "chunk_id": 165,
      "paper_id": "ego4d",
      "text": "0 0.0 10.0 20.0 30.0 40.0 50.0 60.0+ window length (seconds) seireuq # Figure 25.Distributionofresponsewindowlength for NLQ.For thesakeofbrevity,we use the lastbintorepresentallwindows longerthanaminute.Seetext for moredetails. 6000 5000 4000 3000 2000 1000 0 0 100 200 300 400 500 Action duration (seconds) secnatsni # 250 200 150 100 50 0 0 10 20 30 40 50 60 70 80 # instances in one video clip Figure 26.Distributionofmomentduration. wherexst and sfor the numberofinstances for aquerycat- egory in one video. This metric measures the percentage of all the correctly predicted instances that have at least one prediction with a t Io U larger than the threshold m in the top-kx results of the action category. This metric has a similar idea to the multi-label metric proposed in [240] spilc oediv # Figure 27.Distributionofinstancenumbersinonevideoclip. whendealing with multipleinstances for aquery. we use k = 1,2,3 andm = 0.3,0.5,0.7 inthemetric. Compared toaverageprecision,thismetriconlyevaluates the recall for the query categories, and does not penalize for false posi- tivepredictionsgivenacategory that hasnoinstancesin the video. F.6 Baselines Wedeveloped base linemodels for each task. Wedesigned these model stoaddress our tasks,usingstate-of-the-artcom- ponentswhererelevant. Theyrepresentastartingpointupon whichfuturework can build. Visualqueries 2 Dlocalization base line Wetreatvisualqueries with 2 Dlocalization(VQ 2 D) asa detection + tracking problem (see Figure 28). At a high level,ourapproachconsistsofthreesteps. First,weperform frame-leveldetectionover the inputvideowherewedetect the presence of the query object in each frame using an object detection model (Figure 28 top). For each frame, wegettheboundingbox that ismostsimilarto the visual crop and ascoreindicatingitsvisualsimilarity. Second,we consider the sequenceofper-framesimilarityscoresover the entirevideo and identifythemostrecentpeakin the sescores (Figure 28 bottom-left). Finally,weinitializeatrackerat the video-framecorrespondingto the peakdetection,andtrack thequeryobjectonboth for ward and backwarddirections torecover the completeresponsetrack(Figure 28 bottom- right). Step 1: Frame-leveldetection Wepropose Siam-RCNN, a Faster-RCNN [189] based approach to detect the query object in a given image. See Figure 28 top. Given a video frame at time t, a pre-trained Region Proposal Network (RPN) [189] with a Feature Pyramid Network (FPN) [142] backbone is used to generate bounding box proposals b , ,b . The Ro I-Align operation [94] is 1 N { \u00b7\u00b7\u00b7 } thenusedtoextractvisualfeatures for eachboundingbox (b ), , (b ) . we use the same FPNbackboneto 1 N {F \u00b7\u00b7\u00b7 F } extractfeatures for the visualcropv. Todetect the presence ofthequeryobjectinframet,eachproposalfeature (b )is i F 33 Step 1: Frame-level detection with Siam-RCNN BBo Bx Bporxopporsoaplossals { b 1, {\u00b7 b \u00b71\u00b7 ,b \u00b7 N \u00b7\u00b7} b N } BBoxfeatures RPN Ro IAlign (b 1), , (b N) {F \u00b7\u00b7\u00b7 F } Backbone( ) F Backbone( ) (v) F F Inputvideo Visual crop (v) Step 2: Temporal detection )s( erocs ytiralimi S Prediction @ frame t Top -1 retrieval (bt,st) Per-frame predictions [(b ,s ), ,(b ,s )] 1 1 Q-1 Q-1 \u00b7\u00b7\u00b7 Step 3: Tracking Signal peaks Initialize tracker with Tracker Query Forward tracking frame Response track prediction Nearest peak detection Initialize tracker with Tracker Q Backward tracking Video time (t) Figure 28.Visualqueries 2 Dlocalization base line.Ourapproachconsistsofthreesteps.Step 1:Weper for mframe-leveldetection for the entireinputvideotodetectthepresenceof the queryobject(specifiedvia the visualcropv).Foreachframet,weextract the regionproposals {b 1 ,\u00b7\u00b7\u00b7 ,b N }using are gionproposalnetwork(RPN),andextractfeatures for eachproposal{F(b 1 ),\u00b7\u00b7\u00b7 ,F(b N )}.Eachproposalfeature iscomp are dwith the visualcropfeature F(v)usinga Siamesehead S,and",
      "start_pos": 12936,
      "end_pos": 13448
    },
    {
      "chunk_id": 166,
      "paper_id": "ego4d",
      "text": "tracker with Tracker Q Backward tracking Video time (t) Figure 28.Visualqueries 2 Dlocalization base line.Ourapproachconsistsofthreesteps.Step 1:Weper for mframe-leveldetection for the entireinputvideotodetectthepresenceof the queryobject(specifiedvia the visualcropv).Foreachframet,weextract the regionproposals {b 1 ,\u00b7\u00b7\u00b7 ,b N }using are gionproposalnetwork(RPN),andextractfeatures for eachproposal{F(b 1 ),\u00b7\u00b7\u00b7 ,F(b N )}.Eachproposalfeature iscomp are dwith the visualcropfeature F(v)usinga Siamesehead S,and the mostsimilarproposalb tisretrievedalong with itsscore s t. Thisprocessisrepeated for allframes. Step 2: Wetreat the similarityscoress={s 1 ,\u00b7\u00b7\u00b7 ,s q\u22121 }asatemporalsignal and perform temporaldetectiontoobtain the\u2018mostrecentoccurrence\u2019ofthequeryobject.Wedetect the peaks(localmaxima)inthesignal and recover thepeakpne are stto the queryframe.Step 3:Given the detectedpeakp and itscorrespondingproposalb p,weinitializetwotrackers with b pandrunthemalongthe for ward and backwarddirectionstorecoveracontiguoustrackof the object,i.e.,theresponsetrackprediction. comp are dwith the visualcropfeature (v)usinga Siamese b with the highest similarity score s for frame t can be t t F head thatpredictsa 0-1 similarityscore obtainedasfollows: S s i = S ( F (b i ), F (v)) (9) b t = argmax { s 1 , \u00b7\u00b7\u00b7 ,s N } (12) b\u2208{b 1,\u00b7\u00b7\u00b7,b N} The Siamesenetworkprojectseachproposal/visual-crop s =max s , ,s (13) t 1 N feature to a 1024-D feature vector using a convolutional { \u00b7\u00b7\u00b7 } projectionmodule , After repeating the above steps for all the video P frames, we can obtain the final per-frame predictions as p = ( (b )); p = ( (v)) (10) b i v P F P F [(b ,s ), ,(b ,s )]. 1 1 q\u22121 q\u22121 \u00b7\u00b7\u00b7 andpredictsa 0-1 similarityscoreusingabilinearopera- Step 2: Temporaldetection Sofar,we used Siam-RCNN tion: togetthemostsimilarproposals and the irsimilarityscores s =\u03c3(p TWp +b) (11) i b v foreveryframein the video. Next,thegoalistotemporally where \u03c3 is a sigmoid non-linearity. After computing the detect the\u2018mostrecentoccurrence\u2018oftheobjectin the video similarities to each bounding box proposal, the proposal (see Figure 28 bottom-left). Thisisachallengingproblem 34 since our goal is not to identify the best detection of the Wenextdetail the trainingprocedure for the Siam Head object,butinstead the mostrecentone,evenif the similarity ( ). we useasimilarityretrievalapproach were the model S isnotashigh. Totackle this problem,wetreat the per-frame is trained to predict high visual similarity between the vi- similarityscoress = s , ,s asatemporalsignal, sual crop v and positives, and low visual similarity be- 1 q\u22121 { \u00b7\u00b7\u00b7 } and use a signal peak detection approach to identify the tween v and negatives. The loss function for is a bi- S salientpeaks(a.k.a. localmaxima)ins. Toavoidspurious nary cross entropy loss defined over each (v,D ,D ) tu- p n peaks,wefirstsmoothsusingamedianfilter with awindow ple (see Eqn. 16), where D = p |Dp| are positive sizeof 5. detections, D = n |Dn| a p re neg { ati i v } e i= d 1 etections, and n { j }j=1 s = ( (x), (v)): s\u00af=median filter(s) (14) x,v S F F p , ,p =find peaks(s\u00af) (15) 1 k \u00b7\u00b7\u00b7 (cid:18) (cid:19) 1 (cid:88) (cid:88) = log(s )+ log(1 s ) Dependingon the video,thealgorithmmayreturnmultiple L S \u2212 D D p,v \u2212 n,v p n peaksspreadthroughout the video(seesignalpeaksin Fig- | \u222a | p\u2208Dp n\u2208Dn (16) ure 28 bottom-right). Since our goalistodetect the most Bothpositives and negatives are defined base donpropos- recentoccurrenceof the object,weselect the peakpthatis alsgeneratedby the RPN.Givenavisualcropv,aproposal temporallyne are stto the queryframe. p fori (s,e)isapositiveif",
      "start_pos": 13398,
      "end_pos": 13910
    },
    {
      "chunk_id": 167,
      "paper_id": "ego4d",
      "text": "S \u2212 D D p,v \u2212 n,v p n peaksspreadthroughout the video(seesignalpeaksin Fig- | \u222a | p\u2208Dp n\u2208Dn (16) ure 28 bottom-right). Since our goalistodetect the most Bothpositives and negatives are defined base donpropos- recentoccurrenceof the object,weselect the peakpthatis alsgeneratedby the RPN.Givenavisualcropv,aproposal temporallyne are stto the queryframe. p fori (s,e)isapositiveif the Io U(p ,r ) 0.5,where i i i Step 3: Tracking Aftertemporaldetection,wehaveidenti- r is the \u2208 response track box in frame i. We r \u2265 emove all r i i fiedapeak-framepin the videowhichisestimatedto have which are toosmall, orhavesignifi can tly differentaspect the most recent occurrence of the object. For this frame ratios from thelargestboxinrsince the setypicallycorre- p,we canobtain the highest-scoringboundingboxb p from spondtoobstructedviewsof the object. Aproposalp j isa theper-framedetectionsinstep 1. Note that thisonlyrep- negativeifitsatisfiesanyof the followingtwoconditions: resentsoneframewhere the objectmostrecentlyoccurred. However,the task objectiveistoobtain the responsetrack, 1. j (s,e)and Io U(p j ,r j )<0.5 \u2208 i.e.,thecontiguoussetofallframes,starting from when the 2. p issampled from ano the rvideo. j objectfirstentered the field-of-viewuntil the objectexits the field-of-view. See Figure 28 bottom-right. Tocompute the We also found it beneficial to use hard-negative mining, restof the responsetrack,we useb asastartingpoint,and whereweinitiallysamplealargenumberofnegatives and p runasingle-objecttracker for ward and backwarduntil the thenselect the top-Knegatives with the highestlossvalue. trackingfails(i.e.,theobjectexits the field-of-view). Forbothdirections,weinitialize the apperancemodelof We employ a few different augmentation strategies to thetrackerusing the proposalb . For the forwardtracking, artificiallyexp and the dataset. First,weaugmenteach data p werun the trackerstarting from framep+1 toq 1 andobtain samplebyreplacing the visualcropvbyaboundingboxr i thetrackedregions: b =[\u00afb , ,\u00afb ]. For \u2212 thebackward from the responsetrack. Thisworksbecause the response f p+1 e \u00b7\u00b7\u00b7 tracking, we run the tracking starting from frame p 1 track and the visual crop correspond to the same object. to 0 and obtain the tracked regions: b = [\u00afb , ,\u00afb \u2212 ]. Next, we augment the visual crop v by applying random b s p\u22121 \u00b7\u00b7\u00b7 Wethenconcatenateb ,b ,andb toobtain the complete rotations between 120\u25e6 to 120\u25e6. This exploits the fact b p f \u2212 response track prediction. We use the KYS tracker [22], thatobjectscan have signifi can tviewpointvariationsinego- whichwasshowntoachievestate-of-the-artresults for single- centricvideos(unlikeinternetphotos). Finally,weapplya objecttracking. randombrightnessaugmentationto the videoframes and the visualcroptosimulatedifferinglighting. VQ 2 Dbaselinetrainingsetup Wenowdiscuss the train- ingprocedure for the VQ 2 Dbaseline. Each data point for the Implementation details We train the Siam Head using S VQ 2 Dtask(definedon Ego 4 Dvideos)consistsof the fol- the Detectron 2 library[225]. we use the defaultconfigura- lowing: video V,visualcropimagev,queryframenumber tionfile and make the followingchanges for ourexperiments. q,andresponsetrackboxesr = r ,r , ,r ,where Foreachexperiment,we use 8 GPUs,64 visualcropsper s s+1 e { \u00b7\u00b7\u00b7 } sande are the start and endframesofr,andr isabounding batch,andtrain for 300,000 iterations with aninitiallearn- i boxdefinedonframeiofvideo V. ing rate of 0.02 followed by a 0.1 decay after 200,000 \u00d7 As a high-level overview, we initialize and freeze the iterations. Weextractbackbonefeatures from the\u201cp 3\u201dlayer backbone and RPNusingweightsfroman MS-COCOpre- of FPN. Based on validation per for mance, we use 6 pos- F trained Mask-RCNN model. we usethe VQ 2 Dannotations itives and 64 negatives for each visual crop. Specifically, to train the Siam Head ( ). We initialize and freeze the we sample 58",
      "start_pos": 13860,
      "end_pos": 14372
    },
    {
      "chunk_id": 168,
      "paper_id": "ego4d",
      "text": "3\u201dlayer backbone and RPNusingweightsfroman MS-COCOpre- of FPN. Based on validation per for mance, we use 6 pos- F trained Mask-RCNN model. we usethe VQ 2 Dannotations itives and 64 negatives for each visual crop. Specifically, to train the Siam Head ( ). We initialize and freeze the we sample 58 negatives per video frame which results in S KYS tracker using weights pre-trained on GOT-10 k [99], 58 64=3712 negativesperbatch. Foreachvisualcrop, \u00d7 La SOT[62],and Tracking Net[162]datasets. wesample the 64 hardestnegativesoutof 3712. 35 Inthe Siam Head architecture,theprojectionmodule itremainsrelativelystable for the secondstrategywherewe S P consistsoff our residualblocksfollowedbyaveragepooling, previewafractionofframesclosestto the query. Forexam- anda 2-layermulti-layerperceptron(MLP)withahidden ple,we canachieveasearchefficiencyof 48.0%withonlya sizeof 1024-Dand Re LUactivation. 6 16%relativedropinper for mancewithk =50%inthe \u2212 For signal peak detection, we utilize the find peaks 2 ndstrategy. However,theper for mancedropssignifi can tly function from the scipylibrary 12 with the followinghyper- ifwereducekfurther. Forexample,weobserve are duction parametersselectedthroughvalidation: distance=25,width of 38 60%fork =10%with the 2 ndstrategy. Thissug- \u2212 =3,andprominence=0.2. geststhatmoreintelligentmethods that per for mcontextual search are neededtoimprove the searchefficiency for VQ 2 D Experimentalresults Weevaluate the per for manceofmul- whilemaintaininggoodper for mance. tiple base lineson the VQ 2 Dtaskin Tab.9. Thefirstcolumn inthetableshows the detection and trackingmethods,and thesecondcolumnshows the Siam Headprojectionarchitec- Visualqueries 3 Dlocalization base line ture . Inadditionto the KYStracker,wealsoexperiment P Nextwedescribethe base line for the visualquery with 3 D withasimpleparticlefiltertracker(denoted\u2018PF\u2019)toassess localization task. Recall the taskdefinition: givenavideo,a theimpactof the trackingquality. Asanablationof Siam R- queryframe,andavisualcropofatargetobject,thegoalis CNN, we replace the 4 residual blocks in the Siam Head tooutputa 3 Ddisplacementvector from the cameracenter projection module with a simple 3-layer CNN which has ofthequeryframetothecenterof the targetobjectin 3 D. lowercapacity with noresidualconnections(indicatedby The 3 D position of the target object is defined at its most \u2018Simple\u2019). recentappearancein the video. Figure 31 showsasampleof We make several observations. When we use a simple thetask. projection model with a particle filter tracker, we already Our base linestrategyhasthreesteps. Wefirstestimate observe a good validation per for mance of 32.4% success, thecameraposesof the video. Then were trieve the most and 0.14 t AP . These can beattributedtousingastrong 25 recentinstanceofthetargetobjectin the video. Lastly,we proposal generator (RPN pre-trained on MS-COCO) and estimatethedepthof the detectedobject and retrieveits 3 D alearnedsiamesecomparison model. Uponreplacing the position from the queryframe. particlefiltertrackerwitha So TAKYStracker[22],while thevalidationsuccessrateremainssimilarat 33.0%,weob- Cameraposeestimation Thecameraposes are estimated serve significant gains (absolute) in all other metrics: 2% usingakeypointmatchingstrategyalongwitha Perspective- t AP, 2% st AP 25 , and 14.3% recovery. This suggests that n-Point(Pn P)resolutionapproach. Atahighlevelourap- a good tracker is necessary to accurately capture the full proachconsistsof the followingf our steps. Firstweestimate responsetrackafterlocalizingasingleframe with init. Fi- thecameraintrinsicparametersusing Structure-from-Motion nally,uponreplacing the\u2018Simple\u2019siameseprojection with (Sf M).Secondly,weextract and matchkeypoints from each 4 residualblocks,weobserveasignifi can tgainsof 6.8%in framein the videotokeypointsextracted from the Matter- success, 5% in t AP 25 , 4% in st AP 25 , and 5% in recovery port 3 Dpanoramas. Then,using the matchedkeypointswe %. Thissuggests that usingahighercapacity model for the setup and solvea Pn Pproblem for eachframein the video Siam Headishelpful for improving the per-framedetection toestimate the correspondingcamerapose. Lastly,werefine per for mance for the VQ 2 Dtask. Weobservesimilartrends theposesusingtemporalconstraints. onthetestset. Pleasesee Fig.29 forqualitativeexamplesof Step 1: Camera intrinsics estimation We start by ex- themodel\u2019spredictions. tractingasetofcontiguousnon-blurryframes from",
      "start_pos": 14322,
      "end_pos": 14834
    },
    {
      "chunk_id": 169,
      "paper_id": "ego4d",
      "text": "that usingahighercapacity model for the setup and solvea Pn Pproblem for eachframein the video Siam Headishelpful for improving the per-framedetection toestimate the correspondingcamerapose. Lastly,werefine per for mance for the VQ 2 Dtask. Weobservesimilartrends theposesusingtemporalconstraints. onthetestset. Pleasesee Fig.29 forqualitativeexamplesof Step 1: Camera intrinsics estimation We start by ex- themodel\u2019spredictions. tractingasetofcontiguousnon-blurryframes from the video. Inallcases from Tab.9,thesearchefficiencyis 0%since Inordertoselectnon-blurryframeswecompute the variance thedetectors are usedoneveryframein the searchwindow. ofthe Laplacianoneachimage and select the oneswitha In Fig.30 weexperiment with twosimpletechniquesforim- valuehigherthana 100 threshold. Wethenselect the largest proving the searchefficiency. Thefirstapproachuni for mly contiguous set of non-blurry images. We cap the number subsamplesk%oftheframesin the searchwindow(denoted ofselectedframesto 10 tolimit the computationaltimeof as\u2018SS\u2019).Thesecondapproachsearchesoveronlyk%ofthe the Sf M module. Once we have selected the images we mostrecentframesin the searchwindow,i.e.,frames that are run the automaticreconstructionmoduleof COLMAP[196] nearestto the query(denotedas\u2018N\u2019).Weconsider 3 values to estimate the camera instrinsic parameters with a radial ofkinbothcases:10%,25%,and 50%.Consider the results fisheyecamera model. in Fig.30. Inbothstrategies,thesearchefficiencyimproves Step 2: Keypointextraction and matching we use Su- aswereducek. Theper for mancedropsdrastically for the per Glue [195] to extract and match keypoints. We first 1 ststrategywherewesubsample the searchwindow,while extract keypoints from the scan panoramas k ,p {p,n} { \u2208 12 Peak detection: https://docs.scipy.org/doc/scipy/ P ,n \u2208 N} where P is the number of panoramas and N reference/generated/scipy.signal.find_peaks.html is the number of keypoints. The scan panoramas are gen- 36 validation set test set Detector+Tracker Succ t AP t AP 25 st AP st AP 25 rec% Succ t AP t AP 25 st AP st AP 25 rec% P Siam-RCNN+PF Simple 32.4 0.06 0.14 0.02 0.06 13.2 32.7 0.06 0.14 0.02 0.06 12.9 Siam-RCNN+KYS Simple 33.0 0.08 0.15 0.03 0.08 27.2 33.4 0.09 0.16 0.03 0.08 26.9 Siam-RCNN+KYS Residual 39.8 0.12 0.20 0.04 0.12 32.2 41.6 0.12 0.21 0.05 0.13 34.0 Table 9.Visualqueries 2 Dlocalizationresults.Wecomp are the per for manceofvarious base lineson the VQ 2 Dvalidation and test data sets. Column 1 indicates the detector and tracker.Column 2 indicates the projectionarchitectureusedincaseof the Siam-RCNN model. Predicted response track Query: When did I last see this object? . . . . . . Predicted response track Query: When did I last see this object? . . . . . . Figure 29. Qualitativeexamples for visualqueries 2 Dlocalization.Oneachrow,weshowthevisualcropofthequeryobjecton the right andthepredictedresponsetrackin the center(3 uni for mlysamplesimages). The model wasabletocorrectlylocalize the mostrecent occurrenceoftheobject and accuratelytrackitthroughout the occurrence. (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:49)(cid:12)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92) (cid:8)(cid:3)(cid:86)(cid:86)(cid:72)(cid:70)(cid:70)(cid:88)(cid:54) (cid:24)(cid:19) (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) (cid:23)(cid:19) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:49)(cid:12)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:22)(cid:19) (cid:21)(cid:19) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:20)(cid:19) (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) (cid:19) (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19) (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92) (cid:24)(cid:21)(cid:17)(cid:19)(cid:3)(cid:35)(cid:3)(cid:51)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:82)(cid:83)(cid:80)(cid:72)(cid:55) (cid:19)(cid:17)(cid:21)(cid:24) (cid:19)(cid:17)(cid:21)(cid:19) (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) (cid:49)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:19)(cid:17)(cid:20)(cid:24) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) (cid:19)(cid:17)(cid:20)(cid:19) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:19)(cid:17)(cid:19)(cid:24) (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) (cid:19)(cid:17)(cid:19)(cid:19) (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19) (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92) (cid:24)(cid:21)(cid:17)(cid:19)(cid:3)(cid:35)(cid:3)(cid:51)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:82)(cid:83)(cid:80)(cid:72)(cid:55)(cid:82)(cid:76)(cid:87)(cid:68)(cid:83)(cid:54) (cid:19)(cid:17)(cid:20)(cid:25) View from the response track (cid:19)(cid:17)(cid:20)(cid:21) (cid:19)(cid:17)(cid:19)(cid:27) (cid:19)(cid:17)(cid:19)(cid:23) (cid:19)(cid:17)(cid:19)(cid:19) (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19) Figure 30. Searchefficiency for visualqueries 2 Dlocalization. Weevaluatesimpletechniques for improving the searchefficiency, andplot the corresponding VQ 2 Dper for mance.Theblue data point isthe Siam RCNNper for mancewhenwepreview the entiresearch window.Thered data points are the Siam RCNNper for mancewhen View from the query frame wesearchoverk%oftheframesuni for mlysubsampled(SS)from thesearchwindow. Theyellow data points are the Siam RCNN Figure 31.Visualqueries 3 Dlocalization task demo.Thetopview per for mancewhenwesearchoverk%oftheframesnearest(N)to istheview from thelastframeoftheresponsetrack with the target thequery(withoutanysubsampling).Thevalueofkisindicated",
      "start_pos": 14784,
      "end_pos": 15296
    },
    {
      "chunk_id": 170,
      "paper_id": "ego4d",
      "text": "data point isthe Siam RCNNper for mancewhenwepreview the entiresearch window.Thered data points are the Siam RCNNper for mancewhen View from the query frame wesearchoverk%oftheframesuni for mlysubsampled(SS)from thesearchwindow. Theyellow data points are the Siam RCNN Figure 31.Visualqueries 3 Dlocalization task demo.Thetopview per for mancewhenwesearchoverk%oftheframesnearest(N)to istheview from thelastframeoftheresponsetrack with the target thequery(withoutanysubsampling).Thevalueofkisindicated objectannotatedwitha 2 Dredboundingbox. Thebottomview aboveeach data point. istheview from the queryframe. Thetargetobjectisannotated with a 3 D red bounding box at the top right of the figure. The figureshows the ground-truth(green)and the predicted(red)3 D erated using the Matterport SDK.13 We render RGB and displacementvectors. depth images at each scan position and sweep over pitch 13 Matterport-SDK: https://matterport.github.io/ values \u2208 [ \u2212 30,30]withastepsizeof 5 deg. andyawval- showcase-sdk/sdk_intersection_inspector.html ues [ 180,180]withastepsizeof 15 deg. Wegenerate \u2208 \u2212 37 on average 7 K images per scan. Note that while we are Video frame View from the scan Superposition not releasing the panoramas because of data anonymiza- tionconcerns,weareproviding the precomputedkeypoints. Similarily, we extract keypoints from the video frames k ,i ,m where is the number of im- {i,m} { \u2208 I \u2208 M} I agesin the video and isthenumberofkeypoints. Once M the keypoints are extracted we loop through each frame i in the video and match the extracted frame key- \u2208 I points k ,m to all the panoramas keypoints {i,m} { \u2208 M} k ,p ,n . We use the pretained models {p,n} { \u2208 P \u2208 N} available 14 of Super Point[50]forkeypoints and descriptors extraction and Super Glue[195]formatching. Step 3:Pn Presolution Wecompute the camerapose for thevideoframeshavingatleast 20 matchedkeypoints. We empiricallyfind that athresholdof 20 providesagoodtrade- Figure 32. Samplesofcameraposeestimation. Leftshows the off between the number of overall pose estimates and the frame from the egocentricvideo,middlehas the viewrendered from qualityof the estimations. Thepositionsof the 3 Dkeypoints theestimatedviewpointinthes can andrightis the superpositionof both.Weobserve that even with bigscenedifferencesbetween the arecomputed from apinholecameramodelof the Matterport video and thes can(e.g.,thewheelin the secondrow),thealgorithm camerausing the renderedpanoramadepth,cameraintrin- isabletoaccuratelyretrieve the camerapose. sics,andcamerapose. Thepositionsof the 2 Dkeypoints are directlyextracted from the videoframespixels. Wethenuse the Open CVlibrarytosolve the Pn Psetup and estimate the Theremainingunlocalizedframes are duetoabruptmo- camerapose from the matchedpairsof 3 Dand 2 Dpoints and tion (lost track) and when the view is too close-up to the using the estimatedcameraintrinsicparameters. Using this scene(notenoughkeypointsmatched). methodwe canestimate the cameraposeofroughly 2%of Targetobjectretrieval Webuild our solutionontopof the thetotalnumberofframesin the video. Nextweincorporate visualqueries 2 Dlocalization base line. The 2 Dlocalization temporalconstraintstoincrease this number. Step 4: Temporal constraints and final pose estima- baselineoutputs are sponsetrack with 2 Ddetectionsof the tion Toincreasethenumberofestimates were fine the pose target object. Our baseline combines these 2 D detections along with dep the stimation and cameraposeestimationto estimationpipelinebyincorporatingtemporalconstraintsin retrieve the 3 Dpositionof the object. aniterativeprocedure. Westartbyextracting and matching 2 Dkeypoints from localizedframestonon-localizedonesin Depth estimation We estimate the depth of the most re- thevideo. Thisstepissimilarto the above Step 2;we use centframeof the responsetrack for whichwe have apose thesame Super Glue[195]. Using the matchedkeypoints and estimate. We use the DPT network [185] with pretrained current estimated poses we triangulate new 3 D keypoints weightson NYU v 2[202]. Figure 33 showsdep the stima- for the non-localized images. We then solve a new Pn",
      "start_pos": 15246,
      "end_pos": 15758
    },
    {
      "chunk_id": 171,
      "paper_id": "ego4d",
      "text": "the responsetrack for whichwe have apose thesame Super Glue[195]. Using the matchedkeypoints and estimate. We use the DPT network [185] with pretrained current estimated poses we triangulate new 3 D keypoints weightson NYU v 2[202]. Figure 33 showsdep the stima- for the non-localized images. We then solve a new Pn P tionresultswhereleftistheframe from the video,middle setup with the newkeypoints. Weapply this procedureit- istheestimateddepth,andrightis the depth from thes can erativelyuntilconvergence. Afterrefinementweachievea rendered at the estimated viewpoint (not available to the per for manceof 15%ofposeestimatesof the totalnumber baseline model). Note that duetoscenedifferencesbetween offramesaccrossallvideoclips. the video and the scan, the two depths frames will differ Camera pose estimation quality and sources of error in some region of the image. We then compute the depth valueofthetargetcentroidas the medianofasqu are region Wequalitativelyevaluate the cameraposeestimationpipeline centeredat the 2 Ddetection. byrendering the viewsin the 3 Dscans. Recall that the scans andvideos have beenrecordedatdifferenttimes and thus 3 D displacement vector reconstruction Given the esti- thescenes can containlargedifferences. Figure 32 shows mated depth d of the object centroid c in frame f of the camera poses estimates where left is the frame from the response track and the estimated camera instrisics K, we video, middle is the view from the scan, and right is the construct the 3 Dvectordisplacementv\u02c6 inthecurrentframe f superposition. Wesee that even with largescenedifferences f coordinatesystemusingapinholecamera model: betweenthes can andvideo(e.g.,thewheelin the middle example)thealgorithmiscapableofproducinggoodpose \uf8ee \uf8f9 \uf8ee \uf8f9 estimates. x u 14 Super Glue weights: https://github.com/magicleap/ v\u02c6 f =\uf8f0y\uf8fb=d K\u22121 c=d K\u22121 \uf8f0v\uf8fb (17) z 1 Super Glue Pretrained Network 38 Video frame Depth from DPT Depth from the scan RT depth L 2 angle Succ\u2217% Succ% Qw P% ground-truth random 7.93 1.99 0.00 0.00 1.83 ground-truth scan 2.92 1.10 76.47 1.22 1.83 ground-truth DPT 3.33 1.15 76.47 1.22 1.83 Siam-RCNN+PF DPT 6.53 1.64 25.00 0.61 0.61 Siam-RCNN+KYS(sim.) DPT 5.78 0.48 36.36 0.61 0.61 Siam-RCNN+KYS(res.) DPT 5.98 1.60 30.77 1.22 1.83 Table 10.Visualqueries 3 Dlocalizationresults.Wecomp are the per for manceofvarious base lineson the valsetof the VQ 3 Dtask. Column 1 indicates the VQ 2 Dnetworkusedtopredict the response track(RT).Thelastmetric Qw Pmeasures the queryratio for which wehaveposeestimation for theresponsetrack and the queryframe. The L 2 metricisexpressedinmeters and angles are inradians.The firstthreerows are ablationstudiesusing the ground-truthresponse tracks and withdep the stimatedr and omly,usingthes can andvia Figure 33. Samplesofdep the stimation. Leftshows the frame the DPT[185]network. from the egocentricvideo,middlehas the estimateddepth from DPT[185]andrighthas the depth from thes can renderedat the estimatedviewpoint. using DPT(lines 2 and 3). Thissuggests that the reisalso room for improvementindesigningbetterdep the stimators. whereu,varethepixelindicesof the centroidcinframef. Wethenestimatetheobjectcentroidpositiont\u02c6 inthes can Naturallanguagequery base lines s coordinatesystem: Since the naturallanguagequeries can beseenasalanguage- groundingprobleminavideo,weadopttwopriormethods t\u02c6 =Psv\u02c6 (18) s f f inordertoimplement the baselines for this task. where Ps is the camera pose for the frame f. We further (a)2 DTemporal Adjacent Networks(2 D-TAN)[236]: f retrieve the displacement vector v\u02c6 in the query frame Q We apply 2 D-TAN with a sliding window method to im- Q coordinatesystem: plement the naturallanguagequery base line. Thegoalof 2 D-TANistoanswerwhere the semanticallycorresponding v\u02c6 Q =P Q s\u22121 t\u02c6 s (19) videomomentis,givenalanguagequeryinanuntrimmed video. Thelanguagequerystems from oneof the 13 tem- where Ps",
      "start_pos": 15708,
      "end_pos": 16220
    },
    {
      "chunk_id": 172,
      "paper_id": "ego4d",
      "text": "the displacement vector v\u02c6 in the query frame Q We apply 2 D-TAN with a sliding window method to im- Q coordinatesystem: plement the naturallanguagequery base line. Thegoalof 2 D-TANistoanswerwhere the semanticallycorresponding v\u02c6 Q =P Q s\u22121 t\u02c6 s (19) videomomentis,givenalanguagequeryinanuntrimmed video. Thelanguagequerystems from oneof the 13 tem- where Ps isthecameraposeof the queryframe. Q plate questions. The core idea of 2 D-TAN is to consider Experiments and results Wecomp are the per for manceof adjacent moment candidates as the temporal context on a multiple base linesalong with ablationstudies. Wepresent two-dimensionaltemporalmap and retrieve the mostrele- theresultsin Table 10. Numbers are computedon the valida- vantmoment from the can didates.Moreconcretely,2 D-TAN tionset(164 queries)ofthe VQ 3 Dtask. Wereport the query takeseachmoment can didateasoneelementin the 2 Dtem- ratio Qw P,forwhichwe have cameraposeestimates for the poralmapsuch that the adjacentmoment can didateson the responsetrack and queryframe. Additionally,wereport the map can havemuch-overlappedcontentorsh are the same success rate Succ\u2217 which is the success metric computed startorendtimeslot. Itappliesaconvolutionalneuralnet- only for queries with associatedposeestimates. workon the 2 Dmaptopredict the Intersectionover Union Overall,wenoticealow Qw Pratioleadingtoalowsuc- of each moment candidate and the ground-truth moment. cessrate. Theselowmetrics are duetoasmallnumberof Pleasesee[236]formoredetails. cameraposeestimates(15%overall). None the less,weob- Since the 2 D-TAN enumerates all the possible combi- serve that the best VQ 2 Dbaselinemethodcombined with the nationsofstart-endpairs,the O(N 2)spacecomplexityof pretrained DPT[185]depthestimatoryields the bestper for- the 2 D map leads to a heavy model, especially when we mancesintermsof L 2 andsuccess. Thesenumberstell that requireaprecisemomentboundary. Tomake 2 D-TANmore there are opportunities for enhancementindesigningbetter appropriateto our problem,wefur the ruseaslidingwindow cameraposeestimators. Additionally,weper for mablation methodontopof 2 D-TAN.Webreakdown the clipintoa studiesusing the ground-truthresponsetracks and different numberofoverlappingwindows,whereawindowpresentsa dep the stimators(random,fromthes can,using DPT).For smallportionof the clip. Thewindows are takenas the input ther and omexperimentweuni for mlysampleadepthvalue ofthe 2 D-TAN model inbothtraining and testingphases. between 0.1 and 10 meters. From the ablationexperiments During the training of the 2 D-TAN model, we use wenote that rendering the depth from thes can attheesti- Ego 4 D\u2019sprovidedpre-extractedfeatures for both the video mated viewpoint increases the per for mances compared to clip and language query. The clip feature is from a Slow- 39 Figure 34.Baseline model architectures:momentqueries.Itstakesavideosequence and generatesdetectedactions with start/endtime, theircategories,andconfidencescores. Ithastwocomponents: graphpyramidnetwork(GPN),andscoring and localization(So L). GPNiscomposedofmulti-levelencoder and decoderpyramids.Theencoderaggregatesfeaturesindifferentlevelsviaastackofgraph networks(GN)(yellowtrapezoidarea;thedecoderrestores the temporalresolution and generatesmulti-levelfeatures for detection.So L (bluedashedbox)containsf our modules,thetoptwopredictingactionscores and boundaries,thebottomtwoproducingsupplementary scores and adjustingboundaries.Figureisadapted from[237]. Fast[71]networkpretrainedon Kinetics 400 dataset, and Io U=0.3(%) Io U=0.5(%) Baseline the language feature is a based on the BERT model [52]. r@1 r@5 r@1 r@5 Thewindowdurationis 40 s,andstrideis 20 sin the sliding windowmethod. Notably,weonlyusewindows that contain or are next to a ground-truth moment in training, but we useall the windowsintesting. Wekeepalltheo the rhyper- parametersin 2 D-TAN the sameasitsdefaultexceptfort Io U threshold and learningrate. Wedecreasedthet Io Uthreshold from 0.5 to 0.3 toenablemorepositivesamplesduringtrain- ing and empiricallyset the learningrateto 0.001. Wetrain themodel for 100 epochs and report the testsetper for mance onthebestcheckpointon the validationset. 2 D-TANgives top-1 and top-5 recalls of 5.80% and 13.90% at Io U=0.3, respectively. Inaddition,wealsoablate the modeltoobtain per for mance by randomizing the video features ( visual) \u2212 andtextualfeatures( text)for NLQin Tab.11. \u2212 (b)Span-based Localization Network(VSLNet)[235]: Unliketraditionalapproachesinvideonaturallanguagelo- calizationworks,VSLNettreats the inputuntrimmedvideo asatextpassage,andusesaspan-basedapproachtoidentify therelevantsectionssemanticallyrelatedto the givennatural",
      "start_pos": 16170,
      "end_pos": 16682
    },
    {
      "chunk_id": 173,
      "paper_id": "ego4d",
      "text": "testsetper for mance onthebestcheckpointon the validationset. 2 D-TANgives top-1 and top-5 recalls of 5.80% and 13.90% at Io U=0.3, respectively. Inaddition,wealsoablate the modeltoobtain per for mance by randomizing the video features ( visual) \u2212 andtextualfeatures( text)for NLQin Tab.11. \u2212 (b)Span-based Localization Network(VSLNet)[235]: Unliketraditionalapproachesinvideonaturallanguagelo- calizationworks,VSLNettreats the inputuntrimmedvideo asatextpassage,andusesaspan-basedapproachtoidentify therelevantsectionssemanticallyrelatedto the givennatural languagequery.Atitscore,VSLNetfirstencodes the natural languagequery and videofeaturesusingacommon,shared Trans for mer[215]network. Next,ituses the encodedquery tothenattendtotherelevantpartsof the videoclip(akinto atextparagraph). Theattendedsections are fur the rrefined usingaquery-guidedhighlighting(QGH)strategybyextend- ingtheselection for egroundof the videobyahyperparamter tocapturemorevisualcontext.Pleasereferto[235]formore la V (cid:8) 2 D-TAN[236] 5.04 12.89 2.02 5.88 VSLNet[235] 5.45 10.74 3.12 6.63 tse T \uf8f1 2 D-TAN[236] 5.80 13.90 2.34 5.96 \uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2 \u2212 v t i e s x u t al 3 2 . . 4 2 6 9 1 6 0 . . 7 1 7 3 1 1 . . 7 3 8 2 4 3 . . 3 4 8 6 \u2212 VSLNet[235] 5.47 11.21 2.80 6.57 \uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \u2212 v t i e s x u t al 3 1 . . 0 8 5 0 7 5 . . 3 4 9 4 1 0 . . 4 9 5 0 4 2 . . 1 4 2 5 \u2212 Table 11.Per for manceof the NLQ base linesonval and testsplits. detailson the motivation and architecture. For our experiments,wemaintainconsistency with the other NLQ base lines and usepre-extractedfeatures for both thevideoclip(Slow Fastnetwork[70])andnaturallanguage query(BERT[52]). we use the implementationprovided by the authors 15 with the following changes: (a) Set the videofeaturessizeto 2304 dimensionstoaccommodate the featuresextracted from the Slow Fastnetwork,(b)Replace thetextencodertoafrozen,pretrained BERT[52]model, (c)Settheinternaldimensionof the multimodalnetworkto 128,andproject the pre-trained BERTfeatures from 768 to 128. Wetrain the model for 200 epochs and pick the model with the bestper for manceonvalsplit. Thecorresponding 15 https://github.com/Isaac Changhau/VSLNet 40 testper for manceof this VSLNet model isreportedin Tab. Table 12. Momentqueriesresultson the validationset and the 11,along with visual and textualablations. test set,measuredbym AP(%)atdifferentt Io Uthresholds. t Io Uthreshold 0.1 0.2 0.3 0.4 0.5 Average Momentsqueries base line validation set 9.10 7.16 5.76 4.62 3.41 6.03 test set 8.61 6.52 5.43 4.30 3.57 5.68 Weformulateamomentqueries base lineasatemporalac- tion detection method [141,229,237], plus simple post- processing. 5 levelsin the graphpyramidnetwork,each with temporal The MQtaskonlyexpectspredictions for the querycat- length 232,116,58,29,and 14 respectively. Wepre-define egories,whereas the temporalactiondetection task returns two base anchorsofsizes 4 and 12 for Level 1 andincrease the predictions for all categories. Therefore, we can first thesizesby 2 foreachdeeperlayer. Wetrain for 30 epochs use a temporal action detection method to predict for all withabatchsize 32 andlearningrate 0.0001. Ininference, categories,andonlyoutput the resultscorrespondingto the weonlyapplyper-category NMS with aconfidencethreshold querycategories. 0.0005. To predict all categories, we adopt a recent method VSGN[237],whichwasdesigned for temporalactiondetec- Experiments and results We show our baseline perfor- tioninthird-personvideos. we use VSGN with out the VSS manceintermsofm APin Table 12 andrecall@kx,t Io U=m component. Figure 34 illustrates the architecture. Ittakes in Table 13. avideo asinput,extractsfeatures for eachsnippetin the V Weprovidefurtheranalysison the averageprecisionre- videousinganetworksuchas Slow Fast[70],andfeedsthese sultsusing DETAD[9]. In Fig 35,weillustrate the propor- featuresintoagraphpyramidnetwork. Thegraphpyramid tionofeacherrortype for the falsepositivepredictions. It network contains a encoder and a decoder, where the en- shows that both localization and classification are respon- coderiscomprisedofmultiplelevelsofgraphconvolutional sible for the",
      "start_pos": 16632,
      "end_pos": 17144
    },
    {
      "chunk_id": 174,
      "paper_id": "ego4d",
      "text": "Table 13. avideo asinput,extractsfeatures for eachsnippetin the V Weprovidefurtheranalysison the averageprecisionre- videousinganetworksuchas Slow Fast[70],andfeedsthese sultsusing DETAD[9]. In Fig 35,weillustrate the propor- featuresintoagraphpyramidnetwork. Thegraphpyramid tionofeacherrortype for the falsepositivepredictions. It network contains a encoder and a decoder, where the en- shows that both localization and classification are respon- coderiscomprisedofmultiplelevelsofgraphconvolutional sible for the false positive, improving either can increase networks,and the decoderiscomprisedofmultiplelevels theoverallper for mancebyanontrivialamount. In Fig 36, ofde-convolutionalnetworks. Itisananchor-basedmethod wedemonstrate the per for manceofdifferentgroupsofmo- thatpre-definestemporalsegments for eachfeaturelevelas mentinstances basedonmoment duration and number of predictionreference. Itpredicts the scores and refines the instancesbelongingto the samecategorypervideoclip. We locationsof the anchorsintwostages. Inthefirststage,it notice that short moments tend to have low per for mance uses are gionproposalnetwork(RPN)from the decoderto eventhough the yarelargeinnumber. When the reare 2-3 predictclasslabels and regressboundaries for eachanchor; instancesinonevideo,they are easiesttodetect. inthesecondstage,itappliesaboundaryadjustmentmodule torefinetheboundaryoffsets base don the updatedanchors from the firststage. Italsohasstartness/endnesspredictions toprovideauxiliarysupervision and supplementscores for each predicted segment. Its output predictions are formu- M lated as \u03a6 = \u03c6 =(t ,t ,c ,s ) , where m { m m,s m,e m m }m=1 is the number of predictions, t and t are start time m,s m,e andendtimeof the mth predictionrespectively,c isthe m predictedcategory,ands istheconfidencescore. Formore m details,pleasereferto[237]. Given a query category c, the retrieval results for the momentqueries task are obtainedasfollows \u03a6 = \u03c6 =(t ,t ,c ,s ) c =c,1 m M) . c m m,s m,e m m m { | \u2264 \u2264 } (20) Implementation details For feature extraction, we use Ego 4 D\u2019s provided pre-extracted features using a Slow- Fast[70]networkpre-trainedon Kinects 400[108]at 1.87 featurespersecond. Thefeaturedimensionis 2304. Considering that the maximumclipleng this 8 minutes, which has 897 features, we make the input length of our network 928 framestocover the longestvideoclip. Wehave G 1 G 2 G 3 G 4 G 5 G 6 G 7 G 8 G 9 G 01 100 90 80 70 60 50 40 30 20 10 0 Top Predictions )%(nwodkaer Brorr E Background Err Localization Err Double Detection Err Confusion Err Wrong Label Err True Positive False Positive Profile 2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25 0.00 Error Type NPAm-egarev A )%(tnemvorpm I Removing Error Impact 1.2 0.8 0.8 0.8 0.1 Figure 35.Momentqueriesresults:falsepositiveanalysis.The errortypes are determinedbythet Io Ubetweenground-truth and predicted moments, as well as the correctness of the predicted labels,accordingto[9]. Backgrounderror: t Io U<1 e\u22125;confu- sionerror: 1 e\u22125 < t Io U < \u03b1,labeliswrong;wronglabelerror: t Io U>=\u03b1,labeliswrong;localizationerror:1 e\u22125 <t Io U<\u03b1, labeliscorrect,where\u03b1referstothet Io Uthresholds{0.1,0.2,0.3, 0.4,0.5}.\u2018G\u2019refersto the numberofground-truthinstances. 41 Table 13.Momentqueriesresultsonthevalidationset and the testset,measuredbyrecall(R)@kx,t Io U=m(%). m 0.3 0.5 0.7 k 1 3 5 1 3 5 1 3 5 Validation Set 33.45 51.26 58.43 25.16 39.46 46.18 15.36 22.67 25.81 Test Set 33.56 52.23 59.79 24.25 39.22 46.22 14.83 23.15 26.28 80 70 60 50 40 30 20 10 0 XS S M L XL XS S M L XL htur T dnuor G fo % Length #Instances 63.4 52.2 31.0 20.5 12.5 7.5 3.5 5.7 3.1 0.5 20.0 17.5 15.0 12.5 10.0 7.5 5.0 2.5 0.0 XS S M L XL XS S M L XL )%( NPAm-egarev A video clips, moving a step closer",
      "start_pos": 17094,
      "end_pos": 17606
    },
    {
      "chunk_id": 175,
      "paper_id": "ego4d",
      "text": "S M L XL htur T dnuor G fo % Length #Instances 63.4 52.2 31.0 20.5 12.5 7.5 3.5 5.7 3.1 0.5 20.0 17.5 15.0 12.5 10.0 7.5 5.0 2.5 0.0 XS S M L XL XS S M L XL )%( NPAm-egarev A video clips, moving a step closer to augmenting a user\u2019s episodicmemory. Momentqueriesinegocentricvideosisachallenging task dueto the long-taileddistributionofcategories and the large variationinmomentduration. Our base lineachievesarea- sonableresultaccordingto the metricrecall@kx,t Io U=m, whichevaluates the per for manceofeachquerycategory and doesnotrequirecorrectclassificationofallcategories. In contrast, itsaveragem APscoreof 5.96%islowwhenall 13.9 11.4 10.5 11.2 categories are evaluated.Accordingto the falsepositiveanal- 8.0 ysisin Fig 36,errorscausedbywronglabels are significant. 6.9 6.3 6.36 5.1 Amoresophisticatedclassifier for all can didatemoments 3.1 can be explored in future work. In addition, as shown in 0.1 Fig 36, theper for manceofshortmoments, whichoccupy a large proportion in the dataset, is not as good as that of Figure 36. Momentqueriesresults: sensitivityanalysis. Top: long moments. Therefore, improving short moments will Distribution of instance per action characteristic: length; # in- signifi can tlyimprove the overallper for mance. stances. Bottom: averagem APN (%)[9]ineachcharacteristic bucket.The\u2018length\u2019characteristicdividesallmomentinstances 5 buckets base don the momentsdurationinseconds:XS(0,10],S Contributionsstatement (10,60],M(60,180],L(180,300],and XL(300,inf].The\u2018#in- stances\u2019characteristicdividesallmomentinstancesinto 5 buckets Kristen Graumanled the Episodic Memorybenchmark and basedonthenumberofinstancesbelongingto the samecategory paper writing, wrote annotation instructions, contributed inonevideoclip:XS(0,1],S(1,3],M(3,10],L(10,20],and XL todataselection and taxonomy for mation,andco-advised (20,inf]. the VQbaselinedevelopment. Bernard Ghanemco-ledthe Episodic Memorybenchmark,managed base linedevelop- Discussion ment and evaluation for the MQand NLQtasks, andcon- tributed to the annotation instructions, data selection, and Visual queries presents a novel and challenging task for taxonomy for mation for the MQand NLQ data sets. Jackson objectlocalizationinegocentricvideos. While our proposed Hamburger contributed to the development of the annota- baselineachieves are asonablesuccessrateof 42.9%,itonly tioninstructions and taxonomiesof the NLQ,VQ,and MQ achieves a localization per for mance of 0.13 t AP and 0.06 datasetsalong with thedesignof the early VQbaselines. st AP.Fur the rmore, thebestper for manceisachieved with Santhosh Kumar Ramakrishnan led VQ data selection, 0%searchefficiency,andna\u00a8\u0131vetechniquestoimprove the annotation,analysis and auditing,contributedto the formu- searchefficiencyleadtodrasticper for mancereductions. We lation and annotationinstructionsof VQ,dataselection,and hope that this task will spurfutureresearchintoaccurate and implemented the VQbaseline. Vince Cartilliercontributed efficienttechniques for objectsearch. tothe VQ-3 Dformulation and annotationinstructions,led Natural language queries is a challenging multimodal VQ-3 D data selection, annotation, analysis and auditing, task that haswideapplicationsinhelpinguserssearch and and implemented the VQ-3 D baseline. Dhruv Batra co- retrieve relevant pieces of their episodic memory, thanks mentored Vince Cartillierondeveloping base lines and pro- to the flexibility of the queries. The per for mance of the vided guidance on 3 D scans using Matterport. Hyun Soo existingstate-of-the-artvideolocalization model shighlights Parkcontributedto 3 Dreconstructionofegocentricvideos theneedle-in-a-haystacknatureof the task, duetoshorter with respect to 3 D Matterport scans. Tien Do developed response windows of about 10 s in a large video clip of 8 algorithmstoreconstruct 3 Degocentriccameraposes with minutes. Wehope that the NLQ data setopens the doorto respectto 3 DMatterportscans. futureresearch that specializesinidentifying and retrieving James Hillisprovidedbackgroundknowledgeonhuman a large diversity of language queries in longer egocentric episodicmemoryfunction and contributedtoearlydiscus- 42 sionsonbenchmarkdefinition and annotation.Satwik Kottur led the designof NLQ data selection and annotationinstruc- tions,contributedto the NLQtask for mulation,coordinated NLQ data annotation and data analysis, implemented the VSLNet NLQ base line,wrotepartof the NLQsections.Men- meng Xudesigned and implemented",
      "start_pos": 17556,
      "end_pos": 18068
    },
    {
      "chunk_id": 176,
      "paper_id": "ego4d",
      "text": "James Hillisprovidedbackgroundknowledgeonhuman a large diversity of language queries in longer egocentric episodicmemoryfunction and contributedtoearlydiscus- 42 sionsonbenchmarkdefinition and annotation.Satwik Kottur led the designof NLQ data selection and annotationinstruc- tions,contributedto the NLQtask for mulation,coordinated NLQ data annotation and data analysis, implemented the VSLNet NLQ base line,wrotepartof the NLQsections.Men- meng Xudesigned and implemented the experimentpipeline forthe NLQtask,implementedseveral NLQmethods,did NLQresultanalysis and visualization,andwrotepartof the NLQsections. Michael Wraycontributedtoearly for mula- tionof the benchmarktasks,definitionsof NLQqueries and annotationinstructions,providedinput for datasetconstruc- tion and evaluationmetrics,andhelpedin the creationof the MQtaxonomy. Chen Zhaodesigned and implemented the MQbaseline, proposed and implemented the newmetric for MQ,wrote the MQsections,did MQresultanalysis and visualization, contributed to the formulation, data selection and annota- tioninstructionsof MQ.Tushar Nagarajancontributedto the MQformulation and annotationinstructions,developed the MQlabeltaxonomy,andled the dataselection and annota- tionof the MQdataset. Merey Ramazanovamanaged the datasets for the experimentsof MQand NLQ base lines,and assisted with the taxonomy for mation for the MQbaseline. Antonino Furnariprovidedkeypointfeatureextraction from the Matterport 3 Dpanoramas for the VQ 3 Dbaseline. 43 G.Hands and Objects Benchmark Thissectiondetails the Hands and Objectsbenchmarkin- (a) cludingdefinitions,annotations,baselinemodels and results. G.1 Motivation Inavideoofahumanoperating and manipulatinganobject (b) with their hands, there may exist an object state change, i.e.,thepointwherethestateof the objectsbeingoperated changes, either temporarily or permanently in a way that cannot be easily reversed. Examples of temporary state (c) change include turning on a machine, while examples of permanentstatechangesincludephysicalchangessuchas choppingatomatointopieces and chemicalchangessuchas mixingwater and cementpowdertoge the rtocreatea new Figure 37. Examples of object state change. (a) State change composition of cement. Some examples are illustrated in through construction: attaching to two metal plates results in a Figure 37. newobject. (b)Statechangethroughphysicalchange: cuttinga piece of wood results in two smaller pieces of wood. (c) State Theconceptofanobjectstatechangehas been explored changethroughchemicalreaction:combiningtwoobjects,water onlyinalimitedmannerin the videoliterature[8,45,69] andcementpowder,resultsina new object,cement. and the characterizationofstatechangeshasdependedon manybrittlevision-basedcomponenttechnologies,makingit difficulttoanalyzestatechangesat scale. Fortunately,inthe tions that requirerichhum and emonstrations,suchasrobotic lastdecadewe have seentremendousadvancesincomputer manipulation. visionalgorithms for understandingbothobjects and hands. Defining Object State Changes: This benchmark fo- Asaresult,webelieve that nowitistimetoinvestigate the cusesonidentifying and localizing the statechangeofan ideaofcharacterizingstatechangesat scale and indepth. object in an egocentric video. Specifically, a object state Whyisrecognizing the impactofagentsonobjects and change can berepresentedbythethreeaspectsin the video: environments so critical? We believe that underst and ing, temporal,spatial,andsemantic. recognizing,andreplicatingobjectstatechanges are anes- sentialaspectofcreatingartificialintelligence(AI)systems. Whilecurrent AIsystems have the abilitytoreplicatecertain Temporal: An object state change can be represented by typesofhumanactionssuchasassemblingfurniture[116]or three distinct temporal points in the video. (1) Point-of- cuttingtomatoes[200],mostsystemsdonotpossessagen- no-return: Thepoint-of-no-return(PNR)istheframe I pnr eralunderst and ingofhowtheenvironment and the objects in a video that identifies the beginning of an object state can betrans for medas are sultofinteraction. Underst and ing change that cannot be easily reversed. (2) Pre-condition: theimpactofinteractionsonobjects and the environmentis Thepre-conditionisdefinedassomeframe I pre thatmarksa animportantaspectofreasoning and can help AIsystems momentpriorto the state-changeinwhich the relatedobjects per for mmoreadvancedtasks. Forexample,underst and ing werevisible with inthefieldofviewof the camera. (3)Post- theimpactofinteractionson the environment can help AI condition: Thepost-conditionissomeframe I post atwhich systems relate multiple ways to achieve the same change, thecompletionofthestatechangeisvisibleafter the point- discoverefficientmethods for achievinggoalstates,recog- of-no-return. Thesethreeframesmark the distincttemporal nize the completion/incompletionofgoals[58,97],recover stagesof the objectstatechange:before and after the change, fromfailure,andlearn from mistakes. respectively. Thisproposalmatches the Rubicon Boundaries Inegocentricvideosspecifically,theobjectstatechanges proposedin[160]. offerrich and importantin for mation that are relatedtomany otherproblems. Forexample, theobjectundergoingstate Spatial: Anobjectstatechange can berepresentedby the changeinanegocentricvideo can",
      "start_pos": 18018,
      "end_pos": 18530
    },
    {
      "chunk_id": 177,
      "paper_id": "ego4d",
      "text": "same change, thecompletionofthestatechangeisvisibleafter the point- discoverefficientmethods for achievinggoalstates,recog- of-no-return. Thesethreeframesmark the distincttemporal nize the completion/incompletionofgoals[58,97],recover stagesof the objectstatechange:before and after the change, fromfailure,andlearn from mistakes. respectively. Thisproposalmatches the Rubicon Boundaries Inegocentricvideosspecifically,theobjectstatechanges proposedin[160]. offerrich and importantin for mation that are relatedtomany otherproblems. Forexample, theobjectundergoingstate Spatial: Anobjectstatechange can berepresentedby the changeinanegocentricvideo can implyhuman-centricin- boundingboxof the objectat the PNR,pre-condition and formationsuchashumanactivity and intention. Moreover, post-condition,along with anytoolsinvolvedinper for ming the state change of an object shown provides cues about the state change. Tools offer extended capabilities of the human-specificaf for dance and actionablein for mationofan actor\u2019shand,suchasusinganelectricsawtocutapieceof objectortool,which can notbeeasilyinferred from static woodinhalf. Theseboundingboxesrepresent the spatial images. Additionally,ajointunderst and ingofhumanhands dimensionsofhands,tools and the objectsundergoing the and the objectsundergoingstatechange can benefitapplica- statechange. 44 Semantic: Werepresentanobjectstatechangethrough the get task isactivityrecognition[180]. Inthe UT-Egocentric humanaction(verb),theobjectidentity(noun)and the type dataset(UT-Ego),subjectswearahead-mountedcamera and ofstatechangeapplied. Thesamestatechange can beper- per for mlongunscriptedactivitiesinside and outsideof the formedondifferentobjectsusingdifferenttools. Forexam- home,withatotalof 17 hours from 4 subjects(4-5 hoursof ple,cuttingapieceofwoodwi the lectricsaw and cuttinga continuouscapture for eachperson);thetarget task isvideo pieceofpaper with scissors are differentinteractions with summarization[130]. The UTEgocentric Engagement(UT differentobjects and differenttoolsbut the ybothresultin EE)datasetconsistsof 14 hoursofhead-mountedcamera thesameobjectstatechangeofbeingcut. videocapturedinpublicspaceslikemuseums, malls, and grocerystores,andisannotated for momentsofengagement bythecamerawe are rwith the environment. Inthe EGTEA+ G.2 Related Work dataset,32 subjectswearinghead-mountedcamerasinasin- Object State Changes: Existingapproaches for modeling glekitchenenvironmentcapture 28 hoursofvideo;thetask object states and/or their changes can be categorized into is to recognize 44 meal preparation activities [136]. The two research lines. The first deals with collections of im- EPIC-KITCHENS data setconsistsof 100 hoursofkitchen ages. Arepresentative data set for thispurposeis the MIT activitiesrecordedin 45 uniqueenvironments,withatotalof States dataset [103]. By considering object states as ob- 89,977 differentobjectinteractionsacross 97 verb and 330 jectattributes(e.g. burnt,sliced),thislineofworkstudies nounclasses;the task istorecognizeobjects and activities attribute-object composition, e.g. composition with con- andanticipateinteractionsin the nextmomentofvideo[43]. text [158], modeling attributes as operators [164], and an The Charades-Ego dataset consists of 34 hours of video architecture for compositionalreasoning[182]. from 71 participants,withbothfirst-andthird-personpaired Thesecondresearchlinedeals with video and viewsan instanceslabeled for 156 actions[201]. actionasastatetrans for mationovertime. Onedirectionis thediscoveryofobjectstates and/ormanipulatingactions, G.3 Benchmark Definitions e.g.inegocentric[45,69]andinstructionalvideos[8]. Fathi et al. [69] explore object state detection in video using a Wenowdefine the threetasks that comprise the Hands and weaklysupervisedapproach. Anotherdirectionis the mod- Objectsbenchmark. Thethree task scorrespondto the three elingofstatetransitions. Zhouetal.[244]studytemporal aspectsofobjectstatechangesdescribedabove,namely,the trans for mationsofasingleobjectstateintime-lapsevideos. temporal,spatial and semanticaspectsofastatechange. Wangetal.[223]proposeto model statetrans for mationsin (1) PNR Temporal Localization. The goal of Point-of- ahigh-levelfeaturespace with Siamesenetworks. Doughty no-return (PNR) Temporal Localization is to predict I . pnr et al. [55] leverage natural language and treat adverbs as Onepossible for mulationistoview this problemasaper- modifiersforstatetrans for mations. Intermsofapplications, frame classification problem, predicting the Point-of-no- Changetal.[30]showstatetrans for mations can beutilized return frame within a short video clip. The per for mance forprocedureplanning. is evaluated only on the videos that contain object state change,andismeasuredby the absolutetemporalerrorof Human Hand Action Datasets: Several video datasets I predictioninseconds. have been proposed for humanh and actionrecognition. The pnr The PNRwasfirstdiscussedby P.Gollwitzerinhiswell- Yalehumangrasping data set[25]focusesonhumangrasping cited handbook of behavior [89]. Specifically, the book behavior and consistsof 27.7 hoursofannotatedvideos. The proposes the Rubicon Model of Action Phases, focusing Something-Something data set[90]consistsof 220,847 short onhand-objectinteraction. Actionphases are delimitedby videosannotated with 174 categoriesofgeneralh and-object threetransitionpoints: initiationofpriormotion,PNR,and interactions. The Jester data set[214]provides",
      "start_pos": 18480,
      "end_pos": 18992
    },
    {
      "chunk_id": 178,
      "paper_id": "ego4d",
      "text": "pnr The PNRwasfirstdiscussedby P.Gollwitzerinhiswell- Yalehumangrasping data set[25]focusesonhumangrasping cited handbook of behavior [89]. Specifically, the book behavior and consistsof 27.7 hoursofannotatedvideos. The proposes the Rubicon Model of Action Phases, focusing Something-Something data set[90]consistsof 220,847 short onhand-objectinteraction. Actionphases are delimitedby videosannotated with 174 categoriesofgeneralh and-object threetransitionpoints: initiationofpriormotion,PNR,and interactions. The Jester data set[214]provides 148,092 short goalachievement. Thiswaslaterexperimentallyassessedby videos in 27 hand gesture types. Wang et al. [220] con- ourpreviouswork[160],where PNRannotationswereac- structasyn the ticvideo data setofhuman-objectinteraction quired for threeegocentric data sets,demonstratingincreased throughrenderingh and and object CADmodels. Therecent accuracyofannotations(see Fig.10 in[160])andimproved Human Hands data set[198]annotates 100 Ksingleframes robustnessintrainingmodels(see Sec.5 in[160]). Below, fromweb-basedvideos,focusingonh and interactions and wefind PNRcloselyaligns with the narrationtimestamps theoffsetbetweentheh and and the interactingobjectduring thatweindependentlycollected,suggesting PNRisanatural interaction. timepoint for humanunderst and ing(andthusnarration)of Several egocentric video datasets capture daily living theinteraction. activitiesbypeople[43,130,136,180,201,210]. Inthe Ac- tivitiesof Daily Living dataset(ADL),subjectswearchest- (2) State Change Object Detection. We define a State mountedcameras and per for munscriptedactivitiesathome, Change Objectas the object that ismanipulatedbyaperson withatotalof 10 hoursofvideo from 20 participants;thetar- andundergoesachangeinitsstate. Thegoalof this taskis 45 topredict the 2 Dboundingboxesof the State Change Object toselectthreecriticalframesintime: PNR,PRE,and POST. in Point-of-no-return frame I given three frames: Pre- Weask the annotatorstostart with the PNRframe that iden- pnr condition I , Point-of-no-return I , and Post-condition tifiesthebeginningof the statechange. Thisframeisless pre pnr I . We expect that a good solution to this task would ambiguous and helpsprovidethecontext for the interaction. post incorporate the visual information before and after state Wethenasktheannotatorstolabelaframepriorto the state change to detect the State Change Object. The detection change(PRE)andaframeafterthecompletionof the state per for manceisevaluatedon the boundingboxesestimatedin change(POST).Note that the PREand POSTframes are not the Point-of-no-returnframe I andmeasuredby Average uniquelydefined. Welet the annotatorspickany,aslongas pnr Precision(AP). therelevantobjects are fullyvisiblewithin the fieldofview ofthecamera. (3)Object State Change Classification. Thetaskof Ob- ject State Change Classificationclassifiesashortvideoclip Preperiod. Next,welabelboundingboxes for the hands, to a state change type. With N object state change types tools, and objects, as well as the category names for the defined,objectstatechangeclassificationisessentiallyan tools and objects. Wedo this intwosteps. Firstwelabel (N+1)-wayclassificationproblem,where the oneadditional the frames in the pre period, starting at PNR and going category is \u201cwithout state change.\u201d Object State Change backwardto the preframe. Thevideoframes are reversed Classificationisevaluatedbyclassificationaccuracy. and the annotators can play the video.Wefind that itiseasier tostart from the PNRframesince the hands and objects are clearlyvisible. Tospeeduph and boxlabeling,weinitialize G.4 Data Selection theh and boxes with apre-trainedobjectdetector[198]and Nextwedescribe our data selectionprocedure and annotation ask the annotatorstocorrectthese. pipeline, and we present the analysis of the data for the Postperiod. Finally,weask the annotatorstolabelspatial objectstatechangebenchmark. Webeginbydescribing our annotations and categories for the postframe. Asbefore,we procedure for selecting the subsetof data toannotate for this firstpresent the annotators with the PNRframe. Note that benchmark. inthiscase the PNRframeisalreadylabeledwhichhelps Westartwithalargepoolofvideosannotated with high- identifythehands and objectstolabelin the postframe. levelscenariolabels(e.g.,gardening,cooking,landscaping, etc.)andnarrations. Weassesseachscenarioon the scale G.6 Data Analysis of 0 to 3 based on how likely it is to contain hand-object interactions (e.g., 0 for \u201cwatching tv\u201d, 3 for \u201ccarpentery\u201d, Finally,wepresent the analysisof our annotations. etc.).Wethensample data toannotatefollowing the resulting Critical frames. In Figure 40 we show the temporal dis- scenariodistribution. Givenascenario and atargetnumber tributionofcriticalframeswithin the 8 secondh and-object ofhours,wesampleclipsr and omlyinahierarchicalfashion: interactionsnippets. First,weobserve that the PNRframe wefirstsampleaparticipant,",
      "start_pos": 18942,
      "end_pos": 19454
    },
    {
      "chunk_id": 179,
      "paper_id": "ego4d",
      "text": "to contain hand-object interactions (e.g., 0 for \u201cwatching tv\u201d, 3 for \u201ccarpentery\u201d, Finally,wepresent the analysisof our annotations. etc.).Wethensample data toannotatefollowing the resulting Critical frames. In Figure 40 we show the temporal dis- scenariodistribution. Givenascenario and atargetnumber tributionofcriticalframeswithin the 8 secondh and-object ofhours,wesampleclipsr and omlyinahierarchicalfashion: interactionsnippets. First,weobserve that the PNRframe wefirstsampleaparticipant, thenavideo, andfinallya 5 distributioniscenteredaround the middleof the 8 second minuteclip from the video. Ifthevideoisshorterthan 5 min snippet. Interestingly,thiscloselyaligns with the narration wetake the wholevideo. Foreachscenario,webalance the point(4 smark). Next,wesee that mostof the pre and post data across universities to maximize geographic diversity. framescomeshortlybefore and after the PNRframe,respec- Theresultingscenario and universitydistributions are shown tively,highlightingthequicknatureof the sestatechanges, in Figure 38. Intotal,our data sethas 120 hoursrepresenting andthus the challengein this benchmark.Wealsonoticetwo 53 scenarios,7 universities,and 406 participants. additionalmodes for pre and postframes that comeat the start and the endof the 8 sinterval,respectively. Thesecorre- G.5 Data Annotation spondtolongrepetitiveactions that startbe for eorcontinue past the videosnippet(e.g.,knitting). Weannotateh and-objectinteractionscorrespondingtoeach narration within the selected 5 minute clips. We use the Hands and objects. Ourbenchmarkcontainsalargenum- taxonomy from Section D.3 for semantic verb and noun berofhands and objectsannotated with boundingboxes. In labeling.Theannotationpipelineconsistsofthreesequential total, we have 825 K bounding boxes, including 245 K \u223c \u223c stages: criticalframelabeling,pre-periodlabeling,andpost- forlefth and, 260 Kforrighth and, 280 Kforobjects,and \u223c \u223c periodlabeling. 40 Kfortools. In Figure 41 and Figure 42,weshow the \u223c distributions of box sizes and locations, respectively. We Criticalframes. Givenanarration,wecreatean 8 second observe that our data containshands and objectsatavariety videosnippetcenteredat the narrationtimepoint and present ofsizes and locations. itto the annotators. Weask the annotatorstofirstread the narration and select a corresponding verb from the taxon- Actions. Oneofthefeaturesof our benchmarkis the diver- omy. Theannotators can thenplay the videoback and forth sityofinteractions. Wefocusonlow-levelatomicactions 46 (a) Scenarios (b) Universities Figure 38.Numberofhours.Weshowthedistributionof the numberofh our sacrossscenarios(left)anduniversities(right) Figure 39.Labeledactions.Distributionofverbs(left)andnouns(right)inannotatedactioninstances.Top 45 verbs and nouns are shown forclarity.See Section D.3 formoredetails. Figure 42.Hand and objectlocations.Distributionofbounding boxcenters.Showninnormalizedimagecoordinates. Figure 40. Criticalframes. Distributionofcriticalframetimes. Shownrelativeto the 8 shand-objectinteractionsnippet. Wenote that ourobjects are commondailyobjects that are nottypicallypresentinobjectdetection data sets(e.g.,442 outof our 478 objectcategoriescovercategoriesbeyond the 80 COCO[143]categories). G.7 Baselines: Object State Change Classification and PNRTemporal Localization Figure 41. Hand and objectsizes. Distributionofboundingbox sizes.Shownintermsofthesqu are rootof the boxareas. Wepresent the implementationofseveral base linemethods for the Object State Change Classification and PNR Tem- poral Localizationtasks. Among the implemented base line ratherthanhigh-levelactions. Weshow the distributionof models,ingeneral the reareoneortwotypesofoutputnet- verbs(Figure 39,left)andnouns(Figure 39,right). Wesee workheads: aclassificationhead for the videoclipused for thatwe have alargenumberofverbscorrespondingtocom- statechangeclassification,and/oraper-frameclassification monmanipulationactions(e.g.,put,take)andanaturallong head for temporallocalization. One can choosetotraintwo tail. Theobjectdistributionfollows the samegeneraltrend. modelsseparately,oruse the samebackbone model buttwo 47 networkoutputheads and train the joint model with amulti- Table 14. Numberofpositive and negativevideoclipsofobject tasklossfunction. Thefollowing base linemethodsincludes statechangeintrain,validation and testsplits. bothtypesof model designs: I 3 DRes Net-50. we use I 3 D[29]with Res Net-50[95]as Split Positive Negative Total Train 20,041 21,044 41,085 backbonearchitectureof the model for both the Object State Val 13,628 14,720 28,348 Change Classification and the PNRTemporal Localization Test 13,561 14,870 28,431 tasks. The Res Net backbone is followed by two network outpu the ads: astatechangeclassificationheadanda PNR temporallocalizationhead. Thestatechangeclassification Table 15.Resultsof State Change Classificationaccuracy(%). head is produced by global average pooling on the entire spatiotemporal feature tensor followed by a classification Baseline Val",
      "start_pos": 19404,
      "end_pos": 19916
    },
    {
      "chunk_id": 180,
      "paper_id": "ego4d",
      "text": "Classification and the PNRTemporal Localization Test 13,561 14,870 28,431 tasks. The Res Net backbone is followed by two network outpu the ads: astatechangeclassificationheadanda PNR temporallocalizationhead. Thestatechangeclassification Table 15.Resultsof State Change Classificationaccuracy(%). head is produced by global average pooling on the entire spatiotemporal feature tensor followed by a classification Baseline Val Test layer. The PNRtemporallocalizationheadisproducedby Always Positive 48.1 47.7 per-frameaveragepoolingfollowedbyaclassificationlayer. Bi-directional LSTM[91] 65.3 63.8 Theoveralltraininglossofthemodelis the combinationof I 3 DRes Net-50[29] 68.7 67.6 thelossoftwoheadswhich are bothcross-entropyloss for classification. Boundary Matching Network (BMN). We use BMN [140]asa base line for the PNRTemporal Localization task. negativeclips are balancedinnumber. BMNisatemporalsegmentdetectionmethod base doncon- Besides the above learnable baselines, for object state fidencepredictionofdensetemporalsegmentproposals. We changeclassification,wealsopresenttheresultof the naive viewthestartofthevideoasthestartof the temporalseg- baseline of always predicting the positive category as the ment and Point-of-no-return I astheendof the temporal pnr prediction. Forthe PNRtemporallocalization task,wead- segment,sowe canconvert the problemoflocalizing Point- ditionallypresenttheresultof the naive base lineofalways of-no-return I totheproblemofdetecting the temporal pnr selectingthecenterframeof the trimmedvideoas the PNR segment. Inourimplementation,BMNuses Res Netas the frame,giventhepossiblecentrebiasof the data. backbone model. Fur the rmore, BMNisonlyused for the PNRtemporallocalization task. Theresults for objectstatechangeclassification task are Slow Fast+Perceiver. Weimplementa base line model illustratedin Table 15. Thenaive base lineofalwayspositive whosearchitectureconsistsof Slow Fast[70]and Perceiver predictionyieldsstatechangeclassificationaccuracyofclose [105] for both object state change classification and PNR to 50%. All the learnable baselines outperform the naive temporallocalization. Slow Fastactsas the videodeepfea- baseline and achieveaccuracyofmorethan 60%while Bi- tureextractor. Thefeatures are the npassedtoa Perceiver directional LSTM base lineachieves the bestper for mance. model. Similarto the previous BMNmodel,the Slow Fast Thisshows that the learnable base lines can learnmeaningful +Perciever model isonlytrained for temporallocalization informationaboutobjectstatechange,though the reisclearly task. Thetraininglossofthemodelis the cross-entropyloss stillspace for improvement. Onechallengein this taskis forper-frameclassification. thatthereisverylargevarianceintermof the typesofobject statechanges and objectscontainedin the videos. Bi-directional LSTM. Weimplementa Bi-directional LSTM model[91]forboth the objectstatechangeclassifica- The results for the PNR temporal localization task are tion and PNRtemporallocalization. Wefirstpassindivid- illustratedin Table 16. Thenaive base lineofalwayspredict- ualframestoa Res Netmodel[95]toextractdeepfeatures. ing the centerframeyieldsatemporallocalizationerrorof The sequence of per-frame features is then passed to the around 1.1 seconds. Otherlearnable base lines can achieve Bi-directional LSTMasinput,with the outputsenttoboth bettertemporallocalizationerrorofaround 0.85 secondsor the per-frame classification head and the whole-sequence lesswhichshows the baselinemodels can learnmeaningful classificationhead. Theoveralltraininglossof the model information for temporallocalizationofobjectstatechange. isthecombinationof the lossoftwoheadswhich are both Note that the Slow Fast+Perceiver model achieves the best cross-entropyloss for classification. temporallocalizationper for manceof 0.425 secondsonvali- For the objectstatechangeclassificationtasks,inthecur- dationset and 0.489 secondsontestset,whichhighlights the rentversionwefocuson the two-wayclassificationproblem necessityofusingattention-basedmechanismto model the ofwhetherthereisaobjectstatechangein the egocentric changeofobjectstate. Onechallenge for this task isthatin video.In Table 14,weillustrate the numberofpositivevideo someactions,e.g.,cuttingapieceofpaper with scissors,the clips that containsanobjectstatechange and the numberof statechangeofanobjectdoesnotnecessarilycausesignifi- negativevideoclips that donotcontainobjectstatechange cantchangeofvisualappearance and the reforeitisdifficult inthetrain/val/testsplits. Inallthreesplits,thepositive and tolocalize the PNR. 48 Table 16.Resultsof Point-of-no-returntemporallocalizationerror Table 17. Number of State Change Object and hand bounding (seconds). boxesintrain,validation and testsplits. Baseline Val Test Split State Change Object Hand Always Center Frame 1.032 1.056 Train 19,347 33,254 BMN[140] 0.780 0.805 Val 12,912 22,098 I 3 DRes Net-50[29] 0.739 0.755 Test 13,118 22,576 Bi-directional LSTM[91] 0.790 0.759 Slow Fast[70]+Perceiver[105] 0.804 0.828 Table 18.Resultsofsingle-frame State Change Object Detection. Theper for manceismeasuredin Average Precision(AP). G.8 Baselines: State Change Object Detection Baseline Backbone AP AP 50 AP 75 Faster-RCNN[190]",
      "start_pos": 19866,
      "end_pos": 20378
    },
    {
      "chunk_id": 181,
      "paper_id": "ego4d",
      "text": "19,347 33,254 BMN[140] 0.780 0.805 Val 12,912 22,098 I 3 DRes Net-50[29] 0.739 0.755 Test 13,118 22,576 Bi-directional LSTM[91] 0.790 0.759 Slow Fast[70]+Perceiver[105] 0.804 0.828 Table 18.Resultsofsingle-frame State Change Object Detection. Theper for manceismeasuredin Average Precision(AP). G.8 Baselines: State Change Object Detection Baseline Backbone AP AP 50 AP 75 Faster-RCNN[190] Res Net-101[95] 13.4 25.6 12.5 Whileweexpect that newmethodsdeveloped for the tasks DETR[27] Res Net-50[95] 15.5 32.8 13.0 ofstatechangeobjectdetection will utilizeallthreeinput Center Net[241] DLA-34[233] 6.4 11.7 6.1 frames (pre, PNR, post), in this initial stage of the bench- 100 DOHModel[199] Res Net-101[95] 10.7 20.6 10.1 mark, we only evaluate single-frame detection baselines, whereonly the PNRframe I isusedasinput. Welimited pnr ourinputasmanymethods for objectdetection are primarily the 100 DOH model pre-trainedon 100 DOH data set[199] designedtowork with asingleimage. to first detect hand bounding boxes and then predict state Wepresent the implementationofseveral base linemeth- changeobjectboundingboxesgiven the hands. ods for the state change object detection task. In general, We show the number of state change objects and hand the baseline models for the task can be categorized into boundingboxescontainedin our data setin Table 17. The two types: (1) directly detecting the bounding box of the resultsofsingle-frame State Change Object Detection are statechangeobjectincluding Faster-RCNN[190],Center- illustratedin Table 18. All base linesstruggleindetecting Net[241],and DETR[27],and(2)detectingh and bounding the State Change Objects with onlyoneframeasinputas boxesfirst the npredictstatechangeobjectboundingboxes an APof 8-14%. There are severalchallengesin this task. given the handssuchas the 100 DOHmodel[199]. Specifi- First,theboundingboxsizesofstatechangeobjects have cally,the base linemethods are the following: largevariance. Forexample,thesizeofstatechangeobjects Faster-RCNN[190] isatwo-stageanchor-based 2 Dob- can beaslargeashalfofimagein the actionof\u201cpainting the ject detector on a single RGB image. In its classification wall\u201dandassmallasafewpixelsin the actionof\u201cigniting head,thestatechangeobjectis the onlypositivecategory. the match.\u201d Second, when only using one frame as input, Wetrain Faster-RCNNon our benchmark and useittodi- thedetection model sdidnotconsider the changeofobject rectlydetect the boundingboxesofstatechangeobjectsin appearance across different frames. As future work, we PNRframes. hope the researchers will investigateusingmodels that take Center Net[241] isano the robjectdetectionmethodon multipleframesasinput and perhapsdevelopframeworks asingle RGBimage. Itestimatesobjectkeypointstofind thatincorporatetrackingorassociation. objectcenterpoints and regressesallo the robjectproperties, suchassize,3 Dlocation,andorientation. Wetrain Center- G.9 Discussion Net to directly detect the bounding boxes of state change objects. Thisnovelbenchmarkexploresthreeaspectsofobjectsun- DETR[27]isanobjectdetection model onasingle RGB dergoingstatechangesas are sultofh and manipulation: the image base don Trans for mer[216]. Itviewsobjectdetection when(i.e. temporallocalizationofstatechange),where(i.e., as a direct set prediction problem and uses a trans for mer spatial localization of objects that undergo change) and encoder-decoderarchitecturetoproduceasetofobjectpre- what (i.e., semantic notion of action and object trans for- dictionsincludingboundingboxin for mationaswellasother mation). As a first step, we have explored these indepen- information such as category. We train DETR to directly dentlyusingreadilyavailablelocalization and classification detect the boundingboxesofstatechangeobjects. methods. However,approaches that aimtotackle this chal- 100 DOHModel[199] firstdetects the boundingboxes lenge should focus on jointly underst and ing the manipu- of the human hand and objects as well as the relational lation with its spatio-temporal impact on objects as these vectors that links from each hand bounding box center to aretrans for med. Forexample,knowinganobjectisbeing anobjectboundingboxcenter. Thefinalpredictionof the splitshouldofferastrongpriorto the PNRlocalisation and objects are decidedas the objectpredictions that satisfies the detect two or more bounding boxes after the point-of-no- both the predictionsofh and and relationalvectors. we used return. Suchmethods that tackle the",
      "start_pos": 20328,
      "end_pos": 20840
    },
    {
      "chunk_id": 182,
      "paper_id": "ego4d",
      "text": "that links from each hand bounding box center to aretrans for med. Forexample,knowinganobjectisbeing anobjectboundingboxcenter. Thefinalpredictionof the splitshouldofferastrongpriorto the PNRlocalisation and objects are decidedas the objectpredictions that satisfies the detect two or more bounding boxes after the point-of-no- both the predictionsofh and and relationalvectors. we used return. Suchmethods that tackle the dependenciesbetween 49 thetasks are yettobedeveloped. Wehope this benchmark willspurinnovativeapproaches that bridge the gapbetween actionperception and the impactofactionsonobjects and environments. G.10 Contributionsstatement Kris Kitani helped formulate and write the object state changebenchmark,designed the annotations and tasks for the HObenchmark. Dima Damenhelped with the formu- lation and writing of the object state change benchmark, designed the annotations for the Hands and Objects(HO), and Forecastingbenchmarks. Ilija Radosavoviccoordinated HO data annotation, annotation analysis, and contributed to the definition and writing of the HO benchmarks. Ro- hit Girdharhelpedcoordinate the HOdataannotation and annotationanalysis. Abrham Gebreselasieadapted the Slow- Fast+Perceiver model for PNRtemporallocalization.Qichen Fuimplementedallof the statechangeobjectdetection base- lines. Raghava Modhuguimplemented the BMN base line for PNRtemporallocalization.Kristen Graumancontributed tothe for mulation and writingofobjectstatechangebench- mark. Siddhant Bansalhelped with the processingof HO data,developmentof HOdataloader for PNRtemporallo- calization and implemented the I 3 D Res Net-50 baselines. Xingyu Liuwas the leadcoordinator and mentorof the HO benchmark base lineimplementations,andalsocontributed to the definition and writing of HO benchmarks. Xuhua Huangdevelopedof the initial Slow Fast+Perceiver model. Yifei Huangimplemented the Bi-directional LSTM base line forthe PNRtemporallocalization and statechangeclassifi- cation. 50 H.Audio-Visual Diarization Benchmark thevisualstream\u2014somesuchnoiseisstructured and rele- vant for underst and ing the context and semanticcontentin Thissectiondetails the Audio-Visual Diarization(AVD) thescene. benchmark task definitions, annotations, baseline models, andresults. Asnotedin Appendix B,the AVDbenchmark usesonlyvideowhereinformedconsent for capturingiden- H.2 Related Audio Visual Learning Work titiesisexplicitlycollected from allparticipantsin the scene, includingfaces and voice. Thereis are centresurgenceofworkonaudio-visualanalysis within and beyond the computervisioncommunity. These workstacklevariousaspectsofaudio-visualunderst and ing, H.1 Motivation includings our celocalization,cross-modalfeaturelearning, Egocentrichumanperceptionisdrivenbyinferringuseful audio spatialization, and audio source separation, as we information from all the primarysenses. Whilevisualscap- brieflyreviewnext. tured by the eyes are one of the main information chan- On audio-visual detection and tracking, recent works nels, sounds as captured by the ears are equally relevant. on multimodal learning explore ways to localize sounds In particular, for underst and ing humans\u2019 interaction with inagivenvideoframe[14,197,212]andinferspatialized the environment from the first-person perspective, detect- sound from video[80,161]. Capturing and processingmulti- ing, localizing, tracking (both in 3 D space and time) and channelaudioisbeingstudiedinaudio and microphonearray underst and ingsoundsbycombining the necessaryacoustic signal processing communities, specifically from a user\u2019s information with visualsignalsbecomesevenmorecritical. perspectivetounderst and agivenscene[101,169]. Building Severalpsychophysicalstudies have proven that humans are upon these, it is reasonable to expect that human-centric remarkablygoodatlocatingwhereasoundcamefromin 3 D audiohasin for mationcontent that can directlyimprovevi- space with respectto the irheadposition[156]. Sensitivityof sualobjectcategorization and recognition. Indeed, thisis humanstomovingsoundsinhorizontal and verticalplanes observedinsomerecentworkwhereaudiodisambiguates isalsowelldocumented[117,178]. certainvisuallyambiguousactions[110,226]. Foractions For a long time, the computer vision community has andactivity, audioevents can alsobedirectlyusedtoper- studyied the problemofpreciselocalizationofobjects and formsummarization[13]. Inparticular,capturingego-driven people, robustly tracking and segmenting them using im- actionsandactivity and separating the mfromgeneralback- ages. Inthiseffort,weaimtobringaudio(humanspeech groundactions and activityin the sceneiscritical. inparticular)into the mix. Trulyaudio-visualsystemsnot Alternatively,visualin for mationhas been usedtodisam- onlyenablerichercapture and analysisof the environment biguatecertainaudio task slikespeechtranscription. Specifi- (and a user\u2019s interaction with it), but they also help build cally,audio-visualspeechrecognitionhasreceivedalotof technologies for visuallyoracousticallyimpairedusers(e.g., attention",
      "start_pos": 20790,
      "end_pos": 21302
    },
    {
      "chunk_id": 183,
      "paper_id": "ego4d",
      "text": "segmenting them using im- actionsandactivity and separating the mfromgeneralback- ages. Inthiseffort,weaimtobringaudio(humanspeech groundactions and activityin the sceneiscritical. inparticular)into the mix. Trulyaudio-visualsystemsnot Alternatively,visualin for mationhas been usedtodisam- onlyenablerichercapture and analysisof the environment biguatecertainaudio task slikespeechtranscription. Specifi- (and a user\u2019s interaction with it), but they also help build cally,audio-visualspeechrecognitionhasreceivedalotof technologies for visuallyoracousticallyimpairedusers(e.g., attention in the last decade with multiple studies suggest- hearingaids,augmentedreality). ing that automaticspeechrecognition(ASR)couldbenefit Thegoalof this benchmarkistohelpadvance the state from visuals of the scene, or other non-acoustic informa- oftheartinaudio-visualunderst and ingfrom the egocentric tion[5,104]. Asshowninhere, itisreasonabletoexpect viewpoint. Specifically,fromaconversationalperspective, thatlipreading from afirstpersonpointofviewwouldalso thebenchmarkaimstounderst and whoistalkingwhen,and benefit ASRsystems. aboutwhat.Fromavisualperspective,wearealsointerested Inaddition,audio-visualcross-modallearningmaypro- inwhere the speakerislocated. Givenanegocentricvideo, videinsight and solutionstooneof the oldestproblemsin theproposed task srequireextracting the spatiallocationof egocentric human communication ecology, referred to as the speakers, their voice activity across the length of the cocktailpartyproblem(CPP).Theessenceof CPPis\u201cHow video,andthecontentof the irspeech. dowerecognizewhatonepersonissayingwheno the rsare Egocentric data presentsseveraluniqueattributesto this speakingat the sametime?\u201d Humanlistenersmustperceptu- problem. Firstly, sounds our cesmaybevisible with inall, allyintegrate the simultaneoussoundsoriginating from one some,ornoneof the visualframes,dependingon the irmove- person\u2019svoice(e.g., harmonics and speech formants)and ment within the scene and the movement of the camera segregatethese from theconcurrentsoundsofo the rtalkers. wearer. Secondly,although the camerawe are risnevervisi- Insuchsituations,humansleveragevisualin for mationsuch ble(due the headmountedcameradevice)they are clearly asfromlipmovementstobetterunderst and,while the irau- audible and in fact often amplified compared to the other ditory system helps with focusing on a particular speaker conversationparticipantsduetotheclosenessto the micro- characteristic while ignoring other speech/noise. Recent phone that captures the video. Third,naturaldynamicsin workonaudio-visualdiarization[83]andmultimodalsource thescene(camerawe are rwalking,running,rapidchanges separation from videoshow that CPP and itsvariations can inheadmovementetc.) addsignifi can tblur and distortionto benefit from visualsignals[6,57,79,81,82,171,238]. 51 Fur the rmore,humans are prettygoodinunderst and ing annotated audio-based speech activity collection of AVA thecontextofaconversationevenwhenwords are incom- 1.0 third-personvideos,andexplicitlylabels 3 background prehensible. They are abletofillin the missingdetailsusing noiseconditions,resultinginapproximately 46,000 labeled their context knowledge. This can be extended to sound segments spanning 45 hours of data. AVA active speaker sources that are non-humansaswell. Foramoredetailed associatesspeakingactivity with avisibleface,resultingin accountof CPPpleasereferto[17]. Fullyaddressing CPP 3.65 million frames labeled across approximately 39,000 requires not only identifying and separating the different facetracks. sounds our cesin the scene,butalsounderst and ing the audi- AVDIAR:[84]Theclosestegocentric data set for audio- toryattentionof the camerawearer\u2014ino the rwords,which visual diarization is AVDIAR. It consists of 23 staged se- sounds our ceistheuserattendingtoat the moment,orwhich quences,witheachsequencedurationranging from tensec- onemaytheuserwanttoattendtoin the nearfuture. ondstothreeminutes(atotalof 27 minutesofvideo). Each sequencecomprisesof 1-4 speakerssomestanding and some walkingaroundin the visual FOV and havingaconversation. H.3 Related Datasets and Benchmarks Thecaptureisdoneviaaheadmountedcaptureonadummy EPIC-Kitchens:[42,44]EPIC-Kitchensisamong the most head. widely known ego-centric dataset with first-person view EASYCOM:[53]EASYCOMis are cent data setopen events and annotations. The dataset comprises of multi- sourced for the purposeofboostingegocentricaudio-visual faceted,audio-visual,non-scriptedrecordingsinnativeen- learning research with a focus on multi-channel data and vironments, i.e. the wearers\u2019 homes, capturing all daily CPP.The data setcorrespondsto 5 hoursofconversational activitiesin the kitchenovermultipledays. The data setis content with 3 5 participants in a closed room setting. \u2212 100 hours,20 Mframes,90 Kactionsin 700 variable-length The content involves playing games, ordering food from videos, capturinglong-termunscriptedactivitiesin 45 en- a menu, and a general discussion on a prespecified list of vironmentsusinghead-mountedcameras. Annotations are topics. Duringtherecordingof the conversations,restaurant- collected using a Pause-and-Talk narration interface. The likenoisewasplayedonloudspeakersin the roomtomimic dataset is widely used in action recognition,",
      "start_pos": 21252,
      "end_pos": 21764
    },
    {
      "chunk_id": 184,
      "paper_id": "ego4d",
      "text": "Kactionsin 700 variable-length The content involves playing games, ordering food from videos, capturinglong-termunscriptedactivitiesin 45 en- a menu, and a general discussion on a prespecified list of vironmentsusinghead-mountedcameras. Annotations are topics. Duringtherecordingof the conversations,restaurant- collected using a Pause-and-Talk narration interface. The likenoisewasplayedonloudspeakersin the roomtomimic dataset is widely used in action recognition, action detec- arealrestaurantscene. The EASYCOMcapturedeviceuse tion, action anticipation, cross-modal retrieval, as well as glasses with 6 micsattachedto the frame. Althoughrichin unsuperviseddomainadaptation for actionrecognition. termsofmulti-channelegocentricacousticcontent,thesetup Vox Celeb:[40,165]Vox Celeb 1 and 2 compriserecord- isconstrainedintermsofrealism,the data isnotin the wild, andmostimportantly the datasetissmall. ings of more than 6 K speakers spanning a wide range of differentethnicities,accents,professions,andages. Thedata Existingaudio-visual data setsvs. Ego 4 D:Oftheseex- is non-egocentric and is annotated for active speaker face isting data sets,EPIC-Kitchens,AVDIAR and EASYCOM bounding boxes, face tracks, and anonymous person IDs. areegocentric. However,EPIC-Kitchensfocusesonsolitary Vox Celeb 2 inparticularisdefined for boostingresearchin activity by the camera wearer, and neither the video nor speakerrecognition,anditcontainsoveramillionutterances. annotationsaccommodateaudio-visualconversationtasks Videos included in the dataset are shot in a large number requiringmultiplepeople. Although EASYCOMcontains of challenging visual and auditory environments. These audio-visual conversation, it is a small dataset containing includeinterviews from redcarpets,outdoorstadiums and partlyscriptedconversations that are notin-the-wild. The quietindoorstudios,speechesgiventolargeaudiences,ex- participantsin the sessionsalsodonotmovearound. AV- cerpts from professionallyshotmultimedia,andevencrude DIARdoesincludesomeparticipantswhomovearound,but videosshotonh and-helddevices.Audiosegmentspresentin thecamerawe are risadummyhead and,similarto EASY- the data set are degraded with backgroundchatter,laughter, COM,the data isnotin-the-wild(sessionsall are donein overlappingspeech and varyingroomacoustics. thesameenvironment/scene). Ego 4 Daccounts for allthese Vox Converse:[39]Vox Converseis are latedaudio-visual aspects. Lastly,incontrastto Vox Celeb,Vox Converse and AVA,Ego 4 Doffersfirst-personvideo and itsconversation diarization dataset consisting of over 50 hours of multi- videostakeplaceincasualdaily-lifeenvironments with mul- speaker clips of human speech, extracted from You Tube tiplespeakers. videos. Similarto Vox Celeb,this data isalsonon-egocentric. This data setwasproposedtoboostresearchinspeakerdi- arization for audio-visualinputs.Abulkof the datainstances H.4 Tasks: Definition and Annotations are from politicaldebates and newsanchorssoastocapture conversationalscenarios with overlapping and interrupting Herewedetail the taskdefinitions,thecorrespondingannota- speech. tions,and the evaluationmetrics. Weproposeasuiteoftasks AVA:[31,192]The AVAspokenactivity data sets are AVA forthe Audio-Visual Diarization(AVD)benchmark. These speech and AVA active speaker. AVA speech is a densely tasks are abbreviated as: Localization & Tracking, Active 52 Speaker Detection, Diarization and Transcription. These people in the scene are speaking at a given time [192]. It tasks jointly capture who is talking when, to whom, and buildsontopof the previouslocalization and trackingtaskto aboutwhatinagivenegocentricconversationalscene. Ob- recognizeeachof the speakerswhosefaceboundingboxes serve that the setasks are implicitlytiedtoeachother;each are detected. Hence, this task does not take into account subsequent task isdriveninsome for mbyaprevious task speakers who are not visible in the camera\u2019s FOV. Note (asfurtherclarifiedin the taskdescriptionsbelow).16 thatactivespeakerdetectionisalsoanimportantaspectof speakerdiarization(whichisthenexttaskin the benchmark). Task 1: Localization & Tracking: Where is the person Annotations: We provide an anonymous speaker label inthevisualfieldofview? Thisfirsttaskin AVDcaptures (e.g.,speaker 1,2 etc.) foreachspeakervisiblein the clip. thespatialpositionofalltheprobablespeakersin the scene, The camera wearer is assigned the label C. This is done fromthepointofviewof the camerawearer. Thegoalof the byutilizing the faceboundingboxtracksannotations and taskistocomputeboundingboxes for them.Unlikeclassical labeling each track one at a time. Hence, each face track face detection benchmarks, this task is challenging in the getsassignedoneuniquelabel,andmultipletracks with ina sense that thedynamicsof the camerawearer\u2019shead(coming singleclipmaysh are the samelabel(correspondingto the fromnaturalconversations)leadstosignifi can tmovementin samespeaker). However,thelabels are clip-specific,i.e.,a aspeaker\u2019sapp are ntspatiallocation. speakerwhomaybepresentacrossmultipleclipsdoesnot Annotations: Foreachspeakerpresentin the 5 minclip getassignedash are duniquelabelacross the clips.",
      "start_pos": 21714,
      "end_pos": 22226
    },
    {
      "chunk_id": 185,
      "paper_id": "ego4d",
      "text": "at a time. Hence, each face track face detection benchmarks, this task is challenging in the getsassignedoneuniquelabel,andmultipletracks with ina sense that thedynamicsof the camerawearer\u2019shead(coming singleclipmaysh are the samelabel(correspondingto the fromnaturalconversations)leadstosignifi can tmovementin samespeaker). However,thelabels are clip-specific,i.e.,a aspeaker\u2019sapp are ntspatiallocation. speakerwhomaybepresentacrossmultipleclipsdoesnot Annotations: Foreachspeakerpresentin the 5 minclip getassignedash are duniquelabelacross the clips. Again, a bounding box is provided. Each frame of the video is speakerswho are neverin the visual Fo Varenotassigneda annotated for the task. Wefirstutilizedafacedetection and label. tracking model toestimate the seboundingboxes,andthen Evaluation: we use the objectdetectionm APtoquantify ateamofhumanannotatorsvalidated and correctedthese the active speaker detection result. This is a frame-wise machine-generatedboxestoimproveannotationquality. A metric. Inavideoframe,iftheintersectionoverunion(Io U) boundingboxisconsideredavalidhumanannotationifit betweenadetectedfaceboundingbox and the groundtruth captures 80% of the speaker\u2019s face; we peform a quality faceboundingboxexceedsapredefinedthreshold,i.e. 0.5, checksteuptoensure this. Sidewayslookingfaces are also we have a positive face detection. Each detection has an annotated. Note that speakers who are very far from the associated class to indicate whether it corresponds to an camerawearer(oftentimesseveralmetersawayin the scene) active speaker. Active speaker detection methods give a andwhodonotcomeintoconversationalcontact with the confidencescoreof the activespeakerclass for eachdetected wearer are notannotated. faceboundingbox[211]. Evaluation: Recall that thegoalof the taskistolocalize Camera Wearer\u2019s Voice Activity Detection: Note that the aswellastrackthespeakersin the scene. Hence the evalua- camerawearer\u2019sfaceisnevervisiblein the camera\u2019sfield tionmetricsproposedaccount for the accuracyoftrajectory ofview,andso the ydonot have anyfacetracksassociated ofdetectedboundingboxes.Wefollow the standardmultiple withthem. However,inmanycases,they are the dominant objecttracking(MOT)metricstoquantify the speakertrack- speakers. Thisismainlybecausethey are driving the inter- ingresults. There are manydifferent MOTmetrics,inwhich actionsinmanycases,andsincetheirmouths are the closest we are most interested in the MOTA in the CLEARMOT tothemicrophones,theirvoiceisingeneralamplifiedin the metrics[19],and IDF 1,IDP,IDRin the Identitymetrics[18]. audio stream compared to other speakers. We propose to MOTA,themultipleobtecttrackingaccuracy,isacombined alsoconsiderthemasactivespeakers and detect the irvoice. metricoffalsealarms,falsepositives and identityswitches. we use the objectclassificationm APtoquantify the result MOTAisbased onmatching thetracking results with the ofthecamerawearer\u2019svoiceactivitydetection. ground truth at frame level, while the IDP (ID precision), IDR(IDRecall)and IDF 1(IDF 1 score)are base don the Task 3: Diarization: Who spoke when? This next task tracking result to ground truth matching at the trajectory further expands on the temporal aspect of active speaker level. IDmetricsgiveatracker\u2019sper for manceonmaintain- detection(from the previous task). Given the setofspeakers ingcorrectidentification for eachtarget. andtheirspatiallocalizationin the visualfieldofview,this Task 2: Active Speaker Detection: Who is speaking? task aims to capture the voice activity of the speakers. It is identical to speaker diarization, a well studied research Thenexttaskin AVDistodetect the activespeakerin the problem in the speech and audio domains [10,177] and scene. This task is in principle similar to active speaker answers the question, \u201cwho spoke when\u201d. While speech detection\u2014wherethegoalistodetectwhichof the visible from speakers that overlap with each other is one of the 16 Note that althoughspeechtranscription and sourcelocalization are biggestissuestosolvein this task,theegocentricperspective distinct from audio-onlyspeakerdiarization\u2014allofwhich are welldefined addsmorecomplexityintermsofheadmotions and other researchparadigmsinmainstreamaudio,speech and visioncommunity\u2014 dynamicsassociated with naturalconversations. Note that wecumulativelyrefertoallthesetogetherunder the umbrellaofaudio- visualdiarization for Ego 4 D. theoutputsofactivespeakerdetection(theearliertaskin the 53 benchmark)alsodrive this task. arenotthesameas the onesusedindiarizationbecausewe Annotations: Foreveryactivespeakerlabel(where the separatelyannotated the overlappingregionsheretoreduce annotations are from the previous Active Speaker Detection transcriptionerrors and account for speakerstalkinginlow task), a human annotator marks the start and end time of volume. This allows us to also distinguish voice activity thatpersonspeaking. Weaccount for overlappingspeech fromspeechactivity. Inaddition,theuseoftime-segmented segmentswheremultiplespeakerstalkovereachother,but transcriptionsisalsoslightlydifferentfromst and",
      "start_pos": 22176,
      "end_pos": 22688
    },
    {
      "chunk_id": 186,
      "paper_id": "ego4d",
      "text": "the onesusedindiarizationbecausewe Annotations: Foreveryactivespeakerlabel(where the separatelyannotated the overlappingregionsheretoreduce annotations are from the previous Active Speaker Detection transcriptionerrors and account for speakerstalkinginlow task), a human annotator marks the start and end time of volume. This allows us to also distinguish voice activity thatpersonspeaking. Weaccount for overlappingspeech fromspeechactivity. Inaddition,theuseoftime-segmented segmentswheremultiplespeakerstalkovereachother,but transcriptionsisalsoslightlydifferentfromst and ard ASR we ignore speech not relevant to the conversation such as datasetsinspeechcommunitywhichmainly have text and backgroundspeechfroma TVorspeechfur the raway from notimestamps. the camera wearer. Note that speech segments from the Evaluation: We utilize the Word Error Rate (WER), a camerawearer are alsoannotated. Theannotatorsrelyboth standard ASRmetric,forevaluating this task[114].First,the ontheaudio and thevisualstream for creating the selabels. minimumeditor Levenshteindistanceiscomputedbetween Evaluation: Diarizationerrorrate(DER)isthedefacto the reference and hypo the sized transcription. WER then evaluationmetric for speakerdiarization[11],anditiswell measurestheratioof the numberofwordsubstitutions(S), studiedin the audio and speechprocessingcommunity. DER deletions(D)andinsertions(I),i.e.thetotalnumberofedits measures the fractionoftotaltime(inagivenclip)thatis necessarytoconvertthehypo the sizedtranscriptioninto the notattributedcorrectlytoaspeakerortonon-speech. Itis referencerelativeto the totalnumberofwords(N )inthe w definedasfollows: reference: DER(%)=(E +E +E ) 100, (21) S+D+I miss fa spk \u00d7 WER(%)= 100. (22) N \u00d7 where E denotes the fractionoftime that has been pre- w miss dictedtobenon-speechwhile that segmentisattributedto H.5 Data Statistics aspeakerin the reference. E denotes the fractionoftime fa thathas been predictedtobeassociated with aspeaker,but From across the 3,670 hours of video in Ego 4 D, approxi- isactuallylabelledasnon-speechin the reference,and E spk mately 764 hours of data contains conversational content, denotes the fractionoftimewherespeechisassociated with and are directlyrelevant for the AVDand Socialbenchmarks. thewrongspeaker. Allerrors are computedasafractionof Pleasereferto Section I.5 foracompletedescriptionof the thetotalamountofspeech. experimental design and scenarios used in these sessions. Task 4: Transcription: Whatdid the speakersay? The From this set,arandomlychosensubsetof 572 clips(each 5 finaltaskof AVDistotranscribe the speechofeachspeaker, minuteslong)areannotated for thisfirstversionrelease. Of i.e.,per for ming ASR.Similarto the diarization task,someof these 572 clips,389 clips are marked for training,50 clips thechallengesassociated with the transcription task include forvalidation,andtheremainderis the testingset. overlapping speech and environmental noise. In addition, Table 19 and Figure 43 summarize statistics about the thecamerawearer\u2019sheadmovementresultsinasignificant speakercontent from across the seclips. Observe the long change of the audio volume of the speech recorded from tails of mean and maximum number of speakers in the others. dataset. Wenote that inthefirstversionof the dat are lease, Annotations: Since the clipscontainmultiplespeakers due to the fact that the total number of clips is relatively with overlapping speech segments and with different vol- small, the test and/or validation batches may be biased in umes,thefinaltranscriptions are obtainedinmultiplepasses. termsofchangesinspeakers\u2019accents,changesinvocabulary Inthefirstpass,initialhumanannotations base donvoice usage(since the participants are fromdifferentculturalback- segments are merged with automati can notations for regions grounds from across the world),andingeneralchangesin withlowvolume. Inasubsequentpass,humanannotators natureofinteractivenessbetweenspeakersinascene. There hadtocorrect and assignsegmentsoftranscriptionsto the ismarginaldistributionalshiftamong the training,testing corresponding voice activity segments per speaker while andvalidationsplits. Thisismainlybecauseof the smaller also annotating overlapping speech. Note that annotators numberofannotationsin this versionof AVDfor Ego 4 D. hadboth the audio and videoavailable for annotation and, Weexpect the sedistributionalshiftstobelesssignifi can tin besidesspokenwords,theoccurrenceofo the rartifactssuch futurereleases and asmore data will beannotated. asunintelligiblespeechorincompletewords have also been annotated. Thefinaltranscriptionannotations for aclipcon- H.6 Baseline Modeling Framework sistofasequenceofsegmentslabeled with begintime,end time, transcript and speaker IDwithin the clip. Inevalua- Recall that the 4-part task sin this benchmark are tiedtoeach tions,weapplied ASRto the sesegmentsindividually and other,inthesense that representationslearned from onetask computedtheper for manceoverallof the",
      "start_pos": 22638,
      "end_pos": 23150
    },
    {
      "chunk_id": 187,
      "paper_id": "ego4d",
      "text": "data will beannotated. asunintelligiblespeechorincompletewords have also been annotated. Thefinaltranscriptionannotations for aclipcon- H.6 Baseline Modeling Framework sistofasequenceofsegmentslabeled with begintime,end time, transcript and speaker IDwithin the clip. Inevalua- Recall that the 4-part task sin this benchmark are tiedtoeach tions,weapplied ASRto the sesegmentsindividually and other,inthesense that representationslearned from onetask computedtheper for manceoverallof the sesegments.Please mayberelevant for the others. Tothatend, weproposea note that thetimesegmentsassociated with the transcripts baselinelearningframework that addresseseach task ina 54 Figure 43. AVDiarization data statistics.Mean and maximumnumberspeakersin FOV,andnumberspeakersperclip. Figure 44. AVDiarizationbenchmarkannotationssummary.Thef our tasks are annotatedinasequentialfashion,starting with localization andtrackingofspeakers,activespeakerdetectionlabels,diarizationtimestamps,andfinallytranscriptions. Thefigureshows the face detections(highlightedbyboundingboxes),speakerdetection(shownby the anonymousperson IDs 1,2,etc.),activespeaker(highlightedin green)andvoiceactivity(shownbelowingreenhighlightedtimesegments).Speakersin the visual FOVwhoarenottalking are highlighted indottedredboxes.Theclipsused for AVD(and Social Interaction)haveconsent from participantstoleave the irfacesunblurred. Statistic(Avg.) Value sequentialfashion. Theframeworkincludes the following Speakersperclip 4.71 steps: Speakersperframe 0.74 \u2022 Wefirstdetectpeople\u2019sheads and doshorttermtrack- Speakingtimeinclip 219.81 sec ingin the video. Theshorttermtrackerfollowseach Speakingtimeperpersoninclip 43.29 sec detectedheadbyexp and ingasetoftrajectoriesbased Camerawe are rspeakingtime 77.64 sec ontheirpositions,sizes and theappearanceof the per- son. Thetrajectoriesmayendwhenocclusionhappens Table 19. AVDData Statistics. orwhenthetrackedpersongoesoutof the fieldofview. Newtrajectories can alsobeaddedto the trajectoryset. \u2022 Theshorttermtracker\u2019strajectory for eachpersonis 55 Figure 45. Exampleannotationsshowing the facedetections(highlightedbyboundingboxes),speakerdetection(shownby the anonymous person IDs 1,2,etc.),activespeaker(highlightedinred)andvoiceactivity(shownbelowinbluehighlightedtimesegments).Asillustrated here,thedata for AVDincludespeoplewalkingaround and talking,sitting and playinggamesetc.Theclipsused for AVD have consent fromparticipantstoleave the irfacesunblurred. oftenfragmentedintomultipleparts. Hence,wethen across all segments. Evaluating the system by using optimizethegroupingof the trackletsinsteponeso that ano the rsegmentationmethodischallengingespecially thetrajectoriesofeachperson can belinkedtogether. in the case of overlapping speech segments. Jointly We formulate the problem as a constrained combina- modeling time segments and transcriptions will be a torialoptimizationproblem. Integerprogramming can challengingproblem(aswediscussin Section H.7). beusedtosolve the problemdirectlybutithasexpo- Wedescribefurtherdetailsabouteachof the sestepsbe- nentialcomplexity. Forefficiency,wedevelopagreedy low,and Tables 20\u201329 summarize the resultingper for mance approach which is much faster and still gives strong metrics for the tasks. results. \u2022 Wethenclassifyeachperson/headineachvideoframe Audio Only Models for Speaker Diarization The prob- as an active speaker or not. Based on the classifica- lemofspeakerdiarization from audiohas been studiedtoa tion result and the corresponding detected long-term considerableextentin the fieldofspeechprocessing[10,177]. trajectories, we further associate the audio/speech to For the audio-only baseline system, the VBx diarization eachpersonin the video. we use this preliminarylistof approach has been utilized [128] for having shown supe- audiofeatureembeddingstofur the rextract and match rior results on different types of datasets such as CALL- un-associatedaudiosegmentstospeakerlabels. HOME[3](telephoneconversations),AMI[28](meetings) \u2022 Wethenusetwomethodstodetect the camerawearer\u2019s and DIHARDII[59](myriadofdomainsrangingfromau- voiceactivity. Thefirstmethoduseshighenergyaudio diobooksto You Tubevideos). Thismethodrequiresspeech segment in the clip (under the assumption that their activityregions and the sewereobtainedusing the ASp IRE voicehasnaturalamplificationcomp are dto the remain- model base donatimedelayneuralnetwork(TDNN)with ingspeakers). Thesecondmethodisadeepclassifier statisticspooling,available with the Kaldispeechrecogni- thatpredictswhether the wearerisspeaking. tiontoolkit[181]. Wereferto this ask VAD(the Kaldi VAD \u2022 Lastly,weapplied ASRto the speechregions base don model). Althoughthisk VADhas been trainedonslightly thegroundtruthsegmentation and evaluated the WER different data(telephoneconversations),andthusdoesnot 56 provide the bestpossibleresults,ithas been chosen for the 0,itisremoved from the trajectoryset. baselinesystembecauseofitsgeneralavailability. Thekeycomponentof the short-termtrackerismatching Thespeechactivityregions are uni for mlysegmentedto trajectories to the candidate head bounding boxes in each obtainshortersegments and speakerembeddings(so-called frame. This can beformulatedas the followingoptimization x-vectors[206])areextractedonepersubsegment. Thex- problem: vectors are obtainedwitha Res Net 101 extractor[96]trained toproducespeaker-discriminativeembeddings. Theinput (cid:88) min c x (23) tothenetwork are log Mel-filterbankfeaturesevery 10 ms, i,j i,j and given a segment of speech, it computes a single 256 (i,j) dimensionalvector that represents the wholesegment. The s.t. x i,j formsamax-matching, informationof",
      "start_pos": 23100,
      "end_pos": 23612
    },
    {
      "chunk_id": 188,
      "paper_id": "ego4d",
      "text": "the followingoptimization x-vectors[206])areextractedonepersubsegment. Thex- problem: vectors are obtainedwitha Res Net 101 extractor[96]trained toproducespeaker-discriminativeembeddings. Theinput (cid:88) min c x (23) tothenetwork are log Mel-filterbankfeaturesevery 10 ms, i,j i,j and given a segment of speech, it computes a single 256 (i,j) dimensionalvector that represents the wholesegment. The s.t. x i,j formsamax-matching, informationof the wholesegmentisaggregated with asta- x =0, if(i,j) E, i,j tisticalpoolinglayerwhichcomputes the meanandst and ard \u2208 x =0,1, i,j deviation of activations over the time domain. A linear trans for mationisthenusedtoreduce the dimensionalityto wherex is 1 iftrajectoryimatches can didateheadboxj i,j 256. Thetraining data consistedof Vox Celeb 1[165],Vox- and 0 otherwise. E isasetinwhich the pairsoftrajectory Celeb 2[40]and CN-CELEB[64]together,totalling 2877 andc and idate can notmatcheachother, examplesinclude hoursofspeech from 8178 speakers. casessuchas the can didateistoofaraway,thesizeistoo Thex-vectors are initiallyclusteredtoafewdozensof differentor the appearancedoesnotmatch. c isthecostof i,j classes using agglomerative hierarchical clustering. This matchingtrajectoryi and can didateheaddetectionj. This initial clustering is fed as initialization to a Bayesian hid- costofmatching,c ,iscomputedasalinearcombination i,j den Markov model whichestimatesaltogether the number ofthenormalizedboundingboxdistances and the difference of speakers in the recording as well as the assignment of oftheappearancefeatures. Thenormalizedboundingbox x-vectorsto the states. Eachstatein the modelcorresponds distance is defined as the ratio of the Euclidean distance toonespeaker and the probabilityofobservingaparticular between the two corners of the last bounding box in the x-vectorinaparticularstate can beinterpretedas the cor- trajectory and thedetectedheadboundingboxin the image respondingspeakerproducing the correspondingsegment tothesizeof the detectedboundingbox.Eachtrajectoryalso ofspeech. Themostrelevanthyperparametersof the model maintains a feature vector to characterize the most recent werefine-tunedtoobtain the best DERper for manceon the appearance of the tracked person. This feature vector is Ego 4 Dvalidationset. The VBximplementationpublished obtained from a feature embedding network trained on a by Brno Universityof Technologyispubliclyavailableas largepersonhead data set. wellas the trainingrecipepublishedby Phonexia Research. Thisoptimizationproblem can besolvedefficientlyus- ingthe Hungarianalgorithmor the primaldualalgorithm. Short-term People Tracking Thegoalhereistotrackpeo- Due to the imperfect features, the optimization may have ple\u2019sfaces. However,ourmethod can alsobeusedtotrack an identity switching problem if two targets cross paths. thewholebodyofeachperson. Theshort-termtrackermain- Tosolve the problem,weenforce the longertrajectoriesto tains a set of trajectories. The trajectories include the at- havehigherprioritytomatch. we useatwo-stepmatching tributes such as the person-ID, the frames tracked, a life scheme. Wefirstmatchall the trajectories that are longer counter, the appearance features and the positions of the thanaspecificthresholdchosenempirically. Oncedone,we tracked bounding boxes. Throughout, we use the term thenmatch the shortertrajectories. Thisschemenaturally \u201cperson-ID\u201d to refer to an anonmyous tag for a person in giveshigherprioritytolongertrajectories,therebyreducing thevideo(person 1,person 2,etc.);noactualidentities are mismatchesamongthem. Thisismorerobustthanasingle available in the data, and the benchmark does not aim to stagematchingwherealltrajectoriesareh and ledtogether. per for manypersonidentification. There are twokindsof Inourimplementation,thepersondetectorisa Yolo-V 3 trajectories. Ifatrajectory\u2019strackedframes are lessthana detector[187]whichdetects the head and personbounding threshold,e.g.5,itisinprobation and isnotcountedasareal boxsimultaneously. Thedetectoristrainedonimages from trajectoryeventhoughwemaintainall the information for the Google Open Image data set[123]andafisheyeimage them.Whenatrajectory\u2019strackedframes are greaterthan the dataset[73]. we use the detectedheadboundingboxes for threshold,itbecomes are altrajectory. Eachtrajectoryalso people tracking. The person head appearance\u2019s feature is hasalifespan.Thelifeofa new trajectorystarts from afixed extracted using the person embedding network, which is value. Thelifeofatrajectoryisrestoredtoafixedmaximum trainedon the Vox Celeb 2 datasetusing the tripletloss. The value,suchas 10,ifthetrajectoryismatchedtoa can didate networkhas the structureofa Res Net-18. personheadboundingboxes. Otherwise,thetrajectorygoes into a maintenance mode and its life decreases by 1 each Long-term Trackingby Trajectory Matching Theshort timeitfailstofindamatch.",
      "start_pos": 23562,
      "end_pos": 24074
    },
    {
      "chunk_id": 189,
      "paper_id": "ego4d",
      "text": "trajectorystarts from afixed extracted using the person embedding network, which is value. Thelifeofatrajectoryisrestoredtoafixedmaximum trainedon the Vox Celeb 2 datasetusing the tripletloss. The value,suchas 10,ifthetrajectoryismatchedtoa can didate networkhas the structureofa Res Net-18. personheadboundingboxes. Otherwise,thetrajectorygoes into a maintenance mode and its life decreases by 1 each Long-term Trackingby Trajectory Matching Theshort timeitfailstofindamatch. Ifthelifeofatrajectorygoesto termtrackergeneratesfragmentedpersontrajectories. Ifa 57 personisoccludedorgoesoutof the fieldofview and reap- Metric Valid Test pears,itwillreceivea new ID.Thefragmentedtrajectories MOTA 74.52 71.94 arereferredtoastracklets. Weneedtogroup the tracklets MOTP 79.07 79.17 throughoutthewholevideotogenerate the finaltrajectories IDF 1 84.92 80.07 foreachperson. Thegroupingproblem can beformulated IDR 80.40 73.52 asfollows: IDP 89.97 87.90 (cid:88) Table 20. Localization and tracking base linemetricson the valida- min D y (24) m,n m,n tion and the testsetsrespectively. m,n s.t. y =y , m,n, m,n n,m \u2200 would probably never occur and the greedy result would y +y 1+y , m,n, m,k k,n m,n \u2264 \u2200 approach the globallyoptimalsolution. y =0,ifm and noverlapintimeor D >g, m,n m,n Table 20 summarizes the trackingmetrics MOTA,MOTP, y m,n isbinary, IDF 1,IDR,and IDPon the validation and testsets. wherey =1 iftrackletmandn can begroupedtogether Active Speaker Detection: we usetwoapproachesforac- m,n ando the rwisey = 0. D istheappearancedistance tive speaker detection. One approach is based on mouth m,n m,n between the trackeletm and nandg isathreshold. Here region classification, and the second method is a trans- D =min f f 2,where T isthesetof former based audio-visual method for active speaker de- m,n {i\u2208Tm,j\u2208Tn} || i \u2212 j || i personheadboxesintrackletiandf isthecorresponding tection[211]. i featureembedding. Theconstraintsrequire the groupingto Region Cls: Ourfirstapproachis base don the classification bereflective: iftrackletmandn can begroupedtogether ofmouthregions. Itfirstcomputes the 3 Dheadorientation socannandm, transitive: ifmandk can begroupedto- using a regression network. In our implementation, the z gether and so can k and n, then m and n can be grouped direction is into the image; if the head 3 D orientation z together. Twotracklets can notbegroupedtoge the rifthey coordinateon the unitsphereisgreaterthan 0.3,weassume havetimeoverlapor the irdistanceisgreaterthanathreshold the face is away from the camera. If the face is facing g. Theoptimization can besolvedusingintegerprogram- away from the camera,weignoretheimage and the active ming. However, this method has exponential complexity. speakerdetectionresultissettonull. Forfaceslookingat Weproposeafastgreedyalgorithmtosolve the problem. thecamera,ourmethodfirstregresses the facialkeypoints Thegreedyalgorithmstartsbytreatingeachinitialtrack- usingtheimagewithin the person\u2019sheadboundingbox. We let as a trajectory and progressively groups two trajecto- usethemouthkeypointstocropout the mouthimage. The ries with the closest Duntilnotrajectories can begrouped croppedmouthimageis the nsenttoaclassificationnetwork together. Since the distance between two trajectories can toclassifywhether the speakeristalkingornot. becomputedbyfinding the minimumofall the\u201celement\u201d Note that we also explored using multiple images, trackletpairdistances,themergingprocedureisefficientif wherein we stack a short sequence of cropped mouth im- wepre-compute and cache the elementpairdistance. This agesinatimeinterval for activespeakerclassification. Our greedyapproachgivesstrongresultswhilemaintaininglow experimentsshow the multiplemouthimagesinputdonot complexity. signifi can tlyimprove the result. Thisisprobablydueto the The algorithm reduces to the minimum spanning tree fastmovementof the camera and sometimesdifficultangles methodif the reisconflictbetweeneachpairoftrajectories. oftheface. Thiscausesinaccuratecroppedmouthregions. However,ifthere are time-conflictingtracklets,thereisno Talk Net:[211]Talk Netisanend-to-endpipeline that takes guaranteethegreedyalgorithmgives the globallyoptimal the cropped face video and corresponding audio as input, solution. Weillustrate the methodthroughasimpleexample: anddecidesif the personisspeakingineachvideoframe. It Assume the rearetrackelets T ,T ,T ,T ,T and T have consistsofafeaturerepresentationfrontend and aspeaker 1 2 3 4 1 2 { } timeconflict, and T and T havetimeconflict. D(T ,T )",
      "start_pos": 24024,
      "end_pos": 24536
    },
    {
      "chunk_id": 190,
      "paper_id": "ego4d",
      "text": "the globallyoptimal the cropped face video and corresponding audio as input, solution. Weillustrate the methodthroughasimpleexample: anddecidesif the personisspeakingineachvideoframe. It Assume the rearetrackelets T ,T ,T ,T ,T and T have consistsofafeaturerepresentationfrontend and aspeaker 1 2 3 4 1 2 { } timeconflict, and T and T havetimeconflict. D(T ,T ) detectionbackendclassifier,asillustratedin Figure 46. The 3 4 1 3 = 10, D(T ,T ) = 1, D(T ,T ) = 3 and D(T ,T ) = 4. We frontend contains an audio temporal encoder and a video 2 4 1 4 2 3 assume g = 20. Using the proposed greedy method, the temporalencoder. Theyencode the frame-basedinputaudio solution P is T ,T , T ,T whose overall cost is 11. andvideosignalsinto the timesequenceofaudio and video 2 4 1 3 {{ }{ }} However,theoptimalsolutionis T ,T , T ,T whose embeddings, representingtemporalcontext. Thebackend 1 4 2 3 {{ }{ }} overall cost is 7. Even though the greedy method does classifierconsistsofaninter-modalitycross-attentionmech- not guarantee the global optimal solution, empirically we anismtodynamicallyalignaudio and visualcontent,anda observe that the proposed method give strong results. In self-attentionmechanismtoobservespeakingactivities from fact,ifthepersonembeddingisaccurate,thesecornercases thetemporalcontextat the utterancelevel. 58 Algorithm 1 Greedy Tracklet Grouping Initializesets P= S ,S ,...,S ,where S = T ,T isthetrackleti and N isthenumberoftracklets. 1 2 N i i i { } { } for(m,n),m=1..Nandn=1..Ndo compute D(m,n) endfor while Truedo for (S ,S ),S P and S P,and(S ,S )donot have timeconflictdo m n m n m n \u2208 \u2208 compute F(S ,S )=min D(a,b) m n Ta\u2208Sn,Tb\u2208Sm endfor (m\u2217,n\u2217)=argmin(F(S ,S )) m n if(m\u2217,n\u2217)isemptyor F(S m\u2217,S n\u2217)>gthenbreak endif S m\u2217 =S m\u2217 S n\u2217 and P.pop(S n\u2217) \u222a endwhile Pincludes the groundedtrajectories Figure 46. Talk Net:Anaudio-visualtemporalnetwork for detecting and tracking the activespeakerinavideo[211].Figureis from[211]. Tables 21, 22, 23 and 24 summarize the resulting per- Model m AP@0.5 formance. Foreachof the twoproposed base linemodels, Reg Clsw/osmoothing 29.65 wereportper for mancesummaries with pretraining base don Reg Cls+max-filtering 32.77 AVA andalso model strainedusingonlyvideos from the Reg Cls+max-filtering+s VAD 34.35 Ego 4 Dtraining data set. Note that the video-onlyapproach Talk Net 50.90 can becombined with anyvoiceactivitydetectiontoremove Talk Net+s VAD 49.66 falsealarms. Herewe usesuchanalgorithm from[203],and wereferto this ass VADThis can greatlyimprove the active Table 22. Activespeakerdetection base linemetricson the testset speakerdetectionresults. Themax-filteringhasawindow usingtrainingvideosin the Ego 4 Ddataset. sizeof 11. Talk Netalsohasabuilt-insmoothnessfiltering topost-process the rawclassificationresult. Model m AP@0.5 Reg Clsw/osmoothing 22.09 Model m AP@0.5 Reg Cls+max-filtering 22.88 Reg Clsw/osmoothing 29.68 Reg Cls+max-filtering+s VAD 25.53 Reg Cls+max-filtering 31.95 Talk Net 34.36 Reg Cls+max-filtering+s VAD 33.72 Talk Net+s VAD 34.65 Talk Net 34.75 Always Speak 20.94 Talk Net+s VAD 34.56 Always Speak 24.46 Table 23. Activespeakerdetection base linemetricson the valida- tionset with modelstrainedon AVA data set.In Always Speak,all thedetectedfaces are classifiedasactivespeakers. Table 21. Activespeakerdetection base linemetricson the test set with pre-trainingusing AVA.In Always Speak,all the detected faces are classifiedasactivespeakers. Matching Speakers Outside Fo V: Based on the tracked heads and the activespeakerdetectionresults,we canasso- 59 Model m AP@0.5 wearer\u2019s voice often has higher energy than other partici- Reg Clsw/osmoothing 20.33 pant\u2019svoices. we use this heuristictoextract can didatesof Reg Cls+max-filtering 21.93 thewearer\u2019svoicebychoosingportionsofaudiowi the nergy Reg Cls+max-filtering+s VAD 24.60 higher than certain",
      "start_pos": 24486,
      "end_pos": 24998
    },
    {
      "chunk_id": 191,
      "paper_id": "ego4d",
      "text": "Matching Speakers Outside Fo V: Based on the tracked heads and the activespeakerdetectionresults,we canasso- 59 Model m AP@0.5 wearer\u2019s voice often has higher energy than other partici- Reg Clsw/osmoothing 20.33 pant\u2019svoices. we use this heuristictoextract can didatesof Reg Cls+max-filtering 21.93 thewearer\u2019svoicebychoosingportionsofaudiowi the nergy Reg Cls+max-filtering+s VAD 24.60 higher than certain threshold. Since different recordings Talk Net 51.04 have different levels of loudness, we normalize the audio Talk Net+s VAD 50.58 using the maximum energy and then choose the possible wearer\u2019s voice using a fixed percentage of the maximum Table 24. Activespeakerdetection base linemetricson the valida- energy. This threshold percentage is set to be as high as tionsetusingtrainingvideosin the Ego 4 Ddataset. possibletoavoidfalsealarms. Once the can didateaudiois selected,we use the sameaudiomatchingmethoddescribed intheprevioussectiontofindall the audio that belongsto ciatetheaudiotothevisiblepeoplein the scene. However, thecamerawearer. Thissimplemethodworksreasonably thisisstillnotcompletebecause the rearecasesinwhich the wellassummarizedin Table 25. Theapproachfailswhen speakerisoutsideof the visualfieldofview. Tosolve this thewe are rnevertalksortalksinaverylowvoice,andin problem,wefirstcreateanaudio-signature for eachvisible general the baseli new orksbetter for nearrangemicrophones personin the video. thanlongrangemicrophones. Weextractonesecondofaudiocenteredateachvideo Method II: In the second method, we directly classify frametimeinstant. Iftheaudiocorrespondstoaspeaking the audio at each time instant to two categories: wearer\u2019s head in the image, we compute the audio embedding of voiceornotwearer\u2019svoice. Thelogarithmmagnitudeof the theonesecondaudio and insertthefeatureinto the audio spectrogramat 40 mswindowis the input. Thenetworkisa signaturelibraryof the person.Theaudioembeddings can be modified Res Net. Thenetworkistrainedon the Ego 4 d AV obtained from anyspeechrepresentationlearningmethods. training data setusingast and ardcross-entropyloss. Weexploredseveral model sincludingamodified Res Net 18 we useclassificationm APtoquantify the weareraudio whichtakesaudiospectrogramlogarithmmagnitudeinone- activity detection result. We report the average m AP on secondwindowsas the input and trainedon the Vox Celeb 2 both the testvideos and validationvideosin Table 25. datasetusingtripletloss,andaversionofwav 2 vec 2.0[15]\u2014 aself-supervisedapproachtospeechrepresentationlearning. Model Valid Test We parse the video and find instants when a particular Method I 43.95 50.61 person is not in the video frame and match the audio em- Method II 72.00 74.29 bedding to the person\u2019s audio signature library. We find Always Speak 21.30 26.09 the minimum distance of this audio embedding to all the signatureaudioembeddingsin the library. Ifthedistanceis Table 25. Camerawe are ractivitydetection base linemetrics(m AP) lessthanapredefinedthreshold,weclassify the personas onthevalidation and testsetsrespectively.Always Speakassigns speakingando the rwisenot. Note that the audioembedding that the wearerspeakingineachvideoframe. isusedonlywithin the same 5 minutevideoclip and never acrossvideoclips. Person IDs are alwaysanonymoustags (person 1,2,etc.). Speaker Diarization Tables 26,27 and 28 summarize the we use this methodtodetectall the backgroundaudioof speaker diarization DER metrics for the baseline models thepeopleofinterestwhen the yarenotvisible. Thismethod proposedin the earliersections. Wereport the results with assumes that the activespeakerisperfect. Inreality,active trainingonlyon Ego 4 ddataaswellason with trainingon speakergivesnoisyresults. Thiswouldcauseo the rpeople\u2019s existingdiarization data sets. Note that the audio-only DER voicefeaturetobeincludedinaperson\u2019ssignaturelibrary isaggregatedwhile the audio-visual DERisaveraged. Also andaffect the finalaudioclassificationresult. note the impactof the VADon the diarizationper for mance with the audio-only base line. Itshould benoted that amodel Tracking Camera Wearer\u2019s Audio: Thecamerawe are ris more tailored to Ego 4 D-like data could be used to obtain a special participant because their face is invisible in the better per for mance. Never the less, this aspect still poses egocentricvideos. Theactivespeakerdetectionmethodthus challengeson the AVDbenchmark. cannotbeusedtoassociatethewe are rwith the irvoice. We usetwomethodstodetect the camerawearer\u2019svoice. Transcription To obtain baseline",
      "start_pos": 24948,
      "end_pos": 25460
    },
    {
      "chunk_id": 192,
      "paper_id": "ego4d",
      "text": "tailored to Ego 4 D-like data could be used to obtain a special participant because their face is invisible in the better per for mance. Never the less, this aspect still poses egocentricvideos. Theactivespeakerdetectionmethodthus challengeson the AVDbenchmark. cannotbeusedtoassociatethewe are rwith the irvoice. We usetwomethodstodetect the camerawearer\u2019svoice. Transcription To obtain baseline transcriptions, we used Method I:Thefirstmethodusesenergyfilteringfollowed thepre-trained Gigaspeech model providedin the ESPNet byaudiomatching. Thismethoddoesnotneedgroundtruth model zoo [1]. This model is trained on the Gigaspeech labelingof the camerawearer\u2019svoiceactivities. Since the dataset[34]whichcontains 10000 hoursofspeech. Input microphoneofthecameraisusuallycloserto the wearer\u2019s featuresto the model are logmelfeaturesaugmentedusing mouththanothersubjectsin the scene,theamplitudeof the the Spec Augmentmethod[173]andnormalizedbyglobal 60 Model trained s VAD DER[%] language model isused. Fordecoding,we used CTCweight on Ego 4 D of 0.3 andbeamsize 20 whichwedidnotfine-tuneon the Region Cls no no 84.79 Ego 4 D dataset. The pre-trained model obtained from [1] Region Cls no yes 83.88 cannotsupport 5-minvideos,hence,we usedoraclesegment Talk Net no no 86.68 information from the transcriptionannotationstosegment Talk Net no yes 85.85 thedata and wedecodedeachsegmentseparately. Thefinal Region Cls yes,only no 80.52 WERisobtainedbycounting the totalnumberoferrorsover Region Cls yes,only yes 80.17 thewholevalidationortestset. Talk Net yes,only no 73.14 In Table 29,wesummarize the WERresultsdepending Talk Net yes,only yes 73.32 onthe VADsegmentationmethodonbothvalidation and test Always Speak - - >100 sets. Tocompute the final WER,we 1)removedpunctuation Never Talk - - 100 fromboth the reference and the ASRhypo the sis,2)allowed soft-matchoncontractionssuchas(Iwillvs. I\u2019ll)using the Table 26. Diarization Baseline Metricsshowing DERon the testset. Englishglobalmappingfile from Kaldirepository[2],and 3) In Always Speak,all the detectedpeople are labeledas\u201dspeaking\u201d used the NISTsclitetool[72]. Aswe cansee from Table 29, ineachvideoframe. In Never Talk, all the detectedpeople are onboth the test and validationsets,the WERs are quitehigh. labeledas\u201dnotspeaking\u201dineachvideoframe. Thisshows that the datasetischallenging for anoff-the-shelf ASR model becauseofoverlappingspeech,noise,different volumelevels for differentspeakers,occasional for eignword Model trained s VAD DER[%] usage,etc. on Ego 4 D Region Cls no no 98.82 Region Cls no yes 90.98 Speech Segments Valid Test Talk Net no no 99.73 Ground Truth 64.8 59.2 Talk Net no yes 92.14 Region Cls yes,only no 81.66 Table 29. ASRtranscription WERs(%)onthevalidation and test Region Cls yes,only yes 79.97 datausing the referencespeechsegmentation. Talk Net yes,only no 80.58 Talk Net yes,only yes 79.30 Always Speak - - >100 H.7 Discussion Never Talk - - 100 Although AVdiarizationpresentsa task suitecomposedof Table 27. Diarization base linemetricsshowing DERon the val- reasonably well understood tasks from the vision, speech idationset. In Always Speak,all the detectedpeople are labeled andaudiocommunities,our base lineresultsclearlysuggest as\u201dspeaking\u201dineachvideoframe.In Never Talk,all the detected thatefficientspeakerlocalization,tracking,diarization and people are labeledas\u201dnotspeaking\u201dineachvideoframe. transcriptionisarathercomplexproblemin the egocentric perspective and within-the-wild data. Thisisspecifically Typeof VAD Valid Test evident from theper for manceof the jointaudio and video drivendiarization and transcription base lines(with DERof k VAD 67.24 65.28 > 80%and WERof> 60%). Overlappingspeechmakes Ref. Activity 36.56 39.99 both these tasks particularly difficult to annotate as well asevaluateanyproposedmodels. Per for mingsomeaudio- Table 28. Diarizationper for mance with audio-onlymodels for visuals our ceseparationpriorto the setasksmayimprove the validation and testsetsusingk VAD and reference(groundtruth) voiceactivityannotations. efficacy,never the lesssensitivitytochanges and difference inspeechamplitudesofoverlappingspeakerswouldstillbe challengingtoaddress. mean-variancenormalization. Theencoderof the acoustic Novelcross-modallearningapproaches that jointly model model is based on macaron-style con for mer [93] with 12 audio and visual modalities while accounting for such at- blocks and 8 attentionheads and",
      "start_pos": 25410,
      "end_pos": 25922
    },
    {
      "chunk_id": 193,
      "paper_id": "ego4d",
      "text": "the setasksmayimprove the validation and testsetsusingk VAD and reference(groundtruth) voiceactivityannotations. efficacy,never the lesssensitivitytochanges and difference inspeechamplitudesofoverlappingspeakerswouldstillbe challengingtoaddress. mean-variancenormalization. Theencoderof the acoustic Novelcross-modallearningapproaches that jointly model model is based on macaron-style con for mer [93] with 12 audio and visual modalities while accounting for such at- blocks and 8 attentionheads and the decoderis base dona tributes (overlapping speakers, interruptions, noise in the 6-layertrans for mer[217]with 8 attentionheads. Inboth the wildetc.) areneededtofurtherimprove the seper for mances. encoder and decoder,linearlayers have 2048 units and the The base lineframeworkweutilizedherealsodoesnotac- encoderoutputis 512 dimensional. Thedecoderoutputhas countforefficientin for mationsharingacross the fourtasks 5000 sentencepiece[122]units. The model istrainedusing inthebenchmark. Specifically,therelationshipbetweenro- ajoint CTC and attentionobjective[112]. Fordecoding,no bustlocalization and tracking with multi-speakerdiarization 61 is not studied, and this is also not well understood in the thebenchmark for mulation and writing. literature. Weexpect this tobeachallengingproblem. Wealsoobserved that subjectiveattributesinconversa- tions, like speaker accents, changes in vocabulary usage basedonculturaldifferencesetc.,influenceboth the content ofthespeech and the clarity with whichit can becaptured in human annotations. The camera wearer\u2019s head motion addssignifi can tblurtospeakers\u2019faces. Toaccount for such aspectsweper for medqualitychecksonhumanannotations, andweexpectnovelunsupervised and self-supervisedlearn- ing will helpfur the raddresssuchsubjectiveattributes. In future versions, we expect to increase the scope of the task suite (i.e., proposing new tasks and annotations), thereby open ing new avenues for bothcoremachinelearning infirstpersonperspective,andalso for robustmulti-modal representationlearning. Wecouldalsoinvestigateresearch directionsfocusedonspatialaudiobycreating 3 Denviron- ments coupled with Sound Spaces [32]. This enables new research and tasks in audio-visual sound source localiza- tion,audio-visualdirection-of-arrivalestimation and related immersive reality applications. We note that a small frac- tionof our data setdoescompriseofbinauralaudiocaptured using in-ear microphones and an audio recorder (Tascam, Appendix A). H.8 Contributionsstatement Vamsi Krishna Ithapu co-led the audio-visual diarization benchmarkworkstream,thecorresponding task sdefinition, data selection methodology, data annotation tooling and guidelines and writing. Christian Fuegenco-lead the audio- visualbenchmarkworkstream,thediarization and transcrip- tion task sdefinition,thecorrespondingannotationguidelines and paper writing. Hao Jiang worked on data annotation tooling,tasksdefinition for localization and tracking,active speakerdetection and diarization;alsoworkedonbuilding the baseline models for these tasks and writing. Federico Landini and Jachym Kolarworkedon base linemodels for audio-onlyvoiceactivitydetection and diarization,andwrit- ing. Leda Sariworkedontranscription task definition,corre- spondingannotationguidelines and baseline model ing. Eric Zhongcong Xuworkedon data selectionmethodology and the baseline modeling of active speaker detection. Ruijie Taoand Mike Zheng Shouworkedon the modelingofactive speakerdetection. Hanbyul Jooworkedon data annotation tooling and dataselectionmethodology. Christoph Feicht- enhoferworkedon the taskdefinition and metrics. Anurag Kumarworkedonactivespeakerdetection and diarization tasksdefinition,andonaudioembeddings model ing for these tasks. Morrie Doulatyworkedon base linemodels for voice activitydetectionanddiarization and dataanalysisofanno- tations. Lorenzo Torresaniworkedon the tasksdefinition andannotationguidelines. Kristen Graumancontributedto 62 I.Social Interaction Benchmark Thissectiondetails the Social Interactionbenchmark task definitions, annotations,baselinemodels, andresults. We also provide details on the video data collection process formulti-personcapture with participantswhoconsentedto have the irfacesunblurred and conversationrecorded(Ap- pendix I.5). Asnotedin Appendix B,thesocialbenchmark videos were screenedtoremoveanyin for mation(e.g. last namesorsocialmediaaccounts)thatcoulddirectlyidentify participants. However, participants\u2019 faces and voices are presentasperourin for medconsent. I.1 Formal Task Definition LAM and TTM are defined as follows: (1) LAM: y = f(I,B); (2) TTM: y = f(I,A,B) where I = I T 2 , { t }\u2212T 1 (a) Annotationtool A = A T 2 , and B = B T 2 aretime-synchronized { t }\u2212T 1 { t }\u2212T 1 pastsequencesofvideo,audio,andboundingboxes,respec- tively,where T and T arethelengthof the past",
      "start_pos": 25872,
      "end_pos": 26384
    },
    {
      "chunk_id": 194,
      "paper_id": "ego4d",
      "text": "y = f(I,B); (2) TTM: y = f(I,A,B) where I = I T 2 , { t }\u2212T 1 (a) Annotationtool A = A T 2 , and B = B T 2 aretime-synchronized { t }\u2212T 1 { t }\u2212T 1 pastsequencesofvideo,audio,andboundingboxes,respec- tively,where T and T arethelengthof the past and future 1 2 time horizon, respectively, and t = 0 is the center frame. Theboundingboxindicates the targetpersontoclassify. y isabinaryclassificationlabeldefinedby: (cid:26) 1 if targetlooks/talksatcamerawearer y = 0 otherwise. (25) (b) Visualizationofannotations. The LAM and TTM tasks are defined as a frame-level predictiony,whichst and sincontrasttoaudioanalysistasks Figure 47. (Top)The GUIof the annotationtool; (Bottom)Vi- wherelabels are oftenassignedat the levelofaudioframes sualizationofexampleannotations. Note that LAM(denotedby orsegments. Adesired model must beabletomakeacon- magentatext)and TTM(denotedbycyantext)maynotnecessarily solidateddecision base don the video and audiocuesover occurtogetherasshownin the seexamples. thetimec our seofanutterance. Forexample,ifthespeaker turnstheirheadto the sidemomentarilywhilespeakingto thecamera-wearer,thenaframewhere the speakerislooking short-duration LAMor TTMbehaviors,lasting 1 or 2 sec- awaywouldhavey = 0 whiley = 1. Figure 47 onds. The data wasorganizedasfollows for baseline model LAM TTM givessomeframelevelvisualizationofannotationsthatil- trainingin Section I.3: 389 clips were heldout for training, lustrate the taskdefinitions. comprising 32.4 hoursintotal. Anadditional 50 clips(4.2 hours)and 133 clips(11.1 hours)wereheldouttoform the validation and testingsets,respectively. I.2 Annotation Statistics Thesocial task annotations, LAMand TTM,buildon the I.3 Social Baseline Models and Results same video clips used in the AV diarization tasks and de- scribedin Appendix H.5. Fig 48 summarizes the statisticsof LAM Our base line model for LAMisavideo-based model LAMand TTMannotationsacross the seclips. Wecompute using Res Net-18 and Bidirectional LSTM.Our model uses thepercentageof the frames with LAMor TTMannotations thecroppedfaceregionsinvideoasinputinordertofocus ineachclip and show the histogramsin Fig 48(a)and(b), oncuesabout the headpose and socialattentionvisiblein respectively. Inmanyclips,theseeventshappenrarely(10 theface. Thearchitectureof our base lineissimilarto the %orlower),and the frames with LAMannotations are less Gaze 360[111]. Asillustratedin Fig 49(a),weinput 7 con- frequentthan TTMcases. Wealsolist the durationofeach secutiveframes(T =3 and T =3)fromonefacetracklet, 1 2 LAM or TTM annotation (the duration between start and andeachimageisresizedto 224 224. Eachframeisthen \u00d7 end time) in Fig 48 (c) and (d), in order to illustrate the processedby the Res Net-18 backboneindependentlytogen- signifi can tvariationsinlength. Themostfrequentcaseis erate 256 dimensionalfacefeatures. Thefeaturesequenceis 63 120 100 80 60 40 20 0 0 20 40 60 80 Percentage of frames w/ LAM annotations (%) spil C fo rebmu N 100 80 60 40 20 0 0 20 40 60 80 Percentage of frames w/ TTM annotations (%) (a) %of LAMperclips spil C fo rebmu N (b) %of TTMperclips 2000 1500 1000 500 0 0 2 4 6 8 Duration of LAMM annotations (sec) ycneuqer F 2000 1500 1000 500 0 0 2 4 6 8 Duration of TTM annotations (sec) (c) Durationof LAM ycneuqer F val test Acc m AP Acc m AP Random Guess 8.57 51.19 7.98 50.96 Baseline(Gaze 360) 91.78 79.90 87.97 78.07 Baseline(Random) 86.45 72.11 75.38 66.07 Table 30. Resultsof LAM.The base line model wasinitialized from Gaze 360[111](2 ndrow)andatrandom(3 rdrow). thesameas LAM.However,sometimes the speakersleave thefieldofvieworbecomeinvisibledueto the rapidmotion. Inthiscase,wepad the facesequences with blankimages. The MFCCfeatureisextractedevery 10 mswitha 25 mswin- dowlength. Thefeatureisthenfedinto the audiobackbone, a Res Net-18 designed for audiotasks[38]. Following the encoders,weconcatenate the audio and visualembeddings (d) Durationof TTM andpassthemto the finalclassificationheadtoget the TTM result for thevisiblefacesassociated with",
      "start_pos": 26334,
      "end_pos": 26846
    },
    {
      "chunk_id": 195,
      "paper_id": "ego4d",
      "text": "360[111](2 ndrow)andatrandom(3 rdrow). thesameas LAM.However,sometimes the speakersleave thefieldofvieworbecomeinvisibledueto the rapidmotion. Inthiscase,wepad the facesequences with blankimages. The MFCCfeatureisextractedevery 10 mswitha 25 mswin- dowlength. Thefeatureisthenfedinto the audiobackbone, a Res Net-18 designed for audiotasks[38]. Following the encoders,weconcatenate the audio and visualembeddings (d) Durationof TTM andpassthemto the finalclassificationheadtoget the TTM result for thevisiblefacesassociated with the segment. To Figure 48. Social task annotationstatistics.(a)Histogramshowing train the modelinparallel,wefirstsort the shortsegments thenumberofclipsvs.thepercentageofframes with look-at-me basedonthelength and group the segmentsintoabatchif annotations;(b)Histogramshowing the numberofclipsvs. the they have the sameduration. Thebatchsizeisrestrictedby percentageofframes with talk-to-meannotationsineachclip;(c) the GPUmemory;we useabatchsizeof 400. Themodelis Histogram showing the duration of look-at-me annotations; (d) alsooptimizedusing Adam with alearningrateof 5 10\u22124. Histogramshowing the durationoftalk-to-meannotations. \u00d7 Table 31 summarizes the TTMresults. TTMismorechal- lengingthan LAM.we cansee that our base line model only increasesthem APby 9.77%onthetestsplitincomparison encodedbya Bidirectional LSTM,whichhastworecurrent tother and omguess model. layers with dimensionality 256. The output is fed into a classificationheadtopredict the binary LAMresult for the center frame at the t-th timestamp. The LAM task has a I.4 Discussion classimbalanceissue,andwe useweightedcross-entropy While the benchmark task sofdetectingwhenattention and loss. Since the architectureissimilarto Gaze 360,wehave speakingbehaviors are directedtowards the first-person are twooptions for the initialization: first,initializing the back- closely related to existing analysis tasks, it is clear from bone from apretrained Gaze 360 model;second,initializing the base lineper for mance that the reissubstantialroom for the model randomly and training from scratch on Ego 4 D. improvement,withm APof 78.07 for LAMand 55.06 for Duringtraining,wesamplecenterframes with astrideof 3. TTM. Thenetworkisoptimizedby Adam with alearningrateof 5 10\u22124. The TTM task isparticularlychallengingbecauseitre- \u00d7 quiresanalysisoftheaudiocontenttounderst and the target Theresults are shownin Table 30. Our base line model audienceofanutterance,aswellas the fusionofaudio and achievesanm APof 66.07%onthetestsplitwheninitialized videocues. Themostcompletesolutionto this problem will randomly, and the per for manceishigherat 78.07%when requireanunderst and ingofthesemanticsof the utterancein initialized from Gaze 360. Thesefindingshighlight the close thecontextofanevolvingconversationalinteraction. Future relationshipbetween the LAMtask and gazeestimation.The workon this taskmightinvolvemoresophisticatedlanguage randomguess model achievesabout 8%accuracybecause modeling and possiblyhierarchicalanalysisapproaches that thenegativesamplesaccount for 92%ofthetestsplit and allow the integrationofcuesatmultiplelevels,e.g. atthe the model alwayspredictslookingatme. dialogleveltounderst and whoisparticipatinginaconversa- TTM The base line model for TTMdigestsmulti-modalin- tionalexchange,attheutteranceleveltoaccesssemantics, puts: eachaudiosegmentispaired with anassociatedface andat the audioleveltoexploitprosodi can dothercues. crop.Since the audiosegmentsvarysubstantiallyinduration, The LAM task presentsadditionalchallengessuchas the webreak the longutterancesintoshortsegmentswhosemax- need to deal with motion blur and fast head movements, imumdurationislimitedto 1.5 s. Ifthesegmentisshorter andmayalsobenefit from amoreexplicit model ingofhead than 0.15 s,weskipitin the trainingstage. Theassociated movement and the patterns of gaze behavior that arise in faces are alsoresizedto 224 224,and the videoencoderis conversationalinteraction. \u00d7 64 datacollectiontookplaceduring the COVID-19 pandemic, and the resultingstudyprotocols were designedtosafeguard participantsagainstadditionalrisk. Thesocialdataconsistsof data collectedatfivesites: At- lanta,Bloomington,Redmond,Twin Cities,and Singapore. In total, 764 hours of video and audio were collected for (a) LAM thesocialbenchmark task. Adetailedsummaryof the data collectionpracticesateachsite can befoundin Appendix A. I.6 Derived Tasks for Future Social Benchmarks Thecore task sof LAMand TTMdefineastartingpoint for analyzingmulti-modalegocentric data and inferringsocial interactions. Wenowdescribetwogroupsofpotentialfuture tasks,attentiontasks and speakingtasks,thatcouldbesup- portedvia the existingannotationsin Ego 4 DSocial and the (b) TTM gaze data collected from eyetrackers. Egocentric Attention Prediction(EAP) Priorwork[135, Figure 49. Baseline model architectures. (a)LAM model uses 137] has demonstrated the feasibility of predicting where a Res Net-18 asabackbonetoextract the featureofeachframe. A Bidirectional-LSTM then takes the sequence and encode the the camera-wearer is looking (i.e.",
      "start_pos": 26796,
      "end_pos": 27308
    },
    {
      "chunk_id": 196,
      "paper_id": "ego4d",
      "text": "DSocial and the (b) TTM gaze data collected from eyetrackers. Egocentric Attention Prediction(EAP) Priorwork[135, Figure 49. Baseline model architectures. (a)LAM model uses 137] has demonstrated the feasibility of predicting where a Res Net-18 asabackbonetoextract the featureofeachframe. A Bidirectional-LSTM then takes the sequence and encode the the camera-wearer is looking (i.e. their egocentric atten- featuresintooneembedding.Wepass the embeddingto FClayer tion)usingonlyegocentricvideocaptured from ahead-worn thatpredicts the LAMresult. (b)TTM model hastwoencoders. camera. Thisworkleveraged the contextofh and-eyecoor- Thevideoencoderis the sameas LAM.Theaudioencoderextracts dination tasks, which require gaze to be coordinated with the MFCCfrequencymapoftheaudiosegment and the featureis handmovements and objects. Asubsetof the Ego 4 DSocial fedintoa Res Net-18 network.Thevisual and audioembeddings dataincludesgazemeasurementsproducedbywearableeye areconcatenated and passedthrough the FClayertopredict the trackersby Indiana University and Georgia Techparticipants targetof this utterance. (e.g.,Pupil Invisible),which will greatlyexp and the sizeof dataforh and-eyecoordinationin the wild. val test Acc m AP Acc m AP Social Gaze Prediction (SGP) The LAM task addresses Random Guess 32.44 53.82 47.41 50.16 thespecialcaseofsocialgaze: apersonlooksat the camera- Baseline 64.31 56.50 49.75 55.06 wearer. Itispossibletogeneralize the taskbypredicting the socialgazetarget for eachof the visiblefacesinanegocen- Table 31. Resultsof TTM.The base line model isinitializedran- tric video, i.e., yp 0,1,...,M , where M is the total domly. \u2208 { } number of participants in a group social interaction, and p 0,1,...,M . pis the index for socialmembers. The \u2208{ } caseyp = qmeans that targetpwaslookingatparticipant I.5 Social dataset Collection q. The case yp = 0 captures alternative gaze targets, in- The Ego 4 DSocial data collectionprocesswasdesignedto cludingnon-socialgazetargets(e.g. lookingatanobject), achieve: 1)naturalisticinteractions,2)multi-modalcapture, lookingatpeoplewho are notwearinganegocentriccamera and 3)diverseparticipants and environments. Participants (with the result that groundtruthannotations are notavail- consistedoffriendsandfamilygroups and datawascaptured able),andlookingatunknowntargetsnotcapturedinany inresidences and localneighborhoods,ensuringnaturalistic of the egocentric videos. Let y\u02c6q,p denote the LAM label interactions. Capture hardw are varied across sites but in- fortargetpersonpvisibleinframeofegocentricvideo I q cludedwearablecameras,wearableeyetrackers(at Georgia capturedbyparticipantq. Then the SGPlabelisgivenby Tech and Indiana University), binauralrecordingsystems, yp =argmax y\u02c6q,p . The Ego 4 DSocial data includessyn- q{ } and smart watches (at Georgia Tech). Protocols included chronizedvideos from multiplesocialmembers,which will highly-structuredsettings,whereparticipants were askedto allowustoexp and theannotationbymatching the person ID playgamesoveraperiodofafewh our sin are sidence,and with the camera-wearers. Note that since the videorecorders unstructuredsettingswhereparticipantscapturedsocialin- arenotgenlocked,theidentificationofcorrespondingframes teractionsindailylifeoveraperiodaweekormore. Sample willonlybeapproximate. However, sincegazebehaviors socialinteractioncontextsincludedplayingboard and card persistovermultipleframeswedonotbelieve this will be games,preparingmeals,andgoingonwalks.Thebulkof the anissue. 65 Akeyissueindefiningthetaskis the determinationof the tonsite and contributedto the socialbenchmark for mulation participantset. Fora 2 Dversionof SGP,termed SCG-2 D, andpaperwriting. Vamsi Ithapucontributedto the social theparticipantsetisdefinedbyparticipantswho are visible benchmark for mulation and dataannotation. Hyun Soo Park inframet. Thisisasocialversionof the video-basedgaze led data collectionat the Twin Citiessite and contributedto follow task[37],wherethegoalistopredictwhe the reach thesocialbenchmark for mulation and paperwriting. targetparticipantislookingatanyoftheo the rparticipants Hao Jiang contributed to model development and data who are visiblein the frame. Amorechallenging 3 Dversion annotation. Yunyi Zhucontributedto model implementation of the task, SCG-3 D, uses all of the participants who are and experiments. Eric Zhongcong Xu contributed to the presentinthesocialsceneat the timeofframet. This task socialbenchmark data preparation and the modelimplemen- requires the ability to predict which participant the target tation and experiments,andcontributedtoall data collection personpislookingatin the casewhere that participantis relatedtasks for the Singaporesite. Ruijie Taocontributed notvisibleinframet. This can inprinciplebeaccomplished todatacollection for the Singaporesite. Fiona Ryanled the by maintaining a",
      "start_pos": 27258,
      "end_pos": 27770
    },
    {
      "chunk_id": 197,
      "paper_id": "ego4d",
      "text": "presentinthesocialsceneat the timeofframet. This task socialbenchmark data preparation and the modelimplemen- requires the ability to predict which participant the target tation and experiments,andcontributedtoall data collection personpislookingatin the casewhere that participantis relatedtasks for the Singaporesite. Ruijie Taocontributed notvisibleinframet. This can inprinciplebeaccomplished todatacollection for the Singaporesite. Fiona Ryanled the by maintaining a birds-eye view layout map of the social datacollectionef for tfor the Atlantasite,includingprotocol scene, that captures the approximate spatial relationships design,multimodalsensordeployment and synchronization, between the participants. Suchalayoutmapcouldbeused andde-identification. Miao Liucontributedto data collec- inconjunction with anapproachlike Gaze 360[111]tosolve tion and analysis for the Atlantasite. Audrey Sou the rland the SCG-3 Dtask.Note that this task couldpotentiallybenefit contributedto the protocoldesign,IRBauthoring and sub- fromtakingrecordedbinauralaudioasanadditionalinput,as mission, participant recruiting, and data ingestion for the theabilitytolocalizesounds our cescouldprovideadditional Atlanta site. Jayant Sharma contributed to participant re- cues for determining the locationsofgazetargetswhich are cruiting,datacollection,IRBsubmission,analysis,anddata notvisiblein the video. ingestion for the Twin Citiessite. Yuchen Wangcontributed totheprotocoldesign,participantrecruiting,and data collec- Utterance Target Prediction(UTP) The TTMtask can be tion for the Bloomingtonsite. Weslie Khoodeveloped the generalized to the full set of participants in the same way multi-camerasynchronizationandde-identificationpipelines that LAMcan beextendedto SGP.Theinputspaceis the atthe Bloomingtonsite. sameas TTMand the outputspaceissimilarto SGP,where yp = q means that participant p is talking to participant Acknowledgements The social benchmark team would q, and yp = 0 denotes the cases where the participant is liketoacknowledge the followingadditionalcontributions nottalkingtoanyone,oristalkingtosomeo new hoisnot fromindividualsateachsite: Atlanta: Jeffrey Valdez(re- wearinganegocentriccamera(and the reforegroundtruth cruitment and data collection), Gabriella Stripling, Ruth cannotbedetermined). Incontrastto SGP,UTPrequires the Stolovitz,and Andrea Sucre-Pardo(recruitment and dataset identificationofallof the targetrecipientsofanutterance. de-identification). Twin Cities: Reese Kneel and, Angad Infact,our TTMannotationalreadysupports this task,as Cheema, Silong Tan, Anjali Oleksy, Zhiteng Cao, Di- itdifferentiatesthecasewhere the utteranceisdirectedto ana Begelman(datacollection and annotation)Facebook: multiple participants including the camera wearer. This Samuel Clapp and Peter Dodds (binaural audio recording additionallabelisignoredinthedesignof the simpler TTM andmultimodalsynchronization). Bloomington: Zunaeed task. Salahuddin,Zehua Zhang,Ziwei Zhao. Transcript-based Variants For all of the previously- definedsocial task sitispossibletodefineavariantwhich utilizesatranscriptof the audiofileasanadditionalinput modality. For example, the TTM-T task is the variant of TTM with the prediction defined as yp = f(I,A,T,B), where T the transcript (time-stamped sequence of words) obtained from A. This can potentiallysimplify the useof dialogcuestoidentify the intendedtargets for utterances and socialgaze. I.7 Contributionsstatement James M.Rehgco-led the socialbenchmarkeffort and paper writing. Hanbyul Jooco-led the socialbenchmarkeffort and dataannotation. Mike Zheng Shouco-led the socialbench- markeffortandproblem for mulation and modelingexperi- ments. David Crandallled data collectionat the Blooming- 66 J.Forecasting Benchmark Short-Term Object Interaction Anticipation Thissectiondetails the Forecastingbenchmark task defi- This task aimstopredict the nexthuman-objectinteraction nitions,annotations,baselinemodels,andresults. happeningafteragiventimestamp. Givenaninputvideo, thegoalistoanticipate: J.1 Formal task sdefinitions \u2022 Thespatialpositionsof the activeobjects,amongthose whicharein the scene(e.g.,boundingboxesaround the Asnotedin the mainpaper,there are four for ecastingtasks: objects). Weconsider the nextactiveobjecttobe the future locomotion movement prediction, future hand pre- nextobjectwhich will betouchedby the user(either diction,short-termobjectinteractionanticipation,andlong- with the irh and sor with atool)toinitiateaninteraction; termactionanticipation. \u2022 Thecategoryofeachof the detectednextactiveobjects (e.g.,\u201cknife\u201d,\u201ctomato\u201d); Future Locomotion Movements Prediction \u2022 Howeachactiveobject will beused,i.e.,whataction will be per for med on the active objects (e.g., \u201ctake\u201d, This task aims to predict the future locomotion of a user \u201ccut\u201d); givenasequenceofpastimages. Weformulate the problem as: \u2022 Whentheinteractionwi the achobject will begin(e.g., \u201cin 1 second\u201d, \u201cin 0.25 seconds\u201d). This is the time (cid:2) (cid:3)T =",
      "start_pos": 27720,
      "end_pos": 28232
    },
    {
      "chunk_id": 198,
      "paper_id": "ego4d",
      "text": "Howeachactiveobject will beused,i.e.,whataction will be per for med on the active objects (e.g., \u201ctake\u201d, This task aims to predict the future locomotion of a user \u201ccut\u201d); givenasequenceofpastimages. Weformulate the problem as: \u2022 Whentheinteractionwi the achobject will begin(e.g., \u201cin 1 second\u201d, \u201cin 0.25 seconds\u201d). This is the time (cid:2) (cid:3)T = x t+1 x t+F =f(x t\u2212T , ,x t\u22121 ; ), tothefirstframeinwhichtheusertouches the active X \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 I (26) object(timetocontact).Thisprediction can beusefulin scenarioswhichinvolvehuman-machinecollaboration. where isthefuturetrajectory,T and F are the past and For instance, an assistive system could give an alert X future time horizons, respectively, x t is the point on the if a short time to action is predicted for a potentially trajectoryattimet,and istheegocentricimageattimet. dangerousobjecttotouch. I Withanassumption that the personwalksoveramajorplane Inthis task,models are requiredtomakepredictionsat (e.g., ground plane), we represent the trajectory in a 2 D plane,i.e.,x R 2. a specific timestamp, rather than densely throughout the t \u2208 video. Figure 51 illustrates the set-up. The model isallowed Theessenceof the locomotion task istodesignafunc- to process the video up to frame t, at which point it must tion f to predict a set of plausible K future trajectories anticipate the next active objects, and how they will take k given the currentimage. Since the reexistsanumber k {X } partinaninteractionin\u03b4seconds,where\u03b4isunknown. The ofplausiblefuturetrajectories with differenttopology,e.g., model can makezeroormorepredictions. Eachprediction trajectories that bifurcate at an Y-junction, we predict K indicates the next active object in terms of noun class (n\u02c6) futuretrajectories. andboundingbox(\u02c6b),averbindicating the futureaction(v\u02c6), aswellas the timetocontact(\u03b4\u02c6),whichestimateshowmany Future Hand Prediction seconds in the future the interaction with the object will begin. Eachpredictionalsocomprisesaconfidencescore(s\u02c6) In addition to future locomotion movements prediction, used for evaluation. we consider another challenging task of predicting future Specifically,let V beanuntrimmedvideo.Wewilldenote hand positions of key-frames (see visual illustration in with V theframeof V occurringattime-stept and with V t :t Fig. 50). Specifically, we denote the contact frame 17 as thevideosegmentstartingat the beginningof V (timestamp x c , pre-conditionframe 18 asx p , and the threeframespre- 0)andendingattimestampt. Givenatimestampt,denoted ceding the pre-condition frame by 0.5 s, 1 s and 1.5 s as as\u201cstoppingtime\u201d,theshort-termobjectinteractionanticipa- x p 1 , x p 2 , x p 3 , respectively. Formally, given an input ego- tion task requires that amodelisabletoexploit the observed centric video 1.5 s before the pre-condition time step (de- video V topredict N tuples(where N isarbitrary): :t noted as x = x ,...,x , with t referred as { p 3\u2212to\u22121 p 3\u22121 } o (\u02c6b ,n\u02c6 ,v\u02c6,\u03b4\u02c6,s\u02c6) N (27) observation time), this task seeks to predict the positions { i i i i i }i=1 of both hands (hl,hr) in the future key frames, where i i where: i c,p,p ,p ,p . \u2208{ 1 2 3 } \u2022 \u02c6b R 4 isaboundingboxindicating the positionof i \u2208 17 Thecontactframeisdefinedasthefirstframeinwhich the usertouches thepredictednextactiveobject; theobject,hencethemomentinwhich the objectbecomesactive. 18 Asdefinedin Section G,thepre-conditionframemarksamomentprior \u2022 n\u02c6 i \u2208N isanounindicatingtheclassof the nextactive tothestate-changeofanobject. object,where isthesetofpossiblenouns. N 67 { \u210e\ud835\udc59,\u210e\ud835\udc5f} { \u210e\ud835\udc59,\u210e\ud835\udc5f} \ud835\udc56 \ud835\udc56 \ud835\udc57 \ud835\udc57 \u2026 \u2026 Input Video Future Key Frames Figure 50.Exampleoffutureh and prediction. Last observedframe (\ud835\udc7d ) Unobserved future frame (\ud835\udc7d ) \ud835\udc29\ud835\udc2b\ud835\udc1e\ud835\udc1d\ud835\udc22\ud835\udc1c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udfcf \ud835\udc95 \ud835\udc95+\ud835\udf39 \ud835\udc4f\u0de0 = 450,90,510,140 1",
      "start_pos": 28182,
      "end_pos": 28694
    },
    {
      "chunk_id": 199,
      "paper_id": "ego4d",
      "text": "Asdefinedin Section G,thepre-conditionframemarksamomentprior \u2022 n\u02c6 i \u2208N isanounindicatingtheclassof the nextactive tothestate-changeofanobject. object,where isthesetofpossiblenouns. N 67 { \u210e\ud835\udc59,\u210e\ud835\udc5f} { \u210e\ud835\udc59,\u210e\ud835\udc5f} \ud835\udc56 \ud835\udc56 \ud835\udc57 \ud835\udc57 \u2026 \u2026 Input Video Future Key Frames Figure 50.Exampleoffutureh and prediction. Last observedframe (\ud835\udc7d ) Unobserved future frame (\ud835\udc7d ) \ud835\udc29\ud835\udc2b\ud835\udc1e\ud835\udc1d\ud835\udc22\ud835\udc1c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udfcf \ud835\udc95 \ud835\udc95+\ud835\udf39 \ud835\udc4f\u0de0 = 450,90,510,140 1 \ud835\udc5b\u0ddc =\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc54\u210e 1 \ud835\udc63\u0ddc =\ud835\udc61\ud835\udc4e\ud835\udc58\ud835\udc52 1 \ud835\udeff\u1218 =0.75\ud835\udc60 1 \ud835\udc60\u01b8 =0.8 1 \ud835\udc29\ud835\udc2b\ud835\udc1e\ud835\udc1d\ud835\udc22\ud835\udc1c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udfd0 frame of \ud835\udc4f\u0de0 = 500,100,550,150 2 contact \ud835\udc5b\u0ddc =\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc54\u210e 2 \ud835\udc63\u0ddc =\ud835\udc61\ud835\udc4e\ud835\udc58\ud835\udc52 2 \ud835\udeff\u1218 =0.75\ud835\udc60 2 \ud835\udc60\u01b8 =0.75 2 \ud835\udeff \ud835\udc61 \ud835\udc61+\ud835\udeff Input video: \ud835\udc49 :\ud835\udc61 Figure 51.Exampleofshort-termobjectinteractionanticipation. \u2022 v\u02c6 is a verb indicating the action which will be Figure 51 illustrates the proposed task. Givenavideo V , i :t \u2208 V per for med,where isthesetofpossibleverbs; a method should be able to detect the next active objects V (e.g., two instances of \u201cdough\u201d), predict the action which \u2022 \u03b4\u02c6 i R+isthetimetocontact,apositivenumberwhich will beper for med with thatobject(e.g.,\u201ctake\u201d),and the time \u2208 estimateshowmanysecondintothefuture the interac- tocontact(e.g.,0.75 s). tion with the object will begin; \u2022 s\u02c6 [0,1]isaconfidencescoreassociatedto the predic- Long-Term Action Anticipation i \u2208 tion. Objects with alargeconfidencevalue are deemed tobelikelynext-active. Long-term action anticipation aims to predict further into the future. Rather than predict the next action at a given The model isallowedtoperform N predictions for each timestamp,models will berequiredtopredict the sequence observed example (with N arbitrary) both to account for of Z future actions which the camera-wearer is likely to thepresenceofmultiplenext-active-objects and tohandle perform. Thisisimportant for long-horizonplanningwhere the multi-modality of future predictions. Each of the N asequenceofactionsisrequiredtobeper for medinaspecific predictionsisintendedasaplausiblefutureobjectinteraction. ordertoachieveagoal. Critically,theseactionsoccurover 68 \ud835\udc5b\u0ddc ,\ud835\udc63\u0ddc , \ud835\udc5b\u0ddc ,\ud835\udc63\u0ddc , \ud835\udc5b\u0ddc ,\ud835\udc63\u0ddc , \ud835\udc5b\u0ddc ,\ud835\udc63\u0ddc 1,\ud835\udc56 1,\ud835\udc56 2,\ud835\udc56 2,\ud835\udc56 3,\ud835\udc56 3,\ud835\udc56 4,\ud835\udc56 4,\ud835\udc56 \ud835\udc56\ud835\udc61\u210eprediction: kneaddough \u2192 putdough \u2192 packspice \u2192 pourspice Input video \ud835\udc61 Figure 52.Exampleoflong-termactionanticipation.Afterobservingavideouptoaparticulartimestept,amethodshould beableto predict the sequenceofactions that willlikelyoccur,inthecorrectorder(e.g.,first\u201ctakedough\u201d,next\u201cputdough\u201detc.) long time horizons, may be of variable length and do not focuson the handmanipulation. Weconsidervideos from occuruni for mlyacrosstime(e.g.,anactionevery 5 s). Thus, glass-mountedcamerasofwhichfieldofviewapproximately the task is defined at a more abstract level \u2014 models are aligns with the firstperson.(4)3 Dreconstruction and ground required to predict sequences of action classes (verb and planeneedtobeaccurate. Afterrunningstructurefrommo- noun), rather than time to action or to next active objects tion,weensure 3 Dreconstruction from the videosachieves boundingboxesin the currentframe. reasonablequalitybychecking 2 Dreprojectionof the point More for mally,givenanuntrimmedvideo V andastop- cloud and groundplane. Givenasetof the sevideoclips,we pingtimetasdescribedabove,thelong-termactionanticipa- chooseframes for training/testing data for everysecond. tion model mustobserve V andpredict N setsofsequences :t of Z plausiblefutureactions: Remaining Tasks (n\u02c6 ,v\u02c6 ) Z N (28) {{ z,i z,i }z=1}i=1 For the remaining tasks we first manually ranked the sce- narios base dontheirapplicabilityto the forecastingtasks. where: Forinstance,scenarioslikecarpentery were highpriority for \u2022 n\u02c6 is the predicted noun and v\u02c6 is the forecasting whereas walking in the park was low-priority. z,i z,i \u2208 N \u2208 V predictedverbofthez-thfutureaction. Wescoredallscenarios from 1-3 basedon this priority. We imposeconstraintson the minimumnumberof hours and \u2022 { (n\u02c6 z,i ,v\u02c6 z,i ) } Z z=1 represents the sequenceoffutureac- participantstosub-selectscenarios that havesufficient data tionssortedbythepredictedorderinwhich the ywill for training(eachparticipantshould have contributedatleast appearin the video. 15 minutes;and the reshould beatleast 20 minutesofvideos for that scenario). Next,wechunk our videosinto 5 minute Like the",
      "start_pos": 28644,
      "end_pos": 29156
    },
    {
      "chunk_id": 200,
      "paper_id": "ego4d",
      "text": "We imposeconstraintson the minimumnumberof hours and \u2022 { (n\u02c6 z,i ,v\u02c6 z,i ) } Z z=1 represents the sequenceoffutureac- participantstosub-selectscenarios that havesufficient data tionssortedbythepredictedorderinwhich the ywill for training(eachparticipantshould have contributedatleast appearin the video. 15 minutes;and the reshould beatleast 20 minutesofvideos for that scenario). Next,wechunk our videosinto 5 minute Like the short-termobjectinteractionanticipation task, clips,anduse the followingalgorithmtoselectclipstobe the model is allowed to generate N sets of predictions to labeled. Toensuregeographicaldiversity,wedistribute the account for the multi-modalnatureoffutureprediction. Fig- totalh our soveruniversitiesandr and omlyselectclips from ure 52 illustrates the proposed task. each to fill the hours allocated to that university. If there are universities that contributed less, then their hours are J.2 Data Selection distributedacrosstheo the runiversities. Toselect the clips Future Locomotion Movements Prediction givenauniversity and the hoursallocated; wewouldfirst sampleaparticipant,thensampleavideo for thatparticipant, Egocentricvideos for locomotion and hand-objectinterac- andsample 1 clip from thatvideo. Forcertainrepetitivesce- tion are nearlymutuallyexclusive. Among the sevideos,we narios(likebrickmaking),wereject this clipifwealready skim through each video to manually identify video clips haveselectedatleast 2 clips from the samevideo. Werepeat (beginning and endframes)thatsatisfy the followingselec- theprocessuntil the requirednumberofhours are selected. tioncriteria. (1)Locomotion,bydefinition,involvesdiverse activitiesassociated with walking. Theclipshouldinclude J.3 Data Annotation substantialtranslationalmovement. (2)Eachvideoclipmust belongerthan 10 seconds for pasttrajectoryobservation and Future Locomotion Movements Prediction future prediction. (3) The videos must observe surround- ing scenes. This differs from the videos for hand-object We generate the ground truth of future trajectories using interactionwhere the cameraisdeliberatelytilteddownto 3 D reconstruction of the camera trajectories. Given a se- 69 \uf049 o x n Offset t Ground plane (a) Geometry (b) Futuretrajectoryprediction Figure 53.(a)Werepresentthefuturetrajectoryofapersonusing the groundplane.Given the 3 Dreconstructionof the cameratrajectory, weprojectitintotheestimatedgroundplanetoform the futuretrajectory.(b)Thegroundtruthfuturetrajectory(blue)and the predicted trajectories(red and white)areshownintheegocentricimage with the groundplanecoordinate(magentagrid).Wepredicttop 5 trajectories where the toppredictionismarkedinred. Data Outdoor Indoor Mixed Total 1 sand 1.5 sbefore the pre-conditiontimestep. Therefore, Train 34.1 k 0.41 k 16.7 k 51.3 k foreachinteraction the rewill be 5 keyframeslabeled with Val 7.5 k 0.23 k 6 k 13.9 k boundingboxesofhands,including the contactframe. We Test 7.4 k 0.18 k 3 k 10.6 k use the bouding box center as the ground truth of hands Table 32.Wesplit the image data for locomotionpredictionbased positions. onscenes that includingoutdoor,indoor,andmixed. Short-Term Object Interaction Anticipation quence of egocentric images, we reconstruct the 3 D ego- Eachvideo V ofthe data setislabeled with asetofshortterm motionandscenegeometryusingast and ardstructure from object interaction anticipation annotations = S (j) S V { V } j motionpipeline with afewmodificationtoh and lealarge indicatingtheoccurrenceofobjectinteractionsin the video. numberofimages. With the 3 Dscenepointcloud,wees- Eachannotation timate the ground plane using RANSAC with the ground planenormalprior. The 3 Dreconstructedcameratrajectory S V (j) =(t( s j), { n ( h j) } h ,v(j), { A ( h j) } h , { B h (j) } h ) (29) is projected onto the ground plane to form the 2 D future trajectoryasshownin Figure 53. includes: Our image dataset includes locomotion in outdoor, (j) \u2022 t s :thetimestampindicatingthebeginningof the inter- indoor, and mixed scenes. We split the image data action with the activeobjects. Thisis the firstframein into training/validation/testing sets with approximately whichtheusertouchesatleastoneof the activeobjects; 70%/15%/15%,respectively. Theratioacrossscenesdoes notexactlymatchbecause the splitisper for med base don (j) \u2022 n : the set of categories of the h",
      "start_pos": 29106,
      "end_pos": 29618
    },
    {
      "chunk_id": 201,
      "paper_id": "ego4d",
      "text": "\u2022 t s :thetimestampindicatingthebeginningof the inter- indoor, and mixed scenes. We split the image data action with the activeobjects. Thisis the firstframein into training/validation/testing sets with approximately whichtheusertouchesatleastoneof the activeobjects; 70%/15%/15%,respectively. Theratioacrossscenesdoes notexactlymatchbecause the splitisper for med base don (j) \u2022 n : the set of categories of the h interacted ob- the(anonymous)participant ID.Thesummaryof the data { h } h jects; split can befoundin Table 32. \u2022 v(j): theclassoftheactioninvolving the activeobjects; Future Hands Movements Prediction (j) \u2022 A : theboundingboxannotations for the active { h } h (j) Forthe the futurehandposition and trajectoryprediction,the objects. The cardinality of { A h } h is equal to the annotation will beper for medbylabelingboundingboxes cardinalityof n (j) , i.e., A (j) = n (j) . The { h } |{ h } h | |{ h }| around hands in the frame in which the user touches the hthset A (j) containsboundingboxannotations for active objects as well as in frames preceding each object { h } h (j) interactions. Hands bounding boxes will be associated to theactiveobjectsofcategoryn h attimestampt s ; a label useful to distinguish among left and right hands. (j) \u2022 B : the bounding box annotations for the next Therefore,givenanobjectinteraction,wewillannotatekey { h } h (j) activeobjects. Thecardinality B isequalto the framesprecedingthebeginningof the interaction. Specif- { h } h (j) (j) (j) ically,t c andt p denote the timestepofcontactframe and cardinality of { A h } h , i.e., |{ B h } h | = |{ A h } h | . pre-conditionframe,andt ,t ,t ,denotetimesteps 0.5 s, Thejthset B (j) contains the boundingboxannotations p 1 p 2 p 3 h 70 future action short-term annotations S (i) are available (see Section J.3) V andavalue for Zhas been chosen,thelong-termannotations (j) L can beeasilyobtainedbysampling the first Z actions V \ud835\udc61 \ud835\udc60 (\ud835\udc57)\u22124\ud835\udefc \ud835\udc61 \ud835\udc60 (\ud835\udc57)\u22123\ud835\udefc \ud835\udc61 \ud835\udc60 (\ud835\udc57)\u22122\ud835\udefc \ud835\udc61 \ud835\udc60 (\ud835\udc57)\u2212\ud835\udefc \ud835\udc61 \ud835\udc60 (\ud835\udc57) annotatedinvideo V beginningaftertimestampt(j). More (j) formally,thefutureactionlabels for L areobtainedas: Figure 54. Anexampleofhowframes are sampledtobelabeled V withnextactiveobjectannotations.Foragivenactioni,wesample mframesatregularintervals\u03b1.Ifwesetm=4 and\u03b1=0.5,we {(n(iz),v(iz))|(t(iz),{n(iz)} ,v(iz),{A(iz)} ,{B(iz)} )\u2208S \u2227 label the frameofcontactaswellas 4 framesalongasegmentof 0 s h h h h h h V 2 sprecedingthebeginningof the actionataframerateof 2 fps. t( s iz) \u2265t(j)\u2227 t(i 1) \u2264...\u2264t(i Z)\u2227 s s ofnextactiveobjectsofclassn h . Inparticular, B h (j) (cid:64)S V (j) \u2208S V |t(j) \u2208/ {i 1 ,...,i Z },t\u2264t s (j) <t( s i Z)}Z z=1 containsannotations for the sameobjectinstancesanno- (j) (j) wheren (iz) refersto the primaryinteractedobject from the t i a c t a e l d ly, in B A h (j h ) = ,tr { a B ck l ( e , j h d ) | l in = fr 1 am ,.. e . s ,m pr } e , ce w d h in e g re t s B l ( , . j h ) S i p s e t c h if e - s e e x t am of p i l n e t 0 e o r f ac h t o e w do lo b n je g c",
      "start_pos": 29568,
      "end_pos": 30080
    },
    {
      "chunk_id": 202,
      "paper_id": "ego4d",
      "text": "B l ( , . j h ) S i p s e t c h if e - s e e x t am of p i l n e t 0 e o r f ac h t o e w do lo b n je g c - t t s er { m n ( h a iz n ) n } o h t . at F io ig n u s r a e re 56 ob il t l a u i s n t e ra d te f s or a m n setofboundingboxannotationsofnextactiveobjectof short-termannotations. classn annotatedattimestampt l\u03b1. Heremindi- h s \u2212 catesthenumberofframespreceding the beginningof Annotationanalysis theinteractioninwhichobjects are annotated,whereas \u03b1isthetemporaldistancebetweenthesampledframes. Datasetstatistics Asdiscussedearlier,oneof our primary For instance, setting \u03b1 = 0.5 s and m = 4, we will objectives when selecting the data to annotate was to labeltheframeinwhich the objectisinteractedaswell maximize the diversityintermsofactivities and geographic as 4 framesina 2 ssegmentpreceding the interaction. locations. Our data setincludesscenariosspanningawide Figure 54 showsanexampleofhowframes are sampled range of everyday activities (e.g., gardening, cleaning, with the consideredscheme. fishing,etc.). Inadditiontodiversityacrossscenarios,there Figure 55 reportsasampleclip with the discussedannota- isalsogeographicdiversity with inscenarios. Forexample, tions. Thetimestampt isselectedas the firstoneinwhich cookingmaylookverydifferentin Italy,India,Saudi Arabia, s theusertouches the activeobjects.Theframesfollowing this or Japan. In Figure 38,weshow the resultingscenario and timestamp are notlabeled. Activeobjectboundingboxes are universitydistributions. Overall,ourbenchmarkconsistsof labeledattimestampt ,whereasnextactiveobjectbounding 120 hoursofannotatedvideocoming from 53 scenarios,7 s boxes are labeledinframesprecedingt . universities,and 406 participants. s Temporal structure of activities Human activity is goal- Long-Term Action Anticipation driven and structured over time, with certain action se- Eachvideo V islabeled with asetoflong-termactionan- quencesbeingfavoredoverothers.Wemeasure this temporal notations L (j) , corresponding to a stopping time until structureusing Normalized Pointwise Mutual Information { V } j which the video can beobserved,andasequenceof Z future (NPMI)[41]overpairsofactionsfollowingpriorwork[92]. actionlabelsdefinedasfollows: NPMIisameasureofhowlikelyactionsfolloweachother. Inour data set,typicalpatternsinclude\u201cpullgrass throw L ( V j) =(t(j), { (n( z j),v z (j)) } Z z=1 ) (30) grass(0.87)\u201d,\u201choldspinach \u2192 cutspinach(0.83)\u201d, \u2192 \u201cturn-on faucet turn-offfaucet(0.68)\u201d,\u201ctakecloth foldcloth where: \u2192 \u2192 (0.49)\u201detc. Severalactionsalsooccurinsequence with high \u2022 t(j): the timestamp until which the video can be ob- NPMIscoresduetotherepetitivenatureof the activity. For served(i.e.,V )beforemakingpredictionsoffuture example, \u201cflippage flippage(0.83)\u201dwhilereading, or :t(j) \u2192 actions; \u201ccutcarrot cutcarrot(0.82)\u201dwhilecooking. Finally,we \u2192 see common action sequences involving multiple objects (j) \u2022 n z :thenouncategoryof the primaryinteractedobject like\u201cfilltire closevalve(0.89)\u201d,or\u201cliftvacuum-cleaner inthez-thfutureaction; cleanstair \u2192 case(0.87)\u201d. Thisstructureisvaluable and can \u2192 (j) informlong-termactionanticipationmodels. \u2022 v z : theverbdescribinghow the objects will beinter- dataset split To facilitate future research and compar- acted with inthez-thfutureaction. isons, we construct training, validation, and test splits Foreachvideo,t(j)areselected from the lasttimestampof containing 40%, 30%, and 30% of the data, respectively. eachannotatedobjectinteraction.Itisworthnoting that once Wenote,however,thatwedonotrelease the groundtruth 71 \ud835\udc35 1 ( , \ud835\udc56 \u210e )={[0.4,0.6,0.6,1.0]} \ud835\udc35 1 ( , \ud835\udc56 \u210e )={[0.5,0.8,0.7,1.0]} \ud835\udc35 1 ( , \ud835\udc56 \u210e )={[0.4,0.5,0.6,0.9], \ud835\udc35 1 ( , \ud835\udc56 \u210e )={[0.3,0.2,0.6,0.7], \ud835\udc34( \u210e \ud835\udc56)={[0.1,0.2,0.5,0.8], First frame of contact [0.4,0.9,0.6,1.0]} [0.3,0.7,0.6,1.0]} [0.3,0.6,0.6,1.0]} UNLABELED FRAMES \ud835\udc5b(\ud835\udc56)=\ud835\udc4f\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61 \u210e \ud835\udc61(\ud835\udc56)\u22123\ud835\udefc \ud835\udc61(\ud835\udc56)\u22122\ud835\udefc \ud835\udc61(\ud835\udc56)\u2212\ud835\udefc \ud835\udc61(\ud835\udc56) \ud835\udc63(\ud835\udc56)=\ud835\udc61\ud835\udc4e\ud835\udc58\ud835\udc52 \ud835\udc60 \ud835\udc60 \ud835\udc60 \ud835\udc60 Figure 55.Exampleofannotations for the short-termobjectinteractionanticipation task. Video \ud835\udc49 \ud835\udc46(1) \ud835\udc46(2) \ud835\udc46(3) \ud835\udc46(4) \ud835\udc46(5) \ud835\udc46(6) \ud835\udc46(7) \ud835\udc46(8) \ud835\udc46(9) \ud835\udc46(10) \ud835\udc46(11) \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc61(\ud835\udc57) Z=3",
      "start_pos": 30030,
      "end_pos": 30542
    },
    {
      "chunk_id": 203,
      "paper_id": "ego4d",
      "text": "of contact [0.4,0.9,0.6,1.0]} [0.3,0.7,0.6,1.0]} [0.3,0.6,0.6,1.0]} UNLABELED FRAMES \ud835\udc5b(\ud835\udc56)=\ud835\udc4f\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61 \u210e \ud835\udc61(\ud835\udc56)\u22123\ud835\udefc \ud835\udc61(\ud835\udc56)\u22122\ud835\udefc \ud835\udc61(\ud835\udc56)\u2212\ud835\udefc \ud835\udc61(\ud835\udc56) \ud835\udc63(\ud835\udc56)=\ud835\udc61\ud835\udc4e\ud835\udc58\ud835\udc52 \ud835\udc60 \ud835\udc60 \ud835\udc60 \ud835\udc60 Figure 55.Exampleofannotations for the short-termobjectinteractionanticipation task. Video \ud835\udc49 \ud835\udc46(1) \ud835\udc46(2) \ud835\udc46(3) \ud835\udc46(4) \ud835\udc46(5) \ud835\udc46(6) \ud835\udc46(7) \ud835\udc46(8) \ud835\udc46(9) \ud835\udc46(10) \ud835\udc46(11) \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc49 \ud835\udc61(\ud835\udc57) Z=3 \ud835\udc3f(\ud835\udc57)= \ud835\udc61\ud835\udc57, \ud835\udc5b\ud835\udc57,\ud835\udc63\ud835\udc57 , \ud835\udc5b\ud835\udc57,\ud835\udc63\ud835\udc57 , \ud835\udc5b\ud835\udc57,\ud835\udc63\ud835\udc57 \ud835\udc49 1 1 2 2 3 3 Figure 56. An exampleof a long-termannotation L(V:t) for anuntrimmed video V at timestamp t can beobtained fromshort-term annotations S(i).Intheexample,Z =3,hencethelongtermannotationisobtainedbyconsidering the firstthreeactionsbeginningafter V timestampt. annotations for the test set. Following common practice, is smaller than the error tolerance, it is considered as a evaluation on the test set will be supported through the correct trajectory prediction. PCT(cid:15) measures how many publicevaluationserver and leaderboard. Weassigndatato trajectoriesamong K retrievedtrajectories are closeto the splitsr and omlyat the levelof 5 minuteclips. Thisensures groundtruthtrajectory. that all interactions within a 5 minute clip were labeled byanannotator and providesenoughtemporalcontext for Future Hand Movement Asfor the futureh and smovements long-termvideotasks,likelong-termactionanticipation. prediction,weonlyconsider the keyframeprediction,and there for eadopt Mean Key Frame Displacement Error Con- tact(M.Disp.) Key Frame Displacement Errorasevaluation J.4 Evaluationmeasures metrics(C.Disp.): Future Locomotion and Hands Movements Prediction \u2022 Mean Key Frame Displacement Error(M.Disp.): Future Locomotion Wemeasuretheaccuracyof the predic- tion using two metrics. (1) K best mean trajectory error (K-MTE):wemeasure Kbesttrajectoryerror: D m = n 1 (cid:88) (cid:107)h i \u2212h\u02c6 i (cid:107) (33) i\u2208Ht 1 (cid:88) K MTE= argmin (cid:80) v t x t x (cid:98)t , (31) \u2212 v (cid:107) \u2212 (cid:107) {Xk}K k=1 t t t H t refers to the set of visible hand positions of key frames, andnis the lengthofset H . h denotes the t i x R 2 isthepredictedlocationattimet,x istheground t (cid:98)t predictedh and positionin the imagecoordinate,while \u2208 truthlocation,andv t isthevisibility. Thevisibilityindicates h\u02c6idenotesthegroundtruthhandpositions. the availability of the ground truth trajectory, i.e., due to severeegocentricvideos,thegroundtruthtrajectoriesmay includemissing data. v =0 indicatesmissing data attime \u2022 Contact Key Frame Displacement Error(C.Disp.): t t. (2)Probabilityofcorrecttrajectory(PCT):wemeasure thesuccessrateof the correcttrajectoryretrieval: D c =(cid:107)h c \u2212h\u02c6 c (cid:107) (34) (cid:32) (cid:33) 1 1 (cid:88) PCT(cid:15)= \u03b4 (cid:80) v t x t x (cid:98)t <(cid:15) , (32) h c refersto the handpositionsat Contactframe. K v (cid:107) \u2212 (cid:107) t t t where\u03b4()isoneifthestatementistrue and zeroo the rwise. Note that all reports are reported on downsampled video \u00b7 (cid:15)isthetrajectoryerrortolerance,i.e.,ifthetrajectoryerror frames with heightof 256 andoriginalaspectratio. 72 Short-Term Object Interaction Anticipation \u2022 Noun+Verb Top-Km AP:predictioni and annotation j areapossiblematchif the followingconditions are Methods will beevaluatedat the timestampsinwhichnext- satisfied: activeobjects have beenannotated,i.e., * IOU(\u02c6b ,b )>0.5; (cid:110) i j tt = t l \u03b1 s * n\u02c6 =n ; | \u2212 \u00b7 i j \u2200 t s \u2208{ t( s j) |\u2203 h:B h (j) (cid:54) = \u2205} j * v\u02c6 i =v j . (cid:111) l 1,...,m (35) \u2022 Noun+TTCTop-Km AP:predictioni and annotation \u2200 \u2208{ } j areapossiblematchif the followingconditions are (j) (j) satisfied: where t s h:B = j isthesetofalltimestampsin- { |\u2203 h (cid:54) \u2205} dicating the beginningofaninteraction,forwhichatleast * IOU(\u02c6b ,b )>0.5; i j onenextactiveobjecthas been annotated, and\u03b1andmare * n\u02c6 =n ; definedin Appendix J.3. i j Sincedetectingnextactiveobjectsisamajorpartof the * \u03b4\u02c6 \u03b4 <T .",
      "start_pos": 30492,
      "end_pos": 31004
    },
    {
      "chunk_id": 204,
      "paper_id": "ego4d",
      "text": "} j areapossiblematchif the followingconditions are (j) (j) satisfied: where t s h:B = j isthesetofalltimestampsin- { |\u2203 h (cid:54) \u2205} dicating the beginningofaninteraction,forwhichatleast * IOU(\u02c6b ,b )>0.5; i j onenextactiveobjecthas been annotated, and\u03b1andmare * n\u02c6 =n ; definedin Appendix J.3. i j Sincedetectingnextactiveobjectsisamajorpartof the * \u03b4\u02c6 \u03b4 <T . i j \u03b4 | \u2212 | task,webase our evaluationmeasuresonmean Average Pre- \u2022 Overall Top-Km AP:predictioni and annotationjarea cision(m AP),asdefinedin the Pascal VOCchallenge[60]. possiblematchif the followingconditions are satisfied: As in standard m AP, we first match each of the detected nextactiveobjectstogroundtruthannotations. Apredicted * IOU(\u02c6b ,b )>0.5; i j andagroundtruthboundingboxes are apossiblematchif their Intersection Over Union(IOU)valueexceeds 0.5 and * n\u02c6 i =n j ; if some matching criteria are met. We will define match- * v\u02c6 =v ; i j ing criteria later. Predictions are matched to ground truth * \u03b4\u02c6 \u03b4 <T . annotationsbelongingto the sameevaluatedexampleina | i \u2212 j | \u03b4 greedy fashion, prioritizing predictions with higher confi- Where T is a tolerance threshold, parameter of the \u03b4 dencescores and choosingmatchescorrespondingtolarger evaluationmeasure. IOUvalues. Agroundtruthannotation can bematchedat most with onepredictedbox. Allmatchedpredictions are The goal of the different measures is to assess the ability countedastruepositives,whereasallunmatchedpredictions ofthe model topredictnextobjectinteractionsatdifferent arecountedasfalsepositives. Per for manceon the wholetest levelsofgranularity. we use K =5 and T =0.25. \u03b4 setissummarizedusing the meanof the Average Precision valuesobtained for eachclass. Long-Term Action Anticipation Toaccount for the multi-modalnatureoffuturepredic- tions(i.e.,morethanonenextactiveobject can belikely), Methods will be evaluated at the set of timestamps spec- we \u201cdiscount\u201d the number of false positives obtained in a ified by the end of each annotated object interaction in a givenexampleby the numberofavailablegroundtruthan- video V. Let L ( V j) = { (n ( z j) ,v z (j) ) } Z z=1 bethegroundtruth notationsin that examplemultipliedby K 1,where K is annotation related to video V at time-stamp t(j) and let \u2212 a parameter of the evaluation measure. Specifically, if an (n\u02c6 (j) ,v\u02c6 (j) ) Z K be the K predicted sequences of {{ z,k z,k }z=1}k=1 example contains two ground truth annotation, we ignore Z actions. We will consider single noun/verb/action pre- the(K 1) 2 falsepositives with the highestscores. This dictionscorrectfollowing the definitionsdiscussedin Sec- \u2212 \u2217 effectivelyimplementsa\u201cTop-Kmean Average Precision\u201d tion J.4. The K predictedsequences will hencebeevaluated criterion which does not penalize methods for predicting using the editdistancemetricasfollows. upto K 1 possiblylikelynextactiveobjectswhich are For a given k, this is obtained by evaluating the edit not anno \u2212 tated. Given a generic prediction (\u02c6b i ,n\u02c6 i ,v\u02c6 i ,\u03b4\u02c6 i s\u02c6 i ) distancebetweenapredictedsequence and the groundtruth andagenericgroundtruthannotation(b j ,n j ,v j ,\u03b4 j ),wede- sequenceoffutureactions. Theeditdistance fine the followingvariantsof this Top-Kevaluationmeasure consideringdifferentmatchingcriteria: \u2206 ( (n\u02c6 (j) ,v\u02c6 (j) ) Z , (n(j),v(j)) Z ) E { z,k z,k }z=1 { z z }z=1 \u2022 Noun Top-Km AP:predictioni and annotationj area iscomputedas the Damerau-Levenshteindistance[47,133] possiblematchif the followingconditions are satisfied: oversequencesofpredictionsofverbs,nouns and actions. Thegoalof this measureistoassessper for manceinaway * IOU(\u02c6b ,b )>0.5; i j whichisrobusttosomeerrorin the predictedorderoffuture * n\u02c6 =n ; actions. A predicted verb/noun is considered \u201ccorrect\u201d if i j 73 it matches the ground truth verb label at",
      "start_pos": 30954,
      "end_pos": 31466
    },
    {
      "chunk_id": 205,
      "paper_id": "ego4d",
      "text": "annotationj area iscomputedas the Damerau-Levenshteindistance[47,133] possiblematchif the followingconditions are satisfied: oversequencesofpredictionsofverbs,nouns and actions. Thegoalof this measureistoassessper for manceinaway * IOU(\u02c6b ,b )>0.5; i j whichisrobusttosomeerrorin the predictedorderoffuture * n\u02c6 =n ; actions. A predicted verb/noun is considered \u201ccorrect\u201d if i j 73 it matches the ground truth verb label at a specific time- Verb time to contact (softmax) (softplus) step. Theallowedoperationstocompute the editdistance areinsertions,deletions,substitutions and transpositionsof f f e e a a t t s s \ud835\udeff \ud835\udeff any two predicted actions. Following the \u201cbest of many\u201d Video Slow Fast feats \ud835\udeff feats \ud835\udeff criterion, the K predictions are evaluated considering the feature pooling Pre-Trained Detector smallesteditdistancebetween the groundtruth and anyof Detected the K predictions: Last Frame R F - a C s N te N r Next Active Objects attachlabels output \u2206 ( (n\u02c6 (j) ,v\u02c6 (j) ) Z K , (n (j) ,v (j) ) Z )= E {{ z,k z,k }z=1}k=1 { z z }z=1 min \u2206 ( (n\u02c6 (j) ,v\u02c6 (j) ) Z , (n (j) ,v (j) ) Z ) Figure 57.Short-Termobjectinteractionanticipation base line. k=1..K E { z,k z,k }z=1 { z z }z=1 Note that we consider edit distance over simple accu- Future Hands Movements Prediction racy base dmeasures. Treatingpredictions for eachfuture Baseline Description Theproposedfutureh and movement time-stepindependently and calculatingaccuracydoesnot prediction task can be factorized as a regression problem. account for thesequentialnatureof the prediction task where To address this task, we adopt a baseline that utilizes the theorderofpredictionsisimportant. Weevaluateeachmet- I 3 Dnetworkasthebackbonetoextract the spatial-temporal ric independently for verbs, nouns and actions (verb and videorepresentationsof the inputvideosequence,andthen nountogether). Wereporteditdistanceat Z =20(ED@20) usealinearmappingfunctionas the regressortopredict the anduse K = 5 inourexperiments. Weselect Z = 20 as futurekeyframeh and positions. Weadopt the smootherl 1 baselinesbegintopredictactionsatr and omforhighervalues lossas the objectivefunction: of Z. (cid:40) 0.5 w (h h\u02c6)2/\u03b2, if h h\u02c6 <\u03b2 J.5 Baselinedefinitions and implementationdetails L h = w \u2217 (h \u2217 h\u02c6 \u2212 0.5 \u03b2), oth | er \u2212 wise | (38) \u2217 | \u2212 |\u2212 \u2217 Future Locomotion Movements Prediction whereh R 20 isavector that representsthex,ycoordinates Wemakeuseof the methodby Parketal.[175]fora base line ofbothle \u2208 ftandrighth and sin the aforementionedfivefuture algorithm. The method models the trajectory prediction key frames. If the hand is not observed in the keyframe, function in Equation (26) using KNN classification with we pad 0 into the h\u02c6, and adopt a binary mask w to pre- CNNimageencoding,i.e., ventthegradientspropagationof the seunobservedinstances. =KNN( \u03c6( ) ,\u03c6( )) (36) i Training Details Weadopt the I 3 Dmodelas the backbone {X} { I } I network and a regression header, composed of two linear where KNN(A,B)finds the Kne are stneighborof Bgiven operations, to predict the hand positions in the future key the set A, and \u03c6( ) Rn is a function that extracts the I \u2208 frames. For our experiments, we set observation time T o image feature of . We use the Alex Net image feature I as 2 s. For training, we applied several data augmentation extractorfor\u03c6. techniques, including random flipping, rotation, cropping Notably, the baseline algorithm leverages a polar co- andcolorjitteringtoavoidoverfitting. Our base line model",
      "start_pos": 31416,
      "end_pos": 31928
    },
    {
      "chunk_id": 206,
      "paper_id": "ego4d",
      "text": "our experiments, we set observation time T o image feature of . We use the Alex Net image feature I as 2 s. For training, we applied several data augmentation extractorfor\u03c6. techniques, including random flipping, rotation, cropping Notably, the baseline algorithm leverages a polar co- andcolorjitteringtoavoidoverfitting. Our base line model ordinate system to represent the trajectory, i.e., X 2 D = j was trained with a batch size of 64 for 25 epochs using (cid:2) (cid:3)T r j \u03b8 j isa 2 Dtrajectoryon the groundpla new herer i a cosine learning rate decay with a initial learning rate of and\u03b8 i arethepolarcoordinatesof the trajectoryrepresented 0.0375. Weset\u03b2 to 5 intheweightedsmoothed L 1 lossas intheegocentriccoordinatesystem,i.e.,distance(radial)and introducedin Eq.38. direction(angle)withrespectto the person\u2019sfeetlocationas shownin Figure 53: Short-Term Object Interaction Anticipation X 2 D =cart 2 polar(r TX ,r TX ) (37) Data and annotationsused for the experiments Weper- j 1 j 2 j formed our experimentsonasubsetof the data and annota- wherer andr arethetwospanningvectorsof the ground tionstoobtainverb and nountaxonomiesconsistent with the 1 2 plane that are aligned with the rotation matrix R . r is Short-Term Object-Interaction Anticipation task. Westarted t 1 the facing direction and r is lateral direction. Both are by considering all annotated actions for which a contact 2 perpendicular to the ground plane normal n as shown in framehas been specifiedby the annotators. Note that these Figure 53. cart 2 polar is a coordinate transform from constituteabout 30%ofthewholesetofannotatedactions cartesiantopolarcoordinates. andthat the notionofacontactframeisfundamentalto our 74 task. Wethenga the redallannotatedframes and referenced on all frames with annotated next active objects. We use themto the irrespectivecontactframes,computing the time the Faster RCNNdetector base don Res Net 50 using the\u201c3 x\u201d toactiontargets. Wediscardedallthoseannotationswhich training schedule provided with the Detectron 2 library 19. comprisedaverboranounclassmarkedby the annotator After this stage,theweightsof the Faster R-CNNcomponent as\u201cnull\u201d. Wefur the rdiscardedannotationsrelatedtonouns arenotupdatedanymore. Wehencetraina Slow Fast model whichhad been labeledinconsistently and non-objectclasses basedon Res Net 50. Wefollow the configurationprovided such as \u201cwall\u201d or \u201cwallpaper\u201d. We similarly removed all inthe Py Slow Fastlibrary 20 totackle the AVAdetection task annotationsrelatedto the verb\u201ctalk\u201dwhichdonotinvolve (\u201cSLOWFAST 32 x 2 R 50 SHORT.yaml\u201d). The Slow Fast interactions with objects. modeltakesasinputvideoclipsof 32 framessampled with Toavoidhavinganover-specificnountaxonomy,weclus- a temporal stride of 1 frame. During training, we match teredselectednounclassesintohomogeneousgroups. For eachdetectedobjectto the groundtruthinstance with largest instance the nouns\u201cokra\u201d,\u201capple\u201d,\u201ccelery\u201dand\u201cavocado\u201d Intersection Over Union(IOU),provided that itislargerthan haveall been groupedunder the\u201cvegetable fruit\u201dclass. We 0.5. Wehenceattach the verb and timetocontactlabelsof alsogroupedverbswhich have similarsemanticwhenantici- thegroundtruthboxesto the matchedones. Wethentrain pated. Forinstance,theverbs\u201ctake\u201d,\u201ccarry\u201d,\u201clift\u201d,\u201cpull\u201d the model applying the followinglossonlytoboxeswhich and \u201cremove\u201d have all been grouped in the \u201ctake\u201d cluster. have been matchedtogroundtruthinstances: Note that while the seactionsmaybevisuallydifferent,they = +\u03bb (39) v ttc all have similar effects on objects, which makes them in- L L L distinguishablewhenanticipated. Wefur the rremovedall where isthecrossentropyloss for verbprediction, is v ttc L L annotations related to nouns appearing less than 50 times thesmooth L 1 loss[87]appliedtotimetocontactprediction, inthetestset(wefollow the commonsplitdefined for this andweset\u03bb = 10 tocontrolthecontributionsof the two benchmark). Wechoosetoretainonlynounsappearingat losses. Toregulate the numberofframesprocessedby the least 50 timesin the testsettoallow for areliableevaluation slowbranch,weset\u03b1=8.Wetrain the modelon 4 NVIDIA throughthem APmeasure. V 100 GPUs with abatchsizeof 64 for 50 epochsusinga Thefinalsetof data includes 64,798",
      "start_pos": 31878,
      "end_pos": 32390
    },
    {
      "chunk_id": 207,
      "paper_id": "ego4d",
      "text": "times thesmooth L 1 loss[87]appliedtotimetocontactprediction, inthetestset(wefollow the commonsplitdefined for this andweset\u03bb = 10 tocontrolthecontributionsof the two benchmark). Wechoosetoretainonlynounsappearingat losses. Toregulate the numberofframesprocessedby the least 50 timesin the testsettoallow for areliableevaluation slowbranch,weset\u03b1=8.Wetrain the modelon 4 NVIDIA throughthem APmeasure. V 100 GPUs with abatchsizeof 64 for 50 epochsusinga Thefinalsetof data includes 64,798 annotatedexamples cosinelearningratepolicy with abaselearningrateof 0.001. in total with 87 nouns and 74 verbs. Our taxonomy is Wevalidatethemodelat the endofeachepoch and consider adapted from the one presented in Figure 39. Figure 58 theweightswhichachieved the bestoveralltop-5 m AP on and Figure 59 report the distributions of verb and noun thevalidation. annotations in the selected data. Among the 64,798 annotations, 27,801 are in the training set, 17,217 are in Long-Term Action Anticipation thevalidationset,and 19,780 arein the testset. Baseline Description Thegoalof the baseline model isto takeasinputatrimmedvideoofarbitrarylength,andpre- Baseline Description Figure 57 illustrates the proposed dict N differentplausiblesequencesoffutureactions. The baseline for short-termobjectinteractionanticipation. The baseline model sthusconsistofthreecomponents: (1)the baselineincludestwomaincomponents. AFaster R-CNN encoderbackbone for obtainingcliplevelfeatures,(2)the objectdetector[87]isusedtodetectnextactiveobjectsin aggregation module for combining the obtained features thelastframeof the inputvideoclipprocessedat full reso- from different clips, and (3) the decoder network for de- lution. ASlow Fast 3 DCNN[71]ishenceusedtopredict coding the plausible sequences of future actions. For en- averblabel and atimetoaction for eachpredictedobject. coderbackbones,weconsiderstateof the artvideorecog- This is done by obtained a fixed-length representation of nition networks from both convolutional model, namely, each object through ROI pooling [87]. Two linear layers Slow Fast [71] and the newly proposed video trans for mer are hence used to predict a probability distribution over models,namely,MVi T[63]. Foraggregationmodule,we verbs and apositivequantity for timetocontactprediction experiment with simple concatenation operators that con- respectively. Verb probability distributions are obtained catenates the obtainedclipfeatures from multipleinputclips using a softmax layer, whereas a softplus activation is aswellastrans for mer base dself-attentionmodules. Forthe used for time to contact prediction to make sure that the decodernetworksweconsider the followingoptions: prediction is a positive number. The final output of the modelisobtainedbyattaching the predictedverb and time \u2022 No Change: Asimplerecognition base line that assumes to contact to each detected next active object. The noun nofuturechangein the currentaction and simplypredicts label and confidencescores are copied from the outputof thecurrentlyobservedactionasaduplicatedstaticfuture the Faster R-CNNcomponent. sequence for Z steps. 19 https://github.com/facebookresearch/detectron 2 Training Details Wefirsttrain the Faster R-CNNcomponent 20 https://github.com/facebookresearch/Slow Fast 75 ekat tup dloh naelc evom tuc tsujda nepo hcuot esolc nrut ward_tniap hcatta hsup_sserp pid ylppa etarepo tih dnas edivid ffo_nrut llor egnarra no_nrut poocs nethgit nori elffuhs worht riaper kcap tresni htooms eparcs kram hcated daenk llird ruop leep dlof erusaem nesool wes dlom tcepsni wercs eit dlew ezeeuqs wercsnu etirw ekahs elif kcits gnah gid evres tcepsni_hcraes llif evig evird etarg yarps yalp pmup emusnoc llorcs kcik retaw tnalp bmilc kcol gniws 25000 20000 15000 10000 5000 0 Figure 58.Verbdistributionin the Short-Term Object-Interaction Anticipation data. doow tiurf_elbategev tnemudni hguod tnalp reniatnoc tecuaf repap gab hsurb wercs eriw dil leehw doof koob tnemec dlom elttob etalp latem kcirb enohp lwob hcnerw revirdwercs efink puc egnops rood gnirts nikpan dor noops reward yart tekcub top guj nap tniap elbat llird roolf rac elkcis egdirf tun tenibac llaw nep lewot telbat rewolf tam remmah",
      "start_pos": 32340,
      "end_pos": 32852
    },
    {
      "chunk_id": 208,
      "paper_id": "ego4d",
      "text": "tecuaf repap gab hsurb wercs eriw dil leehw doof koob tnemec dlom elttob etalp latem kcirb enohp lwob hcnerw revirdwercs efink puc egnops rood gnirts nikpan dor noops reward yart tekcub top guj nap tniap elbat llird roolf rac elkcis egdirf tun tenibac llaw nep lewot telbat rewolf tam remmah eohs erusaem_epat ksam enigne llab rettuc rewom eveis elcycib sdrac_gniyalp krowtra ssalg dnab_rebbur srossics eldeen epor enots ladep hsart llebbmud eulg moorb rekooc was pom retupmoc pmup repilac elit flehs alutaps 5000 4000 3000 2000 1000 0 Figure 59.Noundistributionin the Short-Term Object-Interaction Anticipation data. \u2022 Multi Head: This model trains Z independent heads in parallel,one for eachfuturetimestep. Thefinalsequence issimply the conjoinedpredictedactionsofeachhead. Finally, to generate N plausible future sequences for constructing multimodal baselines, we simply sample the predictedfutureactiondistribution N times. Theframework for a particular instantiation of the Multi Head baseline is illustratedin Figure 60. Figure 60.Long-Term Action Anticipation base line.Abaseline Training Details Foreachvideo,wesamplemultipleinput modelwitha Slow Fastbackbone,and Z =3 isshownhere.Blue clipstoprocess with ourbackbonenetwork. Asingleclip box:clipencodernetwork.Yellowbox:multipleclassifierheads, length for both the backbones, Slow Fast and MVi T, com- one for eachfutureaction.See Sec.J.5 formoredetails. prises of 16 frames sampled 4 frames apart. Each clip is processedindependentlyby the sameencoderweights and combined with the aggregationmodule. Theaggregatedfea- tureisdecoded with thedecodermodulewhere the output (cid:88) Z = ((pn,pv),(n ,v )) (40) behaviorchangesduringtraining and testing. Intraining,the L lta L v z z z z decoderpredicts the nextactionprobabilitydistributions for z=1 each future step. We calculate the sum of losses for each where is cross entropy loss, p\u2217 refers to the predicted L v z predictionas our totalloss: probabilitydistributionoververbs and nouns,and(n ,v ) z z 76 Set Metric Mean Median Left Hand Right Hand Set Method Val 5-MTE 5.11 m 2.53 m M.Disp.\u2193 C.Disp.\u2193 M.Disp.\u2193 C.Disp.\u2193 Val 3-MTE 6.19 m 2.99 m Val I 3 D+Reg 54.11 57.29 54.73 57.94 Val 1-MTE 8.81 m 4.63 m Test I 3 D+Reg 52.98 56.37 53.68 56.17 Test 5-MTE 4.84 m 2.69 m Test 3-MTE 5.54 m 3.24 m Table 35.Resultsoffutureh and movementprediction task.Note Test 1-MTE 7.66 m 4.73 m that the leftandrighth and smovements are evaluatedseparately.\u2193 indicateslowerisbetter Table 33. Resultsof the locomotionprediction task. Wereport mean/median for 7-15 secondpredictions.we use K =1,3,5. Set Method Noun Noun+Verb Noun+TTC Overall Set (cid:15)=1 m (cid:15)=2 m (cid:15)=3 m (cid:15)=4 m (cid:15)=5 m (cid:15)=6 m Val FRCNN+Rnd. 17.55 1.56 3.21 0.34 Val 0.14 0.29 0.39 0.46 0.51 0.54 Val FRCNN+SF 17.55 5.19 5.37 2.07 Test 0.16 0.31 0.40 0.47 0.53 0.58 Test FRCNN+Rnd. 20.45 2.22 3.86 0.44 Test FRCNN+SF 20.45 6.78 6.17 2.45 Table 34. Resultsof the locomotionprediction task. Wereport the probability of correct trajectory (PCT) as varying the error Table 36.Resultsof the short-termobjectinteractionanticipation threshold(cid:15). task.Seetext for discussion. referto the groundtruthfutureactionlabels. placementerror(C.Disp.) onbothvalidation and testsets During testing, we sample action class labels (n\u02c6 ,v\u02c6 ) z z in Table 35. Our baseline model achieves M.Disp. of from the predicteddistributionindependently for eachfuture (52.98/53.68) and C.Disp. of (56.37/56.17) for left/right step. We repeat this sampling procedure N times to gen- handpositionpredictionon the testset. Itisworthnoting erate multiple cancidate sets of predictions for evaluation thatpredictingh and positionsoncontactframeismorechal- describedin Section J.4. lengingthanono the rkeyframes. Thisisbecause,bythe We use",
      "start_pos": 32802,
      "end_pos": 33314
    },
    {
      "chunk_id": 209,
      "paper_id": "ego4d",
      "text": "model achieves M.Disp. of from the predicteddistributionindependently for eachfuture (52.98/53.68) and C.Disp. of (56.37/56.17) for left/right step. We repeat this sampling procedure N times to gen- handpositionpredictionon the testset. Itisworthnoting erate multiple cancidate sets of predictions for evaluation thatpredictingh and positionsoncontactframeismorechal- describedin Section J.4. lengingthanono the rkeyframes. Thisisbecause,bythe We use the taxonomy presented in Figure 39 for our definitionofcontactframe and pre-conditionframe,thean- experiments. Wefinetunea Kinetics-400[109]pretrained ticipationtemporalfootprintofcontactframeislargerthan encoder backbones on Ego 4 D action recognition and use otherkeyframes. Wefur the rprovidequalitativeresultsof this model for all base linestoextract the cliplevelfeatures. our baseline method in Fig. 61. Notably, the model can Theaggregationmodule and decodernetworks are trained makereasonablepredictionsonfutureh and positions. How- fromr and ominitializationdirectlyon the forecasting task. ever,the model ismorelikelytofailwhen the reisdrastic Theencoderweights are keptunchangedduring the decoder embodiedmotions. network training. We set Z = 20 for long horizon future evaluation and K = 5 as the number of plausible future sequences predicted by the model. For all baselines, we sample 2 inputclipstocapturepastcontextunlesso the rwise Short-Term Object Interaction Anticipation specified. We train the model on 8 NVIDIA V 100 GPUs withabatchsizeof 64 for 30 epochs and abaselearningrate Table 36 reportstheresults for the short-termobjectinterac- of 0.0001. tionanticipation task onboth the validation and testsets. We comp are the proposedbaseline base don Faster RCNN and J.6 Results Slow Fast(\u201cFRCNN+SF\u201dinthetable)withasimpler base- linewhichuses Faster RCNNtodetectobject and predict Future Locomotion Movements Prediction theirclasses,butdrawsverb and TTCpredictionsr and omly Weevaluate the KNNbased base linealgorithmbymeasuring from the training set distribution (\u201cFRCNN+Rnd.\u201d in the meantrajectoryerror(K-MTE)andprobabilityofcorrect table). Results are reportedin Top-5 m AP%accordingto the trajectory (PCT) given an error tolerance. The trajectory differentmatchingcriteriadiscussedin Appendix J.4.Ascan length ranges from 7 to 15 seconds (70-150 points in a benoted,theproposed base lineoutper for msr and ompredic- trajectorygiven 10 FPS).Our base lineachievesmeanerror tionbybigmarginswhenverbs and TTCs are predictedon 8.81 mfor 1 MTEand 0.39 for PCT . Theresultis both the validation and testsets. Thissuggests that,despite (cid:15)=3 m \u2212 summarizedin Table 33 and 34. beingsimple,the base line can leverage the observedvideo to anticipate future object interactions. Figure 62 reports some qualitative examples of the baseline. The model is Future Hands Movements Prediction sometimesabletodetect the nextactiveobjects and predict For future hands movements prediction task, we report suitableverbs and TTCs,butper for mancetendstobelimited meandisplacementerror(M.Disp.) andcontactframedis- especiallyincomplexscenarios. 77 1.5 sec before PRE 1.0 sec before PRE 0.5 sec before PRE PRE Frame Contact Frame : Ground Truth : Prediction Figure 61.Qualitativeexamplesoffutureh and smovementspredictionusing the proposed base line.Thegroundtruthh and spositions are plottedasgreencrosses,while the predictedh and spositions are plottedasredcrosses. Figure 62.Qualitativeexamplesofshort-termobjectinteractionanticipationusing the proposed base line.Thenumbersinbracketsrepresent theconfidencescoresassociatedto the predictions. Thegroundtruthnext-activeobjectishighlightedusingadashedredline,whereas modelpredictions are reportedinbluesolidlines. Long-Term Action Anticipation architecture from Slow Fastto MVi Tgreatlyimprovesverb forecasting prediction per for mance, but deteriorates noun forecastingper for mance,highlighting the trade-offbetween Table 37 shows our resultsonboth the validation and testsets. thetwodespitesimilaractionclassificationper for manceon The No Change base linesimplypredicts the currentactionas Kinetics. Finally,includinglargervideocontextin for mation thenext Z actions,andper for mspoorlyatpredictingfuture inthe for mofmultipleinputclipsbyusing the trans for mer actions. Explicitlytrainingmultipleheadsimprovesper for- basedaggregatormoduleresultsin the bestper for mance. manceonverbs,nouns and actions. Changing the backbone 78 Valset ED@(Z=20) Future Locomotion Movements Prediction Backbone Aggregator Decoder Verb Noun Action Slow Fast Concat No Change 0.766 0.830 0.960 The base linequantitativeresultson the locomotionpredic- Slow Fast Concat",
      "start_pos": 33264,
      "end_pos": 33776
    },
    {
      "chunk_id": 210,
      "paper_id": "ego4d",
      "text": "inthe for mofmultipleinputclipsbyusing the trans for mer actions. Explicitlytrainingmultipleheadsimprovesper for- basedaggregatormoduleresultsin the bestper for mance. manceonverbs,nouns and actions. Changing the backbone 78 Valset ED@(Z=20) Future Locomotion Movements Prediction Backbone Aggregator Decoder Verb Noun Action Slow Fast Concat No Change 0.766 0.830 0.960 The base linequantitativeresultson the locomotionpredic- Slow Fast Concat Multi Head 0.747 0.808 0.952 tion task imply that the visualcues,e.g.,sidewalk,obstacles, MVi T Concat Multi Head 0.707 0.901 0.972 Slow Fast Trans for mer Multi Head 0.745 0.779 0.941 androad,inegocentricimages are highlyindicativeoffu- turemovement. However,the base linemethod that encodes test set ED@(Z=20) the visual semantics of an image with a global feature is Backbone Aggregator Decoder Verb Noun Action notdetailedenoughto model complexwalkingmovement, Slow Fast Concat No Change 0.761 0.810 0.959 e.g., avoiding pedestrians. This opens an opportunity for Slow Fast Concat Multi Head 0.743 0.791 0.948 MVi T Concat Multi Head 0.697 0.904 0.969 challengeparticipantstoincorporateafine-grainedvisual Slow Fast Trans for mer Multi Head 0.739 0.780 0.943 representation. Table 37.Resultsof the long-termactionanticipation task.Lower isbetter.Seetext for discussion. Future Hands Movements Prediction Our base line model for futureh and smovementsprediction suffers from the drasticheadmovementsinegocentricvideo Figure 63 showssomequalitativeresultsof our method. and the stochasticnatureoffuture for ecasting. Wespeculate Ineachrow,thegroundtruthfutureactions are shownalong thatexplicitly model ing the headmovements and next-active with the predictions from ourmodel(for 5 time-steps). Cor- objectsmaycomplement the videorepresentations for pre- rectpredictions are highlightedingreen,whilevalidactions dictingfutureh and smovements. that areincorrectly ordered(or paritallycorrect) arehigh- lightedinblue. Note that thoughnotperfectlyaligned,in- Short-Term Object Interaction Anticipation correctlyorderedsequences are givenpartialcreditvia the edit-distancemetric. Theshort-termobjectinteractionanticipationresultshigh- light that the proposed task ischallenging,with the baseline achievinganoverall Top-5 m AP of 2.07%onthevalidation setand 2.45%onthetestset. Thekeychallenges are likely dueto the uncertainnatureoffuturepredictionsaswellas J.7 Discussion totheinabilityof the objectdetectortocorrectlydetectnext activeobjects and ignore the others. Never the less,thepro- Data Annotation posed baseline, even if simple, allows to greatly improve overacombinationofanobjectdetectorandar and ompre- Annotating the videos for for ecasting task sposedanumber dictionofverbs and timetocontactquantities. Thissuggests ofinterestingchallenges. First,wefound the diversityof the thatmethods can learntoanalyze the inputvideoinorderto dataledtoalarge and diversetaxonomy,whichsomeanno- makereasonablepredictionsabout the future. tatorsfoundhardtonavigate. Hence,wefoundanumberof annotatorsused the\u201dOTHER\u201doption,whichweeventually Long-Term Action Anticipation manuallymappedto the taxonomywherepossible. Infuture annotations, we plan to ask annotators to always pick the Wediscussseveralimportantaspectsof the long-termaction closesttaxonomyitemevenifwritinginafree-form OTHER forecastingproblemthrough our experiments and ablation label,toenc our agethemtostickto the taxonomyasmuch studies. All ablations are run with Slow Fast backbone aspossible. Second,wenoticedannotatorsstruggled with networks,and models are trained for 30 epochs. definingboundingboxesover\u201cstuff\u201dcategories. Forexam- ple,whenlabeling\u201ccuttinggrass\u201d,itwasoftenchallenging Howimportantis Ego 4 Dactionrecognitionpre-training? to draw a box that covers the full extent of the object of Table 38 shows the per for mance of our models when change(i.e.\u201cgrass\u201d). Finally,itwassometimeschallenging pretrained only on Kinetics-400 action recognition (as todefi new hat the objectofchangewas,whenusinglarge opposed to further fine-tuning on Ego 4 D action recogni- tools. For example, if using a lawn mower to clear grass, tion). All models benefit greatly from training on Ego 4 D doesoneconsiderthemowerasthetool and hence the grass data in two ways. First, there is a large domain gap astheobjectofchange,orthelevers and buttonsinside the between Kinetics and Ego 4 D both in terms of visuals mower as the object of change. We chose to rely on the (third-person vs. egocentric viewpoint) and the diver- narratorstodefi new hichinteractiontolabel(i.e.pushing sity of activities they contain, which pre-training helps thelever/buttonvscuttinggrass),andasked the annotators account",
      "start_pos": 33726,
      "end_pos": 34238
    },
    {
      "chunk_id": 211,
      "paper_id": "ego4d",
      "text": "gap astheobjectofchange,orthelevers and buttonsinside the between Kinetics and Ego 4 D both in terms of visuals mower as the object of change. We chose to rely on the (third-person vs. egocentric viewpoint) and the diver- narratorstodefi new hichinteractiontolabel(i.e.pushing sity of activities they contain, which pre-training helps thelever/buttonvscuttinggrass),andasked the annotators account for. Second, action recognition models benefit tolabeltools and objectsaccordingly. frombiasesin the labelstructureoffutureactionsasseen 79 ED@5: 0.60 take \u2192 hold \u2192 cut \u2192 put \u2192 take GT: sickle spinach spinach sickle rubber band take \u2192 cut \u2192 hold \u2192 cut \u2192 throw PRED: sickle spinach spinach spinach sickle ED@5: 0.80 smooth \u2192 remove \u2192 smooth \u2192 sand \u2192 remove GT: wood sander wood wood sander hold \u2192 sand \u2192 hold \u2192 sand \u2192 sand PRED: sander wood sander wood wood Figure 63.Longtermactionanticipation-qualitativeresults.Actionsingreenrepresentcorrectpredictions(correctaction,atthecorrect position).Actionsinbluerepresentincorrectorderingofvalidactions.Ouredit-distancemetricaccounts for bothcases. Val Set ED@(Z=20) Init Backbone Aggregator Verb Noun Action 0.800 K 400 Slow Fast Concat 0.752 0.820 0.958 0.775 +Ego 4 D Slow Fast Concat 0.747 0.808 0.952 0.750 K 400 Slow Fast Trans for mer 0.746 0.809 0.953 +Ego 4 D Slow Fast Trans for mer 0.745 0.779 0.941 0.725 0.700 Table 38.Longtermanticipation-varyingpretraining data.Mul- 0.675 ti Head decoder used for all models. Ego 4 D action recognition 0.650 pretraininggreatlyimprovesdownstreamforecastingper for mance. 0.625 Val Set ED@(Z=20) 1 2 3 4 5 6 7 8 91011121314151617181920 Z #clips Backbone Aggregator Verb Noun Action 2 Slow Fast Trans for mer 0.743 0.790 0.946 4 Slow Fast Trans for mer 0.744 0.796 0.947 8 Slow Fast Trans for mer 0.745 0.779 0.941 Table 39.Longtermanticipation-varyingnumberofinputclips. Multi Headdecoderused for allmodels. Per for manceincreases withmoreinputcontext. from the per for manceof the No Change base linein Table 37. How important is past context for trans for mer based models? Our trans for mer aggregation modules aggregate informationacrossalargertemporalhistorycontrolledby thenumberofinputclipsto the model. Table 39 shows the sensitivity of these models to the amount of past context video that ithasaccessto. Overall,per for manceincreasesas morecontextin for mationisprovidedto the model,however thisincreasecomesat the costofmemoryconsumption\u2014 8 is the maximum number of clips that can be fit in GPU memory. Howfarinto the future can modelspredict? Asmentioned in Section J.4 wereportresults for predictionsat Z = 20 as baselines begin to predict actions at random for higher values of Z. Figure 64 shows the plot of edit distance vs. Z@DE verbs nouns Figure 64.Per for mancevs.numberoffutureactions Z.Predicting furtherinto the futureisnaturallymoredifficult.Modelsbeginto predictclosetor and omactions for veryhighvaluesof Z. Z forour base linemodels. Asexpected,itisf are asierto anticipateactions that occurimmediatelynext,whichgets moredifficultas Z increases,andsteadilyplateaus. Howtogeneratemultiple can didatepredictions? Asmen- tioned in Section J.4 we evaluate the best of K = 5 pre- dictions to arrive at our final results. To generate the K predictions,wesampleeachclassifierheadindependently, however the reareseveralmethodstoimprove this includ- ingheuristicsearchalgorithms(likebeamsearch). Ideally, the multi-modal nature of future prediction should be ac- countedforin the modeldesignitself. Moreover,decoder models that takeintoaccount the sequentialnatureduring inferenceshould beconsidered. Theseincludetrans for mer baseddecoders that are popularinrecentlanguagemodels (e.g.,BERT,GPT)Thisisanimportantfuturedirectionof research. 80 J.8 Contributionsstatement Giovanni Maria Farinellaled the Forecasting Benchmark workingonthedefinitionof the proposedtasks,onthecol- lection,andwriting the paper. Rohit Girdharco-ledthe Forecasting Benchmarkworking onthedefinitionof the proposedtasks,onthecollection,and writing the paper. Antonino Furnari contributed to the definition of the pro- posedbenchmarktasks and inparticularto the Short-Term Object Interaction Anticipation",
      "start_pos": 34188,
      "end_pos": 34700
    },
    {
      "chunk_id": 212,
      "paper_id": "ego4d",
      "text": "for mer baseddecoders that are popularinrecentlanguagemodels (e.g.,BERT,GPT)Thisisanimportantfuturedirectionof research. 80 J.8 Contributionsstatement Giovanni Maria Farinellaled the Forecasting Benchmark workingonthedefinitionof the proposedtasks,onthecol- lection,andwriting the paper. Rohit Girdharco-ledthe Forecasting Benchmarkworking onthedefinitionof the proposedtasks,onthecollection,and writing the paper. Antonino Furnari contributed to the definition of the pro- posedbenchmarktasks and inparticularto the Short-Term Object Interaction Anticipation task and has been key driver of implementation, collection, annotation develop- mentthroughout the project,andwriting the paper. Ilija Radosavovicworkedon the definitionoftasks and has been key driver of implementation, collection, annotation developmentthroughout the project,andwriting the paper. Tushar Nagarajancontributedtothedefinitionof the pro- posedbenchmarktasks and inparticularto the Long-Term Action Anticipation task and has been keydriverofimple- mentation,collection,annotationdevelopmentthroughout theproject,andwriting the paper. Tullie Murrell worked on baseline implementation of the Long-Term Action Anticipation task. Karttikeya Mangalamworkedon base lineimplementation, experiments and writing the Long-Term Action Anticipation task. Christoph Feichtenhofer oversaw the development of the task,baselines and implementationof the Long-Term Action Anticipation task. Miao Liuworkedon the definitionof Future Hands Move- ment Prediction task and has been keydriverofimplemen- tation, collection, annotation development throughout the project,andwriting the paper. Wenqi Jiaworkedon base lineimplementationof the Future Hands Movement Prediction task. Zachary Chavisworkedon the Locomotion Forecasting task andhas been keydriverofimplementation,collection,and annotationdevelopmentthroughout the project. Hyun Soo Park worked on the definition of Locomotion Forecasting tasks, collection, annotation, and writing the paper. 81 K.Societal Impact Ourcontribution can positivelyimpactvideounderst and- ing. Itoffers the researchcommunityalarge-scaleresource captured with rigorousprivacyandethicsst and ards(detailed in Appendix Aand B)toge the rwithadiversityofsubjects, and the benchmarks will promotereproducibletechnicalad- vances. Morebroadly,egocentricperceptionhas the poten- tialtopositivelyimpactsocietyinmanyapplicationdomains, includingassistivetechnology,education,fitness,entertain- ment and gaming,elderc are,robotics,andaugmentedreality. None the less, future research in this area must guard against the potentialnegativesocietalimpactiftechnology foregocentricvision were misused. First, there are riskssurroundingprivacy. Aswebegin toseeaproliferationofwearablecamerasinpublicspaces, producersof the sewearabledevices will needtodevelop and implement protocols for notice and consent regarding the collectionof data inpublicspaces,aswellasusercontrols for how such data may be used, stored, and shared with any third parties. Similarly, models that may be used to transcribespeechorperformo the rtasksrelatedtofootage should include robust user controls such as the ability to removeorobscurepersonal data orsensitivecontent. Note that for all our audio-visual and socialbenchmarking work, the data usedhas full consent from the participants inthevideo,i.e.,touse the irunblurredfaces and audioof their conversation. To date, the research community has lacked any large-scale data resource with which to study thesekindsofproblems;Ego 4 Dwillhelp the communityto consider new solutionswhileleveragingreal-world,diverse data that respects the privacyprotocolsofdifferentcountries. Fur the rmore,the Ego 4 Ddataisavailableonly for userswho signalicense that enumeratestheallowableusesof the data, whichisintendedtohinderpotentialnegativeapplications. Second,thereisarisk that ourlarge-scalecollectioncould inspirefuturecollectionef for tswithout the samelevelofc are orattentionto the privacy and ethicalconcernsas were taken in Ego 4 D.Tomitigate this risk,wehaveaimedtobecompre- hensiveinourdescriptionsofallpartsof our procedures,and wewillinclude our bestpracticesrecommendationswhen publiclydisseminatingtheresultsof the project. Finally,despite our bestef for tsasdiscussedin the main paper,there are stillsomeimbalancesin the dataset. Forex- ample,thedata from Rwandaisrelativelysmall,andthough 74 citiesrepresentsaleapincoverage,theydonotcapture allpossibledemographics. Weacknowledge that nomatter howfaronegoes,fullglobalcoverageofdailylifeactivity iselusive. Still,we canmitigate this riskbycontinuingto growglobalcollaborations with researchers and participants inunderrepresentedareas. 82 References [16] Sven Bambach,Stefan Lee,David J.Crandall,and Chen Yu. Lendingah and:Detectinghands and recognizingactivities [1] Github repository of the ESPNet model zoo. https: incomplexegocentricinteractions. In The IEEEInterna- //github.com/espnet/espnet_model_zoo. We tional Conferenceon Computer Vision(ICCV),December used the Shinji Watanabe/gigaspeech_asr_ 2015. 3 train_asr_raw_en_bpe 5000_valid.acc.ave [17] Mark ABee and Christophe Micheyl. Thecocktailparty model. 60,61 problem:whatisit?how can itbesolved?andwhyshould [2] Kaldi English GLM file. https://github.com/ animalbehavioristsstudyit? Journalofcomparativepsy- kaldi-asr/kaldi/blob/master/egs/ami/s 5/ chology,122(3):235,2008.",
      "start_pos": 34650,
      "end_pos": 35162
    },
    {
      "chunk_id": 213,
      "paper_id": "ego4d",
      "text": "[1] Github repository of the ESPNet model zoo. https: incomplexegocentricinteractions. In The IEEEInterna- //github.com/espnet/espnet_model_zoo. We tional Conferenceon Computer Vision(ICCV),December used the Shinji Watanabe/gigaspeech_asr_ 2015. 3 train_asr_raw_en_bpe 5000_valid.acc.ave [17] Mark ABee and Christophe Micheyl. Thecocktailparty model. 60,61 problem:whatisit?how can itbesolved?andwhyshould [2] Kaldi English GLM file. https://github.com/ animalbehavioristsstudyit? Journalofcomparativepsy- kaldi-asr/kaldi/blob/master/egs/ami/s 5/ chology,122(3):235,2008. 52 local/english.glm. 61 [18] Keni Bernardin,Alexander Elbs,and Rainer Stiefelhagen. [3] NISTSRE 2000 Evaluation Plan. https://www.nist. Multipleobjecttrackingper for mancemetrics and evaluation gov/sites/default/files/documents/2017/ inasmartroomenvironment. In Sixth IEEEInternational 09/26/spk-2000-plan-v 1.0.htm_.pdf. 56 Workshopon Visual Surveillance,inconjunction with ECCV, [4] Yazan Abu Farha, Alexander Richard, and Juergen Gall. volume 90.Citeseer,2006. 8,53 When will youdowhat?-anticipatingtemporaloccurrences [19] Keni Bernardin and Rainer Stiefelhagen. Evaluatingmul- ofactivities. In Computer Vision and Pattern Recognition, tiple object tracking per for mance: the clear mot metrics. pages 5343\u20135352,2018. 3 EURASIPJ our nalon Image and Video Processing,2008:1\u2013 [5] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, 10,2008. 8,53 Oriol Vinyals,and Andrew Zisserman. Deepaudio-visual [20] Gedas Bertasius,Hyun Soo Park,Stella X.Yu,and Jianbo speechrecognition. IEEEtransactionsonpatternanalysis Shi. First-personaction-objectdetectionwi the gonet. In andmachineintelligence,2018. 8,51 Proceedingsof Robotics:Science and Systems,July 2017. 9 [6] Triantafyllos Afouras,Joon Son Chung,and Andrew Zisser- [21] Cigdem Beyan,Francesca Capozzi,Cristina Becchio,and man. Theconversation:Deepaudio-visualspeechenhance- Vittorio Murino. Predictionof the leadershipstyleofan ment. In Interspeech,2018. 51 emergentleaderusingaudio and visualnonverbalfeatures. [7] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, IEEETransactionson Multimedia,20(2):441\u2013456,2018. 9 and Andrew Zisserman. Self-supervised Learningof Audio- [22] Goutam Bhat,Martin Danelljan,Luc Van Gool,and Radu Visual Objects from Video. In Proceedingsof the European Timofte. Know Your Surroundings:Exploiting Scene Infor- Conferenceon Computer Vision(ECCV 20),volume 12363 mation for Object Tracking. ar Xiv:2003.11014[cs],May LNCS,pages 208\u2013224,2020. 9 2020. 35,36 [8] Jean-Baptiste Alayrac,Josef Sivic,Ivan Laptev,and Simon [23] Eric Brachmann, Alexander Krull, Frank Michel, Stefan Lacoste-Julien. Jointdiscoveryofobjectstates and manipu- Gumhold, Jamie Shotton, and Carsten Rother. Learning lationactions. ICCV,2017. 7,44,45 6 dobjectposeestimationusing 3 dobjectcoordinates. In [9] Humam Alwassel,Fabian Caba Heilbron,Victor Escorcia, Europeanconferenceoncomputervision,pages 536\u2013551. and Bernard Ghanem. Diagnosingerrorintemporalaction Springer,2014. 7 detectors. In Proceedingsof the European Conferenceon [24] Tom B.Brown,Benjamin Mann,Nick Ryder,Melanie Sub- Computer Vision(ECCV),2018. 41,42 biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan- [10] Xavier Anguera,Simon Bozonnet,Nicholas Evans,Corinne tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- Fredouille,Gerald Friedl and,and Oriol Vinyals. Speaker hini Agarwal,Ariel Herbert-Voss,Gretchen Krueger,Tom diarization:Areviewofrecentresearch. IEEETransactions Henighan,Rewon Child,Aditya Ramesh,Daniel M.Ziegler, onaudio,speech,andlanguageprocessing,20(2):356\u2013370, Jeffrey Wu,Clemens Winter,Christopher Hesse,Mark Chen, 2012. 8,53,56 Eric Sigler,Mateusz Litwin,Scott Gray,Benjamin Chess, [11] Xavier Anguera Miro\u00b4. Robustspeakerdiarization for meet- Jack Clark,Christopher Berner,Sam Mc Candlish,Alec Rad- ings. Universitat Polite`cnicade Catalunya,2006. 8,54 ford,Ilya Sutskever,and Dario Amodei. Languagemodels [12] Stanislaw Antol,Aishwarya Agrawal,Jiasen Lu,Margaret arefew-shotlearners,2020. 9 Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi [25] Ian MBullock, Thomas Feix, and Aaron MDollar. The Parikh. VQA:Visual Question Answering. In International yalehumangrasping data set:Grasp,object,and task datain Conferenceon Computer Vision(ICCV),2015. 7 household and machinesh open vironments. IJRR,2015. 45 [13] Mehmet Ali Arabac\u0131,Fatih O\u00a8zkan,Elif Surer,Peter Janc\u02c7ovic\u02c7, [26] Minjie Cai,Kris MKitani,and Yoichi Sato. Underst and- and Alptekin Temizel. Multi-modal egocentric activity ingh and-objectmanipulation with grasptypes and object recognition using audio-visual features. ar Xiv preprint attributes. In RSS,2016. 3 ar Xiv:1807.00612,2018. 51 [27] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas [14] Relja Arandjelovic\u00b4 and Andrew Zisserman. Objects that Usunier,Alexander Kirillov,and Sergey Zagoruyko. End- sound. In ECCV,2018. 8,51 to-end object detection with trans for mers. In European [15] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Conferenceon Computer Vision,pages 213\u2013229.Springer,",
      "start_pos": 35112,
      "end_pos": 35624
    },
    {
      "chunk_id": 214,
      "paper_id": "ego4d",
      "text": "preprint attributes. In RSS,2016. 3 ar Xiv:1807.00612,2018. 51 [27] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas [14] Relja Arandjelovic\u00b4 and Andrew Zisserman. Objects that Usunier,Alexander Kirillov,and Sergey Zagoruyko. End- sound. In ECCV,2018. 8,51 to-end object detection with trans for mers. In European [15] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Conferenceon Computer Vision,pages 213\u2013229.Springer, and Michael Auli. wav 2 vec 2.0: A framework for self- 2020. 49 supervisedlearningofspeechrepresentations.ar Xivpreprint [28] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike ar Xiv:2006.11477,2020. 60 Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, 83 Vasilis Karaiskos,Wessel Kraaij,Melissa Kronenthal,etal. [41] Kenneth Church and Patrick Hanks.Wordassociationnorms, The AMI meeting corpus: A pre-announcement. In In- mutualin for mation,andlexicography. Computationallin- ternationalworkshoponmachinelearning for multimodal guistics,16(1):22\u201329,1990. 71 interaction,pages 28\u201339.Springer,2006. 56 [42] Dima Damen, Hazel Doughty, Giovanni Farinella, Sanja [29] Joao Carreira and Andrew Zisserman. Quovadis, action Fidler,Antonino Furnari,Evangelos Kazakos,Davide Molti- recognition?anew model and the kinetics data set. Inpro- santi,Jonathan Munro,Toby Perrett,Will Price,etal. The ceedingsof the IEEEConferenceon Computer Vision and epic-kitchens data set:Collection,challenges and baselines. Pattern Recognition,pages 6299\u20136308,2017. 48,49 IEEETransactionson Pattern Analysis&Machine Intelli- gence,(01):1\u20131,2020. 52 [30] Chien-Yi Chang,De-An Huang,Danfei Xu,Ehsan Adeli, Li Fei-Fei,and Juan Carlos Niebles. Procedureplanning [43] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, ininstructionalvideos. ar Xivpreprintar Xiv:1907.01172, ,Antonino Furnari,Jian Ma,Evangelos Kazakos,Davide 2019. 45 Moltisanti,Jonathan Munro,Toby Perrett,Will Price,and Michael Wray. Rescalingegocentricvision. IJCV,2021. 2, [31] Sourish Chaudhuri,Joseph Roth,Daniel PWEllis,Andrew 3,45 Gallagher,Liat Kaver,Radhika Marvin,Caroline Pantofaru, Nathan Reale,Loretta Guarino Reid,Kevin Wilson,etal. [44] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Ava-speech:Adenselylabeled data setofspeechactivityin Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Da- movies. ar Xivpreprintar Xiv:1808.00606,2018. 8,52 vide Moltisanti,Jonathan Munro,Toby Perrett,Will Price, and Michael Wray. Scaling egocentric vision: The epic- [32] C. Chen, U. Jain, C. Schissler, S. V. Amengual Gari, kitchens dataset. In European Conference on Computer Z. Al-Halah, V. Ithapu, P. Robinson, and K. Grauman. Vision(ECCV),2018. 2,3,5,20,52 Soundspaces:Audio-visualnavigationin 3 denvironments. [45] Dima Damen,Teesid Leelasawassuk,Osian Haines,Andrew In ECCV,2020. 62 Calway,and Walterio Mayol-Cuevas.You-Do,I-Learn:Dis- [33] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi- covering task relevantobjects and the irmodesofinteraction cenc Amengual Gari,Ziad Al-Halah,Vamsi Krishna Ithapu, frommulti-useregocentricvideo. In BMVC,2014. 44,45 Philip Robinson,and Kristen Grauman. Audio-visualem- [46] Dima Damen,Teesid Leelasawassuk,and Walterio Mayol- bodiednavigation. environment,97:103,2019. 8 Cuevas. You-do,i-learn:Egocentricunsuperviseddiscovery [34] Guoguo Chen,Shuzhou Chai,Guanbo Wang,Jiayu Du,Wei- of objects and their modes of interaction towards video- Qiang Zhang,Chao Weng,Dan Su,Daniel Povey,Jan Trmal, basedguidance. CVIU,2016. 3 Junbo Zhang,etal. Gigaspeech:Anevolving,multi-domain [47] Fred JDamerau. Atechnique for computerdetection and asrcorpus with 10,000 hoursoftranscribedaudio. ar Xiv correctionofspellingerrors. Communicationsof the ACM, preprintar Xiv:2106.06909,2021. 60 1964. 73 [35] Xinlei Chen,Hao Fang,Tsung-Yi Lin,Ramakrishna Vedan- [48] Ana Garcia Del Molino,Cheston Tan,Joo-Hwee Lim,and tam,Saurabh Gupta,Piotr Dolla\u00b4r,and CLawrence Zitnick. Ah-Hwee Tan. Summarizationofegocentricvideos:Acom- Microsoft coco captions: Data collection and evaluation prehensivesurvey. IEEETransactionson Human-Machine server. ar Xivpreprintar Xiv:1504.00325,2015. 7 Systems,47(1),2016. 3 [36] Eunji Chong, Elysha Clark-Whitney, Audrey Souther- [49] Jia Deng,Wei Dong,Richard Socher,Li-Jia Li,Kai Li,and land, Elizabeth Stubbs, Chanel Miller, Eliana L Ajodan, Li Fei-Fei. Image Net: A large-scale hierarchical image Melanie RSilverman, Catherine Lord, Agata Rozga, Re- data base. In CVPR,2009. 1,3 becca MJones,and James MRehg.Detectionofeyecontact [50] Daniel De Tone, Tomasz Malisiewicz, and Andrew Rabi- withdeepneuralnetworksisasaccurateashumanexperts. novich. Superpoint:Self-supervisedinterestpointdetection Nature Communications,11(1):6386,dec 2020. 9 anddescription. In CVPRWorkshop,2018. 38 [37] Eunji Chong,Yongxin Wang,Nataniel Ruiz,and James M. [51] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina Rehg. Detecting Attended Visual",
      "start_pos": 35574,
      "end_pos": 36086
    },
    {
      "chunk_id": 215,
      "paper_id": "ego4d",
      "text": "Lord, Agata Rozga, Re- data base. In CVPR,2009. 1,3 becca MJones,and James MRehg.Detectionofeyecontact [50] Daniel De Tone, Tomasz Malisiewicz, and Andrew Rabi- withdeepneuralnetworksisasaccurateashumanexperts. novich. Superpoint:Self-supervisedinterestpointdetection Nature Communications,11(1):6386,dec 2020. 9 anddescription. In CVPRWorkshop,2018. 38 [37] Eunji Chong,Yongxin Wang,Nataniel Ruiz,and James M. [51] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina Rehg. Detecting Attended Visual Targetsin Video. In Pro- Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans- ceedingsof the IEEEConferenceon Computer Vision and formers for language underst and ing. ar Xiv:1810.04805, Pattern Recognition(CVPR 20),pages 5395\u20135405,Seattle, 2018. 29 WA,2020. 9,66 [52] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina [38] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans- Lee,Hee Soo Heo,Soyeon Choe,Chiheon Ham,Sunghwan formers for languageunderst and ing. In Proceedingsof the Jung,Bong-Jin Lee,and Icksang Han. Indefenceofmetric 2019 Conferenceof the North Ameri can Chapterof the As- learning for speakerrecognition. In Interspeech,2020. 64 sociation for Computational Linguistics:Human Language [39] Joon Son Chung,Jaesung Huh,Arsha Nagrani,Triantafyl- Technologies, Volume 1 (Long and Short Papers), pages los Afouras, and Andrew Zisserman. Spot the conver- 4171\u20134186,Minneapolis,Minnesota,June 2019.Associa- sation: speaker diarisation in the wild. ar Xiv preprint tion for Computational Linguistics. 40 ar Xiv:2007.01216,2020. 8,52 [53] Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark [40] J.S.Chung, A.Nagrani, and A.Zisserman. Vox Celeb 2: Broyles,Hao Jiang,Jie Shen,Maja Pantic,Vamsi Krishna Deep Speaker Recognition. In INTERSPEECH,2018. 52, Ithapu,and Ravish Mehra. Easycom:Anaugmentedreality 57 dataset to support algorithms for easy communication in 84 noisyenvironments.ar Xivpreprintar Xiv:2107.04174,2021. [66] Alireza Fathi,Jessica K.Hodgins,and James M.Rehg. So- 8,52 cialinteractions:Afirst-personperspective. In CVPR,2012. [54] Bardia Doosti, Ching-Hui Chen, Raviteja Vemulapalli, 3 Xuhui Jia,Yukun Zhu,and Bradley Green. Boostingimage- [67] A.Fathi,J.K.Hodgins,and J.M.Rehg. Socialinteractions: basedmutualgazedetectionusingpseudo 3 dgaze.In Thirty- Afirst-personperspective. In Proceedingsof the IEEECon- Fifth AAAIConferenceon Artificial Intelligence,pages 1273\u2013 ferenceon Computer Vision and Pattern Recognition(CVPR 1281,2021. 9 12),pages 1226\u20131233.IEEE,jun 2012. 9 [68] A. Fathi and J. Rehg. Modeling actions through state [55] Hazel Doughty,Ivan Laptev,Walterio Mayol-Cuevas,and changes. In CVPR,2013. 7 Dima Damen. Actionmodifiers: Learning from adverbs [69] Alireza Fathi and James MRehg. Modelingactionsthrough ininstructionalvideos. ar Xivpreprintar Xiv:1912.06617, statechanges. In CVPR,2013. 44,45 2019. 45 [70] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and [56] Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Kaiming He. Slowfastnetworks for videorecognition. In Farinella, and Christian Micheloni. Isfirstpersonvision ICCV,2019. 3,5,40,41,48,49 challenging for object tracking? In IEEE/CVF Interna- [71] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and tional Conferenceon Computer Vision Workshops(ICCVW) Kaiming He. Slowfastnetworks for videorecognition. In -Visual Object Tracking Challenge,2021. 3 Proceedingsof the IEEE/CVFinternationalconferenceon [57] Ariel Ephrat,Inbar Mosseri,Oran Lang,Tali Dekel,Kevin computervision,pages 6202\u20136211,2019. 40,75 Wilson, Avinatan Hassidim, William T Freeman, and [72] Jonathan Fiscus. NIST sclite sscoring toolkit. https: Michael Rubinstein. Lookingtolistenat the cocktailparty: //github.com/usnistgov/SCTK. 61 Aspeaker-independentaudio-visual model for speechsepa- [73] Jianglin Fu,Ivan VBajic\u00b4,and Rodney GVaughan. Datasets ration. In SIGGRAPH,2018. 8,51 forface and objectdetectioninfisheyeimages.Datainbrief, [58] Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops! 27:104752,2019. 57 predictingunintentionalactioninvideo. In Arxiv,2019. 44 [74] Antonino Furnari, Sebastiano Battiato, Kristen Grauman, [59] N.Ryantet.al.The Second DIHARDDiarization Challenge: and Giovanni Maria Farinella. Next-active-objectprediction dataset,task,and base lines. In Proceedingsof Interspeech, fromegocentricvideos. Journalof Visual Communication 2019. 56 and Image Representation,49:401\u2013411,2017. 9 [60] Mark Everingham,Luc Van Gool,Christopher KIWilliams, [75] Antonino Furnari and Giovanni Farinella. Rolling-unrolling John Winn,and Andrew Zisserman.Thepascalvisualobject lstms for actionanticipation from first-personvideo. IEEE classes(voc)challenge. Internationalj our nalofcomputer Transactionson Pattern Analysis and Machine Intelligence, vision,88(2):303\u2013338,2010. 1,73 2020. 3",
      "start_pos": 36036,
      "end_pos": 36548
    },
    {
      "chunk_id": 216,
      "paper_id": "ego4d",
      "text": "In Proceedingsof Interspeech, fromegocentricvideos. Journalof Visual Communication 2019. 56 and Image Representation,49:401\u2013411,2017. 9 [60] Mark Everingham,Luc Van Gool,Christopher KIWilliams, [75] Antonino Furnari and Giovanni Farinella. Rolling-unrolling John Winn,and Andrew Zisserman.Thepascalvisualobject lstms for actionanticipation from first-personvideo. IEEE classes(voc)challenge. Internationalj our nalofcomputer Transactionson Pattern Analysis and Machine Intelligence, vision,88(2):303\u2013338,2010. 1,73 2020. 3 [76] Antonino Furnari and Giovanni Maria Farinella.Whatwould [61] Bernard Ghanem Fabian Caba Heilbron,Victor Escorcia and you expect? anticipating egocentric actions with rolling- Juan Carlos Niebles.Activitynet:Alarge-scalevideobench- unrolling lstms and modality attention. In International mark for human activity underst and ing. In Proceedings Conferenceon Computer Vision,2019. 9 ofthe IEEEConferenceon Computer Vision and Pattern [77] Jiyang Gao,Zhenheng Yang,and Ram Nevatia. Red: Re- Recognition,pages 961\u2013970,2015. 1,3,30,32 inforcedencoder-decodernetworks for actionanticipation. [62] Heng Fan,Haibin Ling,Liting Lin,Fan Yang,Peng Chu, BMVC,2017. 9 Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, and Chunyuan [78] R.Gao, R.Feris, and K.Grauman. Learningtoseparate Liao. La SOT:AHigh-Quality Benchmark for Large-Scale objectsoundsbywatchingunlabeledvideo. In ECCV,2018. Single Object Tracking. In 2019 IEEE/CVFConferenceon 8 Computer Vision and Pattern Recognition(CVPR),pages [79] Ruohan Gao,Rogerio Feris,and Kristen Grauman.Learning 5369\u20135378,Long Beach,CA,USA,June 2019.IEEE. 35 toseparateobjectsoundsbywatchingunlabeledvideo. In [63] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao ECCV,2018. 51 Li, Zhicheng Yan, Jitendra Malik, and Christoph Feicht- [80] Ruohan Gaoand Kristen Grauman. 2.5 dvisualsound. In enhofer. Multi scale vision trans for mers. ar Xiv preprint CVPR,2019. 8,51 ar Xiv:2104.11227,2021. 75 [81] Ruohan Gaoand Kristen Grauman. Co-separatingsounds [64] Yue Fan,JWKang,LTLi,KCLi,HLChen,STCheng,PY ofvisualobjects. In ICCV,2019. 51 Zhang,ZYZhou,YQCai,and Dong Wang. CN-CELEB:a [82] R.Gaoand K.Grauman. Visual Voice:Audio-visualspeech challenging Chinesespeakerrecognition data set.In ICASSP separation with cross-modalconsistency. In CVPR,2021. 8, 2020-2020 IEEE International Conference on Acoustics, 51 Speech and Signal Processing(ICASSP),pages 7604\u20137608. [83] I.Gebru,S.Ba,X.Li,and R.Horaud. Audio-visualspeaker IEEE,2020. 57 diarization base donspatiotemporalbayesianfusion. PAMI, [65] Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, 2018. 8,51 Li Song,and Guangtao Zhai. Dual Attention Guided Gaze [84] Israel D.Gebru,Sile`ye Ba,Xiaofei Li,and Radu Horaud. Target Detectionin the Wild. In Proceedingsof the IEEE Audio-visualspeakerdiarization base donspatiotemporal Conferenceon Computer Vision and Pattern Recognition bayesianfusion.IEEETransactionson Pattern Analysis and (CVPR 21),2021. 9 Machine Intelligence,39,2017. 52 85 [85] Georgios Georgakis,Md Alimoor Reza,Arsalan Mousavian, [99] Lianghua Huang,Xin Zhao,and Kaiqi Huang. GOT-10 k:A Phi-Hung Le,and Jana Kos\u02c7ecka\u00b4. Multiviewrgb-ddataset Large High-Diversity Benchmark for Generic Object Track- forobjectinstancedetection. In 2016 Fourth International ingin the Wild. IEEETransactionson Pattern Analysis and Conferenceon 3 DVision(3 DV),pages 426\u2013434.IEEE,2016. Machine Intelligence,43(5):1562\u20131577,May 2021. 35 7 [100] Noureldien Hussein, Efstratios Gavves, and Arnold WM [86] Rohit Girdhar and Kristen Grauman. Anticipative video Smeulders. Timeception for complexactionrecognition. In trans for mer. In ICCV,2021. 3,9 CVPR,2019. 7 [101] Go Irie,Mirela Ostrek,Haochen Wang,Hirokazu Kameoka, [87] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE Akisato Kimura,Takahito Kawanishi,and Kunio Kashino. internationalconferenceoncomputervision,pages 1440\u2013 Seeingthroughsounds:Predictingvisualsemanticsegmen- 1448,2015. 75 tationresults from multichannelaudiosignals. In ICASSP [88] Georgia Gkioxari and Jitendra Malik. Findingactiontubes. 2019-2019 IEEE International Conference on Acoustics, In 2015 IEEEConferenceon Computer Vision and Pattern Speech and Signal Processing(ICASSP),pages 3961\u20133964. Recognition(CVPR),pages 759\u2013768,Boston,MA,USA, IEEE,2019. 51 June 2015.IEEE. 31 [102] Phillip Isola,Joseph J.Lim,and Edward H.Adelson. Dis- [89] P.Gollwitzer. Actionphases and mind-sets,Handbookof coveringstates and trans for mationsinimagecollections. In motivation and cognition:Foundationsofsocialbehavior. CVPR,2015. 7 1990. 45 [103] Phillip Isola,Joseph JLim,and Edward HAdelson. Dis- [90] Raghav Goyal,Samira Ebrahimi Kahou,Vincent Michal- coveringstates and trans for mationsinimagecollections. In ski,Joanna Materzynska,Susanne Westphal,Heuna",
      "start_pos": 36498,
      "end_pos": 37010
    },
    {
      "chunk_id": 217,
      "paper_id": "ego4d",
      "text": "June 2015.IEEE. 31 [102] Phillip Isola,Joseph J.Lim,and Edward H.Adelson. Dis- [89] P.Gollwitzer. Actionphases and mind-sets,Handbookof coveringstates and trans for mationsinimagecollections. In motivation and cognition:Foundationsofsocialbehavior. CVPR,2015. 7 1990. 45 [103] Phillip Isola,Joseph JLim,and Edward HAdelson. Dis- [90] Raghav Goyal,Samira Ebrahimi Kahou,Vincent Michal- coveringstates and trans for mationsinimagecollections. In ski,Joanna Materzynska,Susanne Westphal,Heuna Kim, CVPR,2015. 45 Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz [104] Koji Iwano, Tomoaki Yoshinaga, Satoshi Tamura, and Mueller-Freitag,etal. The\u201dsomethingsomething\u201dvideo Sadaoki Furui. Audio-visualspeechrecognitionusinglip data base for learning and evaluatingvisualcommonsense. information extracted from side-face images. EURASIP In ICCV,2017. 45 Journalon Audio,Speech,and Music Processing,2007:1\u20139, [91] Alex Graves,Santiago Ferna\u00b4ndez,and Ju\u00a8rgen Schmidhuber. 2007. 8,51 Bidirectionallstmnetworks for improvedphonemeclassi- [105] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew fication and recognition. In International conference on Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: artificialneuralnetworks,pages 799\u2013804.Springer,2005. Generalperception with iterativeattention. ar Xivpreprint 48,49 ar Xiv:2103.03206,2021. 48,49 [92] Chunhui Gu,Chen Sun,David ARoss,Carl Vondrick,Car- [106] Baoxiong Jia,Yixin Chen,Siyuan Huang,Yixin Zhu,and oline Pantofaru,Yeqing Li,Sudheendra Vijayanarasimhan, Song-Chun Zhu. Amulti-view data set for learningmulti- George Toderici,Susanna Ricco,Rahul Sukthankar,etal. agentmulti-taskactivities. In ECCV,2020. 3 Ava:Avideo data setofspatio-temporallylocalizedatomic [107] Hao Jiang and Kristen Grauman. Seeinginvisibleposes: visualactions. In Proceedingsof the IEEEConferenceon Estimating 3 dbodypose from egocentricvideo. In CVPR, Computer Vision and Pattern Recognition,pages 6047\u20136056, 2017. 3 2018. 1,3,71 [108] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, [93] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- Chloe Hillier,Sudheendra Vijayanarasimhan,Fabio Viola, mar,Yu Zhang,Jiahui Yu,Wei Han,Shibo Wang,Zheng- Tim Green,Trevor Back,Paul Natsev,etal. Thekineticshu- dong Zhang,Yonghui Wu,etal. Con for mer:Convolution- manactionvideo data set. ar Xivpreprintar Xiv:1705.06950, augmented trans for mer for speech recognition. ar Xiv 2017. 1,3,4,41 preprintar Xiv:2005.08100,2020. 61 [109] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier,Sudheendra Vijayanarasimhan,Fabio Viola, [94] Kaiming He,Georgia Gkioxari,Piotr Dolla\u00b4r,and Ross Gir- Tim Green,Trevor Back,Paul Natsev,etal. Thekineticshu- shick. Mask R-CNN. ar Xiv:1703.06870[cs],Jan.2018. manactionvideo data set. ar Xivpreprintar Xiv:1705.06950, 33 2017. 77 [95] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. [110] Evangelos Kazakos,Arsha Nagrani,Andrew Zisserman,and Deep residual learning for image recognition. In CVPR, Dima Damen. Epic-fusion:Audio-visualtemporalbinding 2016. 48,49 for egocentric action recognition. In Proceedings of the [96] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. IEEEInternational Conferenceon Computer Vision,pages Deep residual learning for image recognition. In CVPR, 5492\u20135501,2019. 3,7,8,51 2016. 57 [111] Petr Kellnhofer,Simon Stent,Wojciech Matusik,and An- [97] Farnoosh Heidarivincheh, Majid Mirmehdi, and Dima tonio Torralba. Gaze 360: Physically Unconstrained Gaze Damen. Detecting the momentofcompletion: Temporal Estimationin the Wild. In Proceedingsof the IEEEInterna- models for localisingactioncompletion. In BMVC,2018. tional Conferenceon Computer Vision(ICCV 19),2019. 9, 44 63,64,66 [98] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, [112] Suyoun Kim,Takaaki Hori,and Shinji Watanabe. Jointctc- and Adriane Boyd. spa Cy:Industrial-strength Natural Lan- attention base dend-to-endspeechrecognitionusingmulti- guage Processingin Python,2020. 20 tasklearning. In 2017 IEEEinternationalconferenceon 86 acoustics,speech and signalprocessing(ICASSP),pages [126] Kevin Lai,Liefeng Bo,and Dieter Fox. Unsupervisedfea- 4835\u20134839.IEEE,2017. 61 ture learning for 3 d scene labeling. In 2014 IEEE Inter- [113] Kris M.Kitani,Brian Ziebart,James D.Bagnell,and Martial national Conferenceon Robotics and Automation(ICRA), Hebert. Activity for ecasting. In ECCV,2012. 9 pages 3050\u20133057.IEEE,2014. 7 [114] Dietrich Klakow and Jochen Peters. Testing the correlation [127] Tian Lan,Tsung-Chuan Chen,and Silvio Savarese.Ahierar- ofworderrorrate and perplexity. Speech Communication, chicalrepresentation for futureactionprediction. In ECCV, 38(1-2):19\u201328,2002. 8,54 2014. 9 [115] Mark L. Knapp, Judith A. Hall, and",
      "start_pos": 36960,
      "end_pos": 37472
    },
    {
      "chunk_id": 218,
      "paper_id": "ego4d",
      "text": "Robotics and Automation(ICRA), Hebert. Activity for ecasting. In ECCV,2012. 9 pages 3050\u20133057.IEEE,2014. 7 [114] Dietrich Klakow and Jochen Peters. Testing the correlation [127] Tian Lan,Tsung-Chuan Chen,and Silvio Savarese.Ahierar- ofworderrorrate and perplexity. Speech Communication, chicalrepresentation for futureactionprediction. In ECCV, 38(1-2):19\u201328,2002. 8,54 2014. 9 [115] Mark L. Knapp, Judith A. Hall, and Terrence G. Hor- [128] Federico Landini,Ja\u00b4n Profant,Mireia Diez,and Luka\u00b4s\u02c7Bur- gan. Nonverbal Communication in Human Interaction. get.Bayesianhmmclusteringofx-vectorsequences(vbx)in Wadsworth Cengage Learning,8 thedition,2014. 8 speakerdiarization:theory,implementation and analysison [116] Ross A Knepper, Todd Layton, John Romanishin, and standardtasks. Computer Speech&Language,71:101254, Daniela Rus. Ikeabot: Anautonomousmulti-robotcoor- 2022. 56 dinatedfurnitureassemblysystem. In 2013 IEEEInterna- [129] Y.J.Lee,J.Ghosh,and K.Grauman.Discoveringimportant tionalconferenceonrobotics and automation,pages 855\u2013 people and objects for egocentricvideosummarization. In 862.IEEE,2013. 44 CVPR,2012. 2,3 [117] Andrew JKolarik,Brian CJMoore,Pavel Zahorik,Silvia [130] Y.J.Lee,J.Ghosh,and K.Grauman.Discoveringimportant Cirstea, and Shahina Pardhan. Auditorydistancepercep- people and objects for egocentricvideosummarization. In tioninhumans: areviewofcues,development,neuronal Proceedingsof the IEEEConferenceon Computer Vision bases,andeffectsofsensoryloss. Attention,Perception,& and Pattern Recognition(CVPR),2012. 45 Psychophysics,78(2):373\u2013395,2016. 51 [131] Yong Jae Leeand Kristen Grauman. Predictingimportant [118] Hema S.Koppula and Ashutosh Saxena.Anticipatinghuman objects for egocentricvideosummarization. IJCV,2015. 3 activitiesusingobjectaffordances for reactiveroboticre- [132] Bruno Lepri,Ramanathan Subramanian,Kyriaki Kalimeri, sponse.Pattern Analysis and Machine Intelligence,38(1):14\u2013 Jacopo Staiano, Fabio Pianesi, and Nicu Sebe. Connect- 29,2016. 9 ingmeetingbehaviorwi the xtraversion-asystematicstudy. [119] Ranjay Krishna,Kenji Hata,Frederic Ren,Li Fei-Fei,and IEEETransactionson Affective Computing,3(4):443\u2013455, Juan Carlos Niebles. Dense-captioningeventsinvideos. In 2012. 9 International Conferenceon Computer Vision(ICCV),2017. [133] Vladimir ILevenshteinetal. Binarycodescapableofcor- 7 rectingdeletions,insertions,andreversals. In Sovietphysics [120] Alexei A.Efros Krishna Kumar Singh,Kayvon Fatahalian. doklady,1966. 73 Krishnacam:Usingalongitudinal,single-person,egocentric [134] Cheng Li and Kris Kitani. Model recommendation with dataset for sceneunderst and ingtasks. In IEEEWinter Con- virtualprobes for ego-centrich and detection.In ICCV,2013. ferenceon Applicationsof Computer Vision(WACV),2016. 3 9 [135] Yin Li, Alireza Fathi, and James M. Rehg. Learning to [121] Matej Kristan, Ales Leonardis, Jiri Matas, Michael predict gaze in egocentric video. In Proceedings of the Felsberg, Roman Pflugfelder, Joni-Kristian Kamarainen, Luka C\u02c7ehovin Zajc,Martin Danelljan,Alan Lukezic,On- IEEEInternational Conferenceon Computer Vision,pages 3216\u20133223,2013. 65 drej Drbohlav,Linbo He,Yushan Zhang,Song Yan,Jinyu [136] Y.Li,M.Liu,and J.Rehg. Intheeyeofbeholder: Joint Yang,Gustavo Fernandez,andetal.Theeighthvisualobject learningofgaze and actionsinfirstpersonvideo. In ECCV, tracking VOT 2020 challengeresults,2020. 31 2018. 45 [122] Taku Kudo and John Richardson. Sentencepiece: A [137] Yin Li,Miao Liu,and Jame Rehg.Inthe Eyeof the Beholder: simple and languageindependentsubwordtokenizer and detokenizer for neural text processing. ar Xiv preprint Gaze and Actionsin First Person Video. IEEETransactions ar Xiv:1808.06226,2018. 61 on Pattern Analysis and Machine Intelligence,2021. 65 [123] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui- [138] Yin Li,Miao Liu,and James MRehg.Intheeyeofbeholder: jlings,Ivan Krasin,Jordi Pont-Tuset,Shahab Kamali,Stefan Jointlearningofgaze and actionsinfirstpersonvideo. In Popov,Matteo Malloci,Alexander Kolesnikov,etal. The Proceedingsof the European Conferenceon Computer Vi- openimages data setv 4. International Journalof Computer sion(ECCV),pages 619\u2013635,2018. 2,3 Vision,128(7):1956\u20131981,2020. 57 [139] Yanghao Li,Tushar Nagarajan,Bo Xiong,and Kristen Grau- [124] F.Dela Torre,J.Hodgins,J.Montano,S.Valcarcel,R.For- man. Ego-exo: Transferring visual representations from cada,and J.Macey. Guideto the carnegiemellonuniversity third-persontofirst-personvideos. In CVPR,2021. 3,7 multimodalactivity(cmu-mmac)data base. In Tech.report [140] Tianwei Lin,Xiao Liu,Xin Li,Errui Ding,and Shilei Wen. CMU-RI-TR-08-22,Robotics Institute,Carnegie Mellon Uni- Bmn:Boundary-matchingnetwork for temporalactionpro- versity,2009. 3 posalgeneration. In Proceedingsof the IEEE/CVFInterna- [125] Loic Lacheze,Yan Guo,Ryad Benosman,Bruno Gas,and tional Conferenceon Computer Vision,pages 3889\u20133898, Charlie Couverture. Audio/videofusion for objectsrecog- 2019. 48,49 nition. In 2009 IEEE/RSJInternational Conferenceon In- [141] Tianwei Lin,Xu Zhao,Haisheng Su,Chongjing Wang,and telligent Robots and Systems,pages 652\u2013657.IEEE,2009. Ming Yang. Bsn:Boundarysensitivenetwork for temporal 8 actionproposalgeneration. In Proceedingsof",
      "start_pos": 37422,
      "end_pos": 37934
    },
    {
      "chunk_id": 219,
      "paper_id": "ego4d",
      "text": "3 posalgeneration. In Proceedingsof the IEEE/CVFInterna- [125] Loic Lacheze,Yan Guo,Ryad Benosman,Bruno Gas,and tional Conferenceon Computer Vision,pages 3889\u20133898, Charlie Couverture. Audio/videofusion for objectsrecog- 2019. 48,49 nition. In 2009 IEEE/RSJInternational Conferenceon In- [141] Tianwei Lin,Xu Zhao,Haisheng Su,Chongjing Wang,and telligent Robots and Systems,pages 652\u2013657.IEEE,2009. Ming Yang. Bsn:Boundarysensitivenetwork for temporal 8 actionproposalgeneration. In Proceedingsof the European 87 Conferenceon Computer Vision(ECCV),pages 3\u201319,2018. Conferenceon Applicationsof Computer Vision(WACV), 7,25,32,41 pages 1507\u20131516,January 2021. 7 [142] Tsung-Yi Lin, Piotr Dolla\u00b4r, Ross Girshick, Kaiming He, [156] Christophe Micheyl,Christian Kaernbach,and Laurent De- Bharath Hariharan,and Serge Belongie. Feature Pyramid many. Anevaluationofpsychophysical model sofauditory Networks for Object Detection.ar Xiv:1612.03144[cs],Apr. changeperception.Psychologicalreview,115(4):1069,2008. 2017. 33 51 [143] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, [157] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Pietro Perona,Deva Ramanan,Piotr Dolla\u00b4r,and CLawrence Makar and Tapaswi, Ivan Laptev, and Josef Sivic. Zitnick. Microsoft COCO:Commonobjectsincontext. In How To 100 M:Learninga Text-Video Embeddingby Watch- ECCV,2014. 1,3,47 ing Hundred Million Narrated Video Clips. In ICCV,2019. [144] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Fore- 3,6 castinghuman-objectinteraction:jointpredictionofmotor [158] Ishan Misra,Abhinav Gupta,and Martial Hebert. Fromred attention and actionsinfirstpersonvideo. In ECCV,2020. winetoredtomato: Composition with context. In CVPR, 3 2017. 45 [145] Wen Liu,Weixin Luo,Dongze Lian,and Shenghua Gao.Fu- [159] Yu Mitsuzumi, Atsushi Nakazawa, and Toyoaki Nishida. tureframeprediction for anomalydetection\u2013anew base line. Deepeyecontactdetector:Robusteyecontactbiddetection In Proceedingsof the IEEEConferenceon Computer Vision usingconvolutionalneuralnetwork. In BMVC,2017. 9 and Pattern Recognition,pages 6536\u20136545,2018. 9 [160] Davide Moltisanti,Michael Wray,Walterio Mayol-Cuevas, [146] William Lotter, Gabriel Kreiman, and David Cox. Deep and Dima Damen. Trespassing the boundaries: Labelling predictivecodingnetworks for videoprediction and unsu- temporalbounds for objectinteractionsinegocentricvideo. pervisedlearning. ar Xivpreprintar Xiv:1605.08104,2016. In ICCV,2017. 44,45 9 [161] Pedro Morgado,Nono Vasconcelos,Timothy Langlois,and [147] Cewu Lu,Renjie Liao,and Jiaya Jia. Personalobjectdis- Oliver Wang.Self-supervisedgenerationofspatialaudio for coveryinfirst-personvideos. TIP,2015. 3 360\u25e6video. In Neur IPS,2018. 8,51 [148] Zheng Luand Kristen Grauman. Story-drivensummariza- [162] Matthias Mu\u00a8ller,Adel Bibi,Silvio Giancola,Salman Alsub- tion for egocentricvideo. In CVPR,2013. 3 aihi, and Bernard Ghanem. Tracking Net: ALarge-Scale [149] Tahmida Mahmud, Mahmudul Hasan, and Amit K Roy- Dataset and Benchmark for Object Trackingin the Wild. In Chowdhury. Jointpredictionofactivitylabels and starting Vittorio Ferrari,Martial Hebert,Cristian Sminchisescu,and times in untrimmed videos. In Proceedings of the IEEE Yair Weiss,editors,Computer Vision\u2013ECCV 2018,volume International Conferenceon Computer Vision,pages 5773\u2013 11205,pages 310\u2013327.Springer International Publishing, 5782,2017. 9 Cham,2018. 35 [150] Manuel JMarin-Jimenez,Vicky Kalogeiton,Pablo Medina- [163] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen Suarez,and Andrew Zisserman. Laeo-net:revisitingpeople Grauman.Groundedhuman-objectinteractionhotspots from lookingateacho the rinvideos. In Proceedingsof the IEEE video. ICCV,2019. 3 Conferenceon Computer Vision and Pattern Recognition, [164] Tushar Nagarajan and Kristen Grauman. Attributesasop- pages 3477\u20133485,2019. 9 erators: factorizing unseen attribute-object compositions. [151] Manuel Jesu\u00b4s Mar\u00b4\u0131n-Jime\u00b4nez,Andrew Zisserman,Marcin In Proceedingsof the European Conferenceon Computer Eichner,and Vittorio Ferrari. Detectingpeoplelookingat Vision(ECCV),pages 169\u2013185,2018. 7,45 eacho the rinvideos. International Journalof Computer [165] A. Nagrani, J. S. Chung, and A. Zisserman. Vox Celeb: Vision,106(3):282\u2013296,2014. 9 a large-scale speaker identification dataset. In INTER- [152] Manuel JMar\u00b4\u0131n-Jime\u00b4nez,Andrew Zisserman,and Vittorio SPEECH,2017. 52,57 Ferrari.Here\u2019slookingatyou,kid.Detectingpeoplelooking ateacho the rinvideos.In BMVC,5,2011. 9 [166] Katsuyuki Nakamura,Serena Yeung,Alexandre Alahi,and Li Fei-Fei. Jointlylearningenergyexpenditures and activ- [153] Michael Mathieu,Camille Couprie,and Yann Le Cun. Deep itiesusingegocentricmultimodalsignals. In CVPR,2017. multi-scale video prediction beyond mean square error. 3 ar Xivpreprintar Xiv:1511.05440,2015. 9 [154] Iain Mc Cowan,Jean Carletta,Wessel Kraaij,Simone Ashby, [167] Lukas Neumann,Andrew Zisserman,and Andrea Vedaldi. Sebastien Bourban,Mike Flynn,Mael Guillemot,Thomas Futureeventprediction:Ifandwhen.",
      "start_pos": 37884,
      "end_pos": 38396
    },
    {
      "chunk_id": 220,
      "paper_id": "ego4d",
      "text": "Katsuyuki Nakamura,Serena Yeung,Alexandre Alahi,and Li Fei-Fei. Jointlylearningenergyexpenditures and activ- [153] Michael Mathieu,Camille Couprie,and Yann Le Cun. Deep itiesusingegocentricmultimodalsignals. In CVPR,2017. multi-scale video prediction beyond mean square error. 3 ar Xivpreprintar Xiv:1511.05440,2015. 9 [154] Iain Mc Cowan,Jean Carletta,Wessel Kraaij,Simone Ashby, [167] Lukas Neumann,Andrew Zisserman,and Andrea Vedaldi. Sebastien Bourban,Mike Flynn,Mael Guillemot,Thomas Futureeventprediction:Ifandwhen. In Proceedingsof the Hain,Jaroslav Kadlec,Vasilis Karaiskos,Melissa Kronen- IEEEConferenceon Computer Vision and Pattern Recogni- thal,Guillaume Lathoud,Mike Lincoln,Agnes Lisowska, tion Workshops,pages 0\u20130,2019. 9 Wilfried Post, Dennis Reidsma, and Pierre Wellner. The [168] Evonne Ng,Donglai Xiang,Hanbyul Joo,and Kristen Grau- AMImeetingcorpus. In Proceedingsof Measuring Behav- man. You 2 me:Inferringbodyposeinegocentricvideovia ior 2005,the 5 th International Conferenceon Methods and first and secondpersoninteractions. In CVPR,2020. 3 Techniquesin Behavioral Research,pages 137\u2013140,2005. 9 [169] Joonas Nikunen and Tuomas Virtanen. Directionofarrival [155] Jean-Philippe Mercier,Mathieu Garon,Philippe Giguere, basedspatialcovariance model for blindsounds our cesep- and Jean-Francois Lalonde. Deep template-based object aration. IEEE/ACMTransactionson Audio, Speech, and instancedetection. In Proceedingsof the IEEE/CVFWinter Language Processing,22(3):727\u2013739,2014. 51 88 [170] C.Northcutt,S.Zha,S.Lovegrove,and R.Newcombe.Ego- inanindustrial-likedomain. In IEEEWinter Conferenceon com: Amulti-personmulti-modalegocentriccommunica- Applicationof Computer Vision(WACV),2021. 3 tions data set. PAMI,2020. 3 [185] Rene\u00b4Ranftl,Alexey Bochkovskiy,and Vladlen Koltun. Vi- [171] Andrew Owens and Alexei AEfros.Audio-visualsceneanal- siontransformers for denseprediction.In Proceedingsof the ysis with self-supervisedmultisensoryfeatures. In ECCV, IEEE/CVFInternational Conferenceon Computer Vision, 2018. 51 pages 12179\u201312188,2021. 38,39 [172] Cristina Palmero,Elsbeth Avan Dam,Sergio Escalera,Mike [186] Adria Recasens,Aditya Khosla,Carl Vondrick,and Antonio Kelia,Guido FLichtert,Lucas PJJNoldus,Andrew JSpink, Torralba. Where are the ylooking? In Advancesin Neural and Astridvan Wieringen. Automaticmutualgazedetec- Information Processing Systems,pages 199\u2013207,2015. 9 tioninface-to-facedyadicinteractionvideos. Measuring [187] Joseph Redmon and Ali Farhadi. Yolov 3:Anincremental Behavior 2018,2018. 9 improvement. ar Xivpreprintar Xiv:1804.02767,2018. 57 [173] Daniel SPark,William Chan,Yu Zhang,Chung-Cheng Chiu, [188] James M. Rehg, Agata Rozga, Gregory D. Abowd, and Barret Zoph,Ekin DCubuk,and Quoc VLe. Specaugment: Matthew S. Goodwin. Behavioral Imaging and Autism. Asimple data augmentationmethod for automaticspeech IEEEPervasive Computing,13(2):84\u201387,2014. 8 recognition. ar Xivpreprintar Xiv:1904.08779,2019. 60 [189] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun. [174] H.S.Park,J.-J.Hwang,Y.Niu,and J.Shi.Egocentricfuture Fasterr-cnn:Towardsreal-timeobjectdetection with region localization. In CVPR,2016. 9 proposalnetworks. In Neur IPS,2015. 33 [175] H.S.Park,J.-J.Hwang,Y.Niu,and J.Shi.Egocentricfuture [190] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun. localization. In Conferenceon Computer Vision and Pattern Faster r-cnn: Towards real-time object detection with re- Recognition(CVPR),2016. 74 gionproposalnetworks. Advancesinneuralin for mation [176] Hyun Soo Park,Eakta Jain,and Yaser Sheikh. 3 Dsocial processingsystems,28:91\u201399,2015. 49 saliency from head-mountedcameras.In Advancesin Neural [191] Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, and Information Processing Systems,volume 1,pages 422\u2013430, Giovanni Maria Farinella. Predicting the future from first 2012. 9 person(egocentric)vision:Asurvey. Computer Vision and [177] Tae Jin Park,Naoyuki Kanda,Dimitrios Dimitriadis,Kyu J Image Underst and ing,2021. 9 Han,Shinji Watanabe,and Shrikanth Narayanan. Areview [192] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad- ofspeakerdiarization:Recentadvances with deeplearning. hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh ar Xivpreprintar Xiv:2101.09624,2021. 53,56 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, [178] David R Perrott and Kourosh Saberi. Minimum audible Zhonghua Xi, et al. Ava-activespeaker: An audio-visual anglethresholds for sourcesvaryinginbo the levation and dataset for active speaker detection. ar Xiv preprint azimuth. The Journalof the Acoustical Societyof America, ar Xiv:1901.01342,2019. 8,52,53 87(4):1728\u20131731,1990. 51 [193] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad- [179] Hamed Pirsiavash and Deva Ramanan. Detecting activi- hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh tiesofdailylivinginfirst-personcameraviews. In 2012 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, IEEEconferenceoncomputervision and patternrecogni- Zhonghua Xi,and",
      "start_pos": 38346,
      "end_pos": 38858
    },
    {
      "chunk_id": 221,
      "paper_id": "ego4d",
      "text": "Xiv preprint azimuth. The Journalof the Acoustical Societyof America, ar Xiv:1901.01342,2019. 8,52,53 87(4):1728\u20131731,1990. 51 [193] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad- [179] Hamed Pirsiavash and Deva Ramanan. Detecting activi- hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh tiesofdailylivinginfirst-personcameraviews. In 2012 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, IEEEconferenceoncomputervision and patternrecogni- Zhonghua Xi,and Caroline Pantofaru. Ava Active Speaker: tion,pages 2847\u20132854.IEEE,2012. 2,3 An Audio-Visual dataset for Active Speaker Detection. [180] H.Pirsiavash and D.Ramanan. Detectingactivitiesofdaily In ICASSP, IEEE International Conference on Acoustics, livinginfirst-personcameraviews. In Computer Vision and Speech and Signal Processing-Proceedings,volume 2020- Pattern Recognition(CVPR),2012. 45 May,pages 4492\u20134496,2020. 9 [181] Daniel Povey,Arnab Ghoshal,Gilles Boulianne,Lukas Bur- [194] M.S.Ryoo and L.Matthies. First-personactivityrecogni- get,Ondrej Glembek,Nagendra Goel,Mirko Hannemann, tion: What are the ydoingtome? In IEEEConferenceon Petr Motlicek,Yanmin Qian,Petr Schwarz,Jan Silovsky, Computer Vision and Pattern Recognition(CVPR),2013. 3 Georg Stemmer,and Karel Vesely. The Kaldispeechrecog- [195] Paul-Edouard Sarlin,Daniel De Tone,Tomasz Malisiewicz, nitiontoolkit. In IEEE 2011 Workshopon Automatic Speech and Andrew Rabinovich.Superglue:Learningfeaturematch- Recognition and Underst and ing,2011. 56 ing with graph neural networks. In Proceedings of the [182] Senthil Purushwalkam,Maximilian Nickel,Abhinav Gupta, IEEE/CVFconferenceoncomputervision and patternrecog- and Marc\u2019Aurelio Ranzato. Task-drivenmodularnetworks nition,pages 4938\u20134947,2020. 36,38 forzero-shotcompositionallearning. In Proceedingsof the [196] Johannes LSchonberger and Jan-Michael Frahm. Structure- IEEEInternational Conferenceon Computer Vision,pages from-motion revisited. In Proceedings of the IEEE con- 3593\u20133602,2019. 45 ferenceoncomputervision and patternrecognition,pages [183] F.Ragusa,A.Furnari,S.Battiato,G.Signorello,and G.M. 4104\u20134113,2016. 36 Farinella. Egocentricvisitorslocalizationinculturalsites. [197] A.Senocak,T.-H.Oh,J.Kim,M.Yang,and I.S.Kweon. Journal on Computing and Cultural Heritage (JOCCH), Learningtolocalizesounds our cesinvisualscenes:Analysis 2019. 3 andapplications. TPAMI,2019. 8,51 [184] Francesco Ragusa, Antonino Furnari, Salvatore Livatino, [198] Dandan Shan,Jiaqi Geng,Michelle Shu,and David Fouhey. and Giovanni Maria Farinella. Themec can odataset:Under- Understandinghumanh and sincontactatinternet scale. In standinghuman-objectinteractions from egocentricvideos CVPR,2020. 45,46 89 [199] Dandan Shan,Jiaqi Geng,Michelle Shu,and David Fouhey. [214] Twenty BN. The 20 BN-jester dataset V 1. https:// Understandinghumanh and sincontactatinternet scale. In 20 bn.com/datasets/jester. 45 CVPR,2020. 49 [215] Joost Van Amersfoort,Anitha Kannan,Marc\u2019Aurelio Ran- [200] Mohit Sharma,Kevin Zhang,and Oliver Kroemer. Learning zato, Arthur Szlam, Du Tran, and Soumith Chintala. semanticembeddingspaces for slicingvegetables. ar Xiv Trans for mation-based model sofvideosequences. ar Xiv preprintar Xiv:1904.00303,2019. 44 preprintar Xiv:1701.08435,2017. 9,40 [201] Gunnar ASigurdsson,Abhinav Gupta,Cordelia Schmid,Ali [216] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko- Farhadi,and Karteek Alahari. Charades-ego:Alarge-scale reit,Llion Jones,Aidan NGomez,\u0141ukasz Kaiser,and Illia datasetofpairedthird and firstpersonvideos.ar Xivpreprint Polosukhin.Attentionisallyouneed.In Advancesinneural ar Xiv:1804.09626,2018. 2,3,45 inprocessingsystems,pages 5998\u20136008,2017. 49 [202] Nathan Silberman,Derek Hoiem,Pushmeet Kohli,and Rob [217] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko- Fergus. Indoorsegmentation and supportinference from reit,Llion Jones,Aidan NGomez,\u0141ukasz Kaiser,and Illia rgbdimages. In Europeanconferenceoncomputervision, Polosukhin.Attentionisallyouneed.In Advancesinneural pages 746\u2013760.Springer,2012. 38 informationprocessingsystems,pages 5998\u20136008,2017. 61 [203] Silero Team. Silero vad: Pre-trained enterprise-grade [218] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu voice activity detector (VAD), number detector and lan- Lin, and Honglak Lee. Decomposing motion and con- guageclassifier. https://github.com/snakers 4/ tent for naturalvideosequenceprediction. ar Xivpreprint silero-vad,2021. 59 ar Xiv:1706.08033,2017. 9 [204] Michel Silva,Washington Ramos,Joa\u02dco Ferreira,Felipe Cha- [219] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. mone, Mario Campos, and Erickson R. Nascimento. A Anticipatingvisualrepresentations from unlabeledvideo. In weightedsparsesampling and smoothingframetransition CVPR,2016. 9 approach for semanticfast-forwardfirst-personvideos. In 2018 IEEE/CVFConferenceon Computer Vision and Pat- [220] He Wang,So\u00a8ren Pirk,Ersin Yumer,Vladimir GKim,Ozan tern Recognition(CVPR),2018. 3 Sener,Srinath Sridhar,and Leonidas JGuibas. Learninga [205] Krishna Kumar Singh, Kayvon Fatahalian, and Alexei A generative model for multi-stephuman-objectinteractions Efros. Krishnacam: Using a longitudinal, single-person,",
      "start_pos": 38808,
      "end_pos": 39320
    },
    {
      "chunk_id": 222,
      "paper_id": "ego4d",
      "text": "unlabeledvideo. In weightedsparsesampling and smoothingframetransition CVPR,2016. 9 approach for semanticfast-forwardfirst-personvideos. In 2018 IEEE/CVFConferenceon Computer Vision and Pat- [220] He Wang,So\u00a8ren Pirk,Ersin Yumer,Vladimir GKim,Ozan tern Recognition(CVPR),2018. 3 Sener,Srinath Sridhar,and Leonidas JGuibas. Learninga [205] Krishna Kumar Singh, Kayvon Fatahalian, and Alexei A generative model for multi-stephuman-objectinteractions Efros. Krishnacam: Using a longitudinal, single-person, fromvideos. In Eurographics,2019. 45 egocentric data set for sceneunderst and ingtasks. In WACV, [221] Limin Wang,Yuanjun Xiong,Zhe Wang,Yu Qiao,Dahua 2016. 2,3 Lin,Xiaoou Tang,and Luc Van Gool.Temporalsegmentnet- [206] David Snyder,Daniel Garcia-Romero,Gregory Sell,Daniel works:Towardsgoodpractices for deepactionrecognition. Povey,and Sanjeev Khudanpur. X-vectors: Robust DNN In ECCV,2016. 3,7 embeddings for speakerrecognition. In 2018 IEEEInterna- [222] Qiang Wang,Li Zhang,Luca Bertinetto,Weiming Hu,and tional Conferenceon Acoustics,Speech and Signal Process- Philip H.S.Torr. Fastonlineobjecttracking and segmenta- ing(ICASSP),2018. 57 tion:Aunifyingapproach,2019. 17 [207] Khurram Soomro,Amir Roshan Zamir,and Mubarak Shah. [223] Xiaolong Wang,Ali Farhadi,and Abhinav Gupta. Actions\u02dc Ucf 101:Adatasetof 101 humanactionclasses from videos trans for mations. In CVPR,2016. 45 inthewild. In CRCV-TR-12-01,2012. 3 [224] Xiaolong Wang,Ross Girshick,Abhinav Gupta,and Kaim- [208] Emiliano Spera,Antonino Furnari,Sebastiano Battiato,and ing He. Non-localneuralnetworks. In CVPR,2018. 3 Giovanni Maria Farinella. Egocentricshoppingcartlocal- [225] Yuxin Wu,Alexander Kirillov,Francisco Massa,Wan-Yen ization. In International Conferenceon Pattern Recognition Lo,and Ross Girshick. Detectron 2. 35 (ICPR),2018. 3 [226] Fanyi Xiao,Yong Jae Lee,Kristen Grauman,Jitendra Malik, [209] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, and Christoph Feichtenhofer.Audiovisualslowfastnetworks Erik Wijmans,Simon Green,Jakob JEngel,Raul Mur-Artal, for video recognition. ar Xiv preprint ar Xiv:2001.08740, Carl Ren,Shobhit Verma,etal.Thereplica data set:Adigital 2020. 8,51 replicaofindoorspaces. ar Xivpreprintar Xiv:1906.05797, 2019. 14 [227] SHIXingjian,Zhourong Chen,Hao Wang,Dit-Yan Yeung, [210] Yu-Chuan Suand Kristen Grauman. Detectingengagement Wai-Kin Wong,and Wang-chun Woo. Convolutionallstm inegocentricvideo. In ECCV,2016. 2,3,45 network: A machine learning approach for precipitation [211] Ruijie Tao,Zexu Pan,Rohan Kumar Das,Xinyuan Qian, nowcasting. In Advancesinneuralin for mationprocessing Mike Zheng Shou,and Haizhou Li. Issomeonespeaking? systems,pages 802\u2013810,2015. 9 exploringlong-termtemporalfeatures for audio-visualactive [228] Jun Xu,Tao Mei,Ting Yao,and Yong Rui. Msr-vtt:Alarge speakerdetection. ar Xivpreprintar Xiv:2107.06592,2021. videodescription data set for bridgingvideo and language. 53,58,59 IEEE International Conference on Computer Vision and [212] Y.Tian,J.Shi,B.Li,Z.Duan,and C.Xu. Audio-visual Pattern Recognition(CVPR),June 2016. 7 eventlocalizationinunconstrainedvideos. In ECCV,2018. [229] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, 8,51 and Bernard Ghanem. G-tad: Sub-graphlocalization for [213] E.Tulving. Episodi can dsemanticmemory. In E.Tulv- temporalactiondetection. In Proceedingsof the IEEE/CVF ing and W. Donaldson, editors, Organization of memory. Conferenceon Computer Vision and Pattern Recognition, Academic Press,1972. 6 pages 10156\u201310165,2020. 7,25,32,41 90 [230] Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, and Yoichi Sato.Futurepersonlocalizationinfirst-personvideos. In The IEEEConferenceon Computer Vision and Pattern Recognition(CVPR),June 2018. 9 [231] Ryo Yonetani,Kris M.Kitani,and Yoichi Sato.Recognizing micro-actions and reactions from pairedegocentricvideos. In CVPR,2016. 3 [232] Ryo Yonetani,Kris MKitani,and Yoichi Sato. Visualmotif discoveryviafirst-personvision. In ECCV,2016. 3 [233] Fisher Yu,Dequan Wang,Evan Shelhamer,and Trevor Dar- rell.Deeplayeraggregation.In Proceedingsof the IEEEcon- ferenceoncomputervision and patternrecognition,pages 2403\u20132412,2018. 49 [234] Hua Zhang,Xiaochun Cao,and Rui Wang. Audiovisual attributediscovery for fine-grainedobjectrecognition. In Proceedings of the AAAI Conference on Artificial Intelli- gence,volume 32,2018. 8 [235] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Span-basedlocalizingnetwork for naturallanguagevideo localization. In Proceedingsof the 58 th Annual Meetingof the Association for Computational Linguistics,pages 6543\u2013 6554, Online, July 2020. Association for Computational Linguistics. 40 [236] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2 dtemporaladjacentnetworks for moment localization with naturallanguage.",
      "start_pos": 39270,
      "end_pos": 39782
    },
    {
      "chunk_id": 223,
      "paper_id": "ego4d",
      "text": "Wei Jing, and Joey Tianyi Zhou. Span-basedlocalizingnetwork for naturallanguagevideo localization. In Proceedingsof the 58 th Annual Meetingof the Association for Computational Linguistics,pages 6543\u2013 6554, Online, July 2020. Association for Computational Linguistics. 40 [236] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2 dtemporaladjacentnetworks for moment localization with naturallanguage. In AAAI,2020. 26,31, 32,39,40 [237] Chen Zhao, Ali K Thabet, and Bernard Ghanem. Video self-stitchinggraphnetwork for temporalactionlocalization. In Proceedingsof the IEEE/CVFInternational Conference on Computer Vision,pages 13658\u201313667,2021. 7,25,32, 40,41 [238] Hang Zhao,Chuang Gan,Andrew Rouditchenko,Carl Von- drick,Josh Mc Dermott,and Antonio Torralba. Thesound ofpixels. In ECCV,2018. 51 [239] Bolei Zhou,Alex Andonian,Aude Oliva,and Antonio Tor- ralba. Temporalrelationalreasoninginvideos. In ECCV, 2018. 3 [240] Hao Zhou,Chongyang Zhang,Yan Luo,Yanjun Chen,and Chuanping Hu. Embracinguncertainty: Decoupling and de-bias for robusttemporalgrounding. In Proceedingsof the IEEE/CVFConferenceon Computer Vision and Pattern Recognition,pages 8445\u20138454,2021. 33 [241] Xingyi Zhou,Dequan Wang,and Philipp Kra\u00a8henbu\u00a8hl. Ob- jectsaspoints. ar Xivpreprintar Xiv:1904.07850,2019. 49 [242] Y.Zhou and T.Berg. Learningtemporaltrans for mations fromtime-lapsevideos. In ECCV,2016. 7 [243] Yipin Zhou and Tamara LBerg. Temporalperception and predictioninego-centricvideo. In ICCV,2015. 3,7 [244] Yipin Zhou and Tamara LBerg. Learningtemporaltrans for- mations from time-lapsevideos. In ECCV,2016. 45 [245] Hao Zhu,Man-Di Luo,Rui Wang,Ai-Hua Zheng,and Ran He. Deepaudio-visuallearning: Asurvey. International Journalof Automation and Computing,pages 1\u201326,2021. 8 91",
      "start_pos": 39732,
      "end_pos": 39931
    },
    {
      "chunk_id": 224,
      "paper_id": "egomimic",
      "text": "Ego Mimic: Scaling Imitation Learning via Egocentric Video Simar Kareer 1, Dhruv Patel 1\u2217, Ryan Punamiya 1\u2217, Pranay Mathur 1\u2217, Shuo Cheng 1 Chen Wang 2, Judy Hoffman 1\u2020, Danfei Xu 1\u2020 Fig. 1: Ego Mimic unlocks human embodiment data\u2014egocentric videos paired with 3 D hand tracks\u2014as a new scalable data source for imitation learning. We can capture this data anywhere, without a robot, by wearing a pair of Project Aria glasses while per for ming manipulation tasks with our own hands. Ego Mimic bridges kinematic, distributional, and appearance differences between human embodiment data (left) and traditional robot teleoperation data (right) to learn a unified policy. We find that human embodiment data boosts task per for mance by 34-228% over using robot data alone, and enables generalization to new objects or even scenes. Abstract\u2014The scale and diversity of demonstration data To scale up data for robotics, there have been recent ad- required for imitation learning is a significant challenge. We vancesin data collectionsystems.Forexample,ALOHA[1], present Ego Mimic,afull-stackframeworkwhich scale smanip- [2] and GELLO [3] are intuitive leader-follower controls ulationviahumanembodiment data,specificallyegocentrichu- for collecting teleoperated data. Other works have opted man videos paired with 3 D hand tracking. Ego Mimic achieves this through: (1) a system to capture human embodiment to develop hand-held grippers to collect data without a data using the ergonomic Project Aria glasses, (2) a low-cost robot [4]. Despite these advances, data collected via these bimanual manipulator that minimizes the kinematic gap to systemsstillrequirespecializedhardw are and activeef for tin human data, (3) cross-domain data alignment techniques, and providingdemonstrations.Wehypo the size that akeystep for (4) an imitation learning architecture that co-trains on human achieving Internet-scalerobotdataispassive data collection. and robot data. Compared to prior works that only extract high-level intent from human videos, our approach treats Just as the Internet was not built for curating data to train human and robot data equally as embodied demonstration largevision and languagemodels,anidealrobot data system data and learns a unified policy from both data sources. should allow users to generate sensorimotor behavior data Ego Mimic achieves significant improvement on a diverse set without intending to do so. of long-horizon, single-arm and bimanual manipulation tasks Human videos, especially those captured from an egocen- over state-of-the-art imitation learning methods and enables generalization to entirely new scenes. Finally, we show a tric perspective, present an ideal source of data for passive favorable scaling trend for Ego Mimic, where adding 1 hour of data scalability. This data aligns closely with robot data, additionalh and dataissignifi can tlymorevaluablethan 1 hour as it provides an egocentric camera for vision, 3 D hand ofadditionalrobot data.Videos and additionalin for mation can tracking for actions, and onboard SLAM for localization. be found at https://egomimic.github.io/ The advent of consumer-grade devices capable of capturing I. INTRODUCTION such data, including Extended Reality (XR) devices and End-to-end imitation learning has shown remarkable per- camera-equipped \u201csmart glasses\u201d, opens up unprecedented formance in learning complex manipulation tasks, but it re- opportunities for passive data collection at scale. While mains brittle when facing new scenarios and tasks. Drawing recent works have begun to leverage human",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 225,
      "paper_id": "egomimic",
      "text": "Reality (XR) devices and End-to-end imitation learning has shown remarkable per- camera-equipped \u201csmart glasses\u201d, opens up unprecedented formance in learning complex manipulation tasks, but it re- opportunities for passive data collection at scale. While mains brittle when facing new scenarios and tasks. Drawing recent works have begun to leverage human video data, on the recent success of Computer Vision and Natural Lan- their approaches are limited to extracting high-level intent guage Processing,wehypo the size that for learnedpoliciesto information from videos to build planners that guide low- achieve broad generalization, we must dramatically scale up level conditional policies [5], [6]. As a result, these systems the training data size. While these adjacent domains benefit remainconstrainedby the per for manceoflow-levelpolicies, from Internet-sourced data,roboticslackssuchanequivalent. which are typically trained solely on teleoperation data. Weargue that totruly scale robotper for mance with human SK, DP, RP, PM, SC, JH, DX are with the Georgia Institute of data, we should not consider human videos as an auxiliary Technology and CW is with Stanford University. Email Correspondence: skareer@gatech.edu data source that requires separate handling. Instead, we *Denotesequalcontribution.\u2020Denotesequaladvising. should exploit the inherent similarities between egocentric 4202 tc O 13 ]OR.sc[ 1 v 12242.0142:vi Xra human data and robot data to treat them as equal parts in for instance RT 1 required 17 months of data collection and a continuous spectrum of embodied data sources. Learning 13 robots[13].Ourworkproposesalearningframework that seamlessly from both data sources will require full-stack takes advantage of scalable human embodiment demonstra- innovation, from data collection systems that unify data tions, which has the potential to be larger and more diverse fromboths our cestoimitationlearningarchitectures that can than any dataset consisting of robot demonstrations alone. enable such cross-embodied policy learning. To this end, our work treats human data as a first-class Learning from Video Demonstrations: To satisfy the data data source for robot manipulation. We believe our system requirements of pixel to action IL algorithms, many recent is a key step towards using passive data from wearable works leverage human data because it is highly scalable. smart glasses to train manipulation policies. We present Human data is used at different levels of abstraction, where Ego Mimic (Fig. 1), a framework to collect data and co-train some works use human videos from internet-scale datasets manipulation policies from both human egocentric videos to pretrain visual representations [15], [16], [17]. Other and teleoperated robot data consisting of: worksusehumanvideostomoreexplicitlyunderst and scene (i) A system to collect human data built on Project dynamics through point track prediction, intermediate state Aria glasses [7] that capture egocentric video, 3 D hand hallucination in pixel space, or affordance prediction [18], tracking, and device SLAM. This rich information allows us [19], [6], [20], [21]. And finally, recent works use hand tra- totransformhumanegocentric data intoa for matcompatible jectorypredictionasaproxy for predictingrobotactions[5]. with robot imitation learning. While these approaches leverage hand data, they often have (ii) A capable yet low-cost bimanual robot that minimizes separate modules to process hand and robot data. Instead, the kinematic and camera-to-camera gap to human embodi- by fully leveraging the rich information provided by Aria ment data.",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 226,
      "paper_id": "egomimic",
      "text": "with robot imitation learning. While these approaches leverage hand data, they often have (ii) A capable yet low-cost bimanual robot that minimizes separate modules to process hand and robot data. Instead, the kinematic and camera-to-camera gap to human embodi- by fully leveraging the rich information provided by Aria ment data. In particular, we minimize the camera-to-camera glasses including on-board SLAM, our method is able to device gap (FOV, dynamic ranges, etc) between human and unify and treat human and robot data as equals and co-train robot data by using Project Aria glasses as the main robot from both data sources with a single end-to-end policy. sensor. (iii) To mitigate differences in data distributions, we nor- Data Collection Systems: Various methods have been malize and align action distributions between human and used to scale robot data. Low-cost devices such as the robots. Further, we minimize the appearance gap between Space Mouse offer sensitive and fine-grained teleoperation human arm and robot manipulator via visual masking. of robotic manipulators [22], [10], [23], [11], [24]. Further (iv)Aunifiedimitationlearningarchitecturethatco-trains works improve intuitive control through virtual reality sys- on hand and robot data with a common vision encoder and tems such as the VR headset [25], [26], [27], [28], [29]. Re- policy network. Despite distinct action spaces for human centsystemslike ALOHA and GELLOincreaseergonomics and robot, our model enforces a shared representation to for low-cost and fine-grained bimanual manipulation tasks enable per for mance scaling with human embodiment data, through a leader-follower teleoperation interface [1], [3] outper for ming existing methods that treat human and robot or exoskeletons [30], [31]. Other works attempt to collect data separately. humanembodiment data with richin for mationlike 3 Daction We empirically evaluate Ego Mimic on three challenging tracking, but existing systems face tradeoffs. Those which long-horizon manipulation tasks in the real world: contin- leverage rich information are either not portable (e.g., static uous object-in-bowl, clothes folding, and grocery packing camera [32], [5], [33], [34]) or ergonomic (e.g., require a (Fig.5).Ourresultsdemonstrate that Ego Mimicsignifi can tly hand-heldgripper[4],[35]orbody-worncamera[36],[37]), enhances task per for manceacrossallscenarios,withrelative which prevent the passive scalability of the data collection improvements of up to 200%. Notably, we observe that system. Along these lines, our approach captures egocentric Ego Mimic exhibits generalization to objects and scenes video and 3 Dhandtracking data,butvia the ergonomicform encountered exclusively in human data. Finally, we analyze factorof Project Aria Glasses[7].Whileo the rwearable data thescalingpropertiesof Ego Mimic,andfoundlearning from collection methods like VR headsets capture hand positions an additional hour of hand data signifi can tly outperforms toteleoperatearobot,oursystemdoesnotrequirearobotat training from an additional hour of robot data. all. This system has the potential to passively scale [38], as adoption of similar consumer-grade devices continue to rise. II. RELATEDWORKS Imitation Learning: Imitation Learning (IL) has been used Cross-embodiment Policy Learning: Advances in cross- to perform diverse and contact-rich manipulation tasks [8], embodiment learning show that large models trained on [9], [10]. Recent advancements in IL have led to the de- datasets with diverse robot embodiments are more general- velopment of pixel-to-action IL models, which directly map izable [39]. Some approaches aim to bridge the",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 227,
      "paper_id": "egomimic",
      "text": "perform diverse and contact-rich manipulation tasks [8], embodiment learning show that large models trained on [9], [10]. Recent advancements in IL have led to the de- datasets with diverse robot embodiments are more general- velopment of pixel-to-action IL models, which directly map izable [39]. Some approaches aim to bridge the embodiment raw visual inputs to low-level robot control [1], [11]. These gapthroughobservationreprojection[40],actionabstractions visual IL models have demonstrated impressive reactive [41], and policies conditioned on embodiment, [42]. Recent policies [12], [5]. Scaling these models has displayed strong works view cross-embodiment learning as a domain adapta- generalization in works such as RT 1 and RT 2 [13], [14]. tion problem [43]. Our work argues that human data should However,thesemethodsremainlabor and resource-intensive, be treated as another embodiment in transfer learning. Fig. 2: Our human data system uses Aria glasses to capture Egocentric RGB and uses its side SLAM cameras to localize the device and track hands. The robot consists of two Viper X follower arms with Intel Real Sense D 405 wrist cameras, controlled by two Widow X leader arms. Our robot uses identical Aria glasses as the main vision sensor to help minimize the camera to camera gap. III. EGOMIMIC movements.Priorworksoftenrelyontable-mountedmanip- Ego Mimic is a full-stack framework that captures and ulators such as the Franka Emika Panda [46]. While these learns from both egocentric human embodiment data and systems are capable, they differ signifi can tly from human robot data. We detail each component of our pipeline below, arms in terms of kinematics. Moreover, their substantial starting with our hardw are setup for human and robot data weight andinertia necessitate slow, cautious movements due collection (Sec. III-A), followed by our methods for pro- to safety concerns, largely preventing them from perform- cessing and aligning the data from both sources (Sec. III- ing manipulation tasks at speeds comparable to humans. B), and finally our unified policy architecture (Sec. III-C). In response to these limitations, we have purpose-built a Our design choices throughout are motivated by making bimanual manipulator that is lightweight, agile, and cost- human embodiment data as suitable for robot learning as effective. Drawing inspiration from the ALOHA system [1], tele-operated robot data is. our robot setup comprises two 6-Do F Viper X 300 S arms with Intel Realsense D 405 wrist cameras, mounted in an A. Data Collection Systems and Hardw are Design inverted configuration on a height-adjustable rig as the torso Aria glasses for egocentric demonstration collection. An (Fig 2), kinematically mimicking the upper body of a hu- ideal system for human data needs to capture rich infor- man.The Viper Xarms are lean and relativelysimilarinsize mation about the scene, while remaining passively scalable. to human arms, contributing to their enhanced agility. The Such a system should be wearable, ergonomic, capture a entire rig can be assembled for less than $1,000 excluding wide FOV, track hand positions, device pose, and more. the Viper Xarms(the BOMwill bemadeavailable).Wealso Ego Mimic fills this gap by building on top of the Project built a leader robot rig to collect teleoperation data, similar Aria glasses [7]. Aria glasses",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 228,
      "paper_id": "egomimic",
      "text": "rig can be assembled for less than $1,000 excluding wide FOV, track hand positions, device pose, and more. the Viper Xarms(the BOMwill bemadeavailable).Wealso Ego Mimic fills this gap by building on top of the Project built a leader robot rig to collect teleoperation data, similar Aria glasses [7]. Aria glasses are head-worn devices for to ALOHA [1]. capturing multimodal egocentric data. The device assumes Further, as our method jointly learns visual policies from an ergonomic glasses form factor that weighs only 75 g, egocentric human and robot data, it is essential to align permitting long wearing time and passive data collection. the visual observation space. Thus in addition to alignment Our work leverages the front-facing wide-Fo V RGB camera through data post-processing (Sec. III-B), we directly match forvisualobservation and twomono-colorscenecameras for the camera hardw are by using a second pair of Aria glasses device pose and hand tracking (See Fig. 2 for sample data). as the main sensor for the robot, which we have mounted Inparticular,theside-facingscenecamerastrackh and poses directly to the top of the torso at a location similar to that even when they move out of the main RGB camera\u2019s view, of human eyes (Fig. 2). This enables us to mitigate the signifi can tly mitigating the challenges posed by humans\u2019 observation domain gap associated with the camera devices, natural tendency to move their head and gaze ahead of their including FOVs, exposure levels, and dynamic ranges. hands during sequential manipulation tasks. B. Data Processing and Domain Alignment Further, there are large scale data collection efforts un- derway with Project Aria [44], [45], and the devices are To train unified policies from both human and robot data, made available broadly to the academic community through Ego Mimicbridgesthreekeyhuman-robotgaps:(1)unifying an active research partnership program. In the future, our action coordinate frames, (2) aligning action distributions, system can enableuserstoseamlesslymerge data the ycollect and (3) mitigating visual appearance gaps. with these large datasets. Ultimately, we present a system Raw data streams. We stream raw sensor data from the that enables passive yet feature-rich human data collection hardw are setupasdescribedin Sec.III-A.Ariaglassesworn to help scale up robot manipulation. by the human and robot generate ego-centric RGB image Low-costbimanualmanipulator.Toeffectivelyutilizeego- streams. In addition, the robot generates two wrist camera centric human embodiment data, a robot manipulator should streams. For proprioception, we leverage the Aria Machine be capable of moving in ways that resemble human arm Perception Service (MPS) [47] to estimate 3 D poses of both Source Type Data Human DH Image Egocentricview Proprio 3 Dhandposes(Hp) Action Normalizedh and tracks(Hap) Robot DR Image Egocentric+wristviews Proprio EEFposes(Rp),Jointpositions(Rq) Action EEFactions(Rap),Jointactions(Raq) TABLE I: Comparison of human and robot data streams hands Hp \u2208 SE(3) \u00d7 SE(3). Robot proprioception data includes both its end effector poses Rp \u2208 SE(3)\u00d7SE(3) and joint positions Rq \u2208 R 2\u00d77 (including the gripper jaw joint position). We in addition collect joint-space actions Raq \u2208R 2\u00d77 for teleoperated robot data. Fig.3:a)Actionnormalization:Theposedistributions are differ- Unifying human-robot data coordinate frames. Robot ent between hand and robot data, specifically in the y (left-right) dimension. We apply Gaussian normalization individually to the",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 229,
      "paper_id": "egomimic",
      "text": "\u2208 R 2\u00d77 (including the gripper jaw joint position). We in addition collect joint-space actions Raq \u2208R 2\u00d77 for teleoperated robot data. Fig.3:a)Actionnormalization:Theposedistributions are differ- Unifying human-robot data coordinate frames. Robot ent between hand and robot data, specifically in the y (left-right) dimension. We apply Gaussian normalization individually to the action and proprioception data typically use fixed reference hand and robot pose data before feeding them to the model. b) frames(e.g.,cameraorrobot base frame).However,egocen- Visual masking:Tohelpbridge the appearancegapofhuman and tric hand data from moving cameras breaks this assumption. and the robot arm, we apply a black mask to the hand and robot To unify the reference frames for joint policy learning, we via SAM, then overlay a red line onto the image. transform both human hand and robot end effector trajecto- ries into camera-centered stable reference frames. Following the idea of predicting action chunks [11], [1], we aim to construct action chunks ap for both human hand and t:t+h robot end effector. To simplify the notation, we describe the single-arm case that generalizes to both arms. The raw trajectory is a sequence of 3 D poses [p Ft,p Ft+1,...p Ft+h], t t+1 t+h where F denotes the coordinate frame of the camera when i estimating p . F remains fixed for the robot but changes i i constantly for humanegocentric data.Ourgoalistoconstruct ap bytrans for mingeachpositionin the trajectoryinto the t:t+h observation camera frame F . This allows the policy to pre- Fig.4:Architectureof the jointhuman-robotpolicylearningframe- t work.The model processesnormalizedh and and robot data through dict actions without considering future camera movements. shared vision and ACT encoders, outputting pose predictions for For human data, we use the MPS visual-inertial SLAM to both human and robot data, and joint actions for robot data. The obtain the Ariaglassespose T F W i \u2208SE(3)intheworldframe frameworkusesmaskedimagestomitigatehuman-robotappearance and transform the action trajectory: gaps and incorporates wrist camera views for the robot. Hap =[(TW)\u22121 TWp Fi for i\u2208[t,t+1,...,t+h]] i Ft Fi i A sample trajectory is visualized in Fig. 2 (top-left). Robot empirically effective (Sec. IV-B), though we plan to explore data is trans for med similarly using the fixed camera frame alternatives such as action quantization [13] in the future. estimated by hand-eye calibration. By creating a unified Bridging visual appearance gaps. Despite aligning sen- reference frame, we enable the policy to learn from action sor hardw are for capturing robot and human data, there still supervisions regardless of whether they originate from hu- exists a large visual appearance gap between human hands man videos or teleoperated demonstrations. and robots. Previous works have acknowledged this gap Aligninghuman-robotposedistributions.Despitealign- and attempt to occlude or remove the manipulator in visual ing hand and robot data via hardw are design and data observation[50],[51].Wefollowsimilarideas and maskout processing, we still observe differences in the distributions both the hand and the robot via SAM [52] and overlay a red of hand and robot end effector poses in the demonstra- line to indicate end-effector directions (Fig. 3). The SAM tions collected. These discrepancies arise from biomechani- point prompts are generated by the robot end effector and cal differences, task",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 230,
      "paper_id": "egomimic",
      "text": "hand and the robot via SAM [52] and overlay a red of hand and robot end effector poses in the demonstra- line to indicate end-effector directions (Fig. 3). The SAM tions collected. These discrepancies arise from biomechani- point prompts are generated by the robot end effector and cal differences, task execution variations, and measurement human hand poses trans for med to image frames. precision disparities between human and robotic systems. C. Training Human-Robot Joint Policies Withoutmitigating this gap,thepolicytendstolearnseparate representations for the two data sources[48],[49],preventing Existing approaches often opt for hierarchical architec- per for mance scaling with human data. To address this, we tures, where a high-level policy trained on human data apply Gaussian normalization individually to end effector conditionsalow-levelpolicyoutputtingrobotactions[5],[6]. (hand)poses and actions from each data source,asshownin However, this approach is inherently limited by the perfor- Fig. 3. Echoing [49], we found this simple technique to be manceof the low-levelpolicy,whichdoesnotdirectlybenefit Fig. 5: We evaluate Ego Mimic across three real world, long-horizon manipulation tasks. See Sec. IV-A for description. Algorithm 1 Joint Human-Robot Policy Learning TABLE II: Data collection overview for both Human(H) and Robot(R) data. We report both the number(#) of total task demon- Require: Human dataset D H , Robot dataset D R strations and the time(min) took to collect them. 1: Initialize shared trans for mer encoder f enc (\u00b7), pose de- coder fp(f (\u00b7)), and joint decoder fq(f (\u00b7)) enc enc Task H H H R R R 2: for iteration n=1,2,... do # min #/min # min #/min 3: // Human data Object-in-Bowl 1400 60 23 270 120 2 4: Sample (I t ,p t ,ap t:t+h ) from D H Groceries 160 80 2 300 300 1 5: Predict a\u02c6p t:t+h from f p (f enc (I t ,p t )) Laundry 590 100 6 430 300 1 6: LH =MSE(a\u02c6p ,ap ) p t:t+h t:t+h 7: // Robot data transform the visual and proprioceptive embeddings before 8: Sample (I t ,p t ,q t ,ap t:t+h ,aq t:t+h ) from D R passing to the policy trans for mer. The policy trans for mer 9: Predict a\u02c6q t:t+h from f q (f enc (I t ,p t ,q t )) processes the sefeatures,andthetwooutpu the adstransform 10: Predict a\u02c6p t:t+h from f p (f enc (I t ,p t ,q t )) the trans for mer\u2019s latent output into either pose or joint 11: LR =MSE(a\u02c6q ,aq ) space predictions. The pose loss supervises both human and q t:t+h t:t+h 12: LR =MSE(a\u02c6p ,ap ) robot data via Hap and Rap, whereas the joint action loss p t:t+h t:t+h 13: // Joint policy update only supervises robot data Raq. Since the two branches are 14: Update f enc ,fp,fq with LH p +LR p +LR q separated by only one linear layer, we effectively force the 15: end for model to learn joint representations for both domains. The algorithm is summarized in Alg. 1. Table I summarizes the data used for training. from large-scale human data. To address this limitation, we IV. EXPERIMENTS proposeasimplearchitecture(illustratedin Fig.4)thatlearns from unified data",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 231,
      "paper_id": "egomimic",
      "text": "only one linear layer, we effectively force the 15: end for model to learn joint representations for both domains. The algorithm is summarized in Alg. 1. Table I summarizes the data used for training. from large-scale human data. To address this limitation, we IV. EXPERIMENTS proposeasimplearchitecture(illustratedin Fig.4)thatlearns from unified data and promotes shared representation. Our Weaimtovalidatethreekeyhypo the ses.H 1:Ego Mimicis model builds upon ACT [1], but the design is general and abletoleveragehumanembodiment data toboostin-domain can be applied to other trans for mer based imitation learning per for mance for complex manipulation tasks. H 2: Human algorithms. data helps Ego Mimic generalize to new objects and scenes. A critical challenge in this unified approach is the choice H 3: Given sufficient initial robot data, it is more valuable to of the robot action space. While the robot end-effector collect additional human data than additional robot data. poses are more semantically similar to human hand pose A. Experiment Setup than robot-joint positions, it is difficult to control our robot withend-effectorposesviaacartesian-basedcontroller(e.g., Tasks. We select a set of long-horizon real world tasks to differential IK) because the 6 Do F Viper X arms offer low evaluate our claims. Our tasks require precise alignment, solutionredundancy.Empirically,wefound that robotsoften complex motions, and bimanual coordination (Fig. 5). encounter singularities or non-smooth solutions in a trajec- Continuous Object-in-Bowl:Therobotpicksasmallplush tory. Consequently, we opt for joint-space control (i.e., use toy (about 6 cm long), places it in a bowl, picks up the bowl predicted joint action a\u02c6q to control the robot), while to dump the object onto the table, and repeats continuously t:t+h leveraging pose-space prediction to learn joint human-robot for 40 seconds. We randomly choose from a set of 3 representation. Note that the need both pose- and joint- bowls and 5 toys which randomly positioned on the table spacepredictionsisspecificto our robothardw are,andmore within a 45 cm x 60 cm range. The task stress-tests precise capable robots that better support end-effector space control manipulation, spatial generalization, and robustness in long- can eliminate the need for predicting joint-space actions. horizonexecution.Weaward Ptseachtime the toyisplaced Specifically,allparametersin the policyaresh are dbesides in a bowl, or the bowl is emptied. We perform 45 total the two shallow input and output heads. The input heads evaluation rollouts across 9 bowl-toy-position combinations. TABLE III: Quantitative results for 3 real-world tasks. We report TABLEIV:Ablations-Weablate our method and reportfinal task task success rates (%) and per for mance scores (pts) for all tasks per for mance on the Object-in-Bowl task. and bag grabbing rate for the Groceries tasks. Method Cotrained(Points) Method Bowl Laundry Groceries Ego Mimic 128 Pts Pts SR Pts SR Open Bag Ego Mimicw/o Line 112 Ego Mimicw/o Line and Mask 95 ACT[1] 39 82 55% 82 22% 54% Ego Mimicw/o Action Norm 79 Mimicplay[5] 71 78 50% 53 8% 40% Ego Mimicw/o Hand Data 68 Ego Mimic(w/ohuman) 68 104 73% 92 28% 60% Ego Mimic 128 114 88% 110 30% 70% Laundry: A bimanual task that requires the robot to fold a t-shirt placed with random pose in a 90 cm \u00d7 60 cm range and a rotation",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 232,
      "paper_id": "egomimic",
      "text": "8% 40% Ego Mimicw/o Hand Data 68 Ego Mimic(w/ohuman) 68 104 73% 92 28% 60% Ego Mimic 128 114 88% 110 30% 70% Laundry: A bimanual task that requires the robot to fold a t-shirt placed with random pose in a 90 cm \u00d7 60 cm range and a rotation range of \u00b130 deg. The robot must use both armstofold the rightsidesleeve,theleftsidesleeve,then the whole shirt in half. We award Pts for each of these stages, and calculate Success Rate (SR) based as the percentage of runs where all stages were successful. We perform 40 total evaluation rollouts across 8 shirt-position combinations. Groceries: The robot fills a grocery bag with 3 packs of chips. It uses its left arm to grab the top side of the bag handle to create an opening, then uses the right arm to pick Fig.6:Wehighlight Ego Mimic\u2019ssuccess,aswellasfailuremodes, thechippacks and placestheminto the bag.The task requires forinstance(e)failuretocorrectlyalign with the toy,(f)failureto high-precision manipulation (picking up a deformable bag grasp the bag\u2019shandle,or(g)policyonlygrabs 1 sideof the shirt. handle) and robustness in long-horizon rollout. We award Ego Mimicreducesthefrequencyof the sefailuremodes,improving success rates by 8-33% over the baselines. Pts for picking the handle and for each pack placed in the grocery bag. We report SR as the percentage of runs where all three packs were successfully placed in the bag, and 8-33% over ACT. Our largest improvement is on the Cont. Open Bag as the percentage of runs where the handle of Object-in-Bowl task,inwhichweyielda 228%improvement the bag was grasped, which is a difficult stage of this task. intaskscoreover ACT.Weobserve the baselinesoftenmiss We perform 50 evaluations across 10 bag positions. the toy or bowl by a few inches, which seems to indicate We detail the amount of data collected for each task in that our use of hand data helps the policy precisely reach Table II. While collecting robot data in particular, we make the toy. We show qualitative results in Fig. 6. sure to randomly perturb the robot\u2019s position, which we To ensure this increase was due to leveraging hand data foundempiricallytoimproverobustness.Forhum and ata,we rather than architectural changes, we comp are to Ego Mimic note that while tasks like Continuous Object-in-Bowl were (0% human). We observe a 10-88% improvement in score particularly easy to scale, tasks like Groceries were slower and 2-15% improvement in success rate. because of resetting time. Ego Mimic enables generalization to new objects and Baselines. To evaluate that Ego Mimic can improve in- even scenes. We evaluate our method on two domain shifts: domain success rate by leveraging human data, we bench- attempting to fold shirts of an unseen color, and per for ming mark against ACT [1], a state of the art imitation learning the Cont. Object-in-Bowl task in an entirely different scene. algorithm. Further, we comp are against Mimicplay [5], a re- Asshownin Fig.7,weobserve that ACTstrugglesonshirts cent state of the art method that learns planners from human of unseen colors (25% SR) whereas Ego Mimic fully retains data to guide low-level policies, to show that our unified its per for mance (85% SR). Surprisingly, by learning from architecture learns",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 233,
      "paper_id": "egomimic",
      "text": "[5], a re- Asshownin Fig.7,weobserve that ACTstrugglesonshirts cent state of the art method that learns planners from human of unseen colors (25% SR) whereas Ego Mimic fully retains data to guide low-level policies, to show that our unified its per for mance (85% SR). Surprisingly, by learning from architecture learns more effectively from human and robot hum and ataina new scene(unseenbackground and lighting), data. For fair comparisons, we implement Mimicplay with Ego Mimicisabletogeneralizeto this newenvironment with- the same Trans for mer backbone as our method, and we re- out any additional robot data, scoring 63 points. In contrast, moved goal conditioning because Ego Mimic is designed for Mimicplay, which had access to the same information but single-task policies. Since Ego Mimic contains architectural instead leverages a hierarchical framework for using hand changes to ACT, namely the simultaneous joint and pose dataonlyscored 4 points.Thissuggests that ourarchitecture actionprediction,wealsobenchmarkagainst Ego Mimic(0% promotes joint hand-robot representation, whereas hierarchi- Human). This helps us conclude that improvements come cal architectures pose a generalization bottleneck. from leveraging human data rather than the architecture. Scaling human vs. robot data. To investigate the scaling effect of human and robot data sources on per for mance, we B. Results conducted additional data collection for the Cont. Object-in- Ego Mimic improves in-domain task per for mance.Across bowl task. As illustrated in Fig. 7, Ego Mimic trained on 2 all tasks we observed a relative improvement in score of 34- hours of robot data and 1 hour of human data signifi can tly 228%,andanimprovementinabsolute task successrate from outperforms ACTtrainedon 3 hoursofrobot data(128 vs 74 V. CONCLUSIONS We presented Ego Mimic, a framework to co-train ma- nipulation policies from human egocentric videos and tele- operated robot data. By leveraging Project Aria glasses, a low-costbimanualrobotsetup,cross-domainalignmenttech- niques,andaunifiedpolicylearningarchitecture,Ego Mimic improves over state-of-the-art baselines on three challenging real-world tasks and shows generalization to new scenes as well as favorable scaling properties. For future work, we plan to explore the possibility of generalizing to new robotembodiments and entirely new behaviorsdemonstrated only in human data, such as folding pants instead of shirts. Overall, we believe our work opens up exciting new venues Fig. 7: Evaluation Results on Policy Generalization. (a) We of research on scaling robot data via passive data collection. evaluate the policy on the laundry task using unseen cloth colors andreport the successrate for eachmethod.(b)Wetest the policy on the Object-in-Bowl task in unseen scenes. REFERENCES [1] T.Z.Zhao,V.Kumar,S.Levine,and C.Finn,\u201cLearningfine-grained bimanual manipulation with low-cost hardw are,\u201d 2023. [Online]. Available:https://arxiv.org/abs/2304.13705 [2] A. . Team, J. Aldaco, T. Armstrong, R. Baruch, J. Bingham, S. Chan, K. Draper, D. Dwibedi, C. Finn, P. Florence, S. Goodrich, W. Gramlich, T. Hage, A. Herzog, J. Hoech, T. Nguyen, I. Storz, B. Tabanp our, L. Takayama, J. Tompson, A. Wahid, T. Wahrburg, S. Xu, S. Yaroshenko, K. Zakka, and T. Z. Zhao, \u201cAloha 2: An enhanced low-cost hardw are for bimanual teleoperation,\u201d 2024. [Online].Available:https://arxiv.org/abs/2405.02292 [3] P.Wu,Y.Shentu,Z.Yi,X.Lin,and P.Abbeel,\u201cGello:Ageneral,low- cost, and intuitive teleoperation framework for robot manipulators,\u201d 2024.[Online].Available:https://arxiv.org/abs/2309.13037 [4] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, \u201cUniversal manipulation interface:",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 234,
      "paper_id": "egomimic",
      "text": "Zakka, and T. Z. Zhao, \u201cAloha 2: An enhanced low-cost hardw are for bimanual teleoperation,\u201d 2024. [Online].Available:https://arxiv.org/abs/2405.02292 [3] P.Wu,Y.Shentu,Z.Yi,X.Lin,and P.Abbeel,\u201cGello:Ageneral,low- cost, and intuitive teleoperation framework for robot manipulators,\u201d 2024.[Online].Available:https://arxiv.org/abs/2309.13037 [4] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, \u201cUniversal manipulation interface: In- the-wild robot teaching without in-the-wild robots,\u201d 2024. [Online]. Available:https://arxiv.org/abs/2402.10329 [5] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar, \u201cMimicplay: Long-horizon imitation learning by watching human play,\u201d 2023. [Online]. Available: Fig. 8: Scaling robot vs. human data. Ego Mimic trained on 2 https://arxiv.org/abs/2302.12422 hours robot data + 1 hour hand data (Blue) strongly outperforms [6] H. Bharadhwaj, A. Gupta, V. Kumar, and S. Tulsiani, \u201cTowards ACT [1] trained on 3 hours of robot data (Orange). generalizablezero-shotmanipulationviatranslatinghumaninteraction plans,\u201d2023.[Online].Available:https://arxiv.org/abs/2312.00775 [7] J. Engel, K. Somasundaram, M. Goesele, A. Sun, A. Gamino, A. Turner, A. Talattof, A. Yuan, B. Souti, B. Meredith, C. Peng, points).Notably,oneh our ofhum and atayields 1400 demon- C.Sweeney,C.Wilson,D.Barnes,D.De Tone,D.Caruso,D.Valleroy, strations,comp are dtoonly 135 demonstrations from anhour D.Ginjupalli,D.Frost,E.Miller,E.Mueggler,E.Oleinik,F.Zhang, G. Somasundaram, G. Solaira, H. Lanaras, H. Howard-Jenkins, of robot data. These results demonstrate Ego Mimic\u2019s ability H.Tang,H.J.Kim,J.Rivera,J.Luo,J.Dong,J.Straub,K.Bailey, to effectively leverage the efficiency of human embodiment K. Eckenhoff, L. Ma, L. Pesqueira, M. Schwesinger, M. Monge, data collection, leading to a more pronounced scaling effect N.Yang,N.Charron,N.Raina,O.Parkhi,P.Borschowa,P.Moulon, P. Gupta, R. Mur-Artal, R. Pennington, S. Kulkarni, S. Miglani, that substantially boosts task per for mance beyond what is S. Gondi, S. Solanki, S. Diener, S. Cheng, S. Green, S. Saarinen, achievable with robot data alone. We note that Ego Mimic at S. Patra, T. Mourikis, T. Whelan, T. Singh, V. Balntas, V. Baiyya, 2 hours of robot data outperforms ACT at 2 hours of robot W. Dreewes, X. Pan, Y. Lou, Y. Zhao, Y. Mans our, Y. Zou, Z. Lv, Z.Wang,M.Yan,C.Ren,R.D.Nardi,and R.Newcombe,\u201cProject data, so some improvement is attributed to architecture. aria: A new tool for egocentric multi-modal ai research,\u201d 2023. [Online].Available:https://arxiv.org/abs/2308.13561 Ablationstudies.Weablate our approachtodemonstrate the [8] A.Paraschos,C.Daniel,J.R.Peters,and G.Neumann,\u201cProbabilistic importance of each design decision on the Object-in-Bowl movementprimitives,\u201din Advancesin Neural Information Processing task (Table IV). First, removing action normalization results Systems, C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Weinberger,Eds.,vol.26. Curran Associates,Inc.,2013. in a 38% drop in task score. This highlights the importance [9] C.Finn,T.Yu,T.Zhang,P.Abbeel,and S.Levine,\u201cOne-shotvisual of action distribution alignment for co-training. Next, we imitation learning via meta-learning,\u201d 2017. [Online]. Available: ablate away the visual techniques, specifically masking out https://arxiv.org/abs/1709.04905 [10] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, the hand and robot, as well as drawing the red overlay on L.Fei-Fei,S.Savarese,Y.Zhu,and R.Mart\u00b4\u0131n-Mart\u00b4\u0131n,\u201cWhatmatters the image. Removing these components resulted in 13 and in learning from offline human demonstrations for robot manipula- 26% drops respectively. Finally, Ego Mimic trained without tion,\u201dinar Xivpreprintar Xiv:2108.03298,2021. [11] C.Chi,Z.Xu,S.Feng,E.Cousineau,Y.Du,B.Burchfiel,R.Tedrake, any hand data, yields a large 47% drop, which highlights and S.Song,\u201cDiffusionpolicy:Visuomotorpolicylearningviaaction how effective hand-robot co-training is on our stack. diffusion,\u201d2024.[Online].Available:https://arxiv.org/abs/2303.04137 [12] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and [29] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani, L. Pinto,",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 235,
      "paper_id": "egomimic",
      "text": "any hand data, yields a large 47% drop, which highlights and S.Song,\u201cDiffusionpolicy:Visuomotorpolicylearningviaaction how effective hand-robot co-training is on our stack. diffusion,\u201d2024.[Online].Available:https://arxiv.org/abs/2303.04137 [12] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and [29] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani, L. Pinto, \u201cVisual imitation made easy,\u201d 2020. [Online]. Available: C. Liu, and G. Shi, \u201cOmnih 2 o: Universal and dexterous human- https://arxiv.org/abs/2008.04899 to-humanoid whole-body teleoperation and learning,\u201d ar Xiv preprint [13] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, ar Xiv:2406.08858,2024. K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, [30] H.Fang,H.-S.Fang,Y.Wang,J.Ren,J.Chen,R.Zhang,W.Wang, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, and C. Lu, \u201cAirexo: Low-cost exoskeletons for learning whole-arm D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, manipulationin the wild,\u201dar Xivpreprintar Xiv:2309.14975,2023. U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, [31] S. Yang, M. Liu, Y. Qin, R. Ding, J. Li, X. Cheng, R. Yang, S. Yi, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, and X.Wang,\u201cAce:Across-plat for mvisual-exoskeletonssystem for G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, low-cost dexterous teleoperation,\u201d ar Xiv preprint ar Xiv:2408.11805, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, 2024. T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, \u201cRt-1: Robotics [32] A.Sivakumar,K.Shaw,and D.Pathak,\u201cRobotictelekinesis:Learning transformer for real-worldcontrolat scale,\u201d2023.[Online].Available: a robotic hand imitator by watching humans on youtube,\u201d ar Xiv https://arxiv.org/abs/2212.06817 preprintar Xiv:2202.10448,2022. [14] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, [33] V.Jain,M.Attarian,N.J.Joshi,A.Wahid,D.Driess,Q.Vuong,P.R. K.Choromanski,T.Ding,D.Driess,A.Dubey,C.Finn,P.Florence, Sanketi,P.Sermanet,S.Welker,C.Chan,etal.,\u201cVid 2 robot:End-to- C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, endvideo-conditionedpolicylearning with cross-attentiontransform- A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, ers,\u201dar Xivpreprintar Xiv:2403.12943,2024. D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, [34] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, \u201cHumanplus: Y.Lu,H.Michalewski,I.Mordatch,K.Pertsch,K.Rao,K.Reymann, Humanoidshadowing and imitation from humans,\u201din Conferenceon M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, Robot Learning(Co RL),2024. R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, [35] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chin- P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, tala, and L. Pinto, \u201cOn bringing robots home,\u201d ar Xiv preprint and B. Zitkovich, \u201cRt-2: Vision-language-action models transfer ar Xiv:2311.16098,2023. web knowledge to robotic control,\u201d 2023. [Online]. Available: [36] C. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K. https://arxiv.org/abs/2307.15818 Liu, \u201cDexcap: Scalable and portable mocap data collection [15] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, \u201cR 3 m: system for dexterous manipulation,\u201d 2024. [Online]. Available: A universal visual representation for robot manipulation,\u201d 2022. https://arxiv.org/abs/2403.07788 [Online].Available:https://arxiv.org/abs/2203.12601 [37] G. Papagiannis, N. Di Palo, P. Vitiello, and E. Johns, \u201cR+ x: Re- [16] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Dar- trieval and execution from everyday human videos,\u201d ar Xiv preprint rell,",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 236,
      "paper_id": "egomimic",
      "text": "Available: A universal visual representation for robot manipulation,\u201d 2022. https://arxiv.org/abs/2403.07788 [Online].Available:https://arxiv.org/abs/2203.12601 [37] G. Papagiannis, N. Di Palo, P. Vitiello, and E. Johns, \u201cR+ x: Re- [16] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Dar- trieval and execution from everyday human videos,\u201d ar Xiv preprint rell, \u201cReal-world robot learning with masked visual pre-training,\u201d in ar Xiv:2407.12957,2024. Conferenceon Robot Learning. PMLR,2023,pp.416\u2013426. [38] K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, [17] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and T.Afouras,K.Ashutosh,V.Baiyya,S.Bansal,B.Boote,etal.,\u201cEgo- A. Zhang, \u201cVip: Towards universal visual reward and representa- exo 4 d: Underst and ing skilled human activity from first-and third- tionviavalue-implicitpre-training,\u201dar Xivpreprintar Xiv:2210.00030, personperspectives,\u201din Proceedingsof the IEEE/CVFConferenceon 2022. Computer Vision and Pattern Recognition,2024,pp.19383\u201319400. [18] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha, and [39] E. Collaboration, A. O\u2019Neill, A. Rehman, A. Gupta, A. Maddukuri, A. Garg, \u201cLearning by watching: Physical imitation of manipulation A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, skills from human videos,\u201d 2021. [Online]. Available: https: A. Jain, A. Tung, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, //arxiv.org/abs/2101.07241 A. Rai, A. Gupta, A. Wang, A. Kolobov, A. Singh, A. Garg, [19] C. Wen, X. Lin, J. So, K. Chen, Q. Dou, Y. Gao, and P. Abbeel, A. Kembhavi, A. Xie, A. Brohan, A. Raffin, A. Sharma, A. Yavary, \u201cAny-point trajectory modeling for policy learning,\u201d 2024. [Online]. A. Jain, A. Balakrishna, A. Wahid, B. Burgess-Limerick, B. Kim, Available:https://arxiv.org/abs/2401.00025 B. Scho\u00a8lkopf, B. Wulfe, B. Ichter, C. Lu, C. Xu, C. Le, C. Finn, [20] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, \u201cTrack 2 act: C.Wang,C.Xu,C.Chi,C.Huang,C.Chan,C.Agia,C.Pan,C.Fu, Predicting point tracks from internet videos enables generalizable C.Devin,D.Xu,D.Morton,D.Driess,D.Chen,D.Pathak,D.Shah, robot manipulation,\u201d 2024. [Online]. Available: https://arxiv.org/abs/ D. Bu\u00a8chler, D. Jayaraman, D. Kalashnikov, D. Sadigh, E. Johns, 2405.01527 E. Foster, F. Liu, F. Ceola, F. Xia, F. Zhao, F. V. Frujeri, F. Stulp, [21] S.Bahl,R.Mendonca,L.Chen,U.Jain,and D.Pathak,\u201cAffordances G.Zhou,G.S.Sukhatme,G.Salhotra,G.Yan,G.Feng,G.Schiavi, from human videos as a versatile representation for robotics,\u201d 2023. G.Berseth,G.Kahn,G.Yang,G.Wang,H.Su,H.-S.Fang,H.Shi, [Online].Available:https://arxiv.org/abs/2304.08488 H. Bao, H. B. Amor, H. I. Christensen, H. Furuta, H. Bharadhwaj, H. Walke, H. Fang, H. Ha, I. Mordatch, I. Radosavovic, I. Leal, [22] A. Mandlekar, D. Xu, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, S. Savarese, and L. Fei- J. Liang, J. Abou-Chakra, J. Kim, J. Drake, J. Peters, J. Schneider, Fei, \u201cLearning to generalize across long-horizon tasks from human J. Hsu, J. Vakil, J. Bohg, J. Bingham, J. Wu, J. Gao, J. Hu, J. Wu, demonstrations,\u201dar Xivpreprintar Xiv:2003.06085,2020. J. Wu, J. Sun, J. Luo, J. Gu, J. Tan, J. Oh, J. Wu, J. Lu, J. Yang, [23] V.Dhat,N.Walker,and M.Cakmak,\u201cUsing 3 dmicetocontrolrobot J. Malik, J. Silve\u00b4rio, J. Hejna, J. Booher, J. Tompson, J. Yang, manipulators,\u201d Proceedings of the 2024 ACM/IEEE International J. Salvador, J. J. Lim, J. Han, K. Wang, K. Rao, K. Pertsch, Conference on Human-Robot Interaction, 2024. [Online]. Available: K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne, https://api.semanticscholar.org/Corpus ID:267322988 K.Oslund,K.Kawaharazuka,K.Black,K.Lin,K.Zhang,K.Ehsani, [24] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, \u201cViola: Imitation learning K. Lekkala, K. Ellis, K. Rana, K. Srinivasan, K. Fang, K. P. Singh, for vision-based manipulation",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 237,
      "paper_id": "egomimic",
      "text": "Pertsch, Conference on Human-Robot Interaction, 2024. [Online]. Available: K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne, https://api.semanticscholar.org/Corpus ID:267322988 K.Oslund,K.Kawaharazuka,K.Black,K.Lin,K.Zhang,K.Ehsani, [24] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, \u201cViola: Imitation learning K. Lekkala, K. Ellis, K. Rana, K. Srinivasan, K. Fang, K. P. Singh, for vision-based manipulation with object proposal priors,\u201d 2023. K.-H.Zeng,K.Hatch,K.Hsu,L.Itti,L.Y.Chen,L.Pinto,L.Fei-Fei, [Online].Available:https://arxiv.org/abs/2210.11339 L. Tan, L. J. Fan, L. Ott, L. Lee, L. Weihs, M. Chen, M. Lepert, [25] S. P. Arunachalam, I. Gu\u00a8zey, S. Chintala, and L. Pinto, \u201cHolo-dex: M. Memmel, M. Tomizuka, M. Itkina, M. G. Castro, M. Spero, Teaching dexterity with immersive mixed reality,\u201d in 2023 IEEE M. Du, M. Ahn, M. C. Yip, M. Zhang, M. Ding, M. Heo, M. K. International Conferenceon Robotics and Automation(ICRA). IEEE, Srirama,M.Sharma,M.J.Kim,N.Kanazawa,N.Hansen,N.Heess, 2023,pp.5962\u20135969. N.J.Joshi,N.Suenderhauf,N.Liu,N.D.Palo,N.M.M.Shafiullah, [26] A. George, A. Bartsch, and A. B. Farimani, \u201cOpenvr: Teleoperation O. Mees, O. Kroemer, O. Bastani, P. R. Sanketi, P. T. Miller, for manipulation,\u201d 2023. [Online]. Available: https://arxiv.org/abs/ P. Yin, P. Wohlhart, P. Xu, P. D. Fagan, P. Mitrano, P. Sermanet, 2305.09765 P. Abbeel, P. Sund are san, Q. Chen, Q. Vuong, R. Rafailov, R. Tian, [27] I.A.Tsokalo,D.Kuss,I.Kharabet,F.H.P.Fitzek,and M.Reisslein, R. Doshi, R. Mart\u2019in-Mart\u2019in, R. Baijal, R. Scalise, R. Hendrix, \u201cRemoterobotcontrol with human-in-the-loopoverlongdistancesus- R. Lin, R. Qian, R. Zhang, R. Mendonca, R. Shah, R. Hoque, ingdigitaltwins,\u201din 2019 IEEEGlobal Communications Conference R. Julian, S. Bustamante, S. Kirmani, S. Levine, S. Lin, S. Moore, (GLOBECOM),2019,pp.1\u20136. S.Bahl,S.Dass,S.Sonawani,S.Tulsiani,S.Song,S.Xu,S.Haldar, [28] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, \u201cOpen-television: S. Karamcheti, S. Adebola, S. Guist, S. Nasiriany, S. Schaal, teleoperation with immersive active visual feedback,\u201d ar Xiv preprint S.Welker,S.Tian,S.Ramamoorthy,S.Dasari,S.Belkhale,S.Park, ar Xiv:2407.01512,2024. S.Nair,S.Mirch and ani,T.Osa,T.Gupta,T.Harada,T.Matsushima, T. Xiao, T. Kollar, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, T.Armstrong,T.Darrell,T.Chung,V.Jain,V.Kumar,V.Vanhoucke, W. Zhan, W. Zhou, W. Burgard, X. Chen, X. Chen, X. Wang, X. Zhu, X. Geng, X. Liu, X. Liangwei, X. Li, Y. Pang, Y. Lu, Y. J. Ma, Y. Kim, Y. Chebotar, Y. Zhou, Y. Zhu, Y. Wu, Y. Xu, Y. Wang, Y. Bisk, Y. Dou, Y. Cho, Y. Lee, Y. Cui, Y. Cao, Y.-H. Wu, Y. Tang, Y. Zhu, Y. Zhang, Y. Jiang, Y. Li, Y. Li, Y. Iwasawa, Y. Matsuo, Z. Ma, Z. Xu, Z. J. Cui, Z. Zhang, Z. Fu, and Z. Lin, \u201cOpen x-embodiment: Robotic learning datasets and rt-x models,\u201d 2024.[Online].Available:https://arxiv.org/abs/2310.08864 [40] L. Y. Chen, K. Hari, K. Dharmarajan, C. Xu, Q. Vuong, and K. Goldberg, \u201cMirage: Cross-embodiment zero-shot policy transfer with cross-painting,\u201d 2024. [Online]. Available: https: //arxiv.org/abs/2402.19249 [41] W. Huang, I. Mordatch, and D. Pathak, \u201cOne policy to control them all: Shared modular policies for agent-agnostic control,\u201d 2020. [Online].Available:https://arxiv.org/abs/2007.04976 [42] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, D. Sadigh, and S. Levine, \u201cPushing the limits of cross-embodiment learning for manipulation and navigation,\u201d2024.[Online].Available: https://arxiv.org/abs/2402.19432 [43] J. Yang, D. Sadigh, and C. Finn, \u201cPolybot: Training one policy acrossrobotswhileembracingvariability,\u201d2023.[Online].Available: https://arxiv.org/abs/2307.03719 [44] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M.Xu,E.Z.Xu,C.Zhao,S.Bansal,D.Batra,V.Cartillier,S.Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q.Fu,A.Gebreselasie,C.Gonzalez,J.Hillis,X.Huang,Y.Huang, W. Jia, W.",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 238,
      "paper_id": "egomimic",
      "text": "Sadigh, and C. Finn, \u201cPolybot: Training one policy acrossrobotswhileembracingvariability,\u201d2023.[Online].Available: https://arxiv.org/abs/2307.03719 [44] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M.Xu,E.Z.Xu,C.Zhao,S.Bansal,D.Batra,V.Cartillier,S.Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q.Fu,A.Gebreselasie,C.Gonzalez,J.Hillis,X.Huang,Y.Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Sou the rland, Y. Sugano, R. Tao, M. Vo, Y.Wang,X.Wu,T.Yagi,Z.Zhao,Y.Zhu,P.Arbelaez,D.Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik, \u201cEgo 4 d: Around the world in 3,000 hours of egocentric video,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2110.07058 [45] L.Ma,Y.Ye,F.Hong,V.Guzov,Y.Jiang,R.Postyeni,L.Pesqueira, A. Gamino, V. Baiyya, H. J. Kim, K. Bailey, D. S. Fosas, C. K. Liu, Z. Liu, J. Engel, R. D. Nardi, and R. Newcombe, \u201cNymeria: A massive collection of multimodal egocentric daily motion in the wild,\u201d2024.[Online].Available:https://arxiv.org/abs/2406.09905 [46] S.Haddadin,S.Parusel,L.Johannsmeier,S.Golz,S.Gabl,F.Walch, M. Sabaghian, C. Ja\u00a8hne, L. Hausperger, and S. Haddadin, \u201cThe franka emika robot: A reference platform for robotics research and education,\u201dIEEERobotics and Automation Magazine,vol.29,no.2, pp.46\u201364,2022. [47] Meta Research,\u201cBasics\u2014projectariadocs,\u201dhttps://facebookresearch. github.io/projectariatools/docs/data for mats/mps/mpssummary, 2024,accessed:September 15,2024. [48] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, D. Sadigh, and S. Levine, \u201cPushing the limits of cross- embodimentlearning for manipulation and navigation,\u201dar Xivpreprint ar Xiv:2402.19432,2024. [49] J. Hejna, C. Bhateja, Y. Jian, K. Pertsch, and D. Sadigh, \u201cRe-mix: Optimizing data mixtures for large scale imitation learning,\u201d ar Xiv preprintar Xiv:2408.14037,2024. [50] Y. Zhou, Y. Aytar, and K. Bousmalis, \u201cManipulator-independent representations for visual imitation,\u201d 2021. [Online]. Available: https://arxiv.org/abs/2103.09016 [51] S. Bahl, A. Gupta, and D. Pathak, \u201cHuman-to-robot imitation in the wild,\u201d2022.[Online].Available:https://arxiv.org/abs/2207.09450 [52] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R.Ra\u00a8dle,C.Roll and,L.Gustafson,E.Mintun,J.Pan,K.V.Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dolla\u00b4r, and C. Feichtenhofer, \u201cSam 2: Segment anything in images and videos,\u201d 2024. [Online]. Available:https://arxiv.org/abs/2408.00714 [53] L.Wang,X.Chen,J.Zhao,and K.He,\u201cScalingproprioceptive-visual learning with heterogeneous pre-trained trans for mers,\u201d in Neurips, 2024. Fig. 9: Here we visualize hand and robot data from out dataset side by side with ground truth actions overlayed (purple). Note that the actions are of similar length, despite the hand traveling much faster than the robot. VI. APPENDIX which are used to prompt SAM 2 [52] to generate a mask of the robot arm. After obtaining the mask, we draw a red line A. Data Processing and Domain Alignment onthemasked are afromthegripperto the elbowin the RGB Humans and teleoperated robots complete tasks at differ- image. For the human data, a similar process is followed, entspeeds.Toenablejointtrainingofhuman and robot data, where SAM 2 is prompted using the 3 D coordinates of the we must align these two sources of data temporally. Follow- human hand to generate a mask. A red line is then drawn ing Mimicplay [5], we \u201cslow down\u201d",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 239,
      "paper_id": "egomimic",
      "text": "data, a similar process is followed, entspeeds.Toenablejointtrainingofhuman and robot data, where SAM 2 is prompted using the 3 D coordinates of the we must align these two sources of data temporally. Follow- human hand to generate a mask. A red line is then drawn ing Mimicplay [5], we \u201cslow down\u201d the human data, and along the hand\u2019s cont our, from the bottom right to top left we found empirically that a factor of 4 sufficiently aligned corner of the cont our\u2019s bounding box. both domains. Specifically, for robot data we construct joint During training, both the robot arm and human hand and pose based actions over a four second horizon but for are masked to align their visual representations. During hum and atawe usea 1 secondhorizon.Forbothdomains,our evaluation,SAM 2 isruninrealtimeonadesktoptomask the action chunk size is 100, meaning we construct 100 future robotarm and apply the sameredlineoverlay.Thisapproach actions spaced evenly over the horizon. This alignment is enablesbettervisualalignmentbetween the robot and human independentofdat are cordingfrequencies,wherehum and ata hand, facilitating more effective model generalization across is recorded at 30 hz and robot data is recorded at 50 hz. human and robotic tasks. To co-train on both human and robot data, we indi- vidually normalize the proprioception and actions for both B. Aria Machine Perception Services (MPS) embodiments (as shown in Fig. 3). Given proprioception We leveraged MPS to process human data from the Aria p t \u2208 Rd where d depends on embodiment, we normalize glasses.Theraw data from Ariacontainstimestampedsensor by subtracting the dataset mean and dividing by standard information from the glasses, namely RGB camera, SLAM deviation cameras,IMU,eyetrackingcameras,microphone,andmore. norm(p t )=(p t \u2212\u00b5 p )/\u03c3 p . The raw data is uploaded to the MPS server, where the cloud-hosted service estimates device pose via SLAM, a We perform the identical calculation to normalize actions semi-dense pointcloud of the environment, hand tracking a \u2208Rd\u00d7100. t:t+h relative to the device frame, and even eye gaze. The MPS To bridge the appearance gap between human hand returns SLAM as a timestamped CSV of device poses in and robot arm, we visually mask each embodiment via world frame and hand tracking as a timestamped CSV of SAM 2[52],andoverlay are dlineon the semaskstoenhance cartesian positions in the time-aligned device frame. These alignment (Fig. 3). For the robot, we first use forward hand positions are each in a distinct reference frame due to kinematics to compute the 3 D coordinates of key joints in head movements, so we project future actions to the current robot frame device coordinate frame (described in Sec. III-B). We use p R =FK(q )\u2208R 3\u00d73, t t the undistorted Aria RGB camera data paired with the hand including the wrist, gripper and the forearm. These 3 D tracking and SLAM information to construct an hdf 5 file coordinates are then projected onto the image frame via compatible for training in robomimic [10]. camera intrinsics (Ipixels) and extrinsics (Tcam) to obtain cam R C. Training Human-Robot Joint Policies 2 D keypoints in pixel space We depict our algorithm in detail in Fig. 10. At each ppixel =Ipixels Tcamp",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 240,
      "paper_id": "egomimic",
      "text": "coordinates are then projected onto the image frame via compatible for training in robomimic [10]. camera intrinsics (Ipixels) and extrinsics (Tcam) to obtain cam R C. Training Human-Robot Joint Policies 2 D keypoints in pixel space We depict our algorithm in detail in Fig. 10. At each ppixel =Ipixels Tcamp R \u2208R 3\u00d72, step we sample a batch of hand data as well as a batch of t cam R t Fig. 10: Detailed Architecture of Ego Mimic. TABLE V: Training details - Ego Mimic robot data, and pass each through our unified architecture. Ego Mimicperforms Z-scorenormalizationtoh and and robot proprioception and actionsindividually.Thenormalizedpro- Policy ACT Batch Size 128 prioception is passed through a linear layer to produce a Optimizer adamw proprioception token. Alongside the proprioception, the top learning rate(initial) 5 e-5 down views fromh and androbot arepassed througha SAM Decayfactor 1 Scheduler Linear based masking module. These images, along with the robot Encoderlayers 4 wrist views are passed through a shared Resnet 18 visual Decoderlayers 7 encoder which produces visual tokens. Finally, we add an Hiddendim 512 Feed for warddim 3200 additionalstyletokenz from our CVAEencoderwhichisnot No.ofheads 8 depicted, but directly follows ACT [1]. All these tokens, are Data Augmentations Color Jitter passed through a trans for mer encoder decoder architecture. Thetrans for merdecoder\u2019shiddenoutputispassedthrougha lineardecoderdependingon the outputtype,producingpose grasping action is supervised only via the robot joint predic- actions a\u02c6p or joint based actions a\u02c6j. tion loss L (Ra\u02c6j,Raj), where the gripper is represented as 1 For batches of robot data, we calculate another joint. L robot =L 1 (Ra\u02c6p,Rap)+L 1 (Ra\u02c6j,Raj)+KL D. Training Details and for hand data we have Welist the hyperparameters for Ego Mimicin Table V.All models were trained for 120000 iterations with global batch L =L (Ha\u02c6p,Hap)+KL hand 1 size of 128 across 4 A 40 gpus, which takes about 24 hours. where KL is the CVAE latent regularizer as in ACT [1]. Our code is implemented in the robomimic framework [10]. This yields L=L robot +L hand which we optimize at each More details in Table V step. E. Mimicplay Implementation We leverage the trans for mer\u2019s flexible input sequence to account for differences in the number of visual observations For our implementation of Mimic Play [5], we closely based on the modality; specifically we have wrist images follow the original setup, training the high-level planner and in robot data but not hand data. When the wrist images are low-level control policy separately. present,weconcatenateadditionaltokensto our trans for mers First, we train a Res Net-18 based high-level encoder input sequence as in ACT [1]. In our experiments, we found using a Gaussian Mixture Model (GMM) to generate 3 D that this strategywassufficienttoeffectivelyco-trainonboth trajectories,asdescribedin the originalwork.Thehigh-level hand and robot data, although we plan to experiment with encoder is trained on both human and robot data to predict more sophisticated cross-embodiment learning techniques 3 D trajectories. like HPTs [53]. Once the high-level encoder is trained, we extract the We note that the human data lacks information for the latent representation from the Res Net-18 encoder (i.e., the graspingaction,since Ariaonlyrecordsh and pose.Thus,the high-levelplanner)anduseitas",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 241,
      "paper_id": "egomimic",
      "text": "human and robot data to predict more sophisticated cross-embodiment learning techniques 3 D trajectories. like HPTs [53]. Once the high-level encoder is trained, we extract the We note that the human data lacks information for the latent representation from the Res Net-18 encoder (i.e., the graspingaction,since Ariaonlyrecordsh and pose.Thus,the high-levelplanner)anduseitas the stylevariablez,whichis Fig. 11: Qualitative successes of Ego Mimic on each of our three tasks. TABLE VI: Training details - Mimicplay TABLE VII: Data recording and rollout rates for Human and Robot data. We \u201cslow down\u201d human data by 0.25 to account for differences in task execution speeds. High-level Resnet 18 learning rate(initial) 0.0001 Decayfactor 0.1 Type Human(Hz) Robot(Hz) batch size 50 GMMmodes 5 Recording 30 50 Low-level ACT Rollout(Inference) - 1 learning rate(initial) 5 e-5 Rollout(Control) - 25 Optimizer adamw Decayfactor 1 Scheduler Linear Aria camera which streams at 30 fps. passedto the trans for merencoder-decoder Fig.10.Thelow- level ACT policy is then trained solely on robot data with this additional input from the high level policy as guidance. F. Policy Rollout We rollout our policy with inference at 1 hz and control at 25 hz on a desktop with a an NVIDIA RTX 4090 GPU. The predicted action horizon is 4 seconds, with the first second of predicted actions executed in receding-horizon style. All the robot\u2019s sensors update at 50 hz with the exception of the",
      "start_pos": 7854,
      "end_pos": 8081
    },
    {
      "chunk_id": 242,
      "paper_id": "robgsim",
      "text": "Robo GSim: A Real 2 Sim 2 Real Robotic Gaussian Splatting Simulator Xinhai Li 1*, Jialin Li 2*, Ziheng Zhang 3\u2020, Rui Zhang 4, Fan Jia 3, Tiancai Wang 3, Haoqiang Fan 3, Kuo-Kun Tseng 1\u2021, Ruiping Wang 2\u2021 1 Harbin Instituteof Technology,Shenzhen 2 Instituteof Computing Technology,Chinese Academyof Sciences 3 MEGVIITechnology 4 Zhejiang University Novel View Syn the sis Novel Scene Syn the sis Closed-Loop Evaluation Novel Object Syn the sis Figure 1. Robo GSimisanefficient,low-costinteractiveplat for mwithhigh-fidelityrendering. Itachievesdemonstrationsyn the siswith novel scenes, novel objects, and novel views, facilitating data scaling for policy learning. Additionally, it can perform the closed-loop simulation for safe,fair and realisticevaluationondifferentpolicymodels. Abstract tal Twins Builder,Scene Composer,and Interactive Engine. It can syn the size the simulated data with novel views, ob- Efficientacquisitionofreal-worldembodied data has been jects, trajectories, and scenes. Robo GSim also provides increasinglycritical. However, large-scaledemonstrations an online, reproducible, and safe evaluation for different captured by remote operation tend to take extremely high manipulation policies. The real 2 sim and sim 2 real trans- costs and failtoscaleup the datasizeinanefficientman- ferexperimentsshowahighconsistencyin the texture and ner. Sampling the episodesunderasimulatedenvironment physics. We compared the test results of Robo GSim data is a promising way for large-scale collection while exist- andrealrobot data onboth Robo GSim and realrobotplat- ingsimulatorsfailtohigh-fidelity model ingontexture and forms. The experimental results show that the Robo GSim physics. To address these limitations, we introduce the data model can achieve zero-shot per for mance on the real Robo GSim, a real 2 sim 2 real robotic simulator, powered by robot, with results comparable to real robot data. Addi- 3 DGaussian Splatting and the physicsengine. Robo GSim tionally, in experiments with novel perspectives and novel mainlyincludesf our parts: Gaussian Reconstructor, Digi- scenes, the Robo GSim data model per for med even better on the real robot than the real robot data model. This not *Equal contribution only helps reduce the sim 2 real gap but also addresses the \u2020Project leader limitationsofrealrobot data collection,suchasitssingle- \u2021Corresponding authors 5202 gu A 3 ]OR.sc[ 2 v 93811.1142:vi Xra source and high cost. We hope Robo GSim serves as a ulator that unifies the demonstration syn the sis and closed- closed-loopsimulator for faircomparisononpolicylearn- loopevaluation. Robo GSim can generaterealisticmanipu- ing. More information can be found on our project page lateddemonstrations with novelscenes,views,andobjects https://robogsim.github.io/. forpolicylearning. Itcanalsoper for mclosed-loopevalua- tion for differentpolicynetworks,ensuringfaircomparison under are alisticenvironment. Inconclusion,ourcorecon- 1.Introduction tributions can beconcludedas: \u2022 Realistic 3 DGS-Based Simulator:Wedevelopa 3 DGS- Collecting large-scale manipulated data is of great impor- based simulator that reconstructs scenes and objects tance for efficient policy learning. Some methods propose with realistic textures from multi-view RGB videos. tocapturethedemonstrationsaswellas the actionsthrough Robo GSimisoptimized for somechallengingconditions theremoteoperation[11,37,39].Whilesuchoperationrel- likeweaktextures,lowlight,andreflectivesurfaces. atively improves the collection efficiency, it tends to bring \u2022 Digital Twin System:Weintroduce the layoutalignment extremelylargecosts with the increasing data size.Tosolve modulein the system. With the layout-aligned Isaac Sim, this problem, some works [14, 34] attempt to generate the Robo GSim maps the physical interactions between ob- syn the tic data under the simulated environment, which is jects and roboticarms from Real 2 Simspaces. further used to learn the manipulation policy. However, \u2022 Syn the",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 243,
      "paper_id": "robgsim",
      "text": "the layout-aligned Isaac Sim, this problem, some works [14, 34] attempt to generate the Robo GSim maps the physical interactions between ob- syn the tic data under the simulated environment, which is jects and roboticarms from Real 2 Simspaces. further used to learn the manipulation policy. However, \u2022 Syn the sizer and Evaluator: Robo GSim can synthe- those Sim 2 Real approaches suffer from the large domain size the realistic manipulated demonstrations with novel gapbetweensimulated and real-worldenvironments,mak- scenes,views,andobjects for policylearning. Itcanalso ing the learnedpolicyinvalid. work as the Evaluator to perform model evaluation in a Recently, some works introduce the Real 2 Sim 2 Real physics-consistent manner.The experiment results show (R 2 S 2 R) paradigm for robotic learning [3, 21]. The core that our generated data can achieve the sameper for mance insight is to perform realistic reconstruction via radiance as real robot data, which in a way solves the sim 2 real field methods, such as Ne RF [25] and 3 D Gaussian Splat- gap problem. At the same time, in experiments with ting (3 DGS) [15], and insert learned representations into novel scenes and novel perspectives, our generated data thesimulator. Amongthosemethods,thetypicalapproach, is more effective than real robot data, even with the 2 D Robo-GS[21],presentsa Real 2 Simpipeline and introduces Aug method. Our evaluator also partially verifying the a hybrid representation to generate digital assets enabling per for manceof the realrobot model inaclosed-loop. high-fidelity simulation. However, it lacks the demonstra- tion syn the sis on novel scenes, views, and objects, as well 2.Related Work asverificationaspolicylearning data. Moreover,itfailsto per for mclosed-loopevaluation for differentpoliciesdueto 2.1.Sim 2 Realin Robotics themisalignmentbetween the latentrepresentation,simula- tion,andreal-worldspaces. The Real 2 Sim 2 Real approach fundamentally seeks to ad- In this paper, we develop a Real 2 Sim 2 Real simulator, dress the Sim 2 Realgap,whichremainsapersistentobstacle called Robo GSim,forbothhigh-fidelitydemonstrationsyn- inthetrans for mation from simulationtorealworld[8,27]. thesis and physics-consistent closed-loop evaluation. It In order to bridge the Sim 2 Real gap as much as possible, mainly includes four parts: Gaussian Reconstructor, Digi- manyfeature-richsimulators have emergedinrecentyears, tal Twins Builder,Scene Composer and Interactive Engine. including [7, 23, 28, 35, 38]. To this end, various datasets Given the multi-view RGBimagesequences and MDH [6] andbenchmarks have also been proposed for effectivepol- parameters of the robotic arm, Gaussian Reconstructor is icylearning[12,13,16,26]. built upon 3 DGS [43] and reconstructs the scene and ob- Previous Sim 2 Real methods can be broadly classified jects.Then,the Digital Twins Builderperforms the meshre- intothreecategories: domainr and omization,domainadap- construction and createsadigitaltwinin Isaac Sim. In Dig- tation, and learning with disturbances [40]. Domain ran- ital Twins Builder, we propose the layout alignment mod- domizationmethods are designedtoexp and the operational ule to align the space between the simulation, real-world, envelope of a robot in a simulator by introducing random- and GS representation. After that, the Scene Composer ness. Thesimulationenvironmentshould becapableofmi- combines the scene,roboticarm and objectsinsimulation, grationof the aforementionedcapabilitiesinreal-worldset- and renders the images from new perspective. Finally, in tings[1,10,14,34]. Domainadaptationapproachesaimto the Interactive Engine,Robo GSimworksas the Syn the sizer unify the featurespaceofsimulated and realenvironments, and Evaluatortoper for msthedemonstrationsyn the sisand facilitatingthetraining and migrationwithin the unifiedfea- closed-looppolicyevaluation. ture",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 244,
      "paper_id": "robgsim",
      "text": "Scene Composer ness. Thesimulationenvironmentshould becapableofmi- combines the scene,roboticarm and objectsinsimulation, grationof the aforementionedcapabilitiesinreal-worldset- and renders the images from new perspective. Finally, in tings[1,10,14,34]. Domainadaptationapproachesaimto the Interactive Engine,Robo GSimworksas the Syn the sizer unify the featurespaceofsimulated and realenvironments, and Evaluatortoper for msthedemonstrationsyn the sisand facilitatingthetraining and migrationwithin the unifiedfea- closed-looppolicyevaluation. ture space [2, 19, 41]. The objective of learning methods Robo GSim brings many advantages compared to exist- introducethedisturbancesinto the simulatedenvironment, ing(Real 2)Sim 2 Realframeworks. Itis the firstneuralsim- inwhich the policyofrobotsislearned. Itdevelopstheca- pacitytooperateeffectivelyin the realworld with noise and the manipulated data in simulation using VR/Xbox equip- unpredictability[5,36]. mentof the realworld. 2.2.3 DGaussian Splattingin Robotics 3.2.Gaussian Reconstructor Asasignifi can tadvancementin the fieldof 3 Dreconstruc- We employ the 3 DGS method to reconstruct static scenes, tion, 3 DGS [15] represents the scene as a large set of ex- followed by point cloud segmentation of the robotic arm\u2019s plicit Gaussianpoints and combinesitwi the fficientraster- joints. Subsequently, we utilize the MDH dynamic model izationtoachievehigh-fidelityreal-timerendering,extend- tocontrol the pointcloudscorrespondingtoeachjoint, fa- ing the capabilitiesof Ne RF[25]. cilitatingthedynamicrenderingof the roboticarm. More recently, a number of studies have explored the 3 D Gaussian Splatting (3 DGS) [15] employs a set of use of 3 DGS to perform manipulation tasks within em- multi-view images as input to achieve high-fidelity scene bodied simulators and the real world. For example, Mani- reconstruction. 3 DGSrepresents the sceneasasetof Gaus- Gaussian [22] introduces a dynamic GS framework along- sians and utilizes a differentiable rasterization rendering sidea Gaussianworld model,whichrespectivelyrepresents methodtoenablereal-timerendering. Gaussianpointsimplicitly and parameterizes the mtomodel Specifically, for a scene G = {g }N represented by i i=1 and predict future states and actions. Similarly, Gaussian- N Gaussians, each Gaussian can be represented as g = i Grasper[42]utilizes RGB-Dimagesasinputs and embeds (\u00b5 ,\u03a3 ,o ,c ). Here, \u00b5 \u2208 R 3, \u03a3 \u2208 R 3\u00d73, o \u2208 R and i i i i semanti can dgeometricfeaturesinto 3 DGSthroughfeature c \u2208 SH(4) denote the mean, covariance matrix, opacity distillation and geometric reconstruction, thereby enabling andcolorfactor, representedbysphericalharmoniccoeffi- language-guidedgrasping operations. Toeffectively trans- cients,respectively. fer the knowledge learned in simulation to the real world During the renderingprocess,thefinalcolorvalue C of and reduce the Sim 2 Real gap, recent works [18, 21, 29] thepixel can beobtainedthrough are nderingmethod,sim- basedon 3 DGS have appeared.Amongthem,themostsim- ilar to alpha-blending [15]. It utilizes a sequence of N or- ilartoours are Robo-GS[21]and Splat Sim[29]. Robo-GS dered Gaussians that overlap with the pixel. Such process achieves manipulable robotic arm reconstruction by bind- can beexpressedasfollows: ing Gaussian points, grids, and pixels, with a primary fo- cusonhigh-fidelity Real 2 Simtransfer;however,itprovides i\u22121 (cid:88) (cid:89) limited discussion on the Sim 2 Real phase. Splat Sim re- C = c i \u03b1 i (1\u2212\u03b1 j ) (1) constructs both the robotic arm and objects in the scene i\u2208N j=1 andsimultaneouslyverifiesthefeasibilityof the method for 1 Sim 2 Realtasks. However, itlacksdiscussionsongenerat- \u03b1 =o \u00b7exp( \u03b4\u22a4\u03a3\u22121\u03b4 ) (2) ingdigitaltwinassetsof the objects, which are critical for i i 2 i 2 D i achievingaccuratemanipulation. where \u03b1 is the opacity of the i-th Gaussian. \u03b4 \u2208 R 2 de- i i notes the offset between 2 D Gaussian center and current 3.Methods",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 245,
      "paper_id": "robgsim",
      "text": "However, itlacksdiscussionsongenerat- \u03b1 =o \u00b7exp( \u03b4\u22a4\u03a3\u22121\u03b4 ) (2) ingdigitaltwinassetsof the objects, which are critical for i i 2 i 2 D i achievingaccuratemanipulation. where \u03b1 is the opacity of the i-th Gaussian. \u03b4 \u2208 R 2 de- i i notes the offset between 2 D Gaussian center and current 3.Methods pixel. \u03a3 \u2208R 2\u00d72 represents the 2 Dcovariancematrix. 2 D Modified Denavit-Hartenberg (MDH) [6] convention is 3.1.Overall Architecture aparameterized model todescribe the kinematicchainofa As shown in Fig. 2, Robo GSim mainly includes four manipulator. Each joint and link in the kinematic chain is parts: Gaussian Reconstructor, Digital Twins Builder, characterized by a set of parameters. In MDH, a trans for- Scene Composer,and Interactive Engine. Givenmulti-view mation matrix can be constructed for each link, achieving images and MDH parameters of the robotic arm, Gaus- anaccuraterepresentationof the manipulator\u2019sposeateach sian Reconstructor (Sec. 3.2) reconstructs scenes and ob- stageofmotion. Letx ,y ,z denote the coordinatesof the i i i jectsusing 3 DGS,segments the roboticarm,andbuildsan origin for the i-th joint. For a manipulator, the i-th joint MDH kinematic drive graph structure to enable accurate configuration can berepresentedas: motion modeling of the robotic arm. Digital Twin Builder (Sec.3.3)involvesmeshreconstructionof the sceneandob- \u0398={\u03b2 ,a ,d ,\u03b8 } (3) i i i i jects. Throughlayoutalignment,theasset data flow can be interconnected,facilitating the subsequentevaluationin In- where \u03b2 represents the twist angle, which is the rotation i teractive Engine. Scene Composer (Sec. 3.4) achieves the aroundthex-axis from the(i\u22121)-thjointtothei-thjoint. syn the sis of novel objects, scenes, and views. Interactive a denotes the linklength,measuring the distancealong the i Engine (Sec. 3.5) syn the sizes novel view/scene/object im- x-axis from z to z . d is the link offset, indicating the i\u22121 i i ages for policylearning. Itcanalsoevaluate the policynet- displacementalongthez-axisfromx tox .\u03b8 represents i\u22121 i i works in a closed-loop manner. Moreover, we can collect thejointangle,rotationaroundthez-axisfromx tox . i\u22121 i Figure 2. Overviewof the Robo GSim Pipeline:(1)Inputs:multi-view RGBimagesequences and MDHparametersof the roboticarm. (2)Gaussian Reconstructor: reconstruct the scene and objectsusing 3 DGS,segment the roboticarm and buildan MDHkinematicdrive graphstructure for accuratearmmotion model ing. (3)Digital Twins Builder: per for mmeshreconstructionofboth the scene and objects, thencreateadigitaltwinin Isaac Sim, ensuringhighfidelityinsimulation. (4)Scene Composer: combine the roboticarm and objects inthesimulation, identifyoptimaltestviewpointsusingtracking, andrenderimages from newperspectives. (5)Interactive Engine: (i) Thesyn the sizedimages with novelscenes/views/objects are used for policylearning.(ii)Policynetworks can beevaluatedinaclose-loop manner.(iii)Theembodied data can becollectedby the VR/Xboxequipment. Thetransformationmatrix for eachlink T ,using MDH turntable and extract matching features with GIM [33] to i parameters,can bewrittenas: address issues such as lack of texture and reflections. We thenintegrate the COLMAPpipeline[32]toobtain the ini- \uf8ee \uf8f9 cos\u03b8 \u2212sin\u03b8 cos\u03b2 sin\u03b8 sin\u03b2 a cos\u03b8 i i i i i i i tial SFMpointcloud,whichissubsequentlyused for recon- T i = \uf8ef \uf8ef \uf8f0 sin 0 \u03b8 i cos s \u03b8 in i c \u03b2 o i s\u03b2 i \u2212co c s o \u03b8 s i \u03b2 s i in\u03b2 i a i s d in i \u03b8 i\uf8fa \uf8fa \uf8fb (4) s o t n ru t c h t e io w n e b b y , w 3 D e G in S it . ia M lly or e",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 246,
      "paper_id": "robgsim",
      "text": "s i \u03b2 s i in\u03b2 i a i s d in i \u03b8 i\uf8fa \uf8fa \uf8fb (4) s o t n ru t c h t e io w n e b b y , w 3 D e G in S it . ia M lly or e e m ov p e l r o , y fo g r en n e o r v a e ti l ve ob 3 j D ect r s ec a o v n a s il t a ru b c le - 0 0 0 1 tionmethods[17,20]toprocure 3 Dgaussians and textured meshesof the objects. Subsequently,weutilize the method Bysequentiallymultiplying the setrans for mationmatrices, in Gaussian Editor[4]thatapplies the diffusion model[31] we canobtainthefinaltrans for mationmatrix from the base tofacilitateobjectreconstructionin 3 DGS. to the end effector. We segment each joint and then treat all Gaussianpoints with inajointasapointmass. Wefur- thermoveall Gaussianpoints with inajointaccordingto T , Layout Alignment:Asshownin Fig.2,sincewefollow the i achievingkinematic-driven Controlof the Gaussianpoints. localcoordinatesystemof the roboticarm,theworldcoor- dinates and Isaac Sim are axis-aligned. We first measure 3.3.Digital Twins Builder thereal-worldscenetoalignthesizeof the importedtable Digitaltwinsshouldnotonlymapreal-worldassetsbutalso scene in Isaac Sim. In the GS scene, a downward-facing involve coordinate alignment. Through Real 2 Sim layout camera is placed 1.6 meters above the base joint to render alignment and Sim 2 GSsparsekeypointalignment, we can asegmentationmap. Forcoordinatealignment,weplacea digitize the real world, enabling the flow of digital assets downward-facingcamera 1.6 metersabove the basejointin between the real, simulated, and GS representation. This Isaac Sim. Bycomparing the renderedscene from the BEV, facilitates the conversion of digital assets in all directions, front and sideviewsegmentation,with the views from Isaac achievingcomprehensiveassetflooding. Sim,weadjust the shifttoachievelayoutalignment. 3 D Assets Generation: We employ two methods to gen- erate 3 D object assets. For real-world objects, we cap- Sim 2 GS Alignment: Given the MDH-based transforma- ture high-quality multi-view images of the objects using a tion matrices Tgs and simulated trans for mation matrices i Tsim,thereexistsatrans for mationmatrix Tsim such that: \u03a3\u2032 =R \u03a3R\u22a4 (14) i gs(i) norm norm Tsim =Tsim\u00b7Ti (5) Object Editing: The trans for mation here can extend the gs(i) i gs trans for mation from the scene editing mentioned above. Tocompute the averagetrans for mationmatrix Tsim,we use However,thedifferenceis that the targetobject\u2019scoordinate gs theweightedsum and applynormalization: centerisgivenby Eq.7. Thecoordinatetrans for mation for its Gaussianpoints can berepresented: (cid:80)6 w \u00b7Tsim T g s s im = (cid:13) (cid:13)(cid:80) i 6 =1 w i \u00b7T g s s im i (cid:13) (cid:13) (6) \u00b5\u2032 =R(\u00b5\u2212\u00b5 0 )+\u00b5 0 +t (15) (cid:13) i=1 i gsi(cid:13) 3.5.Interactive Engine wherew istheweightofeachjoint. i For the target object Tsimin Isaac Sim, we can trans- Ourinteractiveengine can workas: Syn the sizer and Evalu- obj ator. As Syn the sizer,itproduceslargevolumesof data with form it into the GS coordinate system using the following low-cost for downstream policy learning. As Evaluator, it formula: Tgs =Tgs \u00b7Tsim (7) canper for msafe,real-time,andreproducibleevaluation. obj sim obj Syn the sizer:we use the enginetogeneratenumeroustrain- Camera Localization: Totransform the real-worldcoordi- ing trajectories, including robotic arm movements and tar- natesysteminto the GScoordinate, weapply the localiza- get object trajectories. These trajectories drive the GS to",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 247,
      "paper_id": "robgsim",
      "text": "policy learning. As Evaluator, it formula: Tgs =Tgs \u00b7Tsim (7) canper for msafe,real-time,andreproducibleevaluation. obj sim obj Syn the sizer:we use the enginetogeneratenumeroustrain- Camera Localization: Totransform the real-worldcoordi- ing trajectories, including robotic arm movements and tar- natesysteminto the GScoordinate, weapply the localiza- get object trajectories. These trajectories drive the GS to tion approach from GS-SLAM [24]. For a pre-trained GS generate massive and photorealistic simulated datasets for model,G = {g }N ,wefroze the attributesof 3 DGSand i i=1 policylearning. Thisdiverse data includesnovelviewren- optimize the externalcameraparameters TW. C derings,scenecombinations,andobjectreplacements. In camera localization, only the current camera pose is Evaluator: For trained models, testing directly on phys- optimized without updates to the map representation. For ical devices may pose safety risks or incur high costs for monocular cases, we minimize the following photometric reproduction. Therefore,weconvert the predictedtrajecto- residual: riesinto GS-renderedresultstoefficiently and rapidlyeval- L pho = (cid:13) (cid:13)I(G,T C W)\u2212I\u00af(cid:13) (cid:13) 1 , (8) uate the model\u2019s prediction quality. Specifically, the Isaac where I(G,TW) represents rendering Gaussians G from Sim [28] outputs an initial state of the target object and C TW,and I\u00afistheobservedimage. robotic arm, and GS renders according to the status. The C rendered images are then fed to the policy to predict the 3.4.Scene Composer next frame\u2019s action. The predicted action is passed to the Scene Editing: To merge the point cloud into the robotic simulation for kinematic inverse parsing, collision detec- armscene,thetrans for mation T[R|t]ofthemarkedpointis tion,ando the rphysicalinteractions. Then,Isaacsimsends firstcalculated. Thenthecoordinatesof the pointcloudin theparsedsix-axisrelativeposeto the GSrenderer, which the new scene are projected into the arm coordinate based thensendstherenderedresultasfeedbackto the policynet- onthetrans for mation. Exp and ing the 3 DCovariance\u03a3in work. Thisservesasvisualfeedback for predicting the next 3 DGSintoscales and rotationquaternionqby: action,andtheprocessiteratesuntil the taskisfinished. \u03a3=qss Tq T (9) 4.Experiments Theratiorof the trans for mation can beisolatedandex- Since the reisnobenchmarksavailable for Real 2 Sim 2 Real, tractedasanindependentcomponent: we construct the following four groups of proxy exper- iments to comprehensively evaluate the per for mance of (cid:113) r = (RRT) (10) Robo GSim under simulation and real-world. We use UR 5 (0,0) robot arm for all experiments. The robot arm rendering is we canfurtheruseittonormalize the rotationmatrix R: partiallybuiltupon the codebaseof Robo-GS[21]. Real 2 Sim Novel Pose Synthesisverifieswhether the robot R R norm = r (11) arm pose captured in the real world can be effectively uti- lizedtoachieveprecisecontrolin the simulator. The scale attributesof the Gaussianpointsisadjusted: Sim 2 Real Trajectory Replaycheckswhether the trajecto- riescollectedin the simulator can beaccuratelyreproduced s=s+log(r) (12) bythereal-worldrobotarm. Apply the Trans for mation T to Gaussianpointcoordinates Robo GSim as Syn the sizer demonstrates the ability of Robo GSim to generate high-fidelity demonstrations with \u00b5\u2032 =R\u00b5+t (13) novelscenes,views,andobjects,aligning with realworld. Grasp Suc. Place Suc. TV NV(MD) NS TV NV NS Real 100% 30% 40% 90% 0% 20% Real+2 DAUG 80% 100% 60% 80% 0% 60% Robo GSim 100% 70% 100% 90% 0% 90% Table 1.Per for manceonring-toss task inrealworld.TVdenotestestview,NVisnovelview and NSis the novelscene.MDmeansminor deviation. Robo GSimas Evaluatorshows that Robo GSim can effec- 4.3.Robo GSimas Syn the sizer tivelyper for mclosed-loopevaluation for policynetworks. In this section, we use the vision-language-action (VLA) model to validate the effectiveness of syn",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 248,
      "paper_id": "robgsim",
      "text": "100% 70% 100% 90% 0% 90% Table 1.Per for manceonring-toss task inrealworld.TVdenotestestview,NVisnovelview and NSis the novelscene.MDmeansminor deviation. Robo GSimas Evaluatorshows that Robo GSim can effec- 4.3.Robo GSimas Syn the sizer tivelyper for mclosed-loopevaluation for policynetworks. In this section, we use the vision-language-action (VLA) model to validate the effectiveness of syn the tic data by Method Grasp Suc. Place Suc. Robo GSim. we usethe LLAMA 3-8 B[9]asthe LLMand Real-to-Real 100% 90% CLIP [30] as the vision encoder. Two-layer MLP is used Real-to-Robo GSim 100% 30% as the projection network. The VLA model is trained on Sim-to-Real 80% 0% 8 x A 100(80 GB)for 1 epoch.Thetrainingprocessisdivided Robo GSim-to-Real 100% 90% into three stages: (1) Pre-training with only the connector Table 2. Cross validation between real world and simulation. enabled,using the LAION-558 Kdataset. (2)Training with \u201cSim-to-Real\u201dmeans that testing the VLA model trained with the LLM unfrozen using the LLa VA 665 K dataset. (3) Super- Isaacsimdataon the realworldrobotarm. vised fine tuning(SFT)withroboticimage-action data and the CLIPweightisfrozen. By using the real machine distribution to guide the 4.1.Real 2 Sim Novel Pose Syn the sis Robo GSimdistribution,weaimtoimprove the model\u2019ssuc- Theobjectiveofthenovelposesyn the sisistovalidate the cess rate. We perform the experiments on a challenging per for mance of Real 2 Sim reconstruction, with a particular ring-toss task (see Fig. 7), which is divided into two sub- focusontheaccuracyof the roboticarm\u2019smovements and tasks: picking up the ring and tossing it onto the target. the fidelity of the image texture. The static scene is re- The accuracy requirement for the Z-axis when picking up constructed using the initial pose of the robotic arm from the ring is within 5 mm. For real data, 1,000 samples are thefirstframeof GT.Thetrajectorycollected from the real collected manually. For a fair comparison, we used 1,000 roboticarmisusedas the drivingforce,andweemploy the syn the tic samples generated by Robo GSim. During test- kinematic control for novel pose rendering. As shown in ing, each model was tested 10 times, with three attempts Fig. 3, the results demonstrate that our reconstruction ac- allowed per trial for grasping. If all three attempts failed, curatelycapturesboththetexture and the physicaldynam- thetrialwasmarkedasunsuccessful. icsof the roboticarm,highlighting the fidelityachievedby As shown in Tab. 1, We compared three models: one Robo GSim. Tocomp are with the videosequencedrivenby trained with real machine data, one trained with real ma- therealrobotunder the newviewpoint,Robo GSimachieves chine data plus 2 DAUG,andonetrained with Robo GSim. a 31.3 PSNR and 0.79 SSIMrenderingresult,whileensur- Thecomparisonwasmadeintermsoftestview,novelview, ingreal-timerendering with 10 FPS. andnovelscene. Theresultsshowthatin the testview,the generated data from Robo GSim can achieve zero-shot ca- 4.2.Sim 2 Real Trajectory Replay pability, withper for mancecomparableto the realmachine To verify whether the trajectories from Issac Sim can per- data(bothat 90%).Inthenovelscenecase,Robo GSimper- fectly align with the real machine and Robo GSim, we de- formed much better than real machine data, reaching 90% signedanexperimentwhere the trajectoryiscollectedusing compared to the 60% of real machine data. In the novel Issac Sim,andthen the trajectoryisusedtodrive GStoren- view experiment, Robo GSim also had less bias compared dera Coke-graspingscene,while the sametrajectoryisused to the real machine data model. We also compared the todrive the realmachinetograspa Coke can. Asshownin effect of adding 2 D AUG",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 249,
      "paper_id": "robgsim",
      "text": "to the 60% of real machine data. In the novel Issac Sim,andthen the trajectoryisusedtodrive GStoren- view experiment, Robo GSim also had less bias compared dera Coke-graspingscene,while the sametrajectoryisused to the real machine data model. We also compared the todrive the realmachinetograspa Coke can. Asshownin effect of adding 2 D AUG to the real machine data. Af- Fig.4, thecomparisonreveals astrongalignment between ter adding AUG, the per for mance inthe test view dropped thesimulatedpolicy and the actualphysicalbehaviorof the (90% \u2192 80%), but in the novel scene, it improved (40% roboticarm,highlighting the effectivenessof the Sim 2 Real \u2192 60%). However, in the novel view, the bias increased. transfer in our system. These results suggest that our sim- Pure 2 DAUGlacksspatialaw are ness,anditsper for mance ulation can reliably model real-worlddynamics,facilitating isfarworsethanthatof Robo GSim,whichhasspatialintel- successfulpolicytransfer from simulationto the realworld. ligence. a Itshould benoted that manualcollectiontakesa Real Robo GSim Depth Diff Dr. Robot PSNR:31.4 SSIM:0.79 FPS:10 Figure 3. Real 2 Sim Novel Pose Syn the sis: \u201dReal\u201drepresentsthecaptureof the realroboticarm from anewviewpoint. \u201dRobo GSim\u201d showstherenderingofthenovelpose from the new viewpointdrivenby the realrecordedtrajectory. \u201dDepth\u201dshows the renderingdepth by GS.\u201dDiff\u201disthedifferencecalculatedbetween the Real and the rendered RGBimages. Wecomputethepixeldistanceof the same pointbetween the Real and Robo GSim,whichis 7.37. Figure 4. Sim 2 Real Trajectory Replay: The\u201dSim\u201drowdisplays the videosequencecollected from Isaac Sim. \u201dReal\u201drepresents the demonstrationdrivenby the trajectoryinsimulation.\u201dRobo GSim\u201disthe GSrenderingresultdrivenby the sametrajectory.\u201dDiff\u201dindicates thedifferencesbetween Real and the renderedresults. totalof 40 hourswhile Robo GSimonlyrequires 4 hours for doorenvironments.Thehigh-fidelitymulti-viewrenderings syn the sis. It is promising to further scale up the data size demonstrate that Robo GSimenables the robotarmtooper- ofsynthesis for fur the rper for manceimprovements. Fig.7 ateseamlesslyacrossdiversescenes. shows the visualization of some success and failure cases. 4.4.Robo GSimas Evaluator Moreover, we also illustrate some more qualitative analy- sis for novel scene syn the sis. As shown in Fig. 5, we dis- Realisticclosed-loopevaluationiscrucial for validation and playtheresultsof the physicalmigrationof the UR 5 robot comparisonofpolicynetworks. Inthissection, wemainly armto new scenes,includingafactory,ashelf,andtwoout- explore the effectiveness of using Robo GSim as an Evalu- Figure 5. Novel Scene Syn the sis: Weshowtheresultsofthephysicalmigrationof the robotarmto new scenes,includingafactory,a shelf,andtwooutdoorenvironments.Thehigh-fidelitymulti-viewrenderingsdemonstrate that Robo GSimenables the robotarmtooperate seamlesslyacrossdiversescenes. Figure 6. Robo GSimas Syn the sizer: Renderingof the sametrainingsetinnovelview and novelscenes. Method L 1 \u2193 PSNR\u2191 tialalignmenttoenables 3 Dassertflow. Withnovelview- 3 DGS 0.01381 34.19939 point, object, trajectory and scene, our Robo GSim engine 2 DGS 0.01798 32.36417 can generate high-fidelity syn the sized data. Additionally, PGSR 0.01925 31.72386 due to our precise spatial alignment, Robo GSim can serve Mip Ne RF 360 0.02618 23.51348 as evaluator that allows real-time online policy evaluation. Despiteitsgreatprogress,thecurrentversionof Robo GSim Table 3.3 DGSachievesbetterstaticreconstructionthan 2 DGS has several limitations. It can only simulate rigid objects andthelighting for syn the sizedobjectsisnotyet full yuni- fied with the robotic arm. Moreover, generating geometri- ator. It aims to show its high consistency with real-world cally consistent object meshes remains challenging, which inference. Given the well-trained VLA model, we deploy isoftenkeytocompletingcomplexmanipulationtasks. In itforbothreal-worldrobots and Robo GSimsimulation. As thenearfuture,wewillexploremoreadvancedmeshextrac- shownin Fig.9, ourclosed-loopsimulator Robo GSim can tionmethods,furtherexp and the taskcategories and estab- reproduceresultssimilartothose from the realworld. For lish the benchmarks to comprehensively evaluate the per- similarbadcases,our Robo GSim can avoid the issuesexist- formanceacrossdiversescenarios. ingin the",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 250,
      "paper_id": "robgsim",
      "text": "Given the well-trained VLA model, we deploy isoftenkeytocompletingcomplexmanipulationtasks. In itforbothreal-worldrobots and Robo GSimsimulation. As thenearfuture,wewillexploremoreadvancedmeshextrac- shownin Fig.9, ourclosed-loopsimulator Robo GSim can tionmethods,furtherexp and the taskcategories and estab- reproduceresultssimilartothose from the realworld. For lish the benchmarks to comprehensively evaluate the per- similarbadcases,our Robo GSim can avoid the issuesexist- formanceacrossdiversescenarios. ingin the realworld,likeviolations and collisions. There- fore,ourevaluatorprovidesafair,safe,andefficientevalu- ationplatform for policy. 6.Acknowledgements 5.Conclusion and Discussion Inthispaper,webuilta Real 2 Sim 2 Realsimulator,basedon Theworkwassupportedby the National Science and Tech- 3 DGS.Wealsointroduce the digitaltwinsystem with spa- nology Major Projectof China(2023 ZD 0121300). kci P ecal P Success Fail Success Fail Figure 7. Robo GSimas Syn the sizer: Thefirsttworowsshowrealrobotvideoscaptured from the testviewpoint,illustratingsuccessful andfailedcasesof the VLAmodelon the Pick task.Thelasttworowsdisplayrealrobotvideoscaptured from the testviewpoint,showing successful and failedcasesof the VLAmodelon the Place task. Real Robo GSim Real Real Hit someone Break gripper in real world in real world Robo GSim Robo GSim Figure 8. Robo GSimas Evaluator: Thefirsttworows,labeled\u201dReal\u201dand\u201dRobo GSim\u201d,showthefootagecaptured from the realrobot and Robo GSim,respectively. They are bothdrivenbythetrajectorygeneratedby the same VLAnetwork. Inthethirdrow,theleftside shows the real-worldinferencewhere the robotarmexceedsitsoperationallimits,resultinginamanualshutdown. Therightsideshows aninstancewhereawrongdecision from the VLAnetwork,causestheroboticarmtocollide with the table. Thef our throwpresents the simulationresults from Robo GSim,which can avoiddangerouscollisions. sus_mi SG liaf_mi SG sus_mi SG liaf_mi SG sus_lae R liaf_lae R sus_lae R liaf_lae R liaf_mi S Figure 9. Expon Realrobot: Thefirstrow,GSim sus,representssuccessfulcasesof Robo GSim data under the testview. Thesecond row, GSim fail, represents failure cases of Robo GSim data under the novel view. The third row, GSim sus, represents successful casesof Robo GSim data under the novelscene. Thef our throw,GSim fail,representsfailurecasesof Robo GSim data under the novel scene.Thefifthrow,Real sus,representssuccessfulcasesofreal-world data under the testview.Thesixthrow,Real fail,represents successful cases of real-world data under the novel view. The seventh row, Real sus, represents successful cases of real-world data under the novelscene. Theeighthrow, Real fail, representsfailurecasesofreal-world data under the novelscene. Theninthrow, Sim fail,representsfailurecasesof Isaac Sim data under the testview. References Moran,Steven Bohez,Fereshteh Sadeghi,Bojan Vujatovic, and Nicolas Heess. Nerf 2 real: Sim 2 realtransferofvision- [1] Open AI: Marcin Andrychowicz, Bowen Baker, Maciek guidedbipedalmotionskillsusingneuralradiancefields. In Chociej, Rafal Jozefowicz, Bob Mc Grew, Jakub Pachocki, 2023 IEEE International Conference on Robotics and Au- Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, tomation(ICRA),pages 9362\u20139369,2023. 2 etal. Learningdexterousin-handmanipulation. The Inter- [4] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xi- national Journalof Robotics Research,39(1):3\u201320,2020. 2 aofeng Yang,Yikai Wang,Zhongang Cai,Lei Yang,Huaping [2] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Liu,and Guosheng Lin. Gaussianeditor: Swift and control- Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel- lable 3 dediting with gaussiansplatting. In Proceedingsof level domain adaptation with generative adversarial net- the IEEE/CVFConferenceon Computer Vision and Pattern works. In Proceedingsof the IEEEconferenceoncomputer Recognition(CVPR),pages 21476\u201321485,2024. 4 vision and patternrecognition,pages 3722\u20133731,2017. 2 [5] Cheng Chi,Zhenjia Xu,Siyuan Feng,Eric Cousineau,Yilun [3] Arunkumar Byravan, Jan Humplik, Leonard Hasenclever, Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Arthur Brussee, Francesco Nori, Tuomas Haarnoja, Ben Diffusionpolicy: Visuomotorpolicylearningviaactiondif- fusion. The International Journal of Robotics Research, [19] Mingsheng Long,Yue Cao,Jianmin Wang,and Michael Jor- 2024. 3 dan.Learningtransferablefeatures with deepadaptationnet- [6] Peter ICorke. Asimple and systematicapproachtoassign- works. In International conference on machine learning, ing denavit\u2013hartenberg parameters. IEEE transactions on pages 97\u2013105.PMLR,2015. 2 robotics,23(3):590\u2013594,2007. 2,3 [20] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, [7]",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 251,
      "paper_id": "robgsim",
      "text": "Journal of Robotics Research, [19] Mingsheng Long,Yue Cao,Jianmin Wang,and Michael Jor- 2024. 3 dan.Learningtransferablefeatures with deepadaptationnet- [6] Peter ICorke. Asimple and systematicapproachtoassign- works. In International conference on machine learning, ing denavit\u2013hartenberg parameters. IEEE transactions on pages 97\u2013105.PMLR,2015. 2 robotics,23(3):590\u2013594,2007. 2,3 [20] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, [7] Erwin Coumans and Yunfei Bai. Pybullet, a python mod- Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, uleforphysicssimulation for games,robotics and machine Marc Habermann,Christian Theobalt,etal. Wonder 3 d:Sin- learning. http://pybullet.org,2016\u20132021. 2 gleimageto 3 dusingcross-domaindiffusion.ar Xivpreprint [8] Konstantinos Dimitropoulos, Ioannis Hatzilygeroudis, and ar Xiv:2310.15008,2023. 4 Konstantinos Chatzilygeroudis. Abriefsurveyofsim 2 real [21] Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng methods for robot learning. In International Conference Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen on Roboticsin Alpe-Adria Danube Region,pages 133\u2013140. Feng,Lu Shi,etal. Robo-gs: Aphysicsconsistentspatial- Springer,2022. 2 temporal model for roboticarm with hybridrepresentation. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- ar Xivpreprintar Xiv:2408.14873,2024. 2,3,5 hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil [22] Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Ji- Mathur,Alan Schelten,Amy Yang,Angela Fan,etal. The wen Lu,and Yansong Tang. Manigaussian: Dynamicgaus- llama 3 herd ofmodels. ar Xiv preprint ar Xiv:2407.21783, sian splatting for multi-task robotic manipulation. In Eu- 2024. 6 ropean Conference on Computer Vision, pages 349\u2013366. [10] Ioannis Exarchos, Yifeng Jiang, Wenhao Yu, and C Karen Springer,2025. 3 Liu. Policy transfer via kinematic domain randomization [23] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, andadaptation. In 2021 IEEEInternational Conferenceon Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Robotics and Automation(ICRA),pages 45\u201351.IEEE,2021. Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel 2 State. Isaac gym: High per for mance GPU based physics [11] Zipeng Fu, Tony Z. Zhao, and Chelsea Finn. Mobile simulation for robotlearning. In Thirty-fifth Conferenceon ALOHA: Learning bimanual mobile manipulation using Neural Information Processing Systems Datasets and Bench- low-cost whole-body teleoperation. In 8 th Annual Confer- marks Track(Round 2),2021. 2 enceon Robot Learning,2024. 2 [24] Hidenobu Matsuki, Riku Murai, Paul H.J. Kelly, and An- [12] Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao drew J.Davison. Gaussiansplattingslam. In Proceedingsof Dong, and He Wang. Partmanip: Learning cross-category the IEEE/CVFConferenceon Computer Vision and Pattern generalizablepartmanipulationpolicy from pointcloudob- Recognition(CVPR),pages 18039\u201318048,2024. 5 servations. ar Xivpreprintar Xiv:2303.16958,2023. 2 [25] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, [13] Minho Heo, Youngwoon Lee, Doohyun Lee, and Joseph J. Jonathan T.Barron,Ravi Ramamoorthi,and Ren Ng. Nerf: Lim. Furniturebench: Reproducible real-world benchmark representingscenesasneuralradiancefields for viewsyn the- for long-horizon complex manipulation. In Robotics: Sci- sis. Commun.ACM,65(1):99\u2013106,2021. 2,3 ence and Systems,2023. 2 [26] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita [14] Johann Huber, Franc\u00b8ois He\u00b4le\u00b4non, Hippolyte Watrelot, Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yun- Fa\u00a8\u0131z Ben Amar, and Ste\u00b4phane Doncieux. Domain ran- rong Guo,Hammad Mazhar,Ajay Mandlekar,Buck Babich, domization for sim 2 realtransferofautomaticallygenerated Gavriel State, Marco Hutter, and Animesh Garg. Orbit: A grasping data sets.In 2024 IEEEInternational Conferenceon unified simulation framework for interactive robot learning Robotics and Automation(ICRA),pages 4112\u20134118.IEEE, environments. IEEERobotics and Automation Letters,8(6): 2024. 2 3740\u20133747,2023. 2 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku\u00a8hler, [27] Jean-Baptiste Mouret and Konstantinos Chatzilygeroudis.20 and George Drettakis. 3 d gaussian splatting for real-time",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 252,
      "paper_id": "robgsim",
      "text": "Orbit: A grasping data sets.In 2024 IEEEInternational Conferenceon unified simulation framework for interactive robot learning Robotics and Automation(ICRA),pages 4112\u20134118.IEEE, environments. IEEERobotics and Automation Letters,8(6): 2024. 2 3740\u20133747,2023. 2 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku\u00a8hler, [27] Jean-Baptiste Mouret and Konstantinos Chatzilygeroudis.20 and George Drettakis. 3 d gaussian splatting for real-time radiancefieldrendering.ACMTransactionson Graphics,42 yearsofrealitygap:afewthoughtsaboutsimulatorsinevo- lutionary robotics. In Proceedings of the genetic and evo- (4),2023. 2,3 lutionarycomputationconferencecompanion, pages 1121\u2013 [16] Vikash Kumar,Rutav Shah,Gaoyue Zhou,Vincent Moens, 1124,2017. 2 Vittorio Caggiano, Abhishek Gupta, and Aravind Ra- jeswaran. Robohive: Aunifiedframework for robotlearn- [28] NVIDIA. Isaacsim. https://developer.nvidia. ing. In Thirty-seventh Conference on Neural Information com/isaac/sim,2024. Softw are. 2,5 Processing Systems Datasets and Benchmarks Track,2023. [29] Mohammad Nomaan Qureshi,Sparsh Garg,Francisco Yan- 2 dun, David Held, George Kantor, and Abhishesh Sil- [17] Xinhai Li, Huaibin Wang, and Kuo-Kun Tseng. Gaus- wal. Splatsim: Zero-shot sim 2 real transfer of rgb manip- siandiffusion: 3 dgaussiansplatting for denoisingdiffusion ulation policies using gaussian splatting. ar Xiv preprint probabilistic models with structured noise. ar Xiv preprint ar Xiv:2409.10161,2024. 3 ar Xiv:2311.11221,2023. 4 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya [18] Ruoshi Liu, Alper can berk, Shuran Song, and Carl Von- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, drick. Differentiable robot rendering. In 8 th Annual Con- Amanda Askell,Pamela Mishkin,Jack Clark,etal.Learning ferenceon Robot Learning,2024. 3 transferable visual models from natural language supervi- sion.In Internationalconferenceonmachinelearning,pages grasping. IEEE Robotics and Automation Letters, 9(9): 8748\u20138763.PMLR,2021. 6 7827\u20137834,2024. 3 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, [43] Licheng Zhong,Hong-Xing Yu,Jiajun Wu,and Yunzhu Li. Patrick Esser, and Bjo\u00a8rn Ommer. High-resolution image Reconstruction and simulationofelasticobjects with spring- syn the sis with latent diffusion models. In Proceedings of mass 3 d gaussians. In European Conference on Computer the IEEE/CVF conference on computer vision and pattern Vision,pages 407\u2013423.Springer,2025. 2 recognition,pages 10684\u201310695,2022. 4 [32] Johannes LSchonberger and Jan-Michael Frahm. Structure- from-motion revisited. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 4104\u20134113,2016. 4 [33] Xuelun Shen,Zhipeng Cai,Wei Yin,Matthias Mu\u00a8ller,Zijun Li,Kaixuan Wang,Xiaozhi Chen,and Cheng Wang. Gim: Learninggeneralizableimagematcher from internetvideos. In The Twelfth International Conferenceon Learning Repre- sentations,2024. 4 [34] Josh Tobin,Rachel Fong,Alex Ray,Jonas Schneider,Woj- ciech Zaremba, and Pieter Abbeel. Domainr and omization fortransferringdeepneuralnetworks from simulationto the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23\u201330. IEEE, 2017. 2 [35] Emanuel Todorov,Tom Erez,and Yuval Tassa. Mujoco: A physicsengine for model-basedcontrol. In 2012 IEEE/RSJ International Conferenceon Intelligent Robots and Systems, pages 5026\u20135033.IEEE,2012. 2 [36] Jingkang Wang,Yang Liu,and Bo Li. Rein for cementlearn- ing with perturbedrewards.In Proceedingsof the AAAIcon- ferenceonartificialintelligence,pages 6202\u20136209,2020. 3 [37] David Whitney, Eric Rosen, Daniel Ullman, Elizabeth Phillips, and Stefanie Tellex. Rosreality: Avirtualreality frameworkusingconsumer-gradehardw are for ros-enabled robots. In 2018 IEEE/RSJInternational Conferenceon In- telligent Robots and Systems(IROS),pages 1\u20139,2018. 2 [38] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu,Fangchen Liu,Minghua Liu,Hanxiao Jiang,Yifu Yuan, He Wang,Li Yi,Angel X.Chang,Leonidas J.Guibas,and Hao Su. SAPIEN:Asimulatedpart-basedinteractiveenvi- ronment. In The IEEEConferenceon Computer Vision and Pattern Recognition(CVPR),2020. 2 [39] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardw are. In Proceedings of Robotics: Science and Systems,Daegu,Republicof Korea,2023. 2 [40] Wenshuai Zhao,",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 253,
      "paper_id": "robgsim",
      "text": "Yuan, He Wang,Li Yi,Angel X.Chang,Leonidas J.Guibas,and Hao Su. SAPIEN:Asimulatedpart-basedinteractiveenvi- ronment. In The IEEEConferenceon Computer Vision and Pattern Recognition(CVPR),2020. 2 [39] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardw are. In Proceedings of Robotics: Science and Systems,Daegu,Republicof Korea,2023. 2 [40] Wenshuai Zhao, Jorge Pen\u02dca Queralta, and Tomi Wester- lund. Sim-to-realtransferindeeprein for cementlearning for robotics:asurvey.In 2020 IEEESymposium Serieson Com- putational Intelligence(SSCI),pages 737\u2013744,2020. 2 [41] Liming Zheng, Wenxuan Ma, Yinghao Cai, Tao Lu, and Shuo Wang. Gpdan:Graspposedomainadaptationnetwork for sim-to-real 6-dof object grasping. IEEE Robotics and Automation Letters,8(8):4585\u20134592,2023. 2 [42] Yuhang Zheng,Xiangyu Chen,Yupeng Zheng,Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zeng- mao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen,Xiaoxiao Long,and Meiqing Wang.Gaussiangrasper: 3 dlanguagegaussiansplatting for open-vocabularyrobotic",
      "start_pos": 5082,
      "end_pos": 5208
    },
    {
      "chunk_id": 254,
      "paper_id": "humanhumanoidph2d",
      "text": "Humanoid Policy \u223c Human Policy Ri-Zhao Qiu*,1 Shiqi Yang*,1 Xuxin Cheng*,1 Chaitanya Chawla*,2 Jialong Li 1 Tairan He 2 Ge Yan 4 David Yoon 3 Ryan Hoque 3 Lars Paulsen 1 Ge Yang 5 Jian Zhang 3 Sha Yi 1 Guanya Shi 2 Xiaolong Wang 1 1 UCSan Diego,2 CMU,3 Apple,4 Universityof Washington,5 MIT https://human-as-robot.github.io/ Egocentric Vision Unified State-Action Space Robot Policies Small-scale Humanoid Data 1.5 k demos Fingers / Wrist Large-scale Human Data 27 k demos Figure 1: This paper advocates high-quality human data as a data source for cross-embodiment learning-task-orientedegocentrichum and ata. Wecollectalarge-scale data set,Physical Human- Humanoid Data (PH 2 D), with hand-finger 3 D poses from consumer-grade VR devices on well- definedmanipulation task sdirectlyaligned with robots. Withoutrelyingonmodularperception,we train a Human Action Trans for mer (HAT) manipulation policy by directly modeling humans as a differenthumanoidembodimentinanend-to-endmanner. Abstract: Trainingmanipulationpolicies for humanoidrobots with diverse data enhances the irrobustnessandgeneralizationacrosstasks and platforms.However, learningsolely from robotdemonstrationsislabor-intensive,requiringexpensive tele-operated data collection, which is difficult to scale. This paper investigates amorescalable data source,egocentrichum and emonstrations,toserveascross- embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset (PH 2 D) that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Trans for mer (HAT). The state- action space of HAT is unified for both humans and humanoid robots and can bedifferentiablyretargetedtorobotactions. Co-trained with smaller-scalerobot data,HATdirectly model shumanoidrobots and humansasdifferentembodiments withoutadditionalsupervision. Weshow that hum and ataimprovebothgeneral- ization and robustnessof HAT with signifi can tlybetter data collectionefficiency. Keywords: Robot Manipulation,Cross-Embodiment,Humanoid 1 Introduction Learning from real robot demonstrations has led to great progress in robotic manipulation re- cently [1, 2, 3, 4]. One key advancement to enable such progress was hardw are / softw are co- designs to scale up data collection using teleoperation [5, 6, 7, 8, 9, 10] and directly controlling 9 th Conferenceon Robot Learning(Co RL 2025),Seoul,Korea. 5202 tc O 5 ]OR.sc[ 3 v 14431.3052:vi Xra the robot end effector [11, 12, 5, 6, 13, 7]. Instead of gathering data on a single robot, collective efforts have beenmadetomergediverserobot data and trainfoundationalpoliciesacrossembodi- ments[11,14,2,1,3,4],which have showntoimprovecross-embodiment and cross-taskgeneral- izability. However, collecting struc- tured real-robot data Human Robot dataset is expensive and time- #Frames #Demos #Frames #Demos consuming. We are still Dex Cap[15] \u223c378 k 787 NA NA far away from building a Ego Mimic[16] \u223c432 k\u2020 2,150 1.29 M\u2020 1,000 robust and generalizable PH 2 D(Ours) \u223c3.02 M 26,824 \u223c668 k 1,552 model as what has been achieved in Computer Table 1: Comparisons of task-oriented egocentric human Vision [17] and NLP [18]. datasets. Besides having the most demonstrations, PH 2 D is col- If we examine humanoid lected on various manipulation tasks, diverse objects and scenes, robot teleoperation more withaccurate 3 Dhand-fingerposes and languageannotations. \u2020: es- timated base donreported data collectiontime with 30 Hz; whereas closely, it involves robots Dex Cap[15]and PH 2 Dreportprocessedframes for training. mimicking human actions using geometric transforms or retargeting to control robot joints and end-effectors. From this perspective,weproposeto model robotsinahuman-centricrepresentation,and",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 255,
      "paper_id": "humanhumanoidph2d",
      "text": "scenes, robot teleoperation more withaccurate 3 Dhand-fingerposes and languageannotations. \u2020: es- timated base donreported data collectiontime with 30 Hz; whereas closely, it involves robots Dex Cap[15]and PH 2 Dreportprocessedframes for training. mimicking human actions using geometric transforms or retargeting to control robot joints and end-effectors. From this perspective,weproposeto model robotsinahuman-centricrepresentation,and the robotaction isjustatrans for mationaway from the humanaction. Ifwe canaccuratelycapture the end-effector and head poses of humans, egocentric human demonstrations will be a more scalable source of training data,aswe cancollect the mefficiently,inanyplace,and with outarobot. Inthispaper,weper for mcross-human and humanoidembodimenttraining for roboticmanipulation. Our key insight is to model bimanual humanoid behaviors by directly imitating human behaviors without using learning surrogates such as affordances [19, 20]. To realize this, we first collect an egocentric task-oriented dataset of Physical Humanoid-Human Data, dubbed PH 2 D. We adapt consumer-grade VRdevicestocollectegocentricvideos with automaticbutaccurateh and pose and end effector(i.e.,hand)annotations. Comp are dtoexistinghum and ailybehavior data sets[21,22], PH 2 D is task-oriented so that it can be directly used for co-training. The same VR hardwares are the nusedtoper for mteleoperationtocollectsmaller-scalehumanoid data for betteralignment. We then train a Human-humanoid Action Trans for mer (HAT), which predicts future hand-finger trajectories in a unified human-centric state-action representation space. To obtain robot actions, wesimplyapplyinversekinematicsandh and retargetingtodifferentiablyconverthumanactionsto robotactions for deployment. We conduct real-robot evaluations on different manipulation tasks with extensive ablation studies toinvestigatehowtobestalignhuman and humanoiddemonstrations. Inparticular,wefound that co-training with diverse human data improves robustness against spatial variance and background perturbation,generalizinginsettingsunseeninrobot data butseeninhum and ata. Webelieve that thesefindingshighlight the potentialofusinghum and ata for large-scalecross-embodimentlearning. Insummary,ourcontributions are: \u2022 Adataset,PH 2 D,whichisalargeegocentric,task-orientedhuman-humanoid data set with accurateh and and wristposes for modelinghumanbehavior(see Tab.1). \u2022 A cross human-humanoid manipulation policy, HAT, that introduces a unified state- actionspaceando the ralignmenttechniques for humanoidmanipulation. \u2022 Improvedpolicyrobustness and generalizationvalidatedbyextensiveexperiments and ablationstudiestoshow the benefitsofco-training with hum and ata. 2 Related Work Imitation Learning for Robot Manipulation. Recently, learningrobotpolicy with dataga the red directly from the multipleandtargetrobotembodimenthasshownimpressiverobustness and dex- 2 terity[23,2,24,1,25,26,9,27,28].Thescaleof data for imitationlearninghasgrownsubstantially withrecentadvancementsin data collection[29,9,7,8],wherehumanoperators can efficientlycol- lectlargeamountsofhigh-quality,task-oriented data.Despite the seadvances,achieving open-world generalizationstillremainsasignifi can tchallengeduetolackofinternet-scaletraining data. Learning from Human Videos. Learningpolicies from humanvideosisalong-standingtopicin bothcomputervision and roboticsdueto the vastexistenceofhum and ata. Existingworks can be approximatelydividedintotwocategories: aligningobservationsoractions. Learn from Human - Aligning Observations. While teleoperating the actual robot platform al- lows learning policy with great dexterity, there is still a long way to go to achieve higher levels of generalization across diverse tasks, environments, and platforms. Unlike fields such as com- puter vision [17] and natural language processing [18] benefiting from internet-scale data, robot datacollectionin the realworldisfarmoreconstrained. Variousapproaches have attemptedtouse internet-scale human videos to train robot policies [30, 31, 32, 33, 34, 35]. Due to various dis- crepancies (e.g., supervision and viewpoints) between egocentric robot views and internet videos, mostexistingwork[19,20]usemodularapproaches with intermediaterepresentationsassurrogates for training. The most representative ones are affordances [19, 20] for object interaction, object keypointspredictions[36,37,38,39,40],oro the rtypesofobjectrepresentations[41,42,43]. Learn from Human - Aligning Actions. Beyond observation alignment, transferring human demonstrationstoroboticplat for msintroducesadditionalchallengesduetodifferencesinembodi- ment,actuation,andcontroldynamics. Specificalignmentofhuman and robotactionsisrequiredto overcome the sedisparities. Approaches have employedmaskinginegocentricviews[16],aligning motiontrajectoriesorflow[44,45],object-centricactions[46,47],orh and tracking with specialized hardw are[15].Mostcloselyrelatedto our work,Human Plus[48]designs are mappingmethod from 3 Dhumanposeestimationtotele-operatehumanoidrobots. Comp are dto Human Plus, theinsight ofourmethodistowaive",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 256,
      "paper_id": "humanhumanoidph2d",
      "text": "object keypointspredictions[36,37,38,39,40],oro the rtypesofobjectrepresentations[41,42,43]. Learn from Human - Aligning Actions. Beyond observation alignment, transferring human demonstrationstoroboticplat for msintroducesadditionalchallengesduetodifferencesinembodi- ment,actuation,andcontroldynamics. Specificalignmentofhuman and robotactionsisrequiredto overcome the sedisparities. Approaches have employedmaskinginegocentricviews[16],aligning motiontrajectoriesorflow[44,45],object-centricactions[46,47],orh and tracking with specialized hardw are[15].Mostcloselyrelatedto our work,Human Plus[48]designs are mappingmethod from 3 Dhumanposeestimationtotele-operatehumanoidrobots. Comp are dto Human Plus, theinsight ofourmethodistowaive the requirement for robothardw are incollectinghuman data and collect diversehum and atadirectlyforco-training. Incontrastto Human Plus, weintentionallyavoidper- formingretargetingonhumandemonstrations and designed the policytodirectlyusehumanh and poses as states/actions. On the other hand, the \u2018human shadowing\u2019 retargeting in Human Plus is a teleoperationmethod that stillrequiresrobots,leadingtolowercollectionefficiencythanours. Cross-Embodiment. Cross-embodimentpre-traininghas been showntoimproveadaptability and generalizationoverdifferentembodiments[49,50,51,52,53,54,55,56,57,58,59,60,61]. When utilizinghumanvideos,introducingintermediaterepresentations can bepronetocompositeerrors. Recent works investigate end-to-end approaches [2, 24, 1, 3] using cross-embodied robot data to reducesuchcompoundingperceptiveerrors. Noticeably,theseworks have found that suchend-to- end learning leads to desired behaviors such as retrying [3]. Some other work [62, 38] enforces viewpoint constraints between training human demonstrations and test-time robot deployment to allowlearningonhum and atabutittradesoffthescalabilityof the datacollectionprocess. Concurrent Work. Some concurrent work [15, 16, 63] also attempts to use egocentric human demonstrations for end-to-endcross-embodimentpolicylearning. Dex Cap[15]usesglovestotrack 3 Dhandposes with achest-mounted RGBDcameratocaptureegocentrichumanvideos. However, Dex Cap relies on 3 D inputs, whereas some recent works [3, 1] have shown the scalability of 2 D visualinputs. Mostrelatedto our work,Ego Mimic[16]alsoproposestocollect data usingwearable device [64] with 2 D visual inputs. However, Ego Mimic requires strict visual sensor alignments; whereasweshow that scalingupdiverseobservations with differentcamerasmakes the policymore robust. In addition, PH 2 D is also greater in dataset scale and object diversity. We also show our policy can be deployed on real robots without strict requirements of visual sensors and heuristics, whichpaves the way for scalable data collection. 3 3 Method To collect more data to train generalizable robot policies, recent research has explored cross- embodimentlearning, enablingpoliciestogeneralizeacrossdiversephysicalforms[3,1,4,2,65, 14]. This paper proposes egocentric human manipulation demonstrations as a scalable source of cross-embodimenttraining data. Sec.3.1 describes our approachtoadaptconsumer-grade VRde- vicesto scale uphum and atacollectionconveniently for adatasetof task-orientedegocentrichuman demonstrations. Sec. 3.2 describesvarious techniques to handle domain gaps to align human data androbot data for learninghumanoidmanipulationpolicy. 3.1 PH 2 D:Task-oriented Physical Humanoid-Human Data Though there has been existing work that collects egocentric human videos [16, 22, 21, 15], they either (1) provide demonstrations mostly for non-task-oriented skills (e.g., dancing) and do not provideworld-frame 3 Dheadandh and posesestimations for imitationlearningsupervision[21,22] or(2)requirespecializedhardw are orrobotsetups[15,16]. To address these issues, we propose PH 2 D. PH 2 Daddress the setwoissuesby(1)collecting task-orientedhum and emonstrations that aredi- rectly related to robot execution, (2) adapting well-engineered SDKs of VR devices (illus- Camera / tratedin Fig.2)toprovidesupervision,and(3) Pose Tracking Camera diversifying tasks, camera sensors, and reduc- Figure 2: Consumer-grade Devices for Data Collec- ing whole-body movement to reduce domain tion.Toavoidrelyingonspecializedhardw are for data gapsinbothvision and behaviors. collectiontomake our methodscalable,wedesign our data collection process using consumer-grade VR de- Adapting Low-cost Commerical Devices vices. With development in pose estimation [66] and systemengineering,modernmobiledevices are capableofprovidingaccurateon-deviceworldframe 3 Dheadposetracking and 3 Dhandkeypointtracking[9],whichhasprovedtobestableenoughto teleoperaterobotinreal-time[9,13]. Wedesignsoftw are and hardw are tosupportconvenient data collectionacrossdifferentdevices. Differentcamerasprovidebettervisualdiversity. \u2022 Apple Vision Pro+Built-in Camera.Wedevelopeda Vision OSApp that uses the built-in camera for visualobservation and uses the Apple ARKit for 3 Dheadandh and poses. \u2022 Meta Quest 3/Apple",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 257,
      "paper_id": "humanhumanoidph2d",
      "text": "pose estimation [66] and systemengineering,modernmobiledevices are capableofprovidingaccurateon-deviceworldframe 3 Dheadposetracking and 3 Dhandkeypointtracking[9],whichhasprovedtobestableenoughto teleoperaterobotinreal-time[9,13]. Wedesignsoftw are and hardw are tosupportconvenient data collectionacrossdifferentdevices. Differentcamerasprovidebettervisualdiversity. \u2022 Apple Vision Pro+Built-in Camera.Wedevelopeda Vision OSApp that uses the built-in camera for visualobservation and uses the Apple ARKit for 3 Dheadandh and poses. \u2022 Meta Quest 3/Apple Vision Pro+ZEDCamera.Wedevelopedaweb-basedapplication based on Open Television [9] to gather 3 D head and hand poses. We also designed a 3 D- printedholdertomount ZEDMini Stereocamerason the sedevices. Thisconfigurationis bothlow-cost(<700$)andintroducesmorediversity with stereocameras. Data Collection Pipeline We collect task-oriented egocentric human demonstrations by asking human operators to perform tasks overlapping with robot execution (e.g., grasping and pouring) when wearing the VR devices. For every demonstration, we provide language instructions (e.g., graspa can ofcokezero with righth and),andsynchronizeproprioceptioninputs and visualinputs byclosesttimestamps. Action Domain Gap. Humanactions and tele-operatedrobotactionsexhibittwodistinctcharacter- istics:(1)humanmanipulationusuallyinvolvesinvoluntarywhole-bodymovement,and(2)humans aremoredexterousthanrobots and havesignifi can tlyfaster task completiontimethanrobots. We mitigatethefirstgapbyrequesting the hum and atacollectorstositinanuprightposition. Forthe secondspeedgap,weinterpolatetranslationandrotationsofhum and ataduringtraining(effectively \u2018slowingdown\u2019actions). Theslow-downfactors\u03b1 areobtainedbynormalizing the average task slow completiontimeofhumans and humanoids,whichisempiricallydistributedaround 4. Forconsis- tency,we use\u03b1 =4 inalltasks. slow 4 HAT \u2026 remrofsnar T \u2026 Robot Observation Robot Human Robot Data / Deployment Human Both Teleoperator Dino V 2\u2744 Humanoid 6 Do FWrist Pose 6 Do F Wrist Pose 3 D Hand 3 D Hand Keypoints Keypoints Dino V 2\u2744 Human Observation Action Prediction Unified Distribution Inverse Kinematics Head Pose Inverse Forward Kinematics Kinematics Retargeting Human Human Data Demonstration Figure 3: Overviewof HAT.Human Action Trans for mer(HAT)learnsarobotpolicyby model ing humans. Duringtraining,wesampleastate-actionpairfromei the rhum and ataorrobot data. The images are encoded by a frozen Dino V 2 encoder [67]. The HAT model makes predictions in a human-centric action space using wrist 6 Do F poses and finger tips, which is retargeted to robot posesduringreal-robotdeployment. 3.2 HAT:Human Action Trans for mer HAT learns cross-embodied robot policy by modeling humans. We demonstrate that treating bi- manualhumanoidrobots and humansasdifferentrobotembodimentsvi are targetingimprovesboth generalizability and robustnessof HAT. More concretely, let D = {(S ,A )}N be the set of data collected from real bimanual robot i i i=1 humanoid robots using teleoperation [9], where S is the states including proprioceptive and vi- i sual observations of i-th demonstration and A be the actions. The collected PH 2 D dataset, i D = {(S\u02dc ,A\u02dc )}M is used to augment the training process. Note that it is reasonable to human i i i=1 assume M \u226bN dueto the signifi can tlybetterhum and atacollectionefficiency. Thegoalistodesignapolicy\u03c0 : S \u2192 Athatpredictsfuturerobotactionsa givencurrentrobot t observations attimet, where the futureactionsa isusuallyachunkofactions for multi-step t t+1 execution (with slight abuse of notation). We model \u03c0 using HAT, which is a trans for mer-based architecture predicting action chunks [5]. The overview of the model is illustrated in Fig. 3. We discusskeydesignchoicesof HATwi the xperimentalablations. Unified State-Action Space. Both bimanual robots and humans have two end effectors. In our case,ourrobots are alsoequipped with anactuated 2 Do Fneck that can rotate,whichresembles the autonomous head movement when humans perform manipulation. Therefore, we design a unified state-actionspace(i.e., (S,A) \u2261 (S\u02dc,A\u02dc))forbothbimanualrobots and humans. Moreconcretely, theproprioceptiveobservationisa 54-dimensionalvector(6 Drotations[68]ofthehead,leftwrist, andrightwrist; x/y/zofleft and rightwrists and 10 fingertips). Inthiswork,sincewedeploy our policyonrobots with 5-fingereddexteroushands(shownin Fig.4),thereexistsabijectivemapping between the fingertipsofrobothands",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 258,
      "paper_id": "humanhumanoidph2d",
      "text": "case,ourrobots are alsoequipped with anactuated 2 Do Fneck that can rotate,whichresembles the autonomous head movement when humans perform manipulation. Therefore, we design a unified state-actionspace(i.e., (S,A) \u2261 (S\u02dc,A\u02dc))forbothbimanualrobots and humans. Moreconcretely, theproprioceptiveobservationisa 54-dimensionalvector(6 Drotations[68]ofthehead,leftwrist, andrightwrist; x/y/zofleft and rightwrists and 10 fingertips). Inthiswork,sincewedeploy our policyonrobots with 5-fingereddexteroushands(shownin Fig.4),thereexistsabijectivemapping between the fingertipsofrobothands and humanhands.Note that injectivemappingisalsopossible (e.g.,mappingdistancebetweenthethumbfingerando the rfingerstoparallelgripperdistance). Visual Domain Gap. Two types of domain gaps exist for co-training on human/humanoid data: camera sensors and end effector appearance. Since our human data collection process includes cameras different from robot deployment, this leads to camera domain gaps such as tones. Also, the appearances of human and humanoid end effectors are different. However, with sufficiently large and diverse data, wefinditnotastrictnecessitytoapplyheuristicprocessingsuchasvisual artifacts[16]orgenerativemethods[69]totrainhuman-robotpolicies-basicimageaugmentations suchascolorjittering and Gaussianblurring are effectiveregularization. 5 Passing Horizontal Grasp Vertical Grasp Pouring Ovr.Succ. Meth. H.Data D.Norm I.D. O.O.D. I.D. O.O.D. I.D. O.O.D. I.D. O.O.D. I.D. O.O.D. ACT \u2717 NA 19/20 36/60 8/10 7/30 7/20 15/70 8/10 1/10 42/60 59/170 HAT \u2713 \u2717 17/20 51/60 9/10 11/30 14/20 30/70 5/10 5/10 45/60 97/170 HAT \u2713 \u2713 20/20 52/60 8/10 12/30 13/20 29/70 8/10 8/10 49/60 101/170 Typeof Generalization Background Texture Obj.Placement Table 2: Success rate of autonomous skill execution. Co-training with human data (H. Data) signifi can tlyimproves the Out-Of-Distribution(O.O.D.)per for mance with nearly 100%relativeim- provementonall task son Humanoid A.Wealsoablate the designchoiceofusingdifferentnormal- izations(D.Norm)fordifferentembodiments. Wedesignateeach task settingtoinvestigateasingle typeofgeneralization. Detailedanalysisofeachtypeofgeneralizationispresentedin Sec.C. Training. Thefinalpolicyisdenotedas\u03c0 : f (\u00b7) \u2192 Aforbothhuman and robotpolicy,wheref \u03b8 \u03b8 isatrans for mer-basedneuralnetworkparametrizedby\u03b8. Thefinallossisgivenby, L=\u2113 (\u03c0(s ),a )+\u03bb\u00b7\u2113 (\u03c0(s ) ,a ), (1) 1 i i 1 i EEF i,EEF where EEF are the indices of the translation vectors of the left and right wrists, and \u03bb = 2 is an (insensitive) hyperparameter used to balance loss to emphasize the importance of end effector positionsoverlearningunnecessarilyprecisefingertipkeypoints. 4 Experiments Hardw are Platforms. We run our experi- ments on two humanoid robots (Humanoid A and Humanoid B shown in Fig. 4) equipped with 6-DOF Inspire dexterous hands. Hu- manoid Aisa Unitree H 1 robot and Humanoid B is a Unitree H 1 2 robot with different arm configurations. Similartohumans,bothrobots (1)areequipped with actuatednecks[9]toget (a) Humanoid A (b) Humanoid B make use of egocentric views and (2) do not have wrist cameras. Unless otherwise noted, Figure 4: Hardw are Illustration. Most robot data mosthumanoid data collectionisdone with Hu- attributes to Humanoid A, a Unitree H 1 robot. Hu- manoid B,a Unitree H 1-2 robot with differentarmmo- manoid A.we use Humanoid Bmainly for test- tor configurations, is used to evaluate few-shot cross- ingcross-humanoidgeneralization. humanoidtransfer.Detailedcomparisonsin Sec.D Implementation Details. Weimplementpolicyarchitecturebyadoptingantrans for mer-basedar- chitecture predicting future action chunks [5]. We use a frozen Dino V 2 Vi T-S [67] as the visual backbone. Weimplementtwovariants: (1)ACT:baselineimplementationusing the Action Chunk Trans for mer[5], trainedusingonlyrobot data. Robotstates are representedasjointpositions. (2) HAT: same architecture as ACT, but the state encoder operates in the unified state-action space. Unless otherwise stated, HAT is co-trained on robot and human data. A checkpoint is trained for each task with approximately 250-400 robotdemonstrations. Experimental Protocol. We collect robot and human demonstrations in different object sets. Sincehum and emonstrations are easiertocollect, thesettingsinhum and emonstrations",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 259,
      "paper_id": "humanhumanoidph2d",
      "text": "state encoder operates in the unified state-action space. Unless otherwise stated, HAT is co-trained on robot and human data. A checkpoint is trained for each task with approximately 250-400 robotdemonstrations. Experimental Protocol. We collect robot and human demonstrations in different object sets. Sincehum and emonstrations are easiertocollect, thesettingsinhum and emonstrations are gener- allymorediverse,whichincludebackground,objecttypes,objectpositions,and the relativeposition ofthehumanto the table. We experimented with four different dexterous manipulation tasks and investigated in-distribution and out-of-distribution setups. The in-distribution (I.D.) setting tests the learned skills with back- grounds and objectarrangementsapproximatelysimilarto the trainingdemonstrationspresentedin thereal-robot data. Inthe Out-Of-Distribution(O.O.D.)setting,wetestgeneralizability and robust- ness by introducing novel setups that were presented in human data but not in robot data. Fig. 7 visualizesdifferentmanipulationtasks and howwedefineout-of-distributionsettings for each task. 6 (a)Per for manceof Humanoid Bco-trained with PH 2 D (b) Co-training consistently outperforms isolated onhorizontalgrasping.o 1 isseenby Humanoid B.o 2 training as Humanoid B demonstrations increase, ando 3 seeninhum and ata.o 4 isunseeninall data. achievinggoodsuccessrateseveninlow-dat are gimes. Figure 5: Few-Shot Adaptation. Co-training consistently outperforms isolated training as Hu- manoid Bdemonstrationsincrease,achievingrobustsuccessrateseveninlow-dat are gimes. 4.1 Main Evaluation Human data has minor effects on I.D. testing. From Tab. 2, we can see that I.D. per for mance with or without co-training with human data gives similar results. In the I.D. setting, we closely match the scene setups as training demonstrations, including both background, object types, and objectplacements. Thus,policiestrained with onlyasmallamountof Humanoid Adataper for med wellin this setting. Thisfindingisconsistent with recentwork[9,7]thatfrozenvisualfoundation models[17,67]improverobustnessagainstcertainexternalperturbationssuchaslighting. Hum and ataimproves the O.O.D.settings with manygeneralizations. Onecommonchallenge inimitationlearningisoverfittingtoonlyin-distribution task settings.Hence,itiscrucial for arobot policy to generalize beyond the scene setups seen in a limited set of single-embodiment data. To demonstrate how co-training with human data reduces such overfitting, we introduce O.O.D. task settings to evaluate such generalization. From Tab. 2, we can see that co-training drastically im- proves O.O.D.settings,achievingnearly 100%relativeimprovementinsettingsunseenby the robot data. In particular, we find that human data improves three types of generalization: background, objectplacement, andappearance. Toisolate the effectofeachvariable, each task focusesona specifictypeofgeneralizationaslistedin Tab.2,within-depthanalysesin Sec.C. 4.2 Few-Shot Transferacross Heterogenous Embodiments Weconductedfew-shotgeneralizationexperimentsonadistincthumanoidplatform(Humanoid B), contrastingit with ourprimaryplatform,Humanoid A.Notably,Humanoid B\u2019sdemonstration data werecollectedinanentirelyseparateenvironment,introducingbo the mbodiment and environmental shifts. We highlight two key advantages of our approach: (1) the ability to unify heterogeneous human-centric data sources(humanoids and humans)intoageneralizablepolicyframework,and(2) thecapacitytorapidlyadaptto new embodiments with drasticallyreduceddat are quirements. Experiment 1: Cross-embodiment co-training efficacy Using only 20 demonstrations from Hu- manoid B, we trained 3 policies - respectively on data from (i) Humanoid B only, (ii) Humanoid B + Humanoid A (cross-embodiment), and (iii) Humanoid B + Humanoid A + Human (cross- embodiment and humanpriors). Asshownin Fig.5 a,co-trainedpolicies(ii)and(iii)substantially outper for med the Humanoid B-only base linesonall task settings,underscoring the method\u2019sability totransferlatent task structureacrossembodiments. Experiment 2: Scaling Demonstrations for Few-Shot Adaptation Wefurtherquantified the relation- shipbetweenrequired for few-shotgeneralization. Wehold Humanoid Aandhum and atasetsfixed 7 Robot Only \u201328/90 Co-Trained \u201335/90 Task State Space Action Speed Success \u2713 \u2717 1/10 Vertical Grasping \u2717 \u2713 0/10 \u2713 \u2713 4/10 Table 3: Importance of unifying policy inputs and out- Figure 6: Human data has better puts. Wereport the numberofsuccessesofverticalgrasp- sampling efficiency. Per-grid vertical ing objects in the upper-left block as illustrated in Fig. 8. graspingsuccessesoutof 10 trials with Baselinesusejointpositionsasstateinputordonotinter-",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 260,
      "paper_id": "humanhumanoidph2d",
      "text": "\u2713 \u2717 1/10 Vertical Grasping \u2717 \u2713 0/10 \u2713 \u2713 4/10 Table 3: Importance of unifying policy inputs and out- Figure 6: Human data has better puts. Wereport the numberofsuccessesofverticalgrasp- sampling efficiency. Per-grid vertical ing objects in the upper-left block as illustrated in Fig. 8. graspingsuccessesoutof 10 trials with Baselinesusejointpositionsasstateinputordonotinter- modelstrained with robot-only data and polatehumanmotions. mixed data. Red boxes indicate where training data iscollected. for the horizontal grasping task and ablate number of demonstrations required for Humanoid B in Fig. 5. Co-training (Humanoid B + A + Human) consistently outper for med isolated training on Humanoid Bacrossallsettings,especiallyin the few-dat are gime. 4.3 Ablation Study Sampling Efficiencyof Human and Humanoid Data. Conceptually,collectinghum and ataisless expensive, not just because it can be done faster, but also because it can be done in in-the-wild scenes;reducessetupcostbe for eevery data collection;andavoids the hardw are costtoequipevery operator with robots. Weper for madditionalexperimentstoshow that evenin the labsetting,hum and ata can havebetter samplingefficiencyinunittime. Inparticular,weprovideasmall-scaleexperimenton the vertical grasping task. Allocating 20 minutes for two settings, we collected (1) 60 Humanoid A demon- strations,(2)30 Humanoid Ademonstrations,and 120 hum and emonstrations. Toavoidconflating diversity and datasize,theobjectplacementsinalldemonstrations are evenlydistributedat the bot- tom 6 cells. The results are given in Fig. 6. The policy trained with mixed robot and human data per for mssignifi can tlybetter,whichvalidates the samplingefficiencyofhum and ataoverrobot data. Eachcellrepresentsa 10 cm\u00d710 cmregionwhere the robotattemptstopickupabox. State-Action Design. In Tab.3,weablatethedesignchoicesof the proprioceptionstatespace and the speed of output actions. In particular, using the same set of robot and human data, we imple- ment two baselines: 1) a unified state-action space, but does not interpolate (i.e., slow down) the humanactions;and 2)abaseline that interpolateshumanactionsbutusesseparatestaterepresenta- tion for humanoid(jointpositions)andhumans(EEFrepresentation). Thepoliciesexhibitdifferent failurepatternsduringtherolloutof the setwo base lines. Withoutinterpolatinghumanactions,the speed of the predicted actions fluctuates between fast (resembling humans) and slow (resembling teleoperation),whichleadstoinstability. Withoutaunifiedstatespace,thepolicyisgivena\u2018short- cut\u2019 to distinguish between embodiments, which leads to on-par in-distribution per for mance and signifi can tlyworse OODper for mance. More Ablation Study. Duetospacelimit,pleaserefertotheappendix and the supplementary for morequalitativevisualization and quantitativeablationstudies. 5 Conclusions Thispaperproposes PH 2 D,anef for ttoconstructalarge-scalehuman task-orientedbehavior data set, along with the trainingpipeline HAT,whichleverages PH 2 Dandrobot data toshowhowhumans can betreatedasa data source for cross-embodimentlearning.Weshow that itispossibletodirectlytrain animitationlearning model with mixedhuman-humanoid data with outanytrainingsurrogateswhen thehum and ata are aligned with the robot data. Thelearnedpolicyshowsimprovedgeneralization androbustnesscomp are dto the counterparttrainedusingonlyreal-robot data. 8 6 Limitations Althoughwealsocollectlanguageinstructionsin PH 2 D,dueto our focusoninvestigatingtheem- bodiment gap between humans and humanoids, one limitation of the current version of the paper uses a relatively simple architecture for learning policy. In the near future, we plan to expand the policylearningprocesstotrainalargelanguage-conditionedcross-embodimentpolicytoinvestigate generalizationtonovellanguageusinghum and emonstrations. Thecollectionofhum and atarelies onoff-the-shelf VRhardwares and the irh and tracking SDKs.Sincethese SDKs were trainedmostly for VR applications, hand keypoint tracking can fail for certain motions with heavy occlusion. In addition, though the proposed method conceptually extends to more robot morphologies, current evaluations are doneonrobotsequipped with dexteroushands. 7 Acknowledgment Thisworkwassupported,inpart,by NSFC ARE ERAward IIS-2240014,NSFCCF-2112665(TI- LOS),andgifts from Amazon,Meta and Apple. 9 References [1] S.Liu,L.Wu,B.Li,H.Tan,H.Chen,Z.Wang,K.Xu,H.Su,and J.Zhu. Rdt-1 b: adiffusion foundation model for bimanualmanipulation. ar Xivpreprintar Xiv:2410.07864,2024. [2] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J.",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 261,
      "paper_id": "humanhumanoidph2d",
      "text": "current evaluations are doneonrobotsequipped with dexteroushands. 7 Acknowledgment Thisworkwassupported,inpart,by NSFC ARE ERAward IIS-2240014,NSFCCF-2112665(TI- LOS),andgifts from Amazon,Meta and Apple. 9 References [1] S.Liu,L.Wu,B.Li,H.Tan,H.Chen,Z.Wang,K.Xu,H.Su,and J.Zhu. Rdt-1 b: adiffusion foundation model for bimanualmanipulation. ar Xivpreprintar Xiv:2410.07864,2024. [2] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, C. Xu, J. Luo, T. Kreiman, Y. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems,2024. [3] K.Black,N.Brown,D.Driess,A.Esmail,M.Equi,C.Finn,N.Fusai,L.Groom,K.Hausman, B. Ichter, et al. \u03c0 : A vision-language-action flow model for general robot control. ar Xiv 0 preprintar Xiv:2410.24164,2024. [4] S.Dasari,O.Mees,S.Zhao,M.K.Srirama,and S.Levine. Theingredients for roboticdiffu- siontrans for mers. ar Xivpreprintar Xiv:2410.10088,2024. [5] T.Z.Zhao, V.Kumar, S.Levine, and C.Finn. Learningfine-grainedbimanualmanipulation withlow-costhardw are. ar Xivpreprintar Xiv:2304.13705,2023. [6] Z.Fu, T.Z.Zhao, and C.Finn. Mobilealoha: Learningbimanualmobilemanipulation with low-costwhole-bodyteleoperation. ar Xivpreprintar Xiv:2401.02117,2024. [7] C.Chi,Z.Xu,C.Pan,E.Cousineau,B.Burchfiel,S.Feng,R.Tedrake,and S.Song. Universal manipulationinterface: In-the-wildrobotteaching with outin-the-wildrobots. ar Xivpreprint ar Xiv:2402.10329,2024. [8] S. Yang, M. Liu, Y. Qin, R. Ding, J. Li, X. Cheng, R. Yang, S. Yi, and X. Wang. Ace: A cross-plat for mvisual-exoskeletonssystem for low-costdexterousteleoperation.ar Xivpreprint ar Xiv:2408.11805,2024. [9] X.Cheng,J.Li,S.Yang,G.Yang,and X.Wang. Open-television: Teleoperation with immer- siveactivevisualfeedback. In Conferenceon Robot Learning(Co RL),2024. [10] T.He,Z.Luo,X.He,W.Xiao,C.Zhang,W.Zhang,K.Kitani,C.Liu,and G.Shi. Omnih 2 o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning. ar Xiv preprintar Xiv:2406.08858,2024. [11] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C.Finn. Robonet: Large-scalemulti-robotlearning. ar Xivpreprintar Xiv:1910.11215,2019. [12] H.Bharadhwaj,J.Vakil,M.Sharma,A.Gupta,S.Tulsiani,and V.Kumar. Roboagent: Gener- alizationandefficiencyinrobotmanipulationviasemanticaugmentations and actionchunking. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 4788\u2013 4795.IEEE,2024. [13] H.Ha,Y.Gao,Z.Fu,J.Tan,and S.Song. Umionlegs: Makingmanipulationpoliciesmobile withmanipulation-centricwhole-bodycontrollers. ar Xivpreprintar Xiv:2407.10353,2024. [14] A.O\u2019Neill,A.Rehman, A.Gupta, A.Maddukuri, A.Gupta, A.Padalkar, A.Lee, A.Pooley, A.Gupta,A.Mandlekar,etal.Openx-embodiment:Roboticlearning data setsandrt-xmodels. ar Xivpreprintar Xiv:2310.08864,2023. [15] C.Wang,H.Shi,W.Wang,R.Zhang,L.Fei-Fei,and C.K.Liu.Dexcap:Scalable and portable mocap data collection system for dexterous manipulation. ar Xiv preprint ar Xiv:2403.07788, 2024. [16] S. Kareer, D. Patel, R. Punamiya, P. Mathur, S. Cheng, C. Wang, J. Hoffman, and D. Xu. Egomimic: Scalingimitationlearningviaegocentricvideo. ar Xivpreprintar Xiv:2410.24221, 2024. URLhttps://arxiv.org/abs/2410.24221. 10 [17] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P.Mishkin,J.Clark,etal. Learningtransferablevisualmodels from naturallanguagesupervi- sion. In ICML.PMLR,2021. [18] Open AI. Gpt-4 technicalreport. Technicalreport,Open AI,2023. [19] R.Mendonca,S.Bahl,and D.Pathak. Structuredworldmodels from humanvideos. In RSS, 2023. [20] S.Bahl,R.Mendonca,L.Chen,U.Jain,and D.Pathak. Affordances from humanvideosasa versatilerepresentation for robotics. In CVPR,2023. [21] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar,J.Hamburger,H.Jiang, M.Liu,X.Liu,etal. Ego 4 d: Around the worldin 3,000 hoursofegocentricvideo. In CVPR, 2022. [22] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray. Scaling egocentric vision: The epic-kitchens dataset. In ECCV,2018. [23] T.Z.Zhao,J.Tompson,D.Driess,P.Florence,K.Ghasemip our,C.Finn,and A.Wahid.Aloha unleashed: Asimplerecipe for robotdexterity. ar Xivpreprintar Xiv:2410.13126,2024. [24] L.Wang,X.Chen,J.Zhao,and K.He. Scalingproprioceptive-visuallearning with heteroge- neouspre-trainedtrans for mers. ar Xivpreprintar Xiv:2409.20537,2024. [25] C.Chi,Z.Xu,S.Feng,E.Cousineau,Y.Du,B.Burchfiel,R.Tedrake,and S.Song. Diffusion policy:Visuomotorpolicylearningviaactiondiffusion. The International Journalof Robotics Research,page 02783649241273668,2023. [26] R.-Z.Qiu,Y.Song,X.Peng,S.A.Suryadevara,G.Yang,M.Liu,M.Ji,C.Jia,R.Yang,X.Zou, etal.Wildlma:Longhorizonloco-manipulationin the wild.ar Xivpreprintar Xiv:2411.15131, 2024. [27] C. Lu, X. Cheng, J. Li, S. Yang, M. Ji, C. Yuan, G. Yang, S. Yi, and X. Wang. Mobile- television: Predictivemotionpriors for humanoidwhole-bodycontrol. In ICRA,2025. [28] Y. Ze, Z. Chen, W. Wang, T. Chen, X. He, Y. Yuan, X.",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 262,
      "paper_id": "humanhumanoidph2d",
      "text": "02783649241273668,2023. [26] R.-Z.Qiu,Y.Song,X.Peng,S.A.Suryadevara,G.Yang,M.Liu,M.Ji,C.Jia,R.Yang,X.Zou, etal.Wildlma:Longhorizonloco-manipulationin the wild.ar Xivpreprintar Xiv:2411.15131, 2024. [27] C. Lu, X. Cheng, J. Li, S. Yang, M. Ji, C. Yuan, G. Yang, S. Yi, and X. Wang. Mobile- television: Predictivemotionpriors for humanoidwhole-bodycontrol. In ICRA,2025. [28] Y. Ze, Z. Chen, W. Wang, T. Chen, X. He, Y. Yuan, X. B. Peng, and J. Wu. Generalizable humanoidmanipulation with improved 3 ddiffusionpolicies.ar Xivpreprintar Xiv:2410.10803, 2024. [29] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto. Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation. In 2023 ieee international conferenceonrobotics and automation(icra),pages 5954\u20135961.IEEE,2023. [30] A.S. Chen, S. Nair, and C.Finn. Learninggeneralizable roboticreward functions from\u201d in- the-wild\u201dhumanvideos. ar Xivpreprintar Xiv:2103.16817,2021. [31] J. Lee and M. S. Ryoo. Learning robot activities from first-person human videos using con- volutionalfutureregression. In Proceedingsof the IEEEConferenceon Computer Vision and Pattern Recognition Workshops,pages 1\u20132,2017. [32] K. Lee, Y. Su, T.-K. Kim, and Y. Demiris. A syntactic approach to robot imitation learning usingprobabilisticactivitygrammars. Robotics and Autonomous Systems,61(12):1323\u20131334, 2013. [33] A. Nguyen, D. Kanoulas, L. Muratore, D. G. Caldwell, and N. G. Tsagarakis. Translating videos to commands for robotic manipulation with deep recurrent neural networks. In 2018 IEEEInternational Conferenceon Robotics and Automation(ICRA),pages 3782\u20133788.IEEE, 2018. 11 [34] J.Rothfuss,F.Ferreira,E.E.Aksoy,Y.Zhou,and T.Asfour. Deepepisodicmemory: Encod- ing,recalling,andpredictingepisodicexperiences for robotactionexecution. IEEERobotics and Automation Letters,3(4):4007\u20134014,2018. [35] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot learning manipulation action plans by\u201d watching\u201d unconstrained videos from the world wide web. In Proceedings of the AAAI conferenceonartificialintelligence,volume 29,2015. [36] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani. Track 2 act: Predicting point tracks frominternetvideosenablesdiversezero-shotrobotmanipulation. In ECCV,2024. [37] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,and P.Abbeel. Any-pointtrajectory model ing forpolicylearning. ar Xivpreprintar Xiv:2401.00025,2023. [38] J.Li,Y.Zhu,Y.Xie,Z.Jiang,M.Seo,G.Pavlakos,and Y.Zhu. Okami: Teachinghumanoid robots manipulation skills through single video imitation. ar Xiv preprint ar Xiv:2410.11792, 2024. [39] N. Das, S. Bechtle, T. Davchev, D. Jayaraman, A. Rai, and F. Meier. Model-based inverse rein for cementlearning from visualdemonstrations. In Conferenceon Robot Learning,pages 1930\u20131942.PMLR,2021. [40] H.Xiong, Q.Li, Y.-C.Chen, H.Bharadhwaj, S.Sinha, and A.Garg. Learningbywatching: Physicalimitationofmanipulationskills from humanvideos. In 2021 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems(IROS),pages 7827\u20137834.IEEE,2021. [41] S. Pirk, M. Khansari, Y. Bai, C. Lynch, and P. Sermanet. Online object representations with contrastivelearning. ar Xivpreprintar Xiv:1906.04312,2019. [42] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,and A.Gupta. R 3 m: Auniversalvisualrepresen- tation for robotmanipulation. ar Xivpreprintar Xiv:2203.12601,2022. [43] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. ar Xiv preprint ar Xiv:2210.00030,2022. [44] L.-H.Lin,Y.Cui,A.Xie,T.Hua,and D.Sadigh. Flowretrieval:Flow-guideddat are trieval for few-shotimitationlearning. ar Xivpreprintar Xiv:2408.16944,2024. [45] J. Ren, P. Sund are san, D. Sadigh, S. Choudhury, and J. Bohg. Motion tracks: A uni- fied representation for human-robot transfer in few-shot imitation learning. ar Xiv preprint ar Xiv:2501.06994,2025. [46] Y. Zhu, A. Lim, P. Stone, and Y. Zhu. Vision-based manipulation from single human video with open-worldobjectgraphs. ar Xivpreprintar Xiv:2405.20321,2024. [47] C.-C.Hsu,B.Wen,J.Xu,Y.Narang,X.Wang,Y.Zhu,J.Biswas,and S.Birchfield. Spot: Se (3)posetrajectorydiffusion for object-centricmanipulation.ar Xivpreprintar Xiv:2411.00965, 2024. [48] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn. Humanplus: Humanoid shadowing and imitation from humans. In Co RL,2024. [49] W. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular policies for",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 263,
      "paper_id": "humanhumanoidph2d",
      "text": "C.-C.Hsu,B.Wen,J.Xu,Y.Narang,X.Wang,Y.Zhu,J.Biswas,and S.Birchfield. Spot: Se (3)posetrajectorydiffusion for object-centricmanipulation.ar Xivpreprintar Xiv:2411.00965, 2024. [48] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn. Humanplus: Humanoid shadowing and imitation from humans. In Co RL,2024. [49] W. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular policies for agent-agnosticcontrol. In International Conferenceon Machine Learning,pages 4455\u20134464.PMLR,2020. [50] L. Y. Chen, K. Hari, K. Dharmarajan, C. Xu, Q. Vuong, and K. Goldberg. Mirage: Cross- embodimentzero-shotpolicytransfer with cross-painting. ar Xivpreprintar Xiv:2402.19249, 2024. 12 [51] J.Yang,C.Glossop,A.Bhorkar,D.Shah,Q.Vuong,C.Finn,D.Sadigh,and S.Levine. Push- ing the limitsofcross-embodimentlearning for manipulation and navigation. ar Xivpreprint ar Xiv:2402.19432,2024. [52] J.Yang,D.Sadigh,and C.Finn. Polybot: Trainingonepolicyacrossrobotswhileembracing variability. ar Xivpreprintar Xiv:2307.03719,2023. [53] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S.Levine. Bridge data: Boostinggeneralizationofroboticskills with cross-domain data sets. ar Xivpreprintar Xiv:2109.13396,2021. [54] T.Franzmeyer,P.Torr,and J.F.Henriques. Learnwhatmatters: cross-domainimitationlearn- ing with task-relevantembeddings. Advancesin Neural Information Processing Systems,35: 26283\u201326294,2022. [55] A.Ghadirzadeh,X.Chen,P.Poklukar,C.Finn,M.Bjo\u00a8rkman,and D.Kragic. Bayesianmeta- learning for few-shotpolicyadaptationacrossroboticplatforms. In 2021 IEEE/RSJInterna- tional Conferenceon Intelligent Robots and Systems(IROS),pages 1274\u20131280.IEEE,2021. [56] T.Shankar,Y.Lin,A.Rajeswaran,V.Kumar,S.Anderson,and J.Oh. Translatingrobotskills: Learning unsupervised skill correspondences across robots. In International Conference on Machine Learning,pages 19626\u201319644.PMLR,2022. [57] M.Xu,Z.Xu,C.Chi,M.Veloso,and S.Song. Xskill: Crossembodimentskilldiscovery. In Conferenceon Robot Learning,pages 3536\u20133555.PMLR,2023. [58] Z.-H.Yin,L.Sun,H.Ma,M.Tomizuka,and W.-J.Li. Crossdomainrobotimitationwithin- variantrepresentation. In 2022 International Conferenceon Robotics and Automation(ICRA), pages 455\u2013461.IEEE,2022. [59] K.Zakka,A.Zeng,P.Florence,J.Tompson,J.Bohg,and D.Dwibedi.Xirl:Cross-embodiment inverse rein for cement learning. In Conference on Robot Learning, pages 537\u2013546. PMLR, 2022. [60] G.Zhang,L.Zhong,Y.Lee,and J.J.Lim. Policytransferacrossvisual and dynamicsdomain gapsviaiterativegrounding. ar Xivpreprintar Xiv:2107.00339,2021. [61] Q.Zhang,T.Xiao,A.A.Efros,L.Pinto,and X.Wang.Learningcross-domaincorrespondence forcontrol with dynamicscycle-consistency. ar Xivpreprintar Xiv:2012.09811,2020. [62] S.Bahl,A.Gupta,and D.Pathak. Human-to-robotimitationin the wild. In RSS,2022. [63] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu,Y.Zhu,and A.Anandkumar.Mimicplay: Long-horizonimitationlearningbywatchinghumanplay. ar Xivpreprintar Xiv:2302.12422, 2023. [64] J.Engel,K.Somasundaram,M.Goesele,A.Sun,A.Gamino,A.Turner,A.Talattof,A.Yuan, B.Souti,B.Meredith,etal. Projectaria: Anewtool for egocentricmulti-modalairesearch. ar Xivpreprintar Xiv:2308.13561,2023. [65] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M.K.Srirama,L.Y.Chen,K.Ellis,etal. Droid: Alarge-scalein-the-wildrobotmanipulation dataset. ar Xivpreprintar Xiv:2403.12945,2024. [66] W. Zhu, X. Ma, Z. Liu, L. Liu, W. Wu, and Y. Wang. Motionbert: A unified perspective on learninghumanmotionrepresentations. In ICCV,2023. [67] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haz- iza,F.Massa,A.El-Nouby,etal.Dinov 2:Learningrobustvisualfeatures with outsupervision. ar Xivpreprintar Xiv:2304.07193,2023. 13 [68] Y.Zhou,C.Barnes,J.Lu,J.Yang,and H.Li. Onthecontinuityofrotationrepresentationsin neuralnetworks. In CVPR,2019. [69] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta, B.Ichter,etal. Scalingrobotlearning with semanticallyimaginedexperience. ar Xivpreprint ar Xiv:2302.11550,2023. 14 Paper Wooden Red Green #1 #2 Diff. Background (a)Therobotperforms the cuppassing task acrossf our differentbackgrounds. Theleftsideshows the four backgroundvariations,whiletherightsideillustrates the twopassingdirections: (#1-Righth and passes the cupto the lefth and,#2-Lefth and passesthecupto the righth and). Diff. Item #1 #2 #3 #4 #5 (b)Therobotperforms the horizontalgrasping task with fourdifferentitems: bottle,box 1,box 2,andcan, asshownon the left. Therightsideillustrates the process: (#1-#3-Therobotgrasps the bottle,#4-#5-The robotplacesitinto the plasticbin). Diff. Position #1 #2 #3 #4 #5 (c)Therobotperforms the verticalgrasping task. Asshownon the left,the Dynamixelboxisplacedinnine differentpositions for grasping.Therightsideillustrates the process:(#1-#3-Therobotgrasps the box,#4-#5 -Therobotplacestheboxinto the plasticbin). Diff. Setting #1 #2 #3 #4 #5 (d)Therobotperforms the pouring task.Theleftsideshowsdifferentsettingsachievedbyvarying the robot\u2019s rotation and the table\u2019s position. The right side illustrates the pouring process: (#1 - Right hand grasps the bottle,#2-Lefth and grasps the cup,#3-Pouring the drink,#4-Lefth and places the cupdown,#5-Right handplaces the bottledown). Figure 7: Illustrations of tasks used in quantitative evaluations. From top to bottom: cup passing, horizontalgrasping,verticalgrasping,andp our ing. Bottle Box Box Can Method 1",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 264,
      "paper_id": "humanhumanoidph2d",
      "text": "right side illustrates the pouring process: (#1 - Right hand grasps the bottle,#2-Lefth and grasps the cup,#3-Pouring the drink,#4-Lefth and places the cupdown,#5-Right handplaces the bottledown). Figure 7: Illustrations of tasks used in quantitative evaluations. From top to bottom: cup passing, horizontalgrasping,verticalgrasping,andp our ing. Bottle Box Box Can Method 1 2 Ovr. Succ. I.D. H.D. H.D. H.D. Withoutwhole-body 8/10 6/10 0/10 7/10 21/40 Withwhole-body 9/10 3/10 3/10 3/10 18/40 Table 4: Ablation of how human whole-body movement in training demonstrations affects policy rollout. We collect the same number of demonstrations on the same set of objects for the grasping task withor with outwhole-bodymovement.Since the robotdoesnot have anaturalwhole- bodymovementlikehumans,itnegativelyinfluences the manipulationsuccessrate. A More Ablation Study-Data Collection Autonomous Whole-body Movement. In Tab.4,wejustify the necessitytominimizebodymove- mentinhum and atacollection.Humanstendtomove the irupperbodyunconsciouslyduringmanip- ulation(includingshoulder and waistmovement). However, existinghumanoidrobots have yetto reachsuchalevelofdexterity.Thus,having the sedifficult-to-replicateactionsin the hum and emon- strations leads to degraded per for mance. We hypo the size that such a necessity would be greatly reduced with the development of both whole-body locomotion methods and mechanical designs, 15 Method Grasping(secs) Pouring(secs) Human Demo 3.79\u00b10.27 4.81\u00b10.35 Human Demo with VR 4.09\u00b10.30 4.90\u00b10.26 Humanoid Demo(VRTeleop) 19.72\u00b11.65 37.31\u00b16.25 Table 5:Amortizedmeanandst and arddeviationof the timerequiredtocollectasingledemon- stration, including scene resets. The first row shows the time for regular human to complete cor- responding tasks in real world. The second row represents our human data when wearing VR for datacollection, demonstrating that egocentrichum and emonstrationsprovideamorescalable data sourcecomp are dtorobotteleoperation. butfor the currentlyavailableplatforms,weinstructoperatorstominimizebodymovementasmuch aspossiblein our data set. Efficiencyof Data Collection. In Tab.5,wecomp are taskcompletiontimesacrossdifferentsetups, includingst and ardhumanmanipulation,hum and emonstrationsper for medwhilewearinga VRde- vice,androbotteleoperation. Thisanalysishighlightshow task-orientedhum and emonstrations can be a scalable data source for cross-embodiment learning. Notably, wearing a VR device does not signifi can tlyimpacthumanmanipulationspeed,asthecompletiontimeremainsnearly the sameas instandardhum and emonstrations. Amongdifferent data collectionschemes, wefind that mostoverheadarisesduring the retargeting process from human actions to robot actions. This is primarily due to latency and the constrained workspaceof 7-Do Froboticarms,which are inherentchallengesinexisting data collectionmethods suchas VRteleoperation[9],motiontracking[48,10],andpuppeting[8,5]. Beyond data collectionspeed,hum and emonstrationsofferseveraladditionaladvantagesovertele- operation. They provide a safer alternative, reducing risks associated with real-robot execution. They are alsomorelabor-efficient,astheydonotrequireadditionalpersonnel for supervision. Fur- thermore, human demonstrations allow for greater flexibility in settings, enabling a diverse range ofenvironments with outrequiringrobot-specificadaptations. Additionally,hum and emonstrations achieveahigherdemonstrationsuccessrate,and the requiredhardw are(suchasmotioncaptureor VR devices) is more accessible and cost-effective compared to full robotic setups. These factors collectivelymakehum and ataamorescalablesolution for large-scale data collection. B Normalizationofdifferentembodiments. Tab. 2 suggests minor differences between using different normalization coefficients for the states andactionsvectorsofhumans and humanoids. Wetakeacloserlookin Fig.8,whereweinvestigate theimpactofdifferentnormalizationstrategiesin the verticalgrasping(picking)task. Noticeably, thesamenormalizationapproachachieved the highestoverallsuccessrate,but the successdistribu- tionisbiasedtowards the upper-rightregionof the grid. Wehypo the size that thisisbecausehumans have alargerworkspacethanhumanoidrobots. Thus, hum and ataencompasseshumanoidproprioceptionasasubset,whichresultsin are lativelysmaller distribution for the robotstate-actionspace. C In-Depth Analysisof Different Typesof Generalization Hum and ataimprovesbackgroundgeneralization. Wechosetouse the cuppassing task totest background generalization. We prepared four different tablecloths as backgrounds, as shown in Fig. 7 a. In terms of training data distribution, the teleoperation data for this task was collected exclusivelyon the paperbackgroundshownin Fig.7 a,whereas the hum and ataincludesmorethan five different backgrounds. This diverse human dataset signifi can tly enhances the generalization ability of the co-trained HAT policy. As shown",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 265,
      "paper_id": "humanhumanoidph2d",
      "text": "as shown in Fig. 7 a. In terms of training data distribution, the teleoperation data for this task was collected exclusivelyon the paperbackgroundshownin Fig.7 a,whereas the hum and ataincludesmorethan five different backgrounds. This diverse human dataset signifi can tly enhances the generalization ability of the co-trained HAT policy. As shown in Tab. 7. , HAT consistently outperforms across all four backgrounds, demonstrating robustness to background variations. In addition, the overall 16 Bottle Box Box Can Method 1 2 Ovr. Succ. I.D. H.D. O.O.D. O.O.D. ACT 8/10 5/10 1/10 1/10 16/40 HAT 8/10 7/10 1/10 4/10 21/40 Table 6: Object Appearance Generalization: In the horizontal grasping task, we evaluated the graspingper for mancebyattemptingtograspeachobject 10 times and recorded the successrate. success rate increases by nearly 50% compared to training without human data, highlighting the advantageofutilizingdiversehum and emonstrations. Hum and ataimprovesappearancegeneralization. Totesthowco-trainingimprovesrobustness toperturbationsinobjecttextures, weevaluate the horizontalgraspingpolicyonnovelobjects, as shown in Fig. 7 b. Specifically, we comp are the policy\u2019s per for mance on the bottle, box 1, box 2, andcan,asshownlefttorightin the firstimagein Fig.7 b. Theseobjectsdiffersignifi can tlyinboth color and shape from thebottleusedin the teleoperation data distribution. Sincegraspingis are lativelysimple task,ouradjustedpolicydemonstratesstronglearningcapabil- itieseven with only 50 teleoperation data samples. Thepolicy can success full ygraspmostbottles despite the limitedtrainingset.Tobetterhighlight the impactofhum and ata,weselectedmorechal- lengingobjects for evaluation. Asshownin Tab.6,hum and atasignifi can tlyenhances the policy\u2019s abilitytograsp the semoredifficultobjects. Notably,box 1 appearsin the hum and ata,whilebox 2 doesnot. Despite this,weobservethatco- training with hum and atastillimprovesoverallper for mance,evenonbox 2,thoughitssuccessrate doesnotincrease.Thissuggests that,beyonddirectexperience with specificobjects,thehum and ata helps the policylearnbroadervisualpriors that enablemoreproactive and stablegraspingbehaviors. Forbox 2,while the successrateremainslow\u2014partiallyduetoitslowheight and colorsimilarity tothetable\u2014theco-trained HATpolicydemonstratesfewerout-of-distribution(OOD)failures and more actively searches for graspable regions. The failures on box 2 are primarily due to unstable grasping and thesmallboxslipping from the hand,ratherthan the inabilitytoperceiveorlocate the object. Fur the rmore, adding more human data not only improves per for mance on objects seen in human training demonstrations (e.g., box 1) but also enhances generalization to completely novel objects (e.g., box 2 and can). We hypo the size that, as the number of objects grows, HAT starts to learn inter-categoryvisualpriors that guideittograspobjectsmoreeffectively,evenwhen the ywerenot explicitlypresentin the trainingset. Hum and ataimprovesobjectplacementgeneralization.Finally,weintroducevariationsinobject placements that are notpresentin the real-robottrainingdemonstrations and specificallyinvestigate this in the vertical grasping (picking) task. In this task, we intentionally constrain the robot data collectiontoobjectplacements with inasubsetofcells,whilehumanverticalgrasping data coversa muchmorediverserangeofsettings. Tosystematicallyanalyze the impactofhum and ata,weevaluate model per for manceonastructured 3\u00d73 grid,whereeachcellrepresentsa 10 cm\u00d710 cmregion for graspingattempts. Thenumbersin eachcellindicate the numberofsuccessfulpicksoutof 10 trials.Real-robottraining data iscollected fromonlytwospecificcells,highlighted with dashedlines. Akeydetailin our teleoperation data distributionis that 50 pickingattempts are collected from the right-handsidegrid and only 10 from the left-handsidegrid. Thisimbalanceexplainswhypolicies trainedpurelyonteleoperation data struggletograspobjectsin the left-sidegrid. Weobserve that models trained solely on robot data fail to generalize to unseen cells, whereas cross-embodiment learning with hum and atasignifi can tlyimprovesgeneralization,doubling the overallsuccessrate. 17 ACT HAT (diff. norm) HAT (same norm) Figure 8:Object Placement Generalization.Per for mancecomparisonsof model strained with and without human data on vertical grasping (picking). Each cell in the 3\u00d73 grid represents a 10 cm \u00d7 10 cmregionwhere the robotattemptstopickupabox,withnumbersindicatingsuccessfulattempts outof 10. Thereal-robot data iscollectedintwocellsinside the dashedlines. Notably,ourteleop- eration data isintentionallyimbalanced. Paper Wooden Red Green Method Ovr.",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 266,
      "paper_id": "humanhumanoidph2d",
      "text": "norm) Figure 8:Object Placement Generalization.Per for mancecomparisonsof model strained with and without human data on vertical grasping (picking). Each cell in the 3\u00d73 grid represents a 10 cm \u00d7 10 cmregionwhere the robotattemptstopickupabox,withnumbersindicatingsuccessfulattempts outof 10. Thereal-robot data iscollectedintwocellsinside the dashedlines. Notably,ourteleop- eration data isintentionallyimbalanced. Paper Wooden Red Green Method Ovr. Succ. I.D. H.D. O.O.D. O.O.D. ACT 19/20 14/20 12/20 10/20 55/80 HAT 20/20 16/20 18/20 18/20 72/80 Table 7:Background Generalization:Inthecuppassing task,weevaluate the passingper for mance byrecording the numberoffailuresorretriesneededtocomplete 20 cup-passingtrials. D In-Depth Comparisonbetween Humanoid Aand Humanoid B configurations Thissectionpresentsadetailedcomparisonof the twohumanoidplatforms,referredtoas Humanoid Aand Humanoid B,withafocusonjointstructure and implications for manipulationcapabilities. We restrict our analysis to the arm configurations, as other parts of the body were not exclusively exploredin this work. While morphologically similar, these two humanoids have drastically different arm configura- tions that create hurdles in direct policy transfer. Besides differences in motor technical specs such as torque and types of encoder (Humanoid B has absolute motor position encoders), they also have different mechanical limits. The range of motion (ROM) for the first four proximal joints\u2014shoulder pitch, shoulder roll, shoulder yaw, and elbow\u2014differs across the two platforms. Humanoid Bexhibitsaconsistentlywider ROM,whichallowsawidersetofreachableconfigura- tions and increasesthemanipulabilityof the arminconstrainedenvironments. Table 8 summarizes the ROMvalues for the sesh are djoints. Amoresignifi can tarchitecturaldivergenceisobservedat the wrist. Humanoid Aincludesasingle distal joint\u2014wrist roll\u2014providing limited wrist articulation. This restricts end-effector dexterity andconstrainsin-handmanipulationstrategiestoasinglerotationaldegreeoffreedom. Incontrast, Humanoid Bisequipped with acompletewristmechanismcomposedofthreeindependentlyactu- atedjoints:wrist pitch,wrist roll,andwrist yaw.Theseadditionaldegreesoffreedomallow for full orientation control of the end-effector, enabling tasks that require precise alignment, rotation, and fineadjustmentofobjectposes. 18 Joint Humanoid A Humanoid B shoulder pitch \u2212164\u25e6to+164\u25e6 \u2212180\u25e6to+90\u25e6 shoulder roll \u221219\u25e6to+178\u25e6 \u221221\u25e6to+194\u25e6 shoulder yaw \u221274\u25e6to+255\u25e6 \u2212152\u25e6to+172\u25e6 elbow \u221271\u25e6to 150\u25e6 \u221254\u25e6to 182\u25e6 wrist roll \u2212175\u25e6to 175\u25e6 \u2212172\u25e6to 157\u25e6 Table 8: Joint Rangeof Motion Comparisonbetween Humanoid Aand B(indegrees) 19",
      "start_pos": 5544,
      "end_pos": 5830
    },
    {
      "chunk_id": 267,
      "paper_id": "GSLTS",
      "text": "GS-LTS: 3 D Gaussian Splatting-Based Adaptive Modeling for Long-Term Service Robots Bin Fu 1 and Jialin Li 1 and Bin Zhang 1 and Ruiping Wang 1,(cid:12) and Xilin Chen 1 Abstract\u20143 D Gaussian Splatting (3 DGS) has garnered sig- nificant attention in robotics for its explicit, high fidelity dense scene representation, demonstrating strong potential for roboticapplications.However,3 DGS-basedmethodsinrobotics primarily focus on static scenes, with limited attention to the dynamic scene changes essential for long-term service robots. These robots demand sustained task execution and efficient scene updates\u2014challenges current approaches fail to meet. To address these limitations, we propose GS-LTS (Gaussian Splatting for Long-Term Service), a 3 DGS-based system en- abling indoor robots to manage diverse tasks in dynamic environments over time. GS-LTS detects scene changes (e.g., object addition or removal) via single-image change detection, employs a rule-based policy to autonomously collect multi- Addition Removal Relocation view observations, and efficiently updates the scene represen- tation through Gaussian editing. Additionally, we propose a Fig.1. Threecommontypesofscenechangesinindoorscenes. simulation-basedbenchmark that automaticallygeneratesscene objects may be added, removed, or relocated over time. In change data ascompactconfigurationscripts,providingastan- dardized, user-friendly evaluation benchmark. Experimental such environments, the robot must continuously observe the results demonstrate GS-LTS\u2019s advantages in reconstruction, scene, autonomously detect changes, and update its scene navigation, and superior scene updates\u2014faster and higher representation to maintain accuracy. quality than the image training baseline\u2014advancing 3 DGS A straight for ward approach to handling scene changes for long-term robotic operations. Code and benchmark are available at: https://vipl-vsu.github.io/3 DGS-LTS would be to periodically recollect images and retrain or fine-tune the 3 DGSrepresentationwhenever the environment I. INTRODUCTION is modified. However, this method is computationally ex- 3 D Gaussian Splatting (3 DGS) [1] is an explicit radiance pensive, requiring frequent reprocessing of large-scale data, field representation based on 3 D Gaussians. It has been and lacks efficiency for real-time or long-term deployment. widely applied in fields such as dense visual SLAM [2] and To address this task, we propose GS-LTS, a 3 DGS-based 3 D reconstruction [3], benefiting from its explicit geometric system designed for Long-Term Service robots in indoor structure and real-time high-quality rendering. By further environments. The GS-LTS framework integrates four key embedding low-dimensional vision-semantic features into modules: (1) Gaussian Mapping Engine, which constructs each 3 DGaussian[4],acomprehensivescenerepresentation asemantic-aware 3 DGSrepresentation,integratinggeometry, integrating geometry, vision, andsemantics can be achieved, visualappearance,andsemantics;(2)Multi-Task Executor, which shows great potential in robotics applications, such which helps robots perform downstream tasks like object as navigation and instruction following. However, current navigation using the informative 3 DGS representation; (3) 3 DGSattemptsin the sefieldsprimarilyfocusonstaticscenes Change Detection Unit, a long-running module that detects [5], which fail to align with the dynamic nature of real- scene changes at a specified frequency by comparing the world environments involving object changes, as illustrated robot\u2019s current RGB observations with historical 3 DGS- in Fig. 1, making these approaches inadequate for long- rendered images, locating altered regions and analyzing term service robots working in dynamic settings. A more change types and positions; and (4) Active Scene Updater, realistic scenario involves a robot utilizing a prebuilt 3 DGS which is guided by a rule-based",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 268,
      "paper_id": "GSLTS",
      "text": "in Fig. 1, making these approaches inadequate for long- rendered images, locating altered regions and analyzing term service robots working in dynamic settings. A more change types and positions; and (4) Active Scene Updater, realistic scenario involves a robot utilizing a prebuilt 3 DGS which is guided by a rule-based policy, directs the robot to representation to perform tasks in an environment where collectmulti-viewimagesarounddetectedareas,andapplies pre-editing and fine-tuning to dynamically update the 3 DGS *This work is partially supported by National Key R&D Program of representation based on the detect change type and new China No.2021 ZD 0111901,and Natural Science Foundationof Chinaunder observations. Together, these components enable the robot contracts Nos.62495082,U 21 B 2025. 1 The authors are with the Key Laboratory of AI Safety of CAS, to adapt to evolving surroundings while maintaining robust Instituteof Computing Technology,Chinese Academyof Sciences(CAS), per for mance over extended periods. Beijing,100190,China,andalso with the Universityof Chinese Academy Evaluating this system places significant demands on of Sciences, Beijing, 100049, China. {bin.fu, jialin.li, bin.zhang}@vipl.ict.ac.cn, {wangruiping, both data availability and environmental support. Real- xlchen}@ict.ac.cn world settings struggle to support both robotic task execu- (cid:12)Correspondingauthor. tion and extensive variations, hindering large-scale dataset 5202 ra M 22 ]OR.sc[ 1 v 33771.3052:vi Xra creation and standardized evaluation. To address this, we Gaussians [11]. This adaptability makes 3 DGS suited for propose a simulation-based benchmark that supports task modeling dynamic environments. Leveraging these proper- execution and policy learning via 3 DGS representations ties, this work explores the integration of 3 DGS with long- while enabling systematic generation of large-scale scene term service robot systems operating in dynamic settings. change data throughobjectinteractions.Thisbenchmarknot only facilitates large-scale evaluation but also serves as a B. Long-Term Robot Autonomy and Change Detection bridge for sim-to-real transfer, allowing models trained in Long-Term Autonomy (LTA) is a critical research area simulation to achieve enhanced per for mance in real-world in robotics, aimed at enabling robots to operate reliably environments. Our approach features two innovations: (1) in complex environments over extended periods [12]. This automated generation of customizable scene change data, capability is essential across various domains, including combining objects (e.g., cups), containers (e.g., tables), and underwater exploration [13], and service robotics [14]. A positions to produce diverse scene change tasks; and (2) major challenge in LTA is adapting to scene changes. storing scene change setups and environment meta data in Our work focuses on medium-term changes [12] in indoor configuration scripts, which ensures efficient storage, easy service environments, where robots must effectively model configuration,andaccuratereproductionofscenes.Thisscal- and update representations of daily object variations. While able, reproducible benchmark reduces data acquisition costs many LTA robotic systems have been deployed in service and provides standardized evaluation, advancing research on scenarios [14], [15], our work introduces a novel approach 3 DGS adaptability in dynamic environments. leveraging 3 DGS for scene representation to enable efficient We conduct extensive validation of the GS-LTS system adaptation to dynamic environments. through a series of experiments. First, we evaluate scene Scenechangedetectionisakeyresearch are aincomputer representation quality via image rendering for visual fidelity vision, aiming to identify scene changes such as object and 3",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 269,
      "paper_id": "GSLTS",
      "text": "DGS for scene representation to enable efficient We conduct extensive validation of the GS-LTS system adaptation to dynamic environments. through a series of experiments. First, we evaluate scene Scenechangedetectionisakeyresearch are aincomputer representation quality via image rendering for visual fidelity vision, aiming to identify scene changes such as object and 3 D localization for semantic accuracy. Additionally, ob- appearance, disappearance, or modifications. It is broadly jectnavigationresultsonanexistingbenchmark[6]highlight classified into 2 D and 3 D approaches based on data type. the potential of 3 DGS for embodied tasks. Finally, on our 2 D change detection employs pairs of before-and-after RGB custom Scene Change Adaptation Benchmark, we comp are images [16], leveraging models from CNNs to foundation our Gaussian editing-based method with the baseline of models for feature extraction and change identification. direct image fine-tuning. Our approach signifi can tly reduces Conversely, 3 D change detection incorporates spatial infor- scene update time while enhancing update quality. These mation, relying on multi-view RGB images [17] or point comprehensive experiments fully demonstrate the efficiency clouds [18]. Recent advances in 3 DGS-based novel-view and robustness of the GS-LTS system in scene reconstruc- syn the sis [19] have demonstrated strong potential, whereas tion, embodied applications, and scene adaptability. our GS-LTS system adopts a distinct approach, leveraging a Insummary,thisworkintroduces GS-LTS,deliveringthree single egocentric RGB image for change detection to reduce key contributions: data and computational demands. \u2022 A 3 DGS-basedsystemenablingindoorrobotstohandle diverse tasks in dynamic environments over time. III. SYSTEMOVERVIEW \u2022 An automatic framework for object-level change detec- In this section, we first introduce the task formulation tion and adaptive scene update via Gaussian editing. for long-term service robot system working in dynamic \u2022 A scalable method for constructing a simulation bench- environments base don 3 DGaussian Splatting(3 DGS)scene mark for object-level scene change detection. representation. Subsequently, we present the overall frame- Together, these advancements enhance 3 DGS applications work of our proposed system designed to address this task. for long-term robotic operations in dynamic environments. II. RELATEDWORK A. Task Formulation A. 3 D Scene Representation The core objectives of the task are twofold: (1) in a Building accurate scene representations is crucial for dynamic environment, construct a 3 DGS representation of robotics, with various methods, such as semantic maps [7], the scene and utilize this representation to control the robot SLAM [8], and Ne RF [9] being widely used. Compared inaccomplishingdownstreamembodiedtasks,suchasobject to these representations, 3 D Gaussian Splatting (3 DGS) navigation; (2) enable the robot to detect object changes in provides an explicit, high-fidelity, and real-time renderable thescene and autonomouslycollect data toupdate the 3 DGS dense representation. Its ability to simultaneously encode representation. geometric, visual, and semantic information has driven its We focus on dynamic indoor settings where primary adoptionin task ssuchas 3 Dreconstruction[3],3 DGS-based structures (e.g., room layouts, large furniture like cabinets SLAM [2], and navigation [10]. However, most existing andrefrigerators)staystatic,whilecertainobjects(e.g.,cups, applications are restricted to static environments [5], where laptops)exhibitperiodicchanges.Weconsiderthreetypesof maps quickly become outdated in the face of scene changes. scene changes: relocation, addition, or removal of objects, Akeyadvantageof 3 DGSisitsinherentlyeditablenature, whichencompass the predominant for msofobjectdynamics enabling dynamic updates through direct",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 270,
      "paper_id": "GSLTS",
      "text": "cabinets SLAM [2], and navigation [10]. However, most existing andrefrigerators)staystatic,whilecertainobjects(e.g.,cups, applications are restricted to static environments [5], where laptops)exhibitperiodicchanges.Weconsiderthreetypesof maps quickly become outdated in the face of scene changes. scene changes: relocation, addition, or removal of objects, Akeyadvantageof 3 DGSisitsinherentlyeditablenature, whichencompass the predominant for msofobjectdynamics enabling dynamic updates through direct modifications of in real world environments. Gaussian Mapping Engine Multi-Task Executor Semantic Map ((\ud835\udc36+2)\u00d7\ud835\udc40\u00d7\ud835\udc40) SAM CLIP obstacles Segmentation category-wise binary grids Object Nav. Autoencoder Encoder Decoder RGB-D Position Feature 3 D Gaussians \ud835\udc60 \ud835\udf19!\"#,\ud835\udf19$! Query: Pose GS-LTS Coffee Machine 3 D Localization Change Detection Unit Long-Term Active Scene Updater Adaptive Modeling Pixel Diff. Original Egocentric Gaussians Removal RGB obs. Scene Change Mask ? Addition 3 DGS Change Type Relocation rendering Change Position Feature Diff. Active Data Collecting Fig. 2. System Overview. GS-LTS is a modular system designed for long-term service robots, which can adapt to object changes in the dynamic environments and update the 3 DGSrepresentationthroughperiodic,automatedoperationof the Change Detection Unit and the Active Scene Updater. During task execution, the robot can only access current methods such as object navigation. RGB-Ddata and poses from the environment.Consequently, Change Detection Unit. Since the robot must au- the robot must rely on single egocentric observations to tonomously detect scene changes relying solely on single actively perform change detection and determine whether egocentric observations, we develop the Change Detection objects in the scene have changed. Upon detecting scene Unit, a lightweight module designed for long-term standby alterations, the robot is further required to autonomously running in parallel with other downstream embodied tasks. collect data to update the 3 DGS scene representation. This module compares the robot\u2019s current RGB observation with a rendered image generated from the 3 DGS at the B. GS-LTS System corresponding pose. By employing a dual-branch strategy To address the aforementioned challenges, we propose that analyzes both pixel-level differences and feature-level GS-LTS, a 3 DGS-based system tailored for Long-Term differences, the module effectively identifies the type and Servicerobotsoperatinginindoorenvironments.Thesystem location of changes between the two compared frames. integrates four key modules as shown in Fig. 2. In the fol- Active Scene Updater. Upon detecting scene changes lowing, we provide an overview of the system\u2019s operational and their locations, the Active Scene Updater module au- workflow,withdetailsofeachmoduleelaboratedin Sec.IV. tonomously collects data and updates the long-term scene Gaussian Mapping Engine. This module is tasked with representation.Initially,therobotfollowsarule-basedheuris- generating the 3 DGS scene representation for the robot tic policy to navigate around the scene change region, during the system\u2019sinitializationphase.Givenasetofmulti- capturing multi-view images. Next, it applies 3 DGS editing view RGB images, depth maps, and their corresponding strategies to edit the target region. Finally, the 3 DGS repre- camera poses, the module trains a 3 DGS model to effec- sentationisrefinedbyfine-tuning with collectedimages.This tively capture the scene\u2019s geometric and visual character- moduleenables GS-LTStoper for msceneupdatesefficiently istics. In addition, to incorporate semantic information, we with minimal computational overhead. leverage the Segment Anything Model (SAM) [20] and IV. METHODOLOGY open-vocabulary vision-language models (e.g., CLIP [21]) to embed semantic features into the 3 DGS representation. In this section, we introduce the implementation and Multi-Task Executor.",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 271,
      "paper_id": "GSLTS",
      "text": "GS-LTStoper for msceneupdatesefficiently istics. In addition, to incorporate semantic information, we with minimal computational overhead. leverage the Segment Anything Model (SAM) [20] and IV. METHODOLOGY open-vocabulary vision-language models (e.g., CLIP [21]) to embed semantic features into the 3 DGS representation. In this section, we introduce the implementation and Multi-Task Executor. This module serves as an interface technical details of the GS-LTS system. between the dense 3 DGS representation and downstream A. 3 DGS Mapping Engine tasks, enabling the robot to leverage the high-fidelity scene information encoded in the 3 DGS for task planning and In this module, we employ semantic-aware 3 D Gaussian execution. For instance, by matching textual features with Splatting (3 DGS) for scene reconstruction. 3 DGS provides 3 D Gaussian semantic features, this module facilitates 3 D an explicit scene representation through anisotropic Gaus- localization for arbitrary text queries. Additionally, it adapts sians characterized by center position \u00b5 \u2208 R 3, covariance the 3 DGSintoa 2 Dsemanticmap for mattosupportexisting matrix \u03a3 \u2208 R 3\u00d73, opacity o \u2208 R, and color c \u2208 SH represented by spherical harmonics. Through differentiable semantic point cloud is voxelized and flattened along the Z- rendering,3 DGSsyn the sizespixelcolorsviaalphablending: axistoforma 2 Dsemanticmap,enablingpathplanning and navigation to target categories. N i\u22121 (cid:88) (cid:89) C = c \u03b1 (1\u2212\u03b1 ), (1) i i j C. Change Detection Unit i=1 j=1 where i \u2265 2 and \u03b1 depends on o and the projected 2 D Asmentionedin Sec.III-A,thismoduleisdesignedto:(1) i i Gaussian\u2019s contribution to the current pixel. detect and classify three types of scene changes; (2) localize We refer [4] to extend 3 DGS for semantic fields. First, the target position p world in world coordinates. construct the vanilla 3 DGS scene representation. Second, 1) 3 DGS-based Change Detection: The most intuitive embedsemanticfeaturesbyleveraging SAM[20]togenerate method for detecting scene change is to analyze the discrep- multi-level masks {Ml}3 and extract high-dimensional anciesbetween the real-worldimage and the 3 DGSrendered l=1 CLIP [21] features Fl = CLIP(I \u2299 Ml) \u2208 RD. An image,although 3 DGSenablesphoto-realisticimagerender- autoencoder is used to compresses these features into low- ing, there often exist pixel-wise errors with the real-world dimensional semantic features Sl = Encoder(Fl) \u2208 Rd observation,making the directcomputationofabsolutepixel while preserving semantics through reconstruction regular- differencesbetween the twoimagesyieldsub-optimalresults. ization F\u02c6l = Decoder(Sl). The low-dimensional semantic Therefore, following the practice of [19], we employ a features are used to supervise the Gaussians\u2019 semantic at- dual-branch strategy for scene change detection. As shown tribute sl \u2208Rd using view-consistent alpha blending: in Fig.2,given the the real-worldcameracapturedimage I real and 3 DGS rendered image I from the current viewpoint, N i\u22121 GS S\u02c6l = (cid:88) sl\u03b1 (cid:89) (1\u2212\u03b1 ). (2) wefirstcalculate the sumofabsolutepixeldifferencesacross i i j all 3 channels between the two images, which is truncated i=1 j=1 via threshold \u03c4 to obtain the pixel-level binary mask: GS Thus, we obtain the semantic-aware 3 DGS representation (cid:18) (cid:19) G = {g }N which enables explicit 3 D semantic repre- (cid:88)3 i i=1 M = |I \u2212I |>\u03c4 . (4) sentation while maintaining real-time rendering capabilities pixel chn=1 real,chn GS,chn GS through the Gaussian",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 272,
      "paper_id": "GSLTS",
      "text": "the pixel-level binary mask: GS Thus, we obtain the semantic-aware 3 DGS representation (cid:18) (cid:19) G = {g }N which enables explicit 3 D semantic repre- (cid:88)3 i i=1 M = |I \u2212I |>\u03c4 . (4) sentation while maintaining real-time rendering capabilities pixel chn=1 real,chn GS,chn GS through the Gaussian representation. Next, we compute normalized Efficient SAM [22] feature B. GS Multi-Task Executor maps I and I thatrobustlyrepresentsignificant real,SAM GS,SAM regions, then calculate their cosine similarity truncated by Below, we illustrate how the 3 DGS representation can be \u03c4 to obtain the feature-level binary mask: applied to robotic tasks, including 3 D localization and ob- feat ject navigation, which are critical capabilities for numerous M =(\u27e8I ,I \u27e9>\u03c4 ). (5) applications and serve to assess the geometric and semantic feat GS,SAM real,SAM feat accuracy of 3 DGS scene representation. Finally, the combined binary mask can be obtained using 1) Semantic Querying and Localization: For an arbitrary pixel-by-pixelmultiplicationof the dual-branchbinarymasks textquery,therelevancescorer(\u03d5 ,\u03d5 )between the CLIP embedding\u03d5 and the semanticf q e r a y tur g e i \u03d5 =Decoder(sl) M comb =M pixel \u2299M feat . We hypo the size that when the total qry gi i area of M comb exceeds the threshold \u03c4 change , a scene change of each 3 D Gaussian is defined as: occurs, thereby triggering scene change prediction. min exp(\u03d5 gi \u00b7\u03d5 qry ) , (3) 2) Scene Change Prediction: We posit that all potential (cid:16) (cid:17) j exp(\u03d5 \u00b7\u03d5 )+exp \u03d5 \u00b7\u03d5j change regions reside within M comp , where noise areas con- gi qry gi canon stitute a small portion. Therefore, we first extract connected where \u03d5j canon is the CLIP embedding of a predefined set components {R i }N i=1 from M comp , sorted in descending of canonical phrases, including \u201cobject\u201d, \u201cthings\u201d, \u201cstuff\u201d, order by their area. Based on the distinct change types, we and \u201ctexture\u201d. The localization of the query is achieved by hypo the size that: (i) relocation operations are geometrically calculating the bounding box of matched Gaussians {g | constrained to the first two largest connected components i r(\u03d5 qry ,\u03d5 gi ) > \u03c4 sim }, where \u03c4 sim is a predefined threshold. (R 1 and R 2 ), while (ii) addition/removal operations mani- Dueto the largenumberof 3 DGaussiansin the scene,sparse fest exclusively within the dominant connected components samplingisappliedinpracticetoper for msemanticquerying. (R 1 ). We first formulate the following dual-region matching 2) 2 D Semantic Mapping and Navigation: The 3 DGS criterion to identify relocation operation: representation can beseamlesslyconvertedintoa 2 Dseman- min(A(R ),A(R )) tic map, ensuring compatibility with existing navigation and \u0393 match = max(A(R 1 ),A(R 2 )) >\u03c4 a (6) pathplanningmethods.The 2 Dsemanticmapisrepresented 1 2 (cid:124) (cid:123)(cid:122) (cid:125) as a (L+2)\u00d7M\u00d7M matrix, where M\u00d7M represents the Areasimilarity map size, L is the number of semantic categories, and the \u2227 \u2225\u03b7(R )\u2212\u03b7(R )\u2225 <\u03c4 , 1 2 2 d additionallayersrepresentobstacles and exploredarea.For L (cid:124) (cid:123)(cid:122) (cid:125) Spatialdistance navigation-relevant categories, we assign each 3 D Gaussian the category with the highest relevance score. The resulting where A(\u00b7)denotesregionarea,\u03b7(\u00b7)forcentroidcoordinates.",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 273,
      "paper_id": "GSLTS",
      "text": "the Areasimilarity map size, L is the number of semantic categories, and the \u2227 \u2225\u03b7(R )\u2212\u03b7(R )\u2225 <\u03c4 , 1 2 2 d additionallayersrepresentobstacles and exploredarea.For L (cid:124) (cid:123)(cid:122) (cid:125) Spatialdistance navigation-relevant categories, we assign each 3 D Gaussian the category with the highest relevance score. The resulting where A(\u00b7)denotesregionarea,\u03b7(\u00b7)forcentroidcoordinates. Then, for the largest connected components R , we com- where \u03c0(\u00b7) is the projection function, the target Gaussians 1 pute its centroid p =(u ,v ), and sample depth from real- are selected based on the proportion \u03c1 of its presence within c c c world depth map D and 3 DGS rendered depth map D : the mask region: G ={g |V(g )>\u03c1\u00b7K}. cam GS target i i (cid:40) The workflow of our pre-editing method is as follows: d =D (p ) real real c (7) For addition: d =D (p ) GS GS c i. Generate object points P = {p } via depth map add j The change type is determined through depth difference: {D real,k \u2299M comb,k }K k=1 . ii. Pass new semantic feature s . \uf8f1 manual \uf8f4\uf8f2 Addition, \u2206d<\u2212\u03f5 iii. Find nearest neighbors and inherit attributes: \u2206d=d real \u2212d GS \u21d2Type= Removal, \u2206d>\u03f5 (8) a) For each p j \u2208P add : g n j n =argmin\u2225\u00b5 m \u2212p j \u2225 2 , \uf8f4\uf8f3Unchanged, |\u2206d|\u2264\u03f5 gm\u2208G b) Extract covariance matrix \u03a3j , opacity oj and color nn nn To obtain p world , here we construct a pseudo depth map cj nn from g n j n . D (u,v) = min(D (u,v),D (u,v)), then generate pseudo real GS iv. Create new Gaussians G and insert to G: camera-coordinate key point p \u2208R 3 based on event type: add \uf8f1 cam Addition/ G add = (cid:8) g i (cid:12) (cid:12)(p j ,\u03a3j nn ,oj nn ,cj nn ,s manual ) (cid:9) . (12) \uf8f4\uf8f4\uf8f4\uf8f4\uf8f2 [u c ,v c ,D pseudo (u c ,v c )]\u22a4, Removal For removal: p cam = \uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 1 2 + (cid:0) [ [ c c ( ( R R 1 ) ) ; ; D D pseudo ( ( c c ( ( R R 1 ) ) ) ) ] ] (cid:1)\u22a4 , Relocation (9) i i i . . D m G e e a l n p e e t { r e a D t G e ta h rg o et le \u2299 fr i o n M m pa G in . tin } g K poi . nts P fill = {p j } via depth 2 pseudo 2 real,k comb,k k=1 iii. Find nearest neighbors and inherit attributes: Then, p can be calculated using camera-to-world trans for mat w io or n ld matrix Tworld: a) For each p j \u2208P fill : g n j n =argmin\u2225\u00b5 m \u2212p j \u2225 2 , cam (cid:20) p (cid:21) b) Extract covariance matrix \u03a3 g j m , \u2208 o G pacity oj , color cj p =Tworld\u00b7 cam . (10) nn nn nn world cam 1 and semantic sj from gj",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 274,
      "paper_id": "GSLTS",
      "text": "fill : g n j n =argmin\u2225\u00b5 m \u2212p j \u2225 2 , cam (cid:20) p (cid:21) b) Extract covariance matrix \u03a3 g j m , \u2208 o G pacity oj , color cj p =Tworld\u00b7 cam . (10) nn nn nn world cam 1 and semantic sj from gj . nn nn D. Active Scene Updater iv. Create new Gaussians G add and insert into G: 1) Active Data Collection: Wedesignarule-basedheuris- G add = (cid:8) g i (cid:12) (cid:12)(p j ,\u03a3j nn ,oj nn ,cj nn ,sj nn ) (cid:9) . (13) ticpolicytoenable the robottoautonomouslycaptureimages For relocation: of scene change regions. The core of this policy involves i. Obtain G and extract colors and semantics C,S = positioningtherobottoface the targetregion,with the region target {c ,s |g \u2208G }. as the circle\u2019s center, and moving left or right along the j j j target ii. Delete G and generate target points P via depth tangential direction. The robot first adjusts its distance from target dest map {D \u2299M }K at destination. thetargetregiontoensureasuccessfultangentialmovement. real,k comb,k k=1 After each movement, the robot reorients its viewpoint iii. For each p j \u2208P dest , g n j n =argmin\u2225\u00b5 m \u2212p j \u2225 2 , create toward the target and readjustsitsdistancetocaptureimages. new Gaussians G and inse g rt m i \u2208 n G to G: add Following this policy, the robot first moves K/2 steps to the left, then K steps to the right, collecting images {I real,i }K i=1 , G add = (cid:8) g i (cid:12) (cid:12)(p i ,\u03a3j nn ,oj nn ,c nn \u223cC,s nn \u223cS) (cid:9) . (14) depth maps {D }K and camera poses {T }K from K real,i i=1 i i=1 iv. Hole inpainting to source region as aforementioned. viewpoints of the scene change region during the process. Finally, we perform post-training to further fine-tune the Next,therobotobtainscombinedmasks{M }K using comb,k k=1 Gaussians and refine the reconstruct quality. the method in Sec. IV-C.2. 2) Gaussian Editing based Scene Update: We adopt a V. EXPERIMENTS pre-editing and fine-tuning strategy to achieve scene update. Thissectiondetailsacomprehensiveevaluationof GS-LTS Distinct pre-editing protocols are implemented for different across simulation and real-world settings. scene change types: (i) For Addition, we directly instanti- ate new Gaussians in target region; (ii) For Removal, we A. Scene Change Adaptation Benchmark first localize target objects and then prune the associated 1) Settings: To assess the robot\u2019s ability to adapt to Gaussians followed by scene inpainting to fix the hole; scene changes in dynamic environments, we present a novel (iii) For Relocation, we execute a delete-then-add strategy Scene Change Adaptation Benchmark constructed on the that removes the associated Gaussians and then instantiate AI 2-THOR simulation platform [23]. AI 2-THOR offers a new Gaussians in target region. To identify the associated comprehensive suite of APIs such as Initial Random Spawn, Gaussians, we refer to [11] for defining a voting function V Disable Object, and Place Object At Point that facilitate direct that localizes target Gaussians within {M comb,k }K k=1 , manipulation of scene objects, which we exploit to design three distinct types",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 275,
      "paper_id": "GSLTS",
      "text": "of APIs such as Initial Random Spawn, Gaussians, we refer to [11] for defining a voting function V Disable Object, and Place Object At Point that facilitate direct that localizes target Gaussians within {M comb,k }K k=1 , manipulation of scene objects, which we exploit to design three distinct types of scene update tasks: relocation, ad- K V(g )= (cid:88) I(cid:2) \u03c0(T \u00b5 )\u2208{M }K (cid:3) , (11) dition, and removal of objects. The benchmark is gener- i k i comb,k k=1 ated by automatically traversing combinations of editable k=1 TABLEI SCENEUPDATEQUALITYEVALUATEDBYIMAGERENDERINGMETRICS(250 FINE-TUNINGITERATIONS). addition relocation removal overall Method View SSIM\u2191 PSNR\u2191 LPIPS\u2193 SSIM\u2191 PSNR\u2191 LPIPS\u2193 SSIM\u2191 PSNR\u2191 LPIPS\u2193 SSIM\u2191 PSNR\u2191 LPIPS\u2193 near 0.87 24.74 0.24 0.89 25.94 0.22 0.91 29.79 0.20 0.89 27.20 0.22 Baseline far 0.88 25.52 0.20 0.89 26.23 0.19 0.91 28.64 0.17 0.89 27.04 0.19 near 0.88 26.60 0.22 0.91 28.58 0.19 0.93 32.08 0.17 0.91 29.40 0.19 GS-LTS far 0.89 26.93 0.18 0.90 28.04 0.17 0.92 30.55 0.15 0.90 28.74 0.16 \u0000\u0016\u0000\u0013 \u0000\u0015\u0000 \u0000\u0015\u0000\u001b \u0000\u0015\u0000\u001a \u0000\u0015\u0000\u0019 \u0000\u0015\u0000\u0018 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000,\u0000W\u0000H\u0000U\u0000D\u0000W\u0000L\u0000R\u0000Q \u00005\u00001\u00006\u00003 \u00001\u0000H\u0000D\u0000U \u0000)\u0000D\u0000U \u0000\u0016\u0000\u0013 \u0000\u0015\u0000 \u0000\u0015\u0000\u001b \u0000\u0015\u0000\u001a \u0000\u0015\u0000\u0019 \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H \u0000*\u00006\u0000\u0010\u0000/\u00007\u00006 \u0000\u0015\u0000\u0018 \u0000*\u00006\u0000\u0010\u0000/\u00007\u00006 \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \u0000,\u0000W\u0000H\u0000U\u0000D\u0000W\u0000L\u0000R\u0000Q Fig.3. Impactoffine-tuningiterationsonsceneupdatequality. objects, containers, and placement positions, which enables GT Baseline 100 Iter GS-LTS 100 Iter Baseline 250 Iter GS-LTS 250 Iter the sampling of an extensive range of scene changes. Each scene change task is recorded via a configuration script containing environment meta data (e.g., initial viewpoint) and a sequential action list specifying the operations to trans for mobjects from theirdefaultstateto the updatedstate. Additionally,each task involves 20 testviewpointscapturing the scene change region (10 near-range and 10 far-range). As changed objects typically occupy a small fraction of the field of view, we generate test images by exp and ing ground- truth change bounding boxes by 50 pixels in all directions. Scene update quality is then assessed using PSNR, SSIM, and LPIPS metrics. For evaluation, the robot is initialized at the starting pose of each scene change task. The Change Detection Unit is first executed to generate predictions, after which we assess whetherthepredictedscenechangetypematches the ground- truth type and whether the prediction error of the scene change region is within 1 meter. For tasks with accurate predictions,active data collection and Gaussianediting-based scene update are per for med. During scene representation updates, GS-LTS first employs pre-editing method, followed by fine-tuning of 3 DGS to refine object geometry and visual details. In contrast, the baseline method directly fine-tunes 3 DGS using multi-view data collected by GS-LTS. In this experiment, we sample 459 scene change tasks, achieving a 74.5% accuracy in predicting change type and target region with GS-LTS. Scene updates are tested on 342 tasks,withresultsafter 250 fine-tuningiterationsreportedin Table I. Experimental results demonstrate that our method achieves superior per for mance for both type of viewpoints, outper for ming the baseline across all metrics. Additionally,asshownin Fig.3,weevaluatevariousfine- tuning iterations and statistically analyze the overall PSNR metrics for both type of viewpoints. The results demon- strate that ourapproachconsistentlyoutperforms the baseline across all settings. Notably, GS-LTS achieves superior scene update quality with fewer fine-tuning",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 276,
      "paper_id": "GSLTS",
      "text": "for both type of viewpoints, outper for ming the baseline across all metrics. Additionally,asshownin Fig.3,weevaluatevariousfine- tuning iterations and statistically analyze the overall PSNR metrics for both type of viewpoints. The results demon- strate that ourapproachconsistentlyoutperforms the baseline across all settings. Notably, GS-LTS achieves superior scene update quality with fewer fine-tuning iterations, highlighting noitidd A noitacole R lavome R Fig.4. Renderingresultsafterdifferentfine-tuningiterations. TABLEII 3 DLOCALIZATIONRESULTS(BOTTOMPART FOR ABLATIONSTUDY). Feature Source m Io U Acc(Io U>0.5) Acc(Io U>0.3) CLIP 40.6 42.9 52.1 GT 60.9 73.6 81.8 CLIP(300\u00d7300 res) 24.6 29.0 30.7 CLIP(8 dim) 32.2 35.7 43.3 CLIP(60%data) 36.6 41.1 46.1 its ability to deliver efficient, low-cost scene updates. Fig. 4 presents the quantitative results of GS-LTS, showcasing one representative case from each of three scene changes. The rendering results more intuitively demonstrate that GS-LTS achieves superior and faster scene update capabilities, while the baseline method requires signifi can tly more iterations to obtain comparable outcomes. B. Multi-Task Experiment To assess the geometry and semantic fidelity of GS-LTS, we conduct experiments on 3 D localization and object nav- igation. We evaluate two 3 DGS representations embedding ground-truth semantics and CLIP semantics, denoted as GS- LTS (GT) and GS-LTS (CLIP), respectively. 1) 3 D Localization: For the 3 D localization task, se- mantic quality is quantitatively assessed by calculating the Intersection over Union (Io U) of the 3 D bounding boxes. A 3 D bounding box is deemed accurately localized if its Io U with the ground-truth bounding box exceeds a pre- defined threshold. Based on this criterion, we compute the Acc (Io U>threshold) metric to evaluate localization accuracy. We evaluate localization per for mance across 12 differentobjectcategories,including Alarm Clock,Arm Chair, Bed, Bread, Chair, Coffee Machine, Desk Lamp, Dining Table, Dresser, Dumbbell, Remote Control and Sofa. Asshownin the toppartof Table II,GS-LTS(GT)signif- icantly outperforms GS-LTS (CLIP) in terms of both m Io U Bread Laptop Alarm Clock Robot View Fig. 5. 3 D Localization Examples. Red bounding boxes indicate the results from GS-LTS(GT),whilegreenones from GS-LTS(CLIP). TABLEIII OBJECTNAVIGATIONRESULTSACROSSTHREEROOMTYPES. kitchen livingroom bedroom overall Method SPL SR SPL SR SPL SR SPL SR SAVN[6] 17.8 43.6 7.7 21.6 8.7 29.2 11.4 31.5 GVE[24] 17.9 45.6 9.4 25.2 8.1 27.6 11.8 32.8 HZG[25] 48.7 74.6 41.5 60.7 32.2 59.1 40.8 64.8 GS-LTS(CLIP) 45.7 59.0 40.0 52.2 41.5 54.6 42.2 55.3 GS-LTS(GT) 64.0 86.4 68.4 90.3 44.8 56.4 59.1 77.7 Fig. 6. Object Navigation Examples. The mid row displays semantic and accuracy metrics, highlighting the critical importance maps and navigationtrajectoriesgeneratedby GS-LTS(CLIP),thebottom rowillustrates the correspondingoutputsof GS-LTS(GT). of precise semantic cues. Fig. 5 presents qualitative results for the 3 D localization task. The 3 D bounding boxes gener- Step 1: Change Detection Step 3: 3 DGS Update 3 DGS Rendered Image Current Observation Old GS ated by GS-LTS (GT) generally exhibit a closer alignment with the objects compared to those generated by GS-LTS Change Detection (CLIP). These precise bounding boxes further highlight the advantages and potential of employing 3 DGS as a scene representation. 2) Object Navigation: For the object navigation task, we Change Detection Unit update adopt the experimental protocol proposed by SAVN [6], with",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 277,
      "paper_id": "GSLTS",
      "text": "objects compared to those generated by GS-LTS Change Detection (CLIP). These precise bounding boxes further highlight the advantages and potential of employing 3 DGS as a scene representation. 2) Object Navigation: For the object navigation task, we Change Detection Unit update adopt the experimental protocol proposed by SAVN [6], with a modification to limit the evaluation to three scene types: kitchens, living rooms, and bedrooms. Bathrooms are excludeddueto the irconstrainedspatial scale and simplistic layouts.Per for manceisassessedusing the Successweighted by Path Length (SPL) and Success Rate (SR). Step 2: Active Data Collection We comp are GS-LTS with classical methods. Notably, New GS these classical methods trained on the AI 2 THOR involve Fig.7. Realrobotper for mingscenechangeadaptation. exploration and navigation within a single episode. In con- an average of 670 images to 60% of the data lowers m Io U trast, GS-LTS leverages prebuilt 3 DGS representations and by 4.0%. Notably, CLIP features computed from SAM- performs training-free navigation directly from a 2 D seman- segmented masks rely heavily on high-resolution images for tic map, employing a deterministic policy (Fast Marching small object recognition. While smaller data volume affect Method). This experimental setup is designed to validate the fine details, the impact is minimal for objects visible from feasibility of 3 DGS-based robotic navigation using existing multiple viewpoints, such that overall per for mance decline benchmark. As shown in Table III, GS-LTS (GT) outper- remains limited. forms other approaches across most metrics, while GS-LTS (CLIP) also demonstrates competitive per for mance, particu- D. Application in Real-world Robot System larly on the SPL metric. Semantic maps and trajectories for three example navigation tasks are illustrated in Fig. 6. Todemonstrate the real-worldapplicabilityof the GS-LTS system, we conducted experiments with a real robot. We C. Ablation Study utilize a Microsoft Azure Kinect DK camera to scan a pre- To examine the effect of initial training data on 3 DGS arrangedroom,capturing data totraina 3 DGSrepresentation representations, we perform an ablation study on the 3 D of the scene. Unlike simulation environments, where precise localization task, with results reported in the bottom part robot poses can be obtained directly from an environment of Table II. We analyze how reduced image resolution, API, such information is unavailable in real-world settings. feature dimension and data volume affect GS-LTS (CLIP) To address this, we augment the GS-LTS system with a per for mance. Experiments reveal that lowering resolution relocalizationmoduletailored for real-worldoperation.Here, from 1,000\u00d71,000 to 300\u00d7300 decreases m Io U by 16.0% we first obtain a coarse pose estimation through ORB visual andsignifi can tlyreducesaccuracy.Decreasing the featuredi- featurematching,thenemployi Com Ma[26]toper for mpose mension from 32 to 8 resultsinaper for mancedropofm Io U refinement to obtain an optimized precise pose estimation. from 40.6%to 32.2%,indicating that lower-dimensionalrep- To assess the robot\u2019s ability to adapt to scene changes, resentationsdegradethequalityof the learnedlatentspace,as we reposition three stacked colored storage bins within the convergenceofautoencodersbecomesmorechallenging with room. As the robot approaches the vicinity of the bins, 8-dimensional features. Reducing the training dataset from the Change Detection Unit identifies discrepancies in the current scene. It then actively collects multi-view images to",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 278,
      "paper_id": "GSLTS",
      "text": "scene changes, resentationsdegradethequalityof the learnedlatentspace,as we reposition three stacked colored storage bins within the convergenceofautoencodersbecomesmorechallenging with room. As the robot approaches the vicinity of the bins, 8-dimensional features. Reducing the training dataset from the Change Detection Unit identifies discrepancies in the current scene. It then actively collects multi-view images to [4] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, \u201cLangsplat: 3 d update 3 DGS. Fig. 7 illustrates the changes in the 3 DGS languagegaussiansplatting,\u201din CVPR,2024,pp.20051\u201320060. [5] S. Zhu, G. Wang, D. Kong, and H. Wang, \u201c3 d gaussian splatting in representation and rendered images before and after the robotics:Asurvey,\u201dar Xivpreprintar Xiv:2410.12262,2024. adaptation. These results validate that the GS-LTS system [6] M.Wortsman,K.Ehsani,M.Rastegari,A.Farhadi,and R.Mottaghi, caneffectivelyoperateinreal-worldenvironments and adapt \u201cLearningtolearnhowtolearn:Self-adaptivevisualnavigationusing meta-learning,\u201din CVPR,2019,pp.6750\u20136759. todynamicscenechanges.Foradetailedexperimentalvideo, [7] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, please refer to our website. \u201cObjectgoalnavigationusinggoal-orientedsemanticexploration,\u201din Neur IPS,2020,pp.4247\u20134258. VI. DISCUSSION [8] X.Chen,A.Milioto,E.Palazzolo,P.Giguere,J.Behley,and C.Stach- niss, \u201cSuma++: Efficient lidar-based semantic slam,\u201d in IROS, 2019, A. Resource Overhead pp.4530\u20134537. [9] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor- Theentiresystemoperatesefficientlyonasingle NVIDIA thi, and R. Ng, \u201cNerf: Representing scenes as neural radiance fields Ge Force RTX 4090 GPU. The GS-LTS system completes forviewsyn the sis,\u201dCommunicationsof the ACM,vol.65,no.1,pp. vanilla 3 DGS reconstruction in \u223c15 minutes, with subse- 99\u2013106,2021. [10] R.Jin,Y.Gao,Y.Wang,Y.Wu,H.Lu,C.Xu,and F.Gao,\u201cGs-planner: quent 32 dimensional Gaussian semantic learning requiring Agaussian-splatting-basedplanningframework for activehigh-fidelity \u223c1 hour. Our experiments show 250 training iterations reconstruction,\u201din IROS. IEEE,2024,pp.11202\u201311209. achieve superior scene updates (0.91 SSIM / 29.07 PSNR) [11] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai, L.Yang,H.Liu,and G.Lin,\u201cGaussianeditor:Swift and controllable versus 1,000-iteration baselines, with \u226410 s training. 3 dediting with gaussiansplatting,\u201din CVPR,2024,pp.21476\u201321485. [12] L. Kunze, N. Hawes, T. Duckett, M. Hanheide, and T. Krajn\u00b4\u0131k, B. Limitation and Future Work \u201cArtificialintelligence for long-termrobotautonomy:Asurvey,\u201dRAL, vol.3,no.4,pp.4023\u20134030,2018. The GS-LTS system advances adaptive modeling for [13] C.P.Jones,\u201cSlocumgliderpersistentoceanography,\u201din AUV. IEEE, 3 DGS-based robotic systems in long-term dynamic envi- 2012,pp.1\u20136. ronments, yet several challenges remain before achieving [14] N. Hawes, C. Burbridge, F. Jovan, L. Kunze, B. Lacerda, L. Mu- drova, J. Young, J. Wyatt, D. Hebesberger, T. Kortner, et al., \u201cThe widespread real-world deployment. Below, we discuss key str and sproject:Long-termautonomyineverydayenvironments,\u201dIEEE limitations and promising directions for improvement. Robotics&Automation Magazine,vol.24,no.3,pp.146\u2013156,2017. First, efficient large-scale representation is a challenge for [15] M.Hanheide,D.Hebesberger,and T.Krajn\u00b4\u0131k,\u201cThewhen,where,and how: An adaptive robotic info-terminal for care home residents,\u201d in vanilla 3 DGS, which struggles with expansive scenes like HRI,2017,pp.341\u2013349. factories, requiring more storage-efficient solutions. [16] P.F.Alcantarilla,S.Stent,G.Ros,R.Arroyo,and R.Gherardi,\u201cStreet- Second, robot control could be improved with learning- view change detection with deconvolutional networks,\u201d Autonomous Robots,vol.42,pp.1301\u20131322,2018. based policies to enhance adaptability in complex scenarios. [17] E. Palazzolo and C. Stachniss, \u201cFast image-based geometric change Finally, highly dynamic environments present an addi- detectiongivena 3 dmodel,\u201din ICRA. IEEE,2018,pp.6308\u20136315. tional challenge. GS-LTS focuses on medium-term changes, [18] J.Wald,A.Avetisyan,N.Navab,F.Tombari,and M.Nie\u00dfner,\u201cRio: 3 d object instance re-localization in changing indoor environments,\u201d not real-time dynamics like moving objects or human in- in ICCV,2019,pp.7658\u20137667. teractions. Future 3 DGS-based dynamic reconstruction will [19] Z. Lu, J. Ye, and J. Leonard, \u201c3 dgs-cd: 3 d gaussian splatting-based enhance support for tasks like cooking or household assis- change detection for physical object rearrangement,\u201d IEEE Robotics and Automation Letters,2025.",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 279,
      "paper_id": "GSLTS",
      "text": "dynamics like moving objects or human in- in ICCV,2019,pp.7658\u20137667. teractions. Future 3 DGS-based dynamic reconstruction will [19] Z. Lu, J. Ye, and J. Leonard, \u201c3 dgs-cd: 3 d gaussian splatting-based enhance support for tasks like cooking or household assis- change detection for physical object rearrangement,\u201d IEEE Robotics and Automation Letters,2025. tance, improving more realistic long-term autonomy. [20] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Roll and, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., \u201cSegment VII. CONCLUSIONS anything,\u201din ICCV,2023,pp.4015\u20134026. In this work, we introduce GS-LTS, a 3 DGS-based sys- [21] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal, G.Sastry,A.Askell,P.Mishkin,J.Clark,etal.,\u201cLearningtransferable tem designed for long-term service robots operating in visualmodels from naturallanguagesupervision,\u201din ICML. Pm LR, dynamic environments. By integrating object-level change 2021,pp.8748\u20138763. detection, multi-view observation, and efficient Gaussian [22] Y.Xiong,B.Varadarajan,L.Wu,X.Xiang,F.Xiao,C.Zhu,X.Dai, D.Wang,F.Sun,F.Iandola,etal.,\u201cEfficientsam:Leveragedmasked editing-basedsceneupdates,GS-LTSenablesrobotstoadapt imagepretraining for efficientsegmentanything,\u201din CVPR,2024,pp. to scene variations over time. Additionally, we propose a 16111\u201316121. scalable simulation benchmark for evaluating object-level [23] E. Kolve, R. Mottaghi, W. Han, E. Vander Bilt, L. Weihs, A. Her- rasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, \u201cAI 2-THOR: scenechanges,facilitatingsystematicassessment and sim-to- An Interactive 3 D Environment for Visual AI,\u201d ar Xiv preprint real transfer. Experimental results demonstrate that GS-LTS ar Xiv:1712.05474,2017. achieves faster and higher-quality scene updates, advancing [24] M. K. Moghaddam, Q. Wu, E. Abbasnejad, and J. Shi, \u201cOptimistic agent: Accurate graph-based value estimation for more successful the applicability of 3 DGS for long-term robotic operations. visualnavigation,\u201din WACV,2021,pp.3733\u20133742. [25] Y. He and K. Zhou, \u201cRelation-wise trans for mer network and re- REFERENCES inforcement learning for visual navigation,\u201d Neural Computing and Applications,vol.36,no.21,pp.13205\u201313221,2024. [1] B.Kerbl,G.Kopanas,T.Leimku\u00a8hler,and G.Drettakis,\u201c3 dgaussian [26] Y.Sun,X.Wang,Y.Zhang,J.Zhang,C.Jiang,Y.Guo,and F.Wang, splatting for real-timeradiancefieldrendering.\u201dACMTrans.Graph., \u201cicomma:Inverting 3 dgaussiansplatting for cameraposeestimation vol.42,no.4,pp.139\u20131,2023. viacomparing and matching,\u201dar Xivpreprintar Xiv:2312.09031,2023. [2] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, D.Ramanan,and J.Luiten,\u201cSplatam:Splattrack&map 3 dgaussians fordensergb-dslam,\u201din CVPR,2024,pp.21357\u201321366. [3] K. Wu, K. Zhang, Z. Zhang, M. Tie, S. Yuan, J. Zhao, Z. Gan, and W.Ding,\u201cHgs-mapping:Onlinedensemappingusinghybridgaussian representationinurbanscenes,\u201dRAL,2024.",
      "start_pos": 5544,
      "end_pos": 5861
    },
    {
      "chunk_id": 280,
      "paper_id": "learningfirsperson",
      "text": "Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression Jangwon Lee and Michael S. Ryoo Abstract\u2014We design a new approach that allows robot a limiting aspect particularly when we want to teach a robot learning of new activities from unlabeled human example new (i.e., previously unseen) activities. videos.Givenvideosofhumansexecuting the sameactivity from In this paper, we present a new CNN-based approach ahuman\u2019sviewpoint(i.e.,first-personvideos),ourobjectiveisto that enables robot learning of its activities from \u2018human\u2019 maketherobotlearnthetemporalstructureof the activityasits futureregressionnetwork,andlearntotransfersuch model for example videos. Human activity videos can be attractive itsownmotorexecution.Wepresenta new deeplearning model: training res our ces because it does not require any hardw are We extend the state-of-the-art convolutional object detection or professional softw are for teaching robots, even though network for the representation/estimation of human hands in it might create other difficulties like transferring learned trainingvideos,and new lyintroduce the conceptofusingafully human-based models to the actual robots. Given videos convolutionalnetworktoregress(i.e.,predict)theintermediate scene representation corresponding to the future frame (e.g., of humans executing the same activity from a human\u2019s 1-2 seconds later). Combining these allows direct prediction of viewpoint (i.e., first-person videos), our objective is to make futurelocationsofhumanhands and objects,whichenables the the robot learn the temporal structure of the activity as its robot to infer the motor control plan using our manipulation future regression network, and learn to transfer such model network. We experimentally confirm that our approach makes foritsownmotorexecution.Theideais that ahuman\u2019sfirst- learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to person video and the video a humanoid robot is expected to execute the learnedcollaborativeactivitiesinreal-timedirectly obtain during its activity execution should be very similar. based on its camera input. Providing first-person human videos to the robot is as if we are providing the robot \u2018visual memory\u2019 of itself per for ming I. INTRODUCTION the activities previously. This enables the robot to directly One of the important abilities of humans (and animals) is learnwhatvisualobservationitisexpectedtoseeduring the that they are able to learn new activities and their motor correctexecutionof the activity and howit will change from controls from others\u2019 behaviors. When a person watches its viewpoint. others per for ming an activity, he/she not only learns to There have beenpreviousworksonrobotactivitylearning visually predict future consequences of the motion during from human videos [3], [4], extending the previous concept the activity but also learns how to execute the activity of\u2018robotlearning from demonstration\u2019[5]whichwasmostly himself/herself. done with direct motor control data. However, these works Recently, approaches taking advantage of \u201cdeep learning\u201d focused on learning grammar representations of human ac- for robot manipulation have been gaining an increasing tivities, modeling human activities as a sequence of atomic amount of attention, directly learning motor control policies actions (e.g., grasping). These approaches were limited in given visual inputs (i.e., images and videos) [1]. The use of the aspect that activities were always represented in terms convolutionalneuralnetworks(CNNs)have been particularly of pre-defined set of atomic actions, and the users had to successful,since the yareabletojointlylearnimagefeatures teach the robot how to recognize those atomic actions from optimized for the task based on their training data. Because humanactivityvideosbyprovidinglabeledtraining data(i.e., of such ability,",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 281,
      "paper_id": "learningfirsperson",
      "text": "aspect that activities were always represented in terms convolutionalneuralnetworks(CNNs)have been particularly of pre-defined set of atomic actions, and the users had to successful,since the yareabletojointlylearnimagefeatures teach the robot how to recognize those atomic actions from optimized for the task based on their training data. Because humanactivityvideosbyprovidinglabeledtraining data(i.e., of such ability, new models incorporating convolutional and supervised learning). This prevented the robot learning of recurrent neural networks (i.e., CNNs and RNNs) is likely activities from scratch, and was also limited in that human to become a major trend in robotics, just like what already had to define new atomic actions when a new activity is happened in computer vision and machine learning. added. Fur the rmore, since it was not trainable in an end-to- However, although these deep learning oriented ap- end fashion, the robot has to somehow figure out how to proaches showed very promising results on learning video execute those atomic actions, which was usually done by prediction [2] and actual motor control policy [1], they hand-coding the motion. have been limited to relatively simple actions such as object We introduce a new robot activity learning model using grasping and pushing. This is because a large amount of a fully convolutional network for future representation re- \u2018robot\u2019dataisnecessary for the directtrainingofthese CNNs gression. We extend the state-of-the-art convolutional object and RNNs with millions of parameters. A large number of detection network (SSD [6]) for the representation of hu- samplesofhumans(ortherobotitself)motorcontrolling the man hand-object information in a video frame, and newly robotisnecessary for generatingtraining data[1],andthisis introduce the concept of using a fully convolutional network to regress (i.e., predict) how such intermediate scene repre- Schoolof Informatics and Computing,Indiana University,Bloomington, IN 47408,USA.{leejang,mryoo}@indiana.edu sentation will change in the future frame (e.g., 1-2 seconds 7102 lu J 42 ]OR.sc[ 2 v 04010.3071:vi Xra \ud835\udc53 \ud835\udc54 \u210e VGG-16 Future hand Through Pool 5 layer Autoencoder Extra Feature Layers Location prediction Convolution Deconvolution 500 Replaced with Image predicted future feature map 500 \u2026 512 x 63 x 63 512 x 63 x 63 Extracted feature map 256 x 25 x 25 256 x 25 x 25 Current Regression Network s s Feature map \ud835\udc5f o L )2 L Concatenate (K) ( n a e d 5 x 5 5 x 5 5 x 5 1 x 1 ilc u E Predicted (256 x K) x 25 x 25 256 x 25 x 25 256 x 25 x 25 \u2026 1024 x 25 x 25256 x 25 x 25 future feature map Future Feature map Fig. 1. Overview of our perception component: Our perception component consists of two fully convolutional neural networks: The first network is anextendedversionof the state-of-the-artconvolutionalobjectdetectionnetwork(SSD[6])for the representationofhumanhands and estimationof the boundingboxes(top).Thesecondnetworkisafutureregressionnetworktoregress(i.e.,predict)theintermediatescenerepresentationcorrespondingto thefutureframe.Thisnetworkdoesnotrequireactivitylabelsorh and/objectlabelsinvideos for itstraining. later). Combining these allows direct and explicit prediction from demonstration (Lf D) [5], [8], [9]. Since it enables of future hand locations (Figure 1), which then allows the robots automatically learn a new task from demonstration robot to infer the motor control plan. That is, not only by non-robotics expert, Lf D is very important in robotics. feature-level prediction of future representations",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 282,
      "paper_id": "learningfirsperson",
      "text": "[5], [8], [9]. Since it enables of future hand locations (Figure 1), which then allows the robots automatically learn a new task from demonstration robot to infer the motor control plan. That is, not only by non-robotics expert, Lf D is very important in robotics. feature-level prediction of future representations (similar to However,there are limitationssincemostof the seapproaches [7])butalsosemantic-levelpredictionofexplicitfutureh and focused on making robots learn motor control polices from locations of humans and robots during the learned activity human data, which usually was in the form of direct control is being jointly per for med in our new network. Such future sequences obtained with actual robots or simulation soft- handpredictionresults are usedby our manipulationnetwork wares [10]. Moreover, it often requires a knowledge about that learns mapping of the 2-D hand locations in the image all primitive actions for teaching high-level tasks [11]. coordinate to the actual motor control. There also have been previous works on robot activity Our activity learning is an unsupervised approach in the learning from visual data [3], [4], [12], extending the previ- aspect that it does not require activity labels or hand/object ous concept of Lf D. These works focused on learning gram- labels in the activity videos. It does require hand-annotated mar representations of human activities from conventional training data for the learning of its hand representation third-person videos (i.e., videos usually taken with static network, but there already exists public datasets for this cameras watching the actors), modeling human activities purpose and it does not require any labels for its future as a sequence of atomic actions (e.g., grasping). Having a regressionnetwork.Thefutureregressionnetworkislearned grammar representation composed of atomic actions allows without supervision by capturing changes in our hand-based transfer of human activity structure to robots, and the robot representations in the training videos. In addition, impor- replication of human activities was possible usually with tantly,all our networks were designedtofunctioninreal-time hand-coded motion transfer from human atomic actions for the actual robot operation, and we show such capability to robot atomic actions. However, activity learning was with our experiments in this paper. generally done in a fully supervised fashion with human annotations in these approaches, and they assumed very II. RELATEDWORK reliable estimation of semantic features from videos such a) Robot learning from humans: There have been a human hands and human body skeletons. [13] studied an considerable amount of previous efforts on robot learning approach to directly learn object manipulation trajectories from human videos, but it was limited to one-robot-one- a hand-based scene representation and estimate bounding object scenarios unlike our approach focusing on very boxes,and(2)afutureregressionnetworkto model howsuch general human-robot collaboration scenarios (e.g., human- intermediate scene representation (should) change in future object-robot interactions). frames.Thesecondcomponentisamanipulationcomponent b) Video prediction: Our approach in this paper is to that maps 2-D hand locations in the image coordinate to the generate proper robot behaviors (particularly for human- actual motor control using fully connected layers. robot collaboration) by predicting \u2018future\u2019 visual represen- The key idea of our approach is that the proposed per- tation. The idea is that such representation leads to the ception component",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 283,
      "paper_id": "learningfirsperson",
      "text": "in the image coordinate to the generate proper robot behaviors (particularly for human- actual motor control using fully connected layers. robot collaboration) by predicting \u2018future\u2019 visual represen- The key idea of our approach is that the proposed per- tation. The idea is that such representation leads to the ception component allows prediction of future (1-2 seconds estimationoffuturepositionsofobjectsandh and sofhumans later)handlocationsgivencurrentvideoinput from acamera. and robots. Visual prediction is one of the core components Suchfutureprediction can belearned base donhumans\u2019first- of our perception system. person activity videos by using them as training data, with There have beenpreviousworkson the predictionoffuture theassumption that the robotcamerahasasimilarviewpoint frames from the computer vision community [7], [14], [15]. with the human first-person videos. This allows the robot However, there has been very limited attempt on applying to directly predict its ideal future hand locations during the such future predictions for robotics systems, since these ap- activity, inferring how the hand should move if the activity proaches in general requires more components for interpret- were to be executed successfully. Next, the manipulation ing predicted representation to generate robot actions. In the componentgeneratesactualrobotcontrolcomm and stomove above works, no robot manipulation was actually attempted. the robot\u2019s hands to the predicted future locations. There exists a recent robotics work that attempted applying B. Perception Component visual prediction for generating robot control actions [16]. This study shows the potential in applying visual predic- Given a video frame X\u02c6 t at time t, the goal of our tion for a robotic manipulation task; it enables transferring perception component is to predict the future hand locations the visual perception to robot manipulation component for Y\u02c6 t+\u2206 . generating motor control commands without any additional a) Hand Representation Network: We first construct componentstointerpret the recognitionresults.However,this a network for the hand-based representation of the image requiresahugeamountoftraining data usingactualphysical scenebyextending the SSDobjectdetectionframework.We robots to make the robot learn activities, and thus is limited extended it by inserting a fully convolutional auto-encoder when the robot needs to learn many new activities. having five convolutional layers followed by five deconvo- c) First-person videos: First-person videos, also called lutional layers for dimensionality reduction. This allows the egocentric videos, are the videos taken from the actor\u2019s own approach to abstract an image (with hands and objects) into viewpoint. Recognition of human/robot activities from such a lower dimensional intermediate representation. first-person videos has been actively studied particularly in All our convolutional/deconvolutionallayersuse 5\u00d75 ker- thepast 5 years,includingrecognitionofhumanactions from nels and the number of filters for each convolutional layer wearable cameras [17]\u2013[20] and human-robot interactions are:512,256,128,64,256.Thegreenconvolutionallayersin from robot cameras [21], [22]. However, these focused on Fig. 1 correspond to them. After such convolutional layers, building discriminative video classifiers, and the attempt to there are deconvolutional layers (yellow layers in Fig. 1), learn\u2018executable\u2019representationsofhumanactivitiesortheir each having the symmetric number of filters: 256, 64, 128, transfer to robots have been very limited. 256, 512. We do not use any pooling layer, and instead use The main contribution of this paper is in enabling robot stride 2 forthelastconvolutionallayer for the dimensionality activity learning from human interaction videos using our reduction. We thus",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 284,
      "paper_id": "learningfirsperson",
      "text": "filters: 256, 64, 128, transfer to robots have been very limited. 256, 512. We do not use any pooling layer, and instead use The main contribution of this paper is in enabling robot stride 2 forthelastconvolutionallayer for the dimensionality activity learning from human interaction videos using our reduction. We thus increase the number of filters for the last newly proposed convolutional future regression. We believe convolutional layer to compensate loss of information. this is the first work to present a deep learning-based Let f denote the hand representation network given an (i.e., entirely CNN-based) method for learning human-robot image at time t. Then, this network can be considered as a interactions fromhuman-human videos. We alsobelieve this combination of two sub functions, f =g\u25e6h: is the first paper to take advantage of human \u2018first-person Y\u02c6 =f(X\u02c6 )=h(F\u02c6 )=h(g(X\u02c6 )), (1) videos\u2019 for the robot activity learning. t t t t where a function g : X\u02c6 \u2192 F\u02c6 denotes a feature extractor III. APPROACH (from an input video frame to encoder) to get compressed A. System Overview intermediate visual representation (i.e., feature map) F\u02c6, and Given a sequence of current frames, our goal is to (i) pre- h : F\u02c6 \u2192 Y\u02c6 indicates a box estimator which uses the dict future hand locations and all interactive objects in front compressed representation as an input for locating hand of the robot, then to (ii) generate robot control commands boxes at time t. With the above formulation, the network for moving robot\u2019s hands to the predicted hand locations. can predict hand locations Y\u02c6 at time t after the training. t Weemploytwocomponents for achieving the goal.Thefirst b) Future Regression Network: Although the above component is a perception component that consists of two hand representation network allows obtaining hand boxes in fully convolutional neural networks: (1) an extended version the \u2018current\u2019 frame, our objective is to get the \u2018future\u2019 hand of the Single Shot Multi Box Detector (SSD) [6] to create locations Y\u02c6 instead of theirs current locations Y\u02c6 . t+\u2206 t g Hand representa,on network: t Fig. 2 summarizes data flow of our perception component Y\u02c6 during testing phase. Given a video frame X\u02c6 t at time t, (1) t we extract the intermediate scene representation F\u02c6 using t X\u02c6 F\u02c6 the feature extractor (g), and then (2) feed it into the future t t \u2026 regression network (r) to get future scene representation r F\u02c6 t+\u2206 . Next, (3) we feed F\u02c6 t+\u2206 into the box estimator (h), F\u02c6 t F\u02c6 t+\u0394 and finally obtain future position of hands Y\u02c6 t+\u2206 at time t. Y\u02c6 =h(F\u02c6 )=h(r(F\u02c6 ))=h(r(g(X\u02c6 ))) (5) t+\u2206 t+\u2206 t t h Y\u02c6 Fur the rmore, instead of using just a single frame (i.e., the t+\u0394 current frame) for the future regression, we extend our F\u02c6 networktotakeadvantageof the previous K framestoobtain t+\u0394 \u2026 F\u02c6 as illustrated in Fig. 1: t+\u2206 Hand representa,on network: t+\u0394 Y\u02c6 =h(r([g(X\u02c6 ),...,g(X\u02c6 )])). (6) t+\u2206 t t\u2212(K\u22121) Fig.2. Dataflowof our perceptioncomponentduringtestphase.Itenables The advantage of our formulation is that it allows us to predictingh and scorrespondingto the futureframe.Only",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 285,
      "paper_id": "learningfirsperson",
      "text": "we extend our F\u02c6 networktotakeadvantageof the previous K framestoobtain t+\u0394 \u2026 F\u02c6 as illustrated in Fig. 1: t+\u2206 Hand representa,on network: t+\u0394 Y\u02c6 =h(r([g(X\u02c6 ),...,g(X\u02c6 )])). (6) t+\u2206 t t\u2212(K\u22121) Fig.2. Dataflowof our perceptioncomponentduringtestphase.Itenables The advantage of our formulation is that it allows us to predictingh and scorrespondingto the futureframe.Only the coloredlayers areused for thepredictionin the testphase. predict future hand locations while considering the implicit activity and object context, even without explicit detection ofobjectsin the scene.Ourauto-encoder-basedintermediate We formulate this problem as a regression problem. The representation F\u02c6i abstracts the scene configuration by inter- t main idea is that the intermediate representation of the hand nally representing what objects/hands are currently in the representationnetwork F\u02c6 abstracts the hand-objectinforma- scene and where they are, and our fully convolutional future t tionin the scene,andthatwe are abletotakeadvantageofit regressor takes advantage of it for the prediction. to infer the future (intermediate) representation F\u02c6 . Once t+\u2206 C. Manipulation Component suchregressionbecomespossible,we cansimplyplug-inthe predicted future representation F\u02c6 to the remaining part Although our perception component is able to predict fu- t+\u2206 of the hand network (i.e., h) to obtain the final future hand ture hand locations of humans in first-person human activity predictionresults.Therefore,wenewlydesignanetwork for videos,itisinsufficient for the robotmanipulation.Here,we predicting the intermediate scene representation correspond- construct another regression network (m) for mapping the ing to the future frame F\u02c6 , as a fully convolutional future predicted 2-Dhumanh and locationsin the imagecoordinate t+\u2206 regression network: Fig. 2. totheactualmotorcontrolcommands.Themainassumption Given a current scene representation F\u02c6 from the hand is that a video frame from a robot\u2019s camera will have a t network,ourfutureregressionnetwork(r)predicts the future similar viewpoint to our training data (first-person human the intermediate scene representation F\u02c6 : videos), allowing us to take advantage of the learned model t+\u2206 for the robot future hand prediction by assuming: F\u02c6 =r (F\u02c6 ). (2) t+\u2206 w t Y\u02c6 (cid:39)Y\u02c6 (7) Rt t Ithassevenconvolutionallayershaving 2565\u00d75 kernels.In where, Y\u02c6 represents robot hand locations. addition,ithasalayer with 102413\u00d713 kernelsfollowedby Rt Our manipulation component (m) predicts future robot thelastlayer that has 2561\u00d71 kernel.Wetrained the weights jointstates(Z\u02c6 )givencurrentrobotjointstates(Z\u02c6 ),robot (w) of the regression network with unlabeled first-person t+\u2206 t hand locations (Y\u02c6 ), and future hand locations (Y\u02c6 ) human activity videos using the following loss function: Rt Rt+\u2206 tellingwhere the robot\u2019shandsshouldmoveto.Thisnetwork (cid:88) w\u2217 =argmin (cid:107)r (F\u02c6i)\u2212F\u02c6i (cid:107)2 can be formulated with the below function: w t t+\u2206 2 w i,t Z\u02c6 =m (Z\u02c6 ,Y\u02c6 ,Y\u02c6 ). (8) (cid:88) t+\u2206 \u03b8 t Rt Rt+\u2206 =argmin (cid:107)r (g(X\u02c6i))\u2212F\u02c6i (cid:107)2 (3) w w t t+\u2206 2 Our manipulation component consists of seven fully con- i,t nected layers having the following number of hidden units where X\u02c6i indicates a video frame at time t from video i, for each layer: 32, 32, 32, 16, 16, 16, 7. The weights (\u03b8) of t and F\u02c6i represents a feature map at time t from video i. this network can be obtained by the same way that used for t Our future regression network can use any intermediate our perception networks: scenerepresentation from anyintermediatelayersof the hand (cid:88) \u03b8\u2217 =argmin (cid:107)m (Z\u02c6j,Y\u02c6j ,Y\u02c6j )\u2212Z\u02c6j (cid:107)2 (9) network,",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 286,
      "paper_id": "learningfirsperson",
      "text": "F\u02c6i represents a feature map at time t from video i. this network can be obtained by the same way that used for t Our future regression network can use any intermediate our perception networks: scenerepresentation from anyintermediatelayersof the hand (cid:88) \u03b8\u2217 =argmin (cid:107)m (Z\u02c6j,Y\u02c6j ,Y\u02c6j )\u2212Z\u02c6j (cid:107)2 (9) network, but we use the one from auto-encoder due to its \u03b8 t Rt Rt+\u2206 t+\u2206 2 \u03b8 j,t lowerdimensionality.Finally,thefuturescenerepresentation F\u02c6 t+\u2206 isfedinto the handnetwork for estimatingh and boxes where Z\u02c6j t indicates robot joint states at time t from training correspondingto the futureframetogetfutureh and locations episode j, and Y\u02c6j represents robot hand locations at time Rt Y\u02c6 t+\u2206 . t from training episode j. Fig. 3 shows our manipulation Y\u02c6 =h(F\u02c6 ) (4) component for generating robot control commands. t+\u2206 t+\u2206 Predicted Future Manipulation Network Current frame Hand locations (2) Perception Motor torques (7) Component L L L L L L L L L L L L L L U U U U U U U F F F F F F F Current Hand locations (2) Current status of a robot 32 32 32 16 16 16 7 Current Arm joint angles (7) Fig.3. Robotmanipulationcomponentof our approach.Itgeneratesrobotcontrolcomm and sgivencurrentrobotjointstate,currentroboth and locations, andpredictedfutureroboth and locations. The combination of our perception component and ma- (u,v) in an image plane and the robot\u2019s corresponding joint nipulation component provides a real-time robotics system angles at time t. We recorded these log files by making that takes raw video frames as its input and generates a human operator move the robot arms (i.e., the human motor control commands for its activity execution. Our grabbed the robot arms and moved them). We obtained such manipulation component can be replaced with a standard robot joint configuration sequences while moving the robot Inverse Kinematics, but our neural network-based model to cover possible arm motion during general human-robot generates more natural arm movements by considering the interactiontasks.Here,weassume that the robotissupposed desired location of the robot\u2019s end-effectors as well as joint to operate in a similar environment during the test phase. configuration sequences (i.e., unlabeled robot logs described Note that thiswasnotrecordedunder the interactionscenario in the next section). (i.e., just the robot itself was moving), and no annotation regarding the activity or motion was provided. We used IV. EXPERIMENTS a Baxter research robot for recording these files and the A. Datasets Baxterhassevendegrees-of-freedomarm:thefilecontains 9 Ourapproachconsistsofthreedifferenttypesofnetworks variables for each arm. In order to estimate the robot\u2019s hand (within the twocomponents),andwe usethreedifferenttypes positionin the imageplane,weprojected the 3-Dpositionsof of datasets for training each model. the Baxter\u2019s grippers into the image plane (based on camera Ego Hands [23]: This is a public dataset containing 48 calibration) and recorded the projected (u,v) positions with first-person videos of people interacting in four types of 7 joint angles at 30 Hz. activities(playingcards,playingchess,solvingapuzzle,and B. Baselines playing Jenga).Ithas 4,800 frames with 15,053 ground-truth hand labels. Here, we added 466 frames with 1,267 ground- Inordertoprovidequantitativecomparisons,wecompared truth annotations to the original dataset to cover more hand our perception component with four different baselines: postures.we use this datasettolearnourh and representation (i) Hand-crafted representation uses a",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 287,
      "paper_id": "learningfirsperson",
      "text": "Hz. activities(playingcards,playingchess,solvingapuzzle,and B. Baselines playing Jenga).Ithas 4,800 frames with 15,053 ground-truth hand labels. Here, we added 466 frames with 1,267 ground- Inordertoprovidequantitativecomparisons,wecompared truth annotations to the original dataset to cover more hand our perception component with four different baselines: postures.we use this datasettolearnourh and representation (i) Hand-crafted representation uses a hand-crafted state network, which is trained to locate hand boxes in a video representation based on explicit object and hand detection. frame. It encodes relative distances between all interactive objects Unlabeled Human-Human Interaction Videos: We col- in our two scenarios, and uses it to predict the future lected a total of 47 first-person videos of human-human hand location using neural network-based regression. More collaboration scenarios, with each video clip ranging from specifically, it detects objects using KAZE features [24] 4 to 10 seconds. This dataset is a main dataset for teaching and hands using CNN based hand detector in [23], then a new task to our robot. It contains two types of tasks: (1) computesrelativedistancesbetweenallobjectsandh and sfor a person wearing the camera cleaning up all objects on a building the state representation which is a 20 dimensional tableasapartner(i.e.,theo the rsubject)approaches the table vector. Then, we built a new network which has five fully while holding a heavy box (to make a room for her/him to connected layers trained using the state representations on put the heavy box on the table), and (2) a person wearing the same interaction dataset we use. (ii) Hands only uses the camera pushing a trivet on a table toward to a partner hand locations for the future regression. It predicts future when he/she is approaching the table while holding a hot hand locations solely based on current hand locations with- cooking pan. These videos are unlabeled videos without any out considering any other visual representations. In order activity/hand annotation and we trained our convolutional to train this baseline model, we extracted hand locations regression network using this dataset. from all frames of the interaction videos using our hand Unlabeled Robot Activity Log Files: We prepared this representation network, then made log files to store detected dataset to train our robot manipulation network. It contains handlocationsineachframe and the irframenumbers.After 50 robot log files. Each log has the robot\u2019s hand positions this, we trained another neural network model for the future TABLEI TABLEII EVALUATIONOFFUTUREH AND PREDICTION MEANPIXELDISTANCEBETWEENGROUNDTRUTH AND PREDICTED POSITIONSOFALLHANDS Evaluation Method Precision Recall F-measure Method Mean Pixel Distance Hand-craftedrepresentation 0.30\u00b10.37 0.15\u00b10.19 0.20\u00b10.25 Hand-craftedrepresentation 143.85\u00b148.77 Handsonly 4.78\u00b13.70 5.06\u00b14.06 4.87\u00b13.81 Handsonly 247.88\u00b1121.94 SSD with futureannotations 1 27.53\u00b123.36 9.09\u00b18.96 13.23\u00b112.62 SSD with futureannotations 1 58.58\u00b136.76 SSD with futureannotations 2 29.21\u00b119.16 7.92\u00b16.45 12.10\u00b19.42 SSD with futureannotations 2 79.95\u00b1102.07 Deep Regressor(ours):K=1 27.04\u00b116.50 21.71\u00b114.71 23.45\u00b114.99 Deep Regressor(ours):K=5 29.97\u00b115.37 23.89\u00b116.45 25.40\u00b115.51 Deep Regressor(ours):K=1 51.31\u00b139.10 Deep Regressor(ours):K=10 36.58\u00b116.91 28.78\u00b117.96 30.90\u00b117.02 Deep Regressor(ours):K=5 51.41\u00b138.46 Deep Regressor(ours):K=10 46.66\u00b136.92 TABLEIII hand location prediction using the log files, which has seven fullyconnectedlayers with the samenumberofhiddenunits MEANPIXELDISTANCEBETWEENGROUNDTRUTH AND PREDICTED as our robot manipulation network. (iii) SSD with future POSITIONOFRIGHTH AND annotations 1 is a baseline that uses the original SSD model Method Mean Pixel Distance [6] trained based on Ego Hands",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 288,
      "paper_id": "learningfirsperson",
      "text": "TABLEIII hand location prediction using the log files, which has seven fullyconnectedlayers with the samenumberofhiddenunits MEANPIXELDISTANCEBETWEENGROUNDTRUTH AND PREDICTED as our robot manipulation network. (iii) SSD with future POSITIONOFRIGHTH AND annotations 1 is a baseline that uses the original SSD model Method Mean Pixel Distance [6] trained based on Ego Hands dataset. Instead of training Hand-craftedrepresentation 121.48\u00b187.36 the model to infer the current hand locations given the Handsonly 264.52\u00b1148.15 input frame, we fine-tuned this model on Ego Hands dataset SSD with futureannotations 1 48.63\u00b139.04 SSD with futureannotations 2 71.36\u00b1104.18 after changing annotations of the dataset to have \u201cfuture\u201d Deep Regressor(ours):K=1 40.08\u00b132.72 locations of hands instead of making it to use current hand Deep Regressor(ours):K=5 40.46\u00b139.52 locations.Wealsousedadditionally 466 frames for thisfine- Deep Regressor(ours):K=10 36.78\u00b136.70 tuning since the original Ego Hands dataset was insufficient (too many repetitive hand movements) for this training. (iv) SSD with future annotations 2 is a baseline also using the tions of hands. The size of the image plane was 1280*720. original SSDmodel,butwetrained this model from scratch. We measured this mean pixel distance only when both the This time we changed all annotations of the Ego Hands ground truths and the predictions are present in the same dataset, then trained the model. After that we fine-tuned the frame. Table II shows the mean pixel distance errors for model as the same way that used for the \u201cSSD with future all four types of hands (my left, my right, your left, and annotations 1\u201d baseline. your right). Once more, we can confirm that our approaches greatly outperform the per for mance of all the baselines. The C. Evaluation of our future hand prediction overall average distance was a bit high due to changes We first evaluated the perception component of our ap- in human hand shapes and their variations, but they were proach in terms of precision, recall, and F-measure, and sufficient in terms of generating robot motion. compared them against the above baselines. In the first Wealsocomp are daccuraciesof the semethodswhileonly evaluation,wemade our approachtopredictboundingboxes considering my right hand predictions, since position of my of human hands in the future frame given the current image right hand is more important for a robot manipulation than frame. We measured the \u201cintersection over union\u201d ratio locationsofo the rtypesofhands.Thisisbecause,inourtest between are asofeachpredictedbox and groundtruth(future) scenarios, the robot\u2019s activities are very focused on its right hand locations. Only when the ratio was greater than 0.5, hand motion. Table III shows mean pixel distance between the predicted box was accepted as a true positive. In this ground truth and predicted position of \u2018my right hand\u2019. We experiment,wer and omlysplit the setof our Human-Human can see that per for mances of our approaches are superior to Interaction Videos into the training and testing sets, so 32 all the baselines. Examples of our visual predictions results videos were used for training sets and remaining 15 videos are illustrated in Fig. 4. were used for testing sets in a total of 47 videos. D. Real-time robot experiments Table I shows quantitative results of our future hand prediction. Here, the plus-minus",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 289,
      "paper_id": "learningfirsperson",
      "text": "baselines. Examples of our visual predictions results videos were used for training sets and remaining 15 videos are illustrated in Fig. 4. were used for testing sets in a total of 47 videos. D. Real-time robot experiments Table I shows quantitative results of our future hand prediction. Here, the plus-minus sign (\u00b1) indicates standard Finally, we conducted a user study to evaluate the success deviation and K represents number of frames we used as an level of robot activities per for med based on our proposed input for ourregressionnetwork.Our\u2206was 30 frames(i.e., approach, with human subjects. A total of 12 participants 1 sec). We are able to clearly observe that our approach (5 undergraduate and 7 graduate students) were recruited signifi can tly outperforms all the baselines, including the from the campus, and were asked to perform one of the state-of-the-art object detector SSD modified for the hand two activities (clearing the table for a partner and preparing prediction. Our proposed network with K = 10 yielded the a trivet for a cooking pan) together with our robot. After best per for mance in terms of all three metrics, at about 30.9 such interactions, the participants were asked to complete a score in F-measure. The best per for mance we can get with questionnaire about the robot behaviors for each task. The SSD was only 13.23. questionnaire had two statements (one statement for each In our second evaluation, we measured mean pixel dis- activity)withscales from 1(totallydonotagree)to 5(totally tancebetweenground truthlocations and the predictedposi- agree) to express their impression on the robot behaviors: \u201cI Time (t) se m a rf tu p n I sn o itc id e r P Predictions overlaid on future frames Time (t) se m a rf tu p n I sn o itc id e r P Predictions overlaid on future frames Fig.4. Twoexamplesof our visualprediction.Thefirstexampleistheactivityofclearing the table,andthesecondexampleis the activityofpushing the trivet toward the person holding a cooking pan. The first row shows the input frames and the second row shows our future hand prediction results. Inthethirdrow,weoverlaid our predictionson\u201cfuture\u201dframes.Redboxescorrespondto the predicted\u2018mylefth and\u2019locations,blueboxescorrespond to \u2018my right hand\u2019, green boxes correspond to the opponent\u2019s left hand, and the cyan boxes correspond to the opponent\u2019s right hand. The frames were capturedeveryonesecond. TABLEIV think the robot cleared the table to make a space for me.\u201d THESUCCESSLEVELOF OUR HUMAN-ROBOTCOLLABORATION for the task 1 and \u201cI think the robot passed a trivet closer to me so that I can put the cooking pan on it.\u201d for the task 2. Method Task 1 Task 2 Average Inadditionto our approach(i.e.,ourperceptioncomponent Base SSD+Basecontrol 1.25\u00b10.43 2.21\u00b11.41 1.72\u00b10.92 Base SSD+Ourcontrol 1.5\u00b10.96 2.33\u00b11.60 1.92\u00b11.28 + manipulation component), we designed and implemented Ourperception+Basecontrol 2.33\u00b11.18 2.25\u00b11.36 2.29\u00b11.27 thefollowingthree base lines and compared the irquantitative Ours 3.17\u00b11.40 3.42\u00b11.61 3.29\u00b11.50 results: (i) Base SSD + Base control uses the baseline SSD with future annotations 1 as a perception component and the base manipulation network trained using the same our real-time robot experiments with human subjects are robot activity log files. This base control network direct illustrated in Fig. 5. maps current hand locations in the",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 290,
      "paper_id": "learningfirsperson",
      "text": "Base control uses the baseline SSD with future annotations 1 as a perception component and the base manipulation network trained using the same our real-time robot experiments with human subjects are robot activity log files. This base control network direct illustrated in Fig. 5. maps current hand locations in the image plane to current Our method operates in slow real-time with our unopti- seven joint angles for each robot arm, without the Z\u02c6 term mized C++ code. It takes \u223c100 ms per frame using one t in Eq. 8. (ii) Base SSD + Our control uses SSD with Nvidia Pascal Titan X GPU, and we were able to conduct future annotations 1 as a perception component and our ma- real-time human-robot collaboration experiments using it. nipulationcomponent(from Section III-C)togeneratemotor commands. (iii) Our perception + Base control used our V. CONCLUSION perception component to predict future hand locations and In this paper, we proposed a new robot activity learning the base controlnetwork for manipulation.Inall the secases, model using a fully convolutional network for future repre- the final control of our robot arm is per for med by taking sentation regression. The main idea was to make the robot advantage of the Baxter API by providing the estimated learn the temporal structure of a human activity as its future future joint angle configuration. regression network, and learn to transfer such model for its Asaresult,eachparticipantinteracted with the robottotal own motor execution using our manipulation network. We of 8 timesinar and omorder.Table IVshows the results.The show that our approach enables the robot to infer the motor resultsindicate that ourparticipantsevaluated the robot with control commands based on the prediction of future human our approach per for med better on both tasks. We received a handlocationsinreal-time.Theexperimentalresultsconfirm higher average score of 3.29 compared to all the baselines that our approach not only predicts the future locations of (1.72, 1.92, and 2.29) from the participants. Examples of human/robot hands more reliably, but also is able to make Time (t) w e iv s\u2019to b o R w e iv n o sre p dr 3 Time (t) w e iv s\u2019to b o R w e iv n o sre p dr 3 Fig.5. Qualitativeresultsof our real-timerobotexperiments.Similarto Fig.4,there are twoexamples:clearing the table,andpushing the trivettoward theperson.Ineachexample,thefirstrowshows the exactframesusedasinputsto our robot(taken from arobotcamera),and the secondrowshows the robot and the humanfroma 3 rdpersonviewpoint.Theframes were capturedeveryonesecond. robots execute the activities based on predictions. The paper [11] K. Mu\u00a8lling, J. Kober, O. Kroemer, and J. Peters, \u201cLearning to focusesonrobotlearningoflocation-basedh and movements select and generalize striking movements in robot table tennis,\u201d The International Journalof Robotics Research,2013. (i.e., translations and natural rotations), and handling more [12] T. Shu, M. S. Ryoo, and S.-C. Zhu, \u201cLearning social affordance dynamic hand posture changes remains as one of our future for human-robot interaction,\u201d in International Joint Conference on challenges. Artificial Intelligence(IJCAI),2016. [13] H. Koppula and A. Saxena, \u201cPhysically-grounded spatio-temporal Acknowledgement: This work was supported by the Army object affordances,\u201d in European Conference on Computer Vision (ECCV),2014. Research Laboratory under Cooperative Agreement Number [14] J.Walker,A.Gupta,and M.Hebert,\u201cPatchto the future:Unsupervised W 911 NF-10-2-0016. visual",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 291,
      "paper_id": "learningfirsperson",
      "text": "human-robot interaction,\u201d in International Joint Conference on challenges. Artificial Intelligence(IJCAI),2016. [13] H. Koppula and A. Saxena, \u201cPhysically-grounded spatio-temporal Acknowledgement: This work was supported by the Army object affordances,\u201d in European Conference on Computer Vision (ECCV),2014. Research Laboratory under Cooperative Agreement Number [14] J.Walker,A.Gupta,and M.Hebert,\u201cPatchto the future:Unsupervised W 911 NF-10-2-0016. visual prediction,\u201d in IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2014. [15] W. Lotter, G. Kreiman, and D. Cox, \u201cDeep predictive coding net- REFERENCES works for videoprediction and unsupervisedlearning,\u201dar Xivpreprint ar Xiv:1605.08104,2016. [1] S. Levine, C. Finn, T. Darrell, and P. Abbeel, \u201cEnd-to-end training [16] C.Finn and S.Levine,\u201cDeepvisualforesight for planningrobotmo- ofdeepvisuomotorpolicies,\u201dJournalof Machine Learning Research, tion,\u201din IEEEInternational Conferenceon Robotics and Automation 2016. (ICRA),2017. [2] C. Finn, I. Goodfellow, and S. Levine, \u201cUnsupervised learning for [17] K.M.Kitani,T.Okabe,Y.Sato,and A.Sugimoto,\u201cFastunsupervised physicalinteractionthroughvideoprediction,\u201din Advances In Neural ego-actionlearning for first-personsportsvideos,\u201din IEEEConference Information Processing Systems(NIPS),2016. on Computer Vision and Pattern Recognition(CVPR),2011. [3] K. Lee, Y. Su, T.-K. Kim, and Y. Demiris, \u201cA syntactic approach [18] A. Fathi, A. Farhadi, and J. M. Rehg, \u201cUnderst and ing egocentric to robot imitation learning using probabilistic activity grammars,\u201d activities,\u201d in International Conference on Computer Vision (ICCV), Robotics and Autonomous Systems,2013. 2011. [4] Y. Yang, Y. Li, C. Fermu\u00a8ller, and Y. Aloimonos, \u201cRobot learning [19] H.Pirsiavash and D.Ramanan,\u201cDetectingactivitiesofdailylivingin manipulation action plans by\u201d watching\u201d unconstrained videos from first-person camera views,\u201d in IEEE Conference on Computer Vision theworldwideweb.\u201din AAAI,2015. and Pattern Recognition(CVPR),2012. [5] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, \u201cA survey [20] M. S. Ryoo, B. Rothrock, and L. Matthies, \u201cPooled motion features of robot learning from demonstration,\u201d Robotics and Autonomous forfirst-personvideos,\u201din IEEEConferenceon Computer Vision and Systems,2009. Pattern Recognition(CVPR),2015. [6] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and [21] M. S. Ryoo, T. J. Fuchs, L. Xia, J. K. Aggarwal, and L. Matthies, A.Berg,\u201cSSD:Singleshotmultiboxdetector,\u201din European Confer- \u201cRobot-centricactivityprediction from first-personvideos:What will enceon Computer Vision(ECCV),2016. they do to me?\u201d in ACM/IEEE International Conference on Human- [7] C.Vondrick,H.Pirsiavash,and A.Torralba,\u201cAnticipatingvisualrep- Robot Interaction(HRI),2015. resentations with unlabeledvideo,\u201din IEEEConferenceon Computer [22] I. Gori, J. K. Aggarwal, L. Matthies, and M. S. Ryoo, \u201cMulti-type Vision and Pattern Recognition(CVPR),2016. activity recognition in robot-centric scenarios,\u201d IEEE Robotics and [8] A.Billard,S.Calinon,R.Dillmann,and S.Schaal,\u201cRobotprogram- Automation Letters(RA-L),2016. mingbydemonstration,\u201din Springerh and bookofrobotics,2008. [23] S. Bambach, S. Lee, D. J. Crandall, and C. Yu, \u201cLending a hand: [9] A.Gupta,C.Eppner,S.Levine,and P.Abbeel,\u201cLearningdexterous Detecting hands and recognizing activities in complex egocentric manipulation for asoftrobotichand from hum and emonstrations,\u201din interactions,\u201d in IEEE International Conference on Computer Vision IEEE/RSJInternational Conferenceon Intelligent Robots and Systems (ICCV),2015. (IROS),2016. [24] P. F. Alcantarilla, A. Bartoli, and A. J. Davison, \u201cKaze features,\u201d in [10] A.L.Thomaz and M.Cakmak,\u201cLearningaboutobjects with human European Conferenceon Computer Vision(ECCV),2012. teachers,\u201d in ACM/IEEE International Conference on Human-Robot Interaction(HRI),2009.",
      "start_pos": 5082,
      "end_pos": 5522
    },
    {
      "chunk_id": 292,
      "paper_id": "ReBot",
      "text": "Teaser Real-to-Sim Trajectory Re Bot: Scaling Robot Learning with Replay Real-to Rea-l-Swoirldm-to-Real Robotic Video Syn the sis Background Inpainting Real-world Real-to-Sim Trajectory Replay Yu Fang 1, Yue Yang 1, X Baicnkggrhouanod In Zpahinuti 2 ng, Kaiyuan Zheng 3, Gedas Bertasius 1, Daniel Szafir 1, Mingyu Ding 1 Sim-to-Real VLA Abstract\u2014Vision-language-action Rea ( l V -to L -S A im ) models present a Open VLA Video Models Trajectory Replay Syn the sis promising paradigm by training policies directly on real robot Octo w/o Re Bot datasets like Open X-Embodimen Rte.al H-woorwldever, the high cost w/ Re Bot of real-world data collection Bhacikngdroeurnsd Infpuarintthinegr data scaling, Bridge Data V 2 DROID Re Bot fine tuned thereby restricting the generalizability of VLAs. In this paper, Real Robot Datasets VLA per for mance VLA Models we introduce Re Bot, a novel real-to-sim-Rteoa-lr-teoa-Slimap proach for Trajectory Replay scaling real robot datasets and adapting VLA models to Episode: Move the yellow mug to New Episode: target domains, which is the last-mile deployment challenge in the front right side of the table Put the spoon on the towel robotmanipulation.Specifically,Re Botrep R l e a a y l s -w r o e r a ld l- worldrobot Put spatula on cutting board Background Inpainting trajectoriesinsimulationtodiversifymanipulatedobjects(real- Real-to-Sim Trajectory Real-world Robot Trajectories to-sim),andintegrates the simulatedmovements with inpainted Replay Sim-to-Real real-world background to syn the size p Shiyms-itcoa-Rlleyal realistic and Video Video Syn the sis Syn the sis temporally consistent robot videos (sim-to-real). Our approach has several advantages: 1) it enjoys the benefit of real data Real Video Real-world Real-to-Sim-to-Real Background to minimize the sim-to-real gap; 2) it leverages the scalability Syn the tic Video Inpainting of simulation; and 3) it can generalize a pretrained VLA to a Take banana out of colander target domain with fully automated data pipelines. Extensive Mug Spoon experiments in both simulation and real-world environments show that Re Bot signifi can tly enhances the per for mance and robustness of VLAs. For example, in Simpler Env with the Fig. 1. An overview of Re Bot. We propose Re Bot, a novel real-to- Widow X robot, Re Bot improved the in-domain per for mance sim-to-real approach for scaling real robot datasets. Re Bot replays real- of Octo by 7.2% and Open VLA by 21.8%, and out-of-domain worldrobottrajectoriesinasimulationenvironmenttodiversifymanipulated generalizationby 19.9%and 9.4%,respectively.Forreal-world objects(real-to-sim),andintegrates the simulatedmovements with inpainted evaluation with a Franka robot, Re Bot increased the success real-world background to produce realistic syn the tic videos (sim-to-real), effectivelyadapting VLA model stotargetdomains. ratesof Octoby 17%and Open VLAby 20%.Morein for mation can be found at our project page. generalizing to real-world applications [12, 13], limiting the I. INTRODUCTION effectiveness of simulated data for advancing VLAs. Large-scale real robot datasets have demonstrated their To tackle these challenges, a straight for ward strategy for significant contribution to the rapid advances of robot learn- scaling robot learning is generating syn the tic robot videos ing [1\u20133], enabling vision-language-action (VLA) models to from real robot datasets. With the rapid development of learn across various tasks, environments, and embodiments. foundation models in computer vision and generative",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 293,
      "paper_id": "ReBot",
      "text": "for significant contribution to the rapid advances of robot learn- scaling robot learning is generating syn the tic robot videos ing [1\u20133], enabling vision-language-action (VLA) models to from real robot datasets. With the rapid development of learn across various tasks, environments, and embodiments. foundation models in computer vision and generative AI, Despite these achievements, VLAs still face challenges researchers have introduced generative models for syn the tic in effectively generalizing to new scenarios, spurring the robot video generation [14\u201316]. For example, methods [17\u2013 need for scaling data to enhance their per for mance in new 19] have leveraged text-to-image inpainting to scale real target domains. However, collecting large-scale real robot robotic images to diverse scenarios. However, they typically datasets is very costly and often demands extensive effort face the issue of AI-generated artifacts such as visible and res our ces, e.g., robots and human teleoperators, which imperfections or inconsistent textures, failing to produce signifi can tly limits the availability and scalability [4, 5]. physically realistic and temporally consistent robot videos. On the other hand, simulated datasets are more accessible Such distortions introduce new domain gaps, making it dif- and cost-effective alternatives, as they can be generated ficult for VLAs to learn stable and continuous robot actions in simulation environments without real-world setups [6\u2013 while raising reliability concerns. Additionally, generated 11]. Unfortunately, the sim-to-real gap in both the action images may not adhere precisely to instruction conditions, space and the observation space hinders robot policies from limiting the effectivenessofsuchmethodsinadapting VLAs to specific target domains, leaving the last-mile deployment 1 Yu Fang,Yue Yang,Gedas Bertasius,Daniel Szafir,and Mingyu Ding challenge in robot manipulation unresolved. arewith Departmentof Computer Science,Universityof North Carolinaat To mitigate these issues, we propose Re Bot, a novel real- Chapel Hill,201 SColumbia St,Chapel Hill,NC 27599,USA.{yufang, yygx, gedas, dszafir, md}@cs.unc.edu to-sim-to-real approach for scaling real robot datasets and 2 Xinghao Zhu is with Robotics and AI Institute, 145 Broadway, adapting VLA models to target domains. Our key insight Cambridge,MA 02142,USA.xizhu@rai-inst.com is to replay real-world robot trajectories in simulation to 3 Kaiyuan Zhengis with Departmentof Electrical and Computer Engi- diversify manipulated objects (real-to-sim), and integrate neering,Universityof Washington,1410 NECampus Parkway,Seattle,WA 98195,USA.kaiyuan 5@uw.edu the simulated movements with inpainted real-world back- 5202 ra M 51 ]VC.sc[ 1 v 62541.3052:vi Xra ground (sim-to-real) to syn the size physically realistic and real robot datasets demands extensive res our ces, making it temporallyconsistentrobotvideos.Notably,Re Botcombines highly challenging to scale across diverse environments and the advantages of both sim and real, i.e., leveraging the tasks.Thislimitationhinders the generalizationper for mance scalability of simulation, while minimizing the sim-to-real of VLA models. On the other hand, simulated datasets gap by grounding both the action and observation spaces offer a more scalable alternative. Well-developed simulation from real robot data. Particularly, in contrast to generation- platforms [31\u201334] facilitate rapid data collection in con- based scaling approaches, Re Bot ensures physical realism trolled environments without the high cost of real-world and temporal consistency, and enables effective adaptation experiments. Unfortunately, these datasets often introduce of VLA models to target domains. significant sim-to-real gap [12], limiting their effectiveness Specifically, as shown in",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 294,
      "paper_id": "ReBot",
      "text": "data collection in con- based scaling approaches, Re Bot ensures physical realism trolled environments without the high cost of real-world and temporal consistency, and enables effective adaptation experiments. Unfortunately, these datasets often introduce of VLA models to target domains. significant sim-to-real gap [12], limiting their effectiveness Specifically, as shown in Fig. 1, Re Bot includes three in real-world applications. Notably, recent works have ex- key components: 1) Real-to-Sim Trajectory Replay. For ploredgenerative model sto scale realrobot data sets[17\u201319]. each real-world episode, we automatically set up digital Yet, these approaches often struggle to provide physically twins in a simulation environment, and replay the real- realistic and temporal consistent robot videos, making them world robot trajectory to obtain simulated movements for unreliable and ineffective for developing VLA models. In manipulating new objects. We validate the scalability of this paper, we propose a real-to-sim-to-real approach for our approach by demonstrating that real-world trajectories scalingrealrobot data sets,offeringanovelsolution for these can be successfully reused to manipulate different shapes of longst and ing challenges. objects in simulation. 2) Real-world Background Inpainting. Real-to-sim and Sim-to-real. Real-to-sim and sim-to-real To obtain task-agnostic real-world background for video strategies have been explored in many applications in syn the sis,weintroduceanautomatedinpaintingmodule with robotics [13, 35\u201338]. Notably, recent work has leveraged Grounded SAM 2 [20] to segment and track the robot and real-to-sim-to-real strategy to develop simulated evaluation object (i.e., task-specific elements) in original real-world platforms for robotics [39], demonstrating a strong cor- videos, and remove them with Pro Painter [21]. 3) Sim-to- relation with real-world robot evaluations. These studies Real Video Syn the sis. We eventually integrate simulated highlight the significant potential of real-to-sim-to-real ap- movements with task-agnostic real-world background, pro- proaches in bridging the gap between simulation and real- ducing syn the tic videos with realistic physics and excellent world environments. However, existing methods often face temporal consistency. scalability challenges due to limited scene and object diver- In summary, our key contributions are three-fold. sity, primarily due to the substantial manual effort for con- \u2022 We introduce Re Bot, which, to our knowledge, is the first structing digital twins in simulation environments [37, 39]. real-to-sim-to-real approach for scaling real robot datasets In this paper, we explore a new application of this strategy, and adapting VLA models to target domains, addressing i.e., for scaling real robot datasets, enabling realistic robotic the last-mile deployment challenge in robot manipulation. video generation without manual intervention. \u2022 Re Bot combines the advantages of both sim and real, i.e., leveraging the scalability of simulation, while minimizing III. METHOD the sim-to-real gap by grounding both the action and In this paper, we propose a novel real-to-sim-to-real ap- observation spaces from real robot data. Notably, Re Bot proach for scaling real robot datasets. We define a real robot is fully automated and requires no manual intervention. dataset as D = {\u03c4 }M , where M episodes are represented \u2022 Extensive evaluations confirm Re Bot\u2019s effectiveness in as \u03c4 = {o ,a ,L i } i T =1 . Here, t denotes the timestep, o both simulation and real-world settings, e.g., it",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 295,
      "paper_id": "ReBot",
      "text": "and requires no manual intervention. dataset as D = {\u03c4 }M , where M episodes are represented \u2022 Extensive evaluations confirm Re Bot\u2019s effectiveness in as \u03c4 = {o ,a ,L i } i T =1 . Here, t denotes the timestep, o both simulation and real-world settings, e.g., it improves i t t t=1 t is the video frame, a is the action, L is the language t Open VLA\u2019s in-domain and generalization per for mance by instruction. Our goal is to produce new syn the tic episodes 21.8% and 9.4% on Simpler Env and achieves a 20% gain \u03c4\u2032 ={o\u2032,a ,L\u2032}T basedon\u03c4 ,tobuildasyn the tic data set inreal-worldtasks,signifi can tlyoutper for mingpriorstate- j t t t=1 i D\u2032 = {\u03c4\u2032}N for adapting VLA models to target domains. of-the-art ROSIE [18]. j j=1 As illustrated in Fig. 2, Re Bot has three key steps: A) Real- II. RELATEDWORK to-Sim Trajectory Replay to obtain simulated movements {osim}T inasimulationenvironment(Sec.III-A);B)Real- Scaling Robot Learning. Although many research insti- t t=1 world Background Inpaintingonvideoframe{o }T toob- tutes have collaborated to construct large-scale real robot t t=1 tain task-agnosticreal-worldbackground{oreal}T (Sec.III- datasets [4, 5], data scale remains a fundamental bottle- t t=1 B);andeventually C)Sim-to-Real Video Syn the sistoobtain neck for VLA models. To address this issue, recent works new frame {o\u2032}T (Sec. III-C). have explored three primary strategies: 1) collecting data t t=1 in real-world environments, 2) collecting data in simulation A. Real-to-Sim Trajectory Replay environments, and 3) scaling real robot datasets with gen- erative models. Real robot datasets can be acquired using The real-to-sim process involves: 1) Creating spatially various methods, including kines the tic teaching [22, 23], aligned digital twins of the scene in the simulation environ- teleoperation [5, 24\u201326], or mixed reality devices [27, 28], ment, 2) Replaying real-world robot trajectory to produce and have signifi can tly contributed to the recent progress simulated robot movements {osim}T , 3) Validating each t t=1 in VLA models [29, 30]. However, collecting large-scale replayed trajectory to ensure successful object manipulation. Framework Latest version Real-to-Sim Trajectory Replay (Sec. III-A) Real-world Background Inpainting (Sec. III-B) Sim-to-Real Video Syn the sis (Sec. III-C) (1) Scene Parsing and Alignment (2) Trajectory Replay Object and Robot Segmentation Real-world Episode Object Container \"robot\" Pr T o e m xt pt Time (e.g., spoon) (e.g., towel) Frames P P ro o m in p t t Grounded SAM 2 Action Instruction : Move the yellow mug to Simulated Movements Syn the size the front right side of the table (3) Replay Validation Syn the tic Episode Robot Camera Semantic Masks Time Pro Painter New Table Height Frames Action Digital Twins (tableis invisible later) Task-agnostic Real-world Instruction : Put the spoon on the towel Point Cloud Filtered Points Background Fig.2. An overview of our framework.Re Botincludesthreekeycomponents:A) Real-to-Sim Trajectory Replay:Foreachreal-worldepisode,we automatically set up digital twins and replay the real-world trajectory to obtain simulated movements for manipulating new objects. Each trajectory can bereused for differentobjects.B)Real-world Background Inpainting:Toobtain task-agnosticreal-worldbackground for videosyn the sis,weintroduce an automated inpainting module to segment and remove the robot and object from the original",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 296,
      "paper_id": "ReBot",
      "text": "Botincludesthreekeycomponents:A) Real-to-Sim Trajectory Replay:Foreachreal-worldepisode,we automatically set up digital twins and replay the real-world trajectory to obtain simulated movements for manipulating new objects. Each trajectory can bereused for differentobjects.B)Real-world Background Inpainting:Toobtain task-agnosticreal-worldbackground for videosyn the sis,weintroduce an automated inpainting module to segment and remove the robot and object from the original real-world video. C) Sim-to-Real Video Syn the sis: We eventuallyintegratesimulatedmovements with task-agnosticreal-worldbackgroundtoproducesyn the ticvideos.Re Botis full yautomated and requiresno manualintervention. Scene Parsing and Alignment.Toensurefaithfultrajectory distance between the object and gripper from t to t . TODO list: start end replay, we construct digital twins of the robot, cameras, and We present a representative example in Fig. 2, showing that - Notation table, and align them to the initial video frame o . The despite the disparity of object shapes, real-world trajectories - Integration in Sim-to-real 1 prototypes o-f Cthhaengreo Ibmoatgeasnd cameras are prepared ahead, can be successfully reused to manipulate various objects, only requiring pose adjustments to complete their setup. demonstrating the scalability of our approach. To determine the table height, we acquire the metric depth B. Real-world Background Inpainting from the initial video frame o and create a point cloud 1 of the scene. Using Grounding DINO [40], we automatically In this step, we prep are task-agnostic real-world back- segment the table with the text prompt (\u201ctable\u201d), and extract ground{or t eal}T t=1 forintegration with simulatedmovements, the subset of the point cloud after removing outliers using by removing task-specific elements (i.e., the original real the interquartile range. We eventually set the average height object and robot) in the original real robot video {o t }T t=1 . of the filtered points as the table height. Object and Robot Segmentation. We automatically seg- ment and track the original real object and robot by using Trajectory Replay. We reuse the real-world trajectory to Grounded SAM 2[20],whichcombines Grounding DINO[40] diversify manipulated objects. First, to ensure the robot can and SAM 2 [41]. More specifically, we first use Ground- successfully reach the simulated object, we need to place it ing DINO to identify and segment the robot using the text exactlywhere the originalrealobjectwasplaced.Weanalyze prompt (\u201crobot\u201d) on o , as we empirically observe the the gripper action sequence to determine t (when the tstart start best per for mance when the robot is most visible. However, gripperclosestograsp the object)andt (when the gripper end automaticallyidentifying the originalrealobjectisextremely opens to place the object). To estimate the object position, challenging, as a detailed description of its appearance, weacquire the gripperpositionatt byreplaying{a }tstart, start t t=1 which is essential for effective text prompts, is typically and place the simulated object accordingly. Similarly, and unavailableinrealrobot data sets.Moreover,textprompts are optionally, we place a container on the table at the gripper highly susceptible to distractors or similar instances, mak- position at t . Finally, we replay the robot trajectory end ing them unreliable for accurately locating the manipulated using the action sequence {a }T , and record simulated t t=1 object. Fortunately, the object position at t is already movements{osim}T formanipulating the newobject.Note start t t=1 estimated during real-to-sim trajectory replay, now serving",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 297,
      "paper_id": "ReBot",
      "text": "Finally, we replay the robot trajectory end ing them unreliable for accurately locating the manipulated using the action sequence {a }T , and record simulated t t=1 object. Fortunately, the object position at t is already movements{osim}T formanipulating the newobject.Note start t t=1 estimated during real-to-sim trajectory replay, now serving that all digital twins are faithfully aligned to the real-world as a crucial cue for segmenting the real object on o . scene, this ensure the recorded movements remain aligned tstart Using the camera pose, we project the 3 D object position with the real-world background. onto o , providing a 2 D point prompt for real object tstart Replay Validation. Notably, trajectory replay may succeed segmentation with SAM 2.Afterobtaining the semanticmask orfailinmanipulatinga new object,dependingon the affor- m (i.e.,therobot and objectmasksatt ),wepropagate tstart start dance compatibility between the new object and the original it to all video frames {o }T using SAM 2, generating the t t=1 real-world object. We automatically validate whether the corresponding semantic masks {m }T . t t=1 objectissuccess full ymanipulatedineachsyn the ticepisode, Object and Robot Removal.Given{o ,m }T ,weeventu- t t t=1 and discard failed episodes by monitoring the Cartesian allyapply Pro Painter[21],astate-of-the-artvideoinpainting Results Original Video ROSIE Re Bot (Ours) Original Video ROSIE Re Bot (Ours) Original Video ROSIE Re Bot (Ours) em i T Move red bull can to the left \u2192 Move coke can to the left Put spatula on cutting board \u2192Put spoon on cutting board Put grape in pink bowl \u2192Put carrot in pink bowl Fig. 3. Comparison of syn the tic videos. We show examples from three datasets: DROID (left), Bridge Data V 2 (mid), and our dataset (right). Re Bot generatesrealisticvideos with physicallyplausiblemovements and excellenttemporalconsistency,signifi can tlyoutper for ming ROSIE. model, to remove the original real object and robot from Implementation Details. We use Isaac Sim 4.1 as our sim- the original video, obtaining the task-agnostic background ulation environment for its excellent rendering quality and {oreal}T . Notice that we also remove the real robot in flexibility. We implement the real-to-sim trajectory replay t t=1 this step and later use the virtual robot in our syn the tic based on Isaac Lab [34]. We pre-build digital twins of the videos{o\u2032}T .Thisensurescorrectocclusions and realistic robotsin Isaac Sim,matching the samerobotplat for msasper t t=1 physical interactions during object manipulation. realrobot data sets,i.e.,using Widow X 2506 DOFrobotarm for Bridge Data V 2 and Franka Panda 7 Do F robot arm with C. Sim-to-Real Video Syn the sis Robotiq 2 F-85 gripper for DROID and our data set.Following We eventually combine simulated movements {osim}T t t=1 the official guidelines of Octo and Open VLA, we use 100 with task-agnosticreal-worldbackground{oreal}T tobuild t t=1 syn the tic episodes per task as the optimal data volume for new video frames {o\u2032}T . Specifically, to obtain o\u2032, we t t=1 t fine tuning. We use four NVIDIA A 6000 GPUs, using full extract the robot and the manipulated object from osim, and t fine tuning with a batch size of 256 and a learning rate of merge them onto",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 298,
      "paper_id": "ReBot",
      "text": "video frames {o\u2032}T . Specifically, to obtain o\u2032, we t t=1 t fine tuning. We use four NVIDIA A 6000 GPUs, using full extract the robot and the manipulated object from osim, and t fine tuning with a batch size of 256 and a learning rate of merge them onto or t eal. We then assign a new language 4\u00d710\u22125 for Octo, and Lo RA fine tuning with a batch size instruction L\u2032 by replacing the object (e.g., \u201cyellow mug\u201d to of 32 and a learning rate of 5\u00d710\u22124 for Open VLA. \u201cspoon\u201d)andcontainer(e.g.,\u201ctable\u201dto\u201ctowel\u201d)intheorigi- Methods for Comparison. We comp are Re Bot with nalinstruction Lto the oneswe usedduringtrajectoryreplay. ROSIE [18], a state-of-the-art generation-based method for Eventually,weconstructanewepisode\u03c4\u2032 ={o\u2032,a ,L\u2032}T . j t t t=1 scalingrealrobotvideos.ROSIEemploysimage-basedfoun- Note that, since we faithfully replay real-world robot trajec- dation models, using Imagen [44] to inpaint manipulated tories, the real-world actions remain unchanged in syn the tic objects directly on original real robot videos. In contrast, episodes. In our experiments (see Sec. IV), we validate the Re Botintroducesanovelreal-to-sim-to-realscalingstrategy, effectiveness of our method for adapting VLA models with producingphysicallyrealisti can dtemporallyconsistentsyn- our syn the tic dataset D\u2032 ={\u03c4\u2032}N . j j=1 thetic robot videos. Since ROSIE is not open-source, we use IV. EXPERIMENTS ourimplementation base don the stablediffusion model[45]. Evaluation with VLA Models. We evaluate the effec- In this section, we evaluate and demonstrate that Re- tiveness of syn the tic videos for adapting VLA models to Bot effectively produces high-fidelity syn the tic robot videos target domains. We mainly discuss two state-of-the-art VLA (Sec. IV-B), and comprehensively enhances the per for mance models, Octo [29] and Open VLA [30], both of which of VLA model sinbothsimulation(Sec.IV-C)andreal-world are trained on large and diverse datasets involving various environments (Sec. IV-D). robotic embodiments [4]. To comp are scaling methods, we A. Experimental Setups evaluate three versions of each VLA model: 1) Octo and Datasets. Forrealrobot data sets,weleveragetabletoppick- Open VLA (zero-shot evaluation, i.e., pre-trained models and-place episodes in Bridge Data V 2 [42] and DROID [5]. without fine tuning), 2) Octo+ROSIE and Open VLA+ROSIE For evaluation in real-world environments in Sec. IV-D, we (fine tuned with episodes from ROSIE), and 3) Octo+Re Bot collect 220 real-world episodes to build our dataset. In the and Open VLA+Re Bot(finetunedwi the pisodes from Re Bot). DROID dataset, we leverage two exterior videos captured B. Evaluation of Video Quality from opposite sides of the robot. For simulated objects used inreal-to-simtrajectoryreplay,wefollow[11,39]andcollect We comp are the generated video quality of ROSIE [18] kitchen assets from Objaverse [43]. and Re Bot across three aspects: Temporal Quality, Imaging Results \u0000/\u0000\u0175\u0000\u0102\u0000\u0150\u0000\u015d\u0000\u0176\u0000\u0150 \u0000\u03f1\u0000\u03ef\u0000\u0358\u0000\u03f0\u0000\u0439 \u0000\u03f2\u0000\u03f2\u0000\u0358\u0000\u03f0\u0000\u0439 \u0000Z \u0000Z \u0000K \u0000\u011e\u0000\u0011 \u0000^ \u0000\u017d \u0000/\u0000 \u0000\u019a\u0000\u0003\u0000\u037e\u0000K\u0000\u01b5\u0000\u018c\u0000\u0190\u0000\u037f o ed i V \u0000Y\u0000\u01b5\u0000\u0102\u0000\u016f\u0000\u015d\u0000\u019a\u0000\u01c7 \u0000\u03f3\u0000\u03ec\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000K\u0000\u018c\u0000\u015d\u0000\u0150\u0000\u015d\u0000\u0176\u0000\u0102\u0000\u016f\u0000\u0003\u0000s\u0000\u015d\u0000\u011a\u0000\u011e\u0000\u017d lan \u0000^\u0000\u01b5\u0000\u010f\u0000\u0169\u0000\u011e\u0000\u0110\u0000\u019a \u0000\u03f2\u0000\u03f1\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03f4\u0000\u03f3\u0000\u0358\u0000\u03f3\u0000\u0439 ig ir O \u0000\u0012\u0000\u017d\u0000\u0176\u0000\u0190\u0000\u015d\u0000\u0190\u0000\u019a\u0000\u011e\u0000\u0176\u0000\u0110\u0000\u01c7 \u0000\u03f5\u0000\u03ef\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u03f4\u0000\u03ef\u0000\u0358\u0000\u03f3\u0000\u0439 \u0000\u0011\u0000\u0102\u0000\u0110\u0000\u016c\u0000\u0150\u0000\u018c\u0000\u017d\u0000\u01b5\u0000\u0176\u0000\u011a \u0000\u03f5\u0000\u03ee\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u0012\u0000\u017d\u0000\u0176\u0000\u0190\u0000\u015d\u0000\u0190\u0000\u019a\u0000\u011e\u0000\u0176\u0000\u0110\u0000\u01c7 \u0000\u03f5\u0000\u03f2\u0000\u0358\u0000\u03ee\u0000\u0439 E IS O \u0000\u03f4\u0000\u03f1\u0000\u0358\u0000\u03ee\u0000\u0439 R \u0000D\u0000\u017d\u0000\u019a\u0000\u015d\u0000\u017d\u0000\u0176 \u0000\u03f5\u0000\u03f5\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000^\u0000\u0175\u0000\u017d\u0000\u017d\u0000\u019a\u0000\u015a\u0000\u0176\u0000\u011e\u0000\u0190\u0000\u0190 \u0000\u03f5\u0000\u03f5\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ec \u0000\u03ee\u0000\u03ec \u0000\u03f0\u0000\u03ec \u0000\u03f2\u0000\u03ec \u0000\u03f4\u0000\u03ec \u0000\u03ed\u0000\u03ec\u0000\u03ec \u0000s\u0000\u0011\u0000\u011e\u0000\u0176\u0000\u0110\u0000\u015a\u0000\u0003\u0000^\u0000\u0110\u0000\u017d\u0000\u018c\u0000\u011e\u0000\u0003\u0000\u037e\u0000\u0439\u0000\u037f )sru O F re i p g o . rt 4. VBen Q c u h a s n c t o i r t e a s ti a v s e ev",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 299,
      "paper_id": "ReBot",
      "text": "E IS O \u0000\u03f4\u0000\u03f1\u0000\u0358\u0000\u03ee\u0000\u0439 R \u0000D\u0000\u017d\u0000\u019a\u0000\u015d\u0000\u017d\u0000\u0176 \u0000\u03f5\u0000\u03f5\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000^\u0000\u0175\u0000\u017d\u0000\u017d\u0000\u019a\u0000\u015a\u0000\u0176\u0000\u011e\u0000\u0190\u0000\u0190 \u0000\u03f5\u0000\u03f5\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ec \u0000\u03ee\u0000\u03ec \u0000\u03f0\u0000\u03ec \u0000\u03f2\u0000\u03ec \u0000\u03f4\u0000\u03ec \u0000\u03ed\u0000\u03ec\u0000\u03ec \u0000s\u0000\u0011\u0000\u011e\u0000\u0176\u0000\u0110\u0000\u015a\u0000\u0003\u0000^\u0000\u0110\u0000\u017d\u0000\u018c\u0000\u011e\u0000\u0003\u0000\u037e\u0000\u0439\u0000\u037f )sru O F re i p g o . rt 4. VBen Q c u h a s n c t o i r t e a s ti a v s e ev c a o l m ua p ti a o r n is m on etr o ic f s. g R e e n B er o a t t o e u d tp v er id fo e r o ms q R ua O l S it I y E . a W nd e to B ( achievesvideoqualitycomparabletooriginalreal-worldvideos. e R Fig.5. Comparisonsofmulti-viewconsistency.Wepresenttwoexamples Quality, and Multi-view Consistency. We present a qualita- from the DROID dataset, each captured from two different camera views. tive comparison in Fig. 3. Meanwhile, as shown in Fig. 4, While ROSIE lacks multi-view consistency, Re Bot naturally preserves this capability inherited from 3 D simulation, ensuring the same object in we use VBench [46], a comprehensive benchmark tool for differentcameraviews,asin the realworld. assessing video generation quality, to evaluate two key as- pectsacrossf our dimensions(pleasereferto[46]fordetailed Multi-view Consistency. Additionally, as shown in Fig. 5, definitions): 1) Temporal Quality - including Subject Con- Re Bot inherently preserves multi-view consistency across sistency, Background Consistency, and Motion Smoothness; multiple camera views, since the syn the tic videos are pro- and 2) Frame-wise Quality, i.e., Imaging Quality. We also duced within a 3 D environment. Notably, this crucial at- evaluate original real videos for reference. tribute is uniquely achievable through our real-to-sim-to-real Temporal Quality. Although ROSIE offers a straight- scaling approach. forward solution, it fails to generate temporally consistent videos, which hinders VLA models from learning stable C. Evaluation in Simulation Environment actions. As shown in the first example of Fig. 3, ROSIE We first evaluate VLA models and their two fine tuned initiallygeneratesaplausiblecoke can inthefirsttwoframes, versions (\u201c+ROSIE\u201d and \u201c+Re Bot\u201d) in Simpler Env [39]. but then fails to maintain consistency, producing irrelevant For fair comparisons, we use ROSIE and Re Bot to scale bottles in later frames. This limitation is further reflected in the same data volume exclusively for evaluation tasks (i.e., its low subject consistency score of only 65.6%, as reported 100 episodes per task), adapting VLA models to the same in Fig. 4. Therefore, although observation history has been target domain. We demonstrate that Re Bot effectively im- shown to enhance VLA models [1, 29], ROSIE remains un- proves VLA per for mance across three key aspects: 1) In- suitable for improving the irabilitytolearn from consecutive domain Per for mance: Direct evaluation on the given tasks; frames. In contrast, Re Bot inherently ensures excellent tem- 2) Generalization Per for mance (following [30, 47]): Eval- poral consistency through the simulation process, achieving uating variations of in-domain tasks across unseen object 99.2%inmotionsmoothness.Surprisingly,thisevenslightly sizes (physical), unseen instructions (semantics), and unseen outper for msrealrobotvideosby 0.2%,possiblybecause the objects (subject); 3) Cross-embodiment Per for mance: Eval- simulationprocessreducesartifactssuchasmotionblur(see uating on one embodiment while fine tuning on another. thesecondframein the secondexamplein Fig.3).Moreover, In-domain Per for",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 300,
      "paper_id": "ReBot",
      "text": "the simulation process, achieving uating variations of in-domain tasks across unseen object 99.2%inmotionsmoothness.Surprisingly,thisevenslightly sizes (physical), unseen instructions (semantics), and unseen outper for msrealrobotvideosby 0.2%,possiblybecause the objects (subject); 3) Cross-embodiment Per for mance: Eval- simulationprocessreducesartifactssuchasmotionblur(see uating on one embodiment while fine tuning on another. thesecondframein the secondexamplein Fig.3).Moreover, In-domain Per for mance. In Table I, we report the grasp real-world background inpainting faithfully uses temporal rates(percentageofsuccessfulobjectgraspsduring the task) context to recover the occlusions, contributing to a 92.2% andsuccessrates(percentageofcompletedtasks)for the four backgroundconsistency.Notably,ourtemporalqualityacross Simpler Env tasks on the Widow X robot. When used out-of- all dimensions, with an average score of 93.0%, is highly the-box, both Octo and Open VLA struggle to report decent comparable to real robot videos (96.1%), indicating that our per for mance on most tasks. Particularly, Open VLA entirely syn the tic videos achieve lifelike temporal consistency. fails on challenging tasks, showing 0.0% success rates (e.g., Imaging Quality. In Fig. 3, ROSIE struggles to generate stack green cube on yellow cube). This demonstrates their high-quality manipulated objects, especially in the last two poor per for mance in the target domain without data scaling, examples. This issue becomes particularly evident when the despiteextensivetrainingon SOTA-scale data sets[3].Mean- newobjectshapepotentiallydeviates from the originalobject while, ROSIE performs poorly across most tasks with 0.0% shape. This is because generative models tend to rely more success rates, as it fails to generate realistic manipulated on the inpainting mask, while paying less attention to the objects and, more importantly, lacks temporal consistency. guidance of the text prompt. By comparison, Re Bot ensures This limitation is particularly problematic for Octo, which physically plausible movements through simulation, while reliesonobservationhistory with twoconsecutiveframes.In demonstratingexcellentimagingqualityin Fig.4,withonlya contrast, Re Bot achieves the best per for mance improving all 3.7%decreasecomp are dtooriginalvideos,whilesurpassing models,increasing the averagesuccessrateby 7.2%for Octo ROSIE by 13.0%. and 21.8%for Open VLA.Notably,Re Botboosts the average TABLEI COMPARISONOFEVALUATIONRESULTSON THE WIDOWXROBOTINSIMPLERENV. Putspoon Putcarrot Stackgreencube Puteggplant Average ontowel onplate onyellowcube inbasket Model Grasp Success Grasp Success Grasp Success Grasp Success Grasp Success Octo [29] 34.7% 12.5% 52.8% 8.3% 31.9% 0.0% 66.7% 43.1% 46.5% 16.0% Octo+ROSIE [18] 20.8% 2.8% 27.8% 0.0% 18.1% 0.0% 22.3% 0.0% 22.3% 0.7% Octo+Re Bot (Ours) 61.1% 54.2% 41.1% 22.0% 63.9% 4.2% 52.8% 12.5% 54.7% 23.2% Open VLA [30] 4.2% 0.0% 33.3% 0.0% 12.5% 0.0% 8.3% 4.2% 14.6% 1.1% Open VLA+ROSIE [18] 12.5% 0.0% 41.7% 0.0% 50.0% 0.0% 20.8% 0.0% 31.3% 0.0% Open VLA+Re Bot (Ours) 58.3% 20.8% 45.8% 12.5% 66.7% 4.2% 66.7% 54.2% 59.4% 22.9% \u0000\u03f3\u0000\u03ec \u0000\u03f2\u0000\u03ec \u0000\u03f1\u0000\u03ec \u0000\u03f0\u0000\u03ec \u0000\u03ef\u0000\u03ec \u0000\u03ee\u0000\u03ec \u0000\u03ed\u0000\u03ec \u0000\u03ec \u0000W\u0000\u015a\u0000\u01c7\u0000\u0190\u0000\u015d\u0000\u0110\u0000\u0102\u0000\u016f \u0000^\u0000\u011e\u0000\u0175\u0000\u0102\u0000\u0176\u0000\u019a\u0000\u015d\u0000\u0110\u0000\u0190 \u0000^\u0000\u01b5\u0000\u010f\u0000\u0169\u0000\u011e\u0000\u0110\u0000\u019a \u0000\u0004\u0000\u01c0\u0000\u011e\u0000\u018c\u0000\u0102\u0000\u0150\u0000\u011e \u0000\u037f\u0000\u0439\u0000\u037e\u0000\u0003\u0000\u011e\u0000\u019a\u0000\u0102\u0000Z\u0000\u0003\u0000\u0190\u0000\u0190\u0000\u011e\u0000\u0110\u0000\u0110\u0000\u01b5\u0000^\u0000\u036c\u0000\u0189\u0000\u0190\u0000\u0102\u0000\u018c\u0000' \u0000K\u0000\u0110\u0000\u019a\u0000\u017d \u0000\u03f1\u0000\u03ed\u0000\u0358\u0000\u03f3\u0000\u0439 \u0000\u03f1\u0000\u03ef\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03f0\u0000\u03f4\u0000\u0358\u0000\u03f4\u0000\u0439 \u0000\u03f0\u0000\u03ec\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03f0\u0000\u03ec\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03f0\u0000\u03ed\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03f0\u0000\u03ed\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03f0\u0000\u03ec\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03ef\u0000\u03f2\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ee\u0000\u03ee\u0000\u0358\u0000\u03f0\u0000\u0439 \u0000\u03ee\u0000\u03f2\u0000\u0358\u0000\u03f3\u0000\u0439 \u0000\u03ee\u0000\u03ec\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000\u03ee\u0000\u03ef\u0000\u0358\u0000\u03f2\u0000\u0439\u0000\u03ee\u0000\u03ef\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03ee\u0000\u03f0\u0000\u0358\u0000\u03ef\u0000\u0439\u0000\u03ee\u0000\u03f2\u0000\u0358\u0000\u03f0\u0000\u0439 \u0000\u03ed\u0000\u03ec\u0000\u0358\u0000\u03f4\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u03f2\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03f0\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000W\u0000\u015a\u0000\u01c7\u0000\u0190\u0000\u015d\u0000\u0110\u0000\u0102\u0000\u016f \u0000^\u0000\u011e\u0000\u0175\u0000\u0102\u0000\u0176\u0000\u019a\u0000\u015d\u0000\u0110\u0000\u0190 \u0000^\u0000\u01b5\u0000\u010f\u0000\u0169\u0000\u011e\u0000\u0110\u0000\u019a \u0000\u0004\u0000\u01c0\u0000\u011e\u0000\u018c\u0000\u0102\u0000\u0150\u0000\u011e \u0000\u037f\u0000\u0439\u0000\u037e\u0000\u0003\u0000\u011e\u0000\u019a\u0000\u0102\u0000Z\u0000\u0003\u0000\u0190\u0000\u0190\u0000\u011e\u0000\u0110\u0000\u0110\u0000\u01b5\u0000^\u0000\u036c\u0000\u0189\u0000\u0190\u0000\u0102\u0000\u018c\u0000' \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004 \u0000\u03f3\u0000\u03ed\u0000\u0358\u0000\u03f5\u0000\u0439 \u0000\u03f2\u0000\u03ef\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03f2\u0000\u03f1\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03f2\u0000\u03f2\u0000\u0358\u0000\u03f4\u0000\u0439 \u0000\u03ef\u0000\u03f3\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03ef\u0000\u03ec\u0000\u0358\u0000\u03f3\u0000\u0439 \u0000\u03ee\u0000\u03f4\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000\u03ef\u0000\u03ee\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000\u03ee\u0000\u03f0\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ed\u0000\u03f1\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03ed\u0000\u03ee\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03ed\u0000\u03ef\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03ed\u0000\u03f1\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03ed\u0000\u03ed\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000\u03f3\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03f3\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03ed\u0000\u0358\u0000\u03ed\u0000\u0439\u0000\u03ed\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439\u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ed\u0000\u0358\u0000\u03ed\u0000\u0439\u0000\u03ee\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03f3\u0000\u0439\u0000\u03ed\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u043d\u0000\u0003\u0000\u036c\u0000\u0003\u0000\u037e\u0000'\u0000\u018c\u0000\u0102\u0000\u0190\u0000\u0189\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000\u0003\u0000\u037e\u0000'\u0000\u018c\u0000\u0102\u0000\u0190\u0000\u0189\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a\u0000\u0003\u0000\u037e\u0000'\u0000\u018c\u0000\u0102\u0000\u0190\u0000\u0189\u0000\u037f \u0000\u043d\u0000\u0003\u0000\u036c\u0000\u0003\u0000\u037e\u0000^\u0000\u01b5\u0000\u0110\u0000\u0110\u0000\u011e\u0000\u0190\u0000\u0190\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000\u0003\u0000\u037e\u0000^\u0000\u01b5\u0000\u0110\u0000\u0110\u0000\u011e\u0000\u0190\u0000\u0190\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a\u0000\u0003\u0000\u037e\u0000^\u0000\u01b5\u0000\u0110\u0000\u0110\u0000\u011e\u0000\u0190\u0000\u0190\u0000\u037f Fig.6. Evaluationofgeneralizationper for mance.Re Botimproves the generalizationper for manceof Octo(left)and Open VLA(right)acrossallthree generalizationtypes(physical,semantics,andsubject)on Widow XRobotin Simpler Env. \u0000\u03ee\u0000\u03ec \u0000\u03ed\u0000\u03ec \u0000\u03ec \u0000W\u0000\u01b5\u0000\u019a\u0000\u0003\u0000\u0190\u0000\u0189\u0000\u017d\u0000\u017d\u0000\u0176 \u0000W\u0000\u01b5\u0000\u019a\u0000\u0003\u0000\u0110\u0000\u0102\u0000\u018c\u0000\u018c\u0000\u017d\u0000\u019a \u0000^\u0000\u019a\u0000\u0102\u0000\u0110\u0000\u016c\u0000\u0003\u0000\u0150\u0000\u018c\u0000\u011e\u0000\u011e\u0000\u0176\u0000\u0003\u0000\u0110\u0000\u01b5\u0000\u010f\u0000\u011e \u0000W\u0000\u01b5\u0000\u019a\u0000\u0003\u0000\u011e\u0000\u0150\u0000\u0150\u0000\u0189\u0000\u016f\u0000\u0102\u0000\u0176\u0000\u019a \u0000\u017d\u0000\u0176\u0000\u0003\u0000\u019a\u0000\u017d\u0000\u01c1\u0000\u011e\u0000\u016f \u0000\u017d\u0000\u0176\u0000\u0003\u0000\u0189\u0000\u016f\u0000\u0102\u0000\u019a\u0000\u011e \u0000\u017d\u0000\u0176\u0000\u0003\u0000\u01c7\u0000\u011e\u0000\u016f\u0000\u016f\u0000\u017d\u0000\u01c1\u0000\u0003\u0000\u0110\u0000\u01b5\u0000\u010f\u0000\u011e \u0000\u015d\u0000\u0176\u0000\u0003\u0000\u010f\u0000\u0102\u0000\u0190\u0000\u016c\u0000\u011e\u0000\u019a \u0000\u037f\u0000\u0439\u0000\u037e\u0000\u0003\u0000\u011e\u0000\u019a\u0000\u0102\u0000Z\u0000\u0003\u0000\u0190\u0000\u0190\u0000\u011e\u0000\u0110\u0000\u0110\u0000\u01b5\u0000^ \u0000t\u0000\u015d\u0000\u011a\u0000\u017d\u0000\u01c1\u0000y \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004 \u0000\u03ee\u0000\u03ec\u0000\u0358\u0000\u03f4\u0000\u0439 \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a \u0000\u03ed\u0000\u03ee\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03f4\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439\u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 301,
      "paper_id": "ReBot",
      "text": "\u0000\u03ed\u0000\u0358\u0000\u03ed\u0000\u0439\u0000\u03ed\u0000\u0358\u0000\u03f2\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439\u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ed\u0000\u0358\u0000\u03ed\u0000\u0439\u0000\u03ee\u0000\u0358\u0000\u03ed\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03f3\u0000\u0439\u0000\u03ed\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u043d\u0000\u0003\u0000\u036c\u0000\u0003\u0000\u037e\u0000'\u0000\u018c\u0000\u0102\u0000\u0190\u0000\u0189\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000\u0003\u0000\u037e\u0000'\u0000\u018c\u0000\u0102\u0000\u0190\u0000\u0189\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a\u0000\u0003\u0000\u037e\u0000'\u0000\u018c\u0000\u0102\u0000\u0190\u0000\u0189\u0000\u037f \u0000\u043d\u0000\u0003\u0000\u036c\u0000\u0003\u0000\u037e\u0000^\u0000\u01b5\u0000\u0110\u0000\u0110\u0000\u011e\u0000\u0190\u0000\u0190\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000\u0003\u0000\u037e\u0000^\u0000\u01b5\u0000\u0110\u0000\u0110\u0000\u011e\u0000\u0190\u0000\u0190\u0000\u037f \u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a\u0000\u0003\u0000\u037e\u0000^\u0000\u01b5\u0000\u0110\u0000\u0110\u0000\u011e\u0000\u0190\u0000\u0190\u0000\u037f Fig.6. Evaluationofgeneralizationper for mance.Re Botimproves the generalizationper for manceof Octo(left)and Open VLA(right)acrossallthree generalizationtypes(physical,semantics,andsubject)on Widow XRobotin Simpler Env. \u0000\u03ee\u0000\u03ec \u0000\u03ed\u0000\u03ec \u0000\u03ec \u0000W\u0000\u01b5\u0000\u019a\u0000\u0003\u0000\u0190\u0000\u0189\u0000\u017d\u0000\u017d\u0000\u0176 \u0000W\u0000\u01b5\u0000\u019a\u0000\u0003\u0000\u0110\u0000\u0102\u0000\u018c\u0000\u018c\u0000\u017d\u0000\u019a \u0000^\u0000\u019a\u0000\u0102\u0000\u0110\u0000\u016c\u0000\u0003\u0000\u0150\u0000\u018c\u0000\u011e\u0000\u011e\u0000\u0176\u0000\u0003\u0000\u0110\u0000\u01b5\u0000\u010f\u0000\u011e \u0000W\u0000\u01b5\u0000\u019a\u0000\u0003\u0000\u011e\u0000\u0150\u0000\u0150\u0000\u0189\u0000\u016f\u0000\u0102\u0000\u0176\u0000\u019a \u0000\u017d\u0000\u0176\u0000\u0003\u0000\u019a\u0000\u017d\u0000\u01c1\u0000\u011e\u0000\u016f \u0000\u017d\u0000\u0176\u0000\u0003\u0000\u0189\u0000\u016f\u0000\u0102\u0000\u019a\u0000\u011e \u0000\u017d\u0000\u0176\u0000\u0003\u0000\u01c7\u0000\u011e\u0000\u016f\u0000\u016f\u0000\u017d\u0000\u01c1\u0000\u0003\u0000\u0110\u0000\u01b5\u0000\u010f\u0000\u011e \u0000\u015d\u0000\u0176\u0000\u0003\u0000\u010f\u0000\u0102\u0000\u0190\u0000\u016c\u0000\u011e\u0000\u019a \u0000\u037f\u0000\u0439\u0000\u037e\u0000\u0003\u0000\u011e\u0000\u019a\u0000\u0102\u0000Z\u0000\u0003\u0000\u0190\u0000\u0190\u0000\u011e\u0000\u0110\u0000\u0110\u0000\u01b5\u0000^ \u0000t\u0000\u015d\u0000\u011a\u0000\u017d\u0000\u01c1\u0000y \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004 \u0000\u03ee\u0000\u03ec\u0000\u0358\u0000\u03f4\u0000\u0439 \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a \u0000\u03ed\u0000\u03ee\u0000\u0358\u0000\u03f1\u0000\u0439 \u0000\u03f4\u0000\u0358\u0000\u03ef\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439\u0000\u03f0\u0000\u0358\u0000\u03ee\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439\u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439\u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439\u0000\u03ec\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03f1\u0000\u03ec \u0000\u03f0\u0000\u03ec \u0000\u03ef\u0000\u03ec \u0000\u03ee\u0000\u03ec \u0000\u03ed\u0000\u03ec \u0000\u03ec \u0000W\u0000\u015d\u0000\u0110\u0000\u016c\u0000\u0003\u0000\u0110\u0000\u017d\u0000\u016c\u0000\u011e\u0000\u0003\u0000\u0110\u0000\u0102\u0000\u0176 \u0000W\u0000\u015d\u0000\u0110\u0000\u016c\u0000\u0003\u0000\u0110\u0000\u017d\u0000\u016c\u0000\u011e\u0000\u0003\u0000\u0110\u0000\u0102\u0000\u0176 \u0000W\u0000\u015d\u0000\u0110\u0000\u016c\u0000\u0003\u0000\u0110\u0000\u017d\u0000\u016c\u0000\u011e\u0000\u0003\u0000\u0110\u0000\u0102\u0000\u0176 \u0000\u037e\u0000^\u0000\u019a\u0000\u0102\u0000\u0176\u0000\u011a\u0000\u015d\u0000\u0176\u0000\u0150\u0000\u037f \u0000\u037e\u0000,\u0000\u017d\u0000\u018c\u0000\u015d\u0000\u01cc\u0000\u017d\u0000\u0176\u0000\u019a\u0000\u0102\u0000\u016f\u0000\u037f \u0000\u037e\u0000s\u0000\u011e\u0000\u018c\u0000\u019a\u0000\u015d\u0000\u0110\u0000\u0102\u0000\u016f\u0000\u037f \u0000\u037f\u0000\u0439\u0000\u037e\u0000\u0003\u0000\u011e\u0000\u019a\u0000\u0102\u0000Z\u0000\u0003\u0000\u0190\u0000\u0190\u0000\u011e\u0000\u0110\u0000\u0110\u0000\u01b5\u0000^ although Open VLA faces greater challenges in Simpler Env, it benefits signifi can tly from Re Bot, with the average grasp raterising from 15.6%to 66.8%,and the averagesuccessrate increasing from 0.7%to 11.1%.Theseresultsfur the rconfirm the effectiveness of Re Bot in improving the generalization per for mance of VLA models. Cross-embodiment Per for mance. We also investigate \u0000'\u0000\u017d\u0000\u017d\u0000\u0150\u0000\u016f\u0000\u011e\u0000\u0003\u0000Z\u0000\u017d\u0000\u010f\u0000\u017d\u0000\u019a \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004 whether Re Bot can enhance the cross-embodiment perfor- \u0000\u03f0\u0000\u03f5\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000\u043d\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000 \u0000\u03f0\u0000\u03ed\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000K\u0000\u0189\u0000\u011e\u0000\u0176\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000\u043d\u0000\u0003\u0000Z\u0000\u011e\u0000\u0011\u0000\u017d\u0000\u019a mance of VLA models. Specifically, we use ROSIE and Re Bot to scale the DROID dataset for the Franka Panda \u0000\u03ee\u0000\u03f0\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ee\u0000\u03f1\u0000\u0358\u0000\u03ec\u0000\u0439 robot, then fine tune Open VLA and evaluate its per for mance \u0000\u03ed\u0000\u03f4\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03ed\u0000\u03f2\u0000\u0358\u0000\u03ec\u0000\u0439 on the Widow X robot and Google Robot in Simpler Env. \u0000\u03f5\u0000\u0358\u0000\u03ec\u0000\u0439 We report the success rates in Fig. 7. On the Widow X \u0000\u03ef\u0000\u0358\u0000\u03ec\u0000\u0439 \u0000\u03f0\u0000\u0358\u0000\u03ec\u0000\u0439 robot,while ROSIEonlyprovidesamarginalincreasein the average success rate from 1.4% to 3.1%, Re Bot achieves a substantialboostto 12.5%.Forthe\u201cpickcoke can\u201dtask with Fig.7. Evaluationofcross-embodimentper for mance. Re Botenhances thecross-embodimentper for manceof Open VLAon the Widow Xrobot(top) varying object poses on Google Robot, Re Bot demonstrates and Google Robot(bottom)in Simpler Env. consistent improvements across all poses, whereas ROSIE fails to achieve such robustness. This highlights that Re Bot grasp rate from 14.6% to 59.4% on Open VLA, further enables Open VLA to learn more precise and adaptable demonstrating its effectiveness. These results highlight that manipulation strategies across diverse object poses. Notably, both VLA models benefit greatly from Re Bot because of its Re Bot consistently improves the per for mance of Open VLA temporal consistent and physically realistic syn the tic videos. despitescaling for adifferentembodiment,demonstratingits Generalization Per for mance. While current VLA models ability to enhance cross-embodiment per for mance. often face generalization challenges, we further validate D. Evaluation in Real-world Environment Re Bot as an effective scaling solution for enhancing their generalization per for mance. As shown in Fig. 6, ROSIE In real-world experiments, we demonstrate that Re Bot remains ineffective on Octo, while Re Bot consistently im- consistently enhances the effectiveness of VLA models, proves both Octo and Open VLA across all three generaliza- delivering superior per for mance over ROSIE. As shown in tion types. Specifically, Re Bot increases the average success Tab. II, we leverage both ROSIE and Re Bot to scale our rate from 6.5% to 26.4% on Octo. On the other hand, real robot dataset for four evaluation tasks (see examples TABLEII COMPARISONOFEVALUATIONRESULTSONTHEFRANKAP AND AROBOTIN THE REALWORLDENVIRONMENT. Results Put carrot Put grape Put fanta can Put black cube Average in blue plate in yellow plate in blue plate in yellow plate Model Grasp Success Grasp Success Grasp Success Grasp Success Grasp Success Octo [29] 0% 0% 30% 20% 10% 0% 20% 10% 15% 8%",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 302,
      "paper_id": "ReBot",
      "text": "AROBOTIN THE REALWORLDENVIRONMENT. Results Put carrot Put grape Put fanta can Put black cube Average in blue plate in yellow plate in blue plate in yellow plate Model Grasp Success Grasp Success Grasp Success Grasp Success Grasp Success Octo [29] 0% 0% 30% 20% 10% 0% 20% 10% 15% 8% Octo+ROSIE [18] 30% 20% 0% 0% 20% 20% 10% 0% 15% 10% Octo+Re Bot (Ours) 40% 20% 40% 30% 30% 20% 30% 30% 35% 25% Open VLA [30] 30% 20% 30% 20% 60% 30% 40% 30% 40% 25% Open VLA+ROSIE [18] 10% 0% 10% 0% 30% 10% 20% 10% 18% 5% Open VLA+Re Bot (Ours) 40% 40% 50% 40% 50% 50% 60% 50% 50% 45% Put carrot in blue plate Put grape in yellow plate Put fanta can in blue plate Put black cube in yellow plate at the bottom of Tab. II), and comp are the per for mance example, extending Re Bot to diverse data settings (e.g., of their fine tuned VLA models. To ensure better adaptation varying camera setups and robots) could potentially benefit to our real-world scene, we also incorporate our real robot cross-embodiment learning. Additionally, exploring more dataset (i.e., 220 real-world episodes) during the fine tuning challenging scenarios beyond tabletop manipulation is also process for all models. We conduct 10 trials per task, and interesting with potentialbroaderreal-worldapplications.We report both the grasp rate and success rate as evaluation take these directions for future work. metrics. While ROSIE provides a marginal improvement, increasing the average success rate of Octo from 8% to REFERENCES 10%, it fails entirely on some tasks (e.g., put the grape in [1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al., \u201cRt-1: yellow plate), and does not show meaningful enhancement Robotics trans for mer for real-world control at scale,\u201d ar Xiv preprint for Open VLA. In contrast, Re Bot consistently achieves sub- ar Xiv:2212.06817,2022. stantial per for mance gains across diverse tasks, improving [2] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choro- manski,T.Ding,D.Driess,A.Dubey,C.Finn,etal.,\u201cRt-2:Vision- the average success rates of Octo by 17% and Open VLA language-action models transfer web knowledge to robotic control,\u201d by 20%. Notably, for challenging tasks where Octo initially ar Xivpreprintar Xiv:2307.15818,2023. has a 0% grasp rate and success rate (e.g., put carrot in [3] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Ir- pan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, et al., \u201cOpen blue plate), Re Bot boosts the grasping rate to 40% and the x-embodiment: Robotic learning datasets and rt-x models,\u201d ar Xiv success rate to 20%, highlighting its robust effectiveness in preprintar Xiv:2310.08864,2023. real-world applications. [4] A. O\u2019Neill, A. Rehman, A. Gupta, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, et al., V. CONCLUSION AND DISCUSSION \u201cOpen x-embodiment: Robotic learning datasets and rt-x models,\u201d ar Xivpreprintar Xiv:2310.08864,2023. We propose Re Bot, a novel real-to-sim-to-real approach [5] A.Khazatsky,K.Pertsch,S.Nair,A.Balakrishna,S.Dasari,S.Karam- for scaling real robot datasets and adapting VLA models to cheti,S.Nasiriany,M.K.Srirama,L.Y.Chen,K.Ellis,etal.,\u201cDroid: A large-scale in-the-wild robot",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 303,
      "paper_id": "ReBot",
      "text": "A. Lee, A. Pooley, A. Gupta, A. Mandlekar, et al., V. CONCLUSION AND DISCUSSION \u201cOpen x-embodiment: Robotic learning datasets and rt-x models,\u201d ar Xivpreprintar Xiv:2310.08864,2023. We propose Re Bot, a novel real-to-sim-to-real approach [5] A.Khazatsky,K.Pertsch,S.Nair,A.Balakrishna,S.Dasari,S.Karam- for scaling real robot datasets and adapting VLA models to cheti,S.Nasiriany,M.K.Srirama,L.Y.Chen,K.Ellis,etal.,\u201cDroid: A large-scale in-the-wild robot manipulation dataset,\u201d ar Xiv preprint target domains. Re Bot replays real-world robot trajectories ar Xiv:2403.12945,2024. in simulation to diversify manipulated objects, and inte- [6] E.Kolve,R.Mottaghi,W.Han,E.Vander Bilt,L.Weihs,A.Herrasti, grates the simulated movements with inpainted real-world M.Deitke,K.Ehsani,D.Gordon,Y.Zhu,etal.,\u201cAi 2-thor:Aninter- active 3 denvironment for visualai,\u201dar Xivpreprintar Xiv:1712.05474, background to syn the size physically realistic and temporally 2017. consistentrobotvideos.Re Botachievesexcellentvideogen- [7] T.Mu,Z.Ling,F.Xiang,D.Yang,X.Li,S.Tao,Z.Huang,Z.Jia,and eration quality, with a VBench temporal consistency score H. Su, \u201cManiskill: Generalizable manipulation skill benchmark with large-scaledemonstrations,\u201dar Xivpreprintar Xiv:2107.14483,2021. of 93.0% and imaging quality score of 66.4%, which are [8] J.Gu,F.Xiang,X.Li,Z.Ling,X.Liu,T.Mu,Y.Tang,S.Tao,X.Wei, comparable to 96.1% and 70.1% for real robot videos. In Y. Yao, et al., \u201cManiskill 2: A unified benchmark for generalizable Simpler Env with the Widow X robot, Re Bot improved the manipulationskills,\u201dar Xivpreprintar Xiv:2302.04659,2023. [9] Y. Wang, Z. Xian, F. Chen, T.-H. Wang, Y. Wang, K. Fragkiadaki, in-domain per for mance of Octo by 7.2% and Open VLA by Z. Erickson, D. Held, and C. Gan, \u201cRobogen: Towards unleashing 21.8%, and enhanced generalization per for mance by 19.9% infinite data for automatedrobotlearningvia generativesimulation,\u201d and 9.4%, respectively. In a real-world environment with a ar Xivpreprintar Xiv:2311.01455,2023. [10] B.Liu,Y.Zhu,C.Gao,Y.Feng,Q.Liu,Y.Zhu,and P.Stone,\u201cLibero: physical Franka Panda, Re Bot increased the success rates of Benchmarkingknowledgetransfer for lifelongrobotlearning,\u201dar Xiv Octo by 17% and Open VLA by 20%. preprintar Xiv:2306.03310,2023. Wehope Re Botcouldserveasavaluableasset and inspire [11] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A.Mandlekar,and Y.Zhu,\u201cRobocasa:Large-scalesimulationofev- future research on real-to-sim-to-real for robot learning. It erydaytasks for generalistrobots,\u201dar Xivpreprintar Xiv:2406.02523, opens several exciting avenues for future exploration. For 2024. [12] W. Zhao, J. P. Queralta, and T. Westerlund, \u201cSim-to-real transfer in ar Xiv:2406.09246,2024. deep rein for cement learning for robotics: a survey,\u201d in 2020 IEEE [31] M.Savva,A.Kadian,O.Maksymets,Y.Zhao,E.Wijmans,B.Jain, symposiumseriesoncomputationalintelligence(SSCI). IEEE,2020, J.Straub,J.Liu,V.Koltun,J.Malik,etal.,\u201cHabitat:Aplat for mfor pp.737\u2013744. embodiedairesearch,\u201din Proceedingsof the IEEE/CVFinternational [13] F. Muratore, F. Ramos, G. Turk, W. Yu, M. Gienger, and J. Peters, conferenceoncomputervision,2019,pp.9339\u20139347. \u201cRobotlearningfromr and omizedsimulations:Areview,\u201dFrontiersin [32] M.Shridhar,J.Thomason,D.Gordon,Y.Bisk,W.Han,R.Mottaghi, Robotics and AI,vol.9,p.799893,2022. L. Zettlemoyer, and D. Fox, \u201cAlfred: A benchmark for interpreting [14] Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and grounded instructions for everyday tasks,\u201d in Proceedings of the V. Kumar, \u201cCacti: A framework for scalable multi-task multi-scene IEEE/CVF conference on computer vision and pattern recognition, visualimitationlearning,\u201dar Xivpreprintar Xiv:2212.05711,2022. 2020,pp.10740\u201310749. [15] S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, \u201cRobo- [33] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, dreamer:Learningcompositionalworldmodels for robotimagination,\u201d Y.Yuan,H.Wang,etal.,\u201cSapien:Asimulatedpart-basedinteractive ar Xivpreprintar Xiv:2404.12377,2024. environment,\u201d in Proceedings of the IEEE/CVF conference on com- [16] Y.Du,S.Yang,B.Dai,H.Dai,O.Nachum,J.Tenenbaum,D.Schu- putervision and patternrecognition,2020,pp.11097\u201311107. urmans, and P. Abbeel, \u201cLearning universal policies via text-guided [34] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, video generation,\u201d Advances in Neural Information Processing Sys- R. Singh, Y. Guo, H. Mazhar, et al., \u201cOrbit: A unified",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 304,
      "paper_id": "ReBot",
      "text": "com- [16] Y.Du,S.Yang,B.Dai,H.Dai,O.Nachum,J.Tenenbaum,D.Schu- putervision and patternrecognition,2020,pp.11097\u201311107. urmans, and P. Abbeel, \u201cLearning universal policies via text-guided [34] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, video generation,\u201d Advances in Neural Information Processing Sys- R. Singh, Y. Guo, H. Mazhar, et al., \u201cOrbit: A unified simulation tems,vol.36,2024. framework for interactiverobotlearningenvironments,\u201dIEEERobotics [17] Z. Chen, S. Kiami, A. Gupta, and V. Kumar, \u201cGenaug: Retargeting and Automation Letters,vol.8,no.6,pp.3740\u20133747,2023. behaviors to unseen situations via generative augmentation,\u201d ar Xiv [35] L. Wang, R. Guo, Q. Vuong, Y. Qin, H. Su, and H. Christensen, preprintar Xiv:2302.06671,2023. \u201cA real 2 sim 2 real method for robust object grasping with neural [18] T.Yu,T.Xiao,A.Stone,J.Tompson,A.Brohan,S.Wang,J.Singh, surfacereconstruction,\u201din 2023 IEEE 19 th International Conference C. Tan, J. Peralta, B. Ichter, et al., \u201cScaling robot learning with on Automation Science and Engineering (CASE). IEEE, 2023, pp. semanticallyimaginedexperience,\u201dar Xivpreprintar Xiv:2302.11550, 1\u20138. 2023. [36] M. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, [19] L. Y. Chen, C. Xu, K. Dharmarajan, M. Z. Irshad, R. Cheng, and P. Agrawal, \u201cReconciling reality through simulation: A real- K. Keutzer, M. Tomizuka, Q. Vuong, and K. Goldberg, \u201cRovi-aug: to-sim-to-real approach for robust manipulation,\u201d ar Xiv preprint Robot and viewpointaugmentation for cross-embodimentrobotlearn- ar Xiv:2403.03949,2024. ing,\u201d in Conference on Robot Learning (Co RL), Munich, Germany, [37] Y. Mu, T. Chen, S. Peng, Z. Chen, Z. Gao, Y. Zou, L. Lin, Z. Xie, 2024. and P. Luo, \u201cRobotwin: Dual-arm robot benchmark with generative [20] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, digitaltwins(earlyversion),\u201dar Xivpreprintar Xiv:2409.02920,2024. Y.Chen,F.Yan,Z.Zeng,H.Zhang,F.Li,J.Yang,H.Li,Q.Jiang, [38] X.Li,J.Li,Z.Zhang,R.Zhang,F.Jia,T.Wang,H.Fan,K.-K.Tseng, and L. Zhang, \u201cGrounded sam: Assembling open-world models for and R.Wang,\u201cRobogsim:Areal 2 sim 2 realroboticgaussiansplatting diversevisualtasks,\u201d2024. simulator,\u201d2024.[Online].Available:https://arxiv.org/abs/2411.11839 [21] S. Zhou, C. Li, K. C. Chan, and C. C. Loy, \u201cPro Painter: Improving [39] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, propagation and transformer for videoinpainting,\u201din Proceedingsof I. Lunawat, I. Sieh, S. Kirmani, et al., \u201cEvaluating real-world robot IEEEInternational Conferenceon Computer Vision(ICCV),2023. manipulationpoliciesinsimulation,\u201dar Xivpreprintar Xiv:2405.05941, [22] H.Ravichandar,A.S.Polydoros,S.Chernova,and A.Billard,\u201cRecent 2024. advances in robot learning from demonstration,\u201d Annual review of [40] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, control, robotics, and autonomous systems, vol. 3, no. 1, pp. 297\u2013 J. Yang, H. Su, J. Zhu, et al., \u201cGrounding dino: Marrying dino with 330,2020. grounded pre-training for open-set object detection,\u201d ar Xiv preprint [23] Y.Yang,L.Chen,Z.Zaidi,S.van Waveren,A.Krishna,and M.Gom- ar Xiv:2303.05499,2023. bolay, \u201cEnhancing safety in learning from demonstration algorithms [41] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, via control barrier function shielding,\u201d in Proceedings of the 2024 R.Ra\u00a8dle,C.Roll and,L.Gustafson,E.Mintun,J.Pan,K.V.Alwala, ACM/IEEE International Conference on Human-Robot Interaction, N. Carion, C.-Y. Wu, R. Girshick, P. Dolla\u00b4r, and C. Feichtenhofer, 2024,pp.820\u2013829. \u201cSam 2: Segment anything in images and videos,\u201d 2024. [Online]. [24] A. Mandlekar, J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, Available:https://arxiv.org/abs/2408.00714 A. Garg, S. Savarese, and L. Fei-Fei, \u201cScaling robot supervision to [42] H.Walke,K.Black,A.Lee,M.J.Kim,M.Du,C.Zheng,T.Zhao, hundredsofhours with roboturk:Roboticmanipulation data setthrough P.Hansen-Estruch,Q.Vuong,A.He,V.Myers,K.Fang,C.Finn,and human reasoning and dexterity,\u201d in 2019 IEEE/RSJ International S. Levine, \u201cBridge data v",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 305,
      "paper_id": "ReBot",
      "text": "and videos,\u201d 2024. [Online]. [24] A. Mandlekar, J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, Available:https://arxiv.org/abs/2408.00714 A. Garg, S. Savarese, and L. Fei-Fei, \u201cScaling robot supervision to [42] H.Walke,K.Black,A.Lee,M.J.Kim,M.Du,C.Zheng,T.Zhao, hundredsofhours with roboturk:Roboticmanipulation data setthrough P.Hansen-Estruch,Q.Vuong,A.He,V.Myers,K.Fang,C.Finn,and human reasoning and dexterity,\u201d in 2019 IEEE/RSJ International S. Levine, \u201cBridge data v 2: A dataset for robot learning at scale,\u201d in Conferenceon Intelligent Robots and Systems(IROS). IEEE,2019, Conferenceon Robot Learning(Co RL),2023. pp.1048\u20131055. [43] M.Deitke,D.Schwenk,J.Salvador,L.Weihs,O.Michel,E.Vander- [25] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, Bilt,L.Schmidt,K.Ehsani,A.Kembhavi,and A.Farhadi,\u201cObjaverse: K.Daniilidis,C.Finn,and S.Levine,\u201cBridge data:Boostinggener- Auniverseofannotated 3 dobjects,\u201dar Xivpreprintar Xiv:2212.08051, alizationofroboticskills with cross-domain data sets,\u201dar Xivpreprint 2022. ar Xiv:2109.13396,2021. [44] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, [26] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, K. Ghasemip our, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, S. Levine, and C. Finn, \u201cBc-z: Zero-shot task generalization with et al., \u201cPhotorealistic text-to-image diffusion models with deep lan- roboticimitationlearning,\u201din Conferenceon Robot Learning. PMLR, guage underst and ing,\u201d Advances in neural information processing 2022,pp.991\u20131002. systems,vol.35,pp.36479\u201336494,2022. [27] D. Whitney, E. Rosen, E. Phillips, G. Konidaris, and S. Tellex, [45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cComparing robot grasping teleoperation across desktop and virtual \u201cHigh-resolution image syn the sis with latent diffusion models,\u201d in reality with rosreality,\u201din Robotics Research:The 18 th International Proceedings of the IEEE/CVF conference on computer vision and Symposium ISRR. Springer,2019,pp.335\u2013350. patternrecognition,2022,pp.10684\u201310695. [28] Y. Yang, B. Ikeda, G. Bertasius, and D. Szafir, \u201cArcade: Scalable [46] Z.Huang,Y.He,J.Yu,F.Zhang,C.Si,Y.Jiang,Y.Zhang,T.Wu, demonstration collection and generation via augmented reality for Q.Jin,N.Chanpaisit,Y.Wang,X.Chen,L.Wang,D.Lin,Y.Qiao,and imitation learning,\u201d in 2024 IEEE/RSJ International Conference on Z.Liu,\u201cVBench:Comprehensivebenchmarksuite for videogenerative Intelligent Robots and Systems(IROS). IEEE,2024,pp.2855\u20132861. models,\u201d in Proceedings of the IEEE/CVF Conference on Computer [29] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, Vision and Pattern Recognition,2024. O.Mees,S.Dasari,J.Hejna,C.Xu,J.Luo,T.Kreiman,Y.Tan,L.Y. [47] Z. Zhang, K. Zheng, Z. Chen, J. Jang, Y. Li, C. Wang, M. Ding, Chen,P.Sanketi,Q.Vuong,T.Xiao,D.Sadigh,C.Finn,and S.Levine, D.Fox,and H.Yao,\u201cGrape:Generalizingrobotpolicyviapreference \u201cOcto: An open-source generalist robot policy,\u201d in Proceedings of alignment,\u201dar Xivpreprintar Xiv:2411.19309,2024. Robotics:Science and Systems,Delft,Netherlands,2024. [30] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al., \u201cOpen- vla: An open-source vision-language-action model,\u201d ar Xiv preprint",
      "start_pos": 6006,
      "end_pos": 6376
    },
    {
      "chunk_id": 306,
      "paper_id": "dexcap",
      "text": "Dex Cap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu Stanford University https://dex-cap.github.io (a)Dex Cap: Portable motion capturesystem (b)Mocap data and 3 Dscene (c)Dex IL: Dexterous imitation learning Fig. 1: DEXCAP facilitates the in-the-wild collection of high-quality human hand motion capture data and 3 D observations. Leveraging this data, DEXIL adapts it to the robot embodiment and trains control policy to perform the same task. Abstract\u2014Imitation learning from human hand motion data supervised training using human demonstration data. One presentsapromisingavenue for imbuingrobots with human-like commonly used way to collect data is to teleoperate robot dexterityinreal-worldmanipulationtasks.Despite this potential, hands to perform the tasks. However, due to the requirement substantialchallengespersist,particularly with the portabilityof ofarealrobotsystem and slowrobotmotion,thisapproachis existingh and motioncapture(mocap)systems and the complexity oftranslatingmocap data intoeffectiveroboticpolicies.Totackle expensive to scale up. An alternative way is to directly track these issues, we introduce DEXCAP, a portable hand motion human hand motions during manipulation without controlling capture system, alongside DEXIL, a novel imitation algorithm the robot. Current system is primarily vision-based with a for training dexterous robot skills directly from human hand single-viewcamera.However,besides the questionofwhether mocap data. DEXCAP offersprecise,occlusion-resistanttracking the tracking algorithm can provide accurate 3 D information ofwrist and fingermotions base don SLAM and electromagnetic fieldtoge the rwith 3 Dobservationsof the environment.Utilizing which is critical for robot policy learning, these systems are this rich dataset, DEXIL employs inverse kinematics and point vulnerable to visual occlusions that frequently occur during cloud-based imitation learning to seamlessly replicate human hand-object interactions. actions with robot hands. Beyond direct learning from human A better alternative to vision-based methods for gathering motion, DEXCAP also offers an optional human-in-the-loop dexterous manipulation data is through motion capture (mo- correctionmechanismduringpolicyrolloutstorefine and further improve task per for mance. Through extensive evaluation across cap). Mocap systems provides accurate 3 D information and six challenging dexterous manipulation tasks, our approach not are robust to visual occlusions. Hence human operators can only demonstrates superior per for mance but also showcases the directly interact with the environment with their hands, which system\u2019s capability to effectively learn from in-the-wild mocap is fast and easier to scale up since no robot hardw are is data, paving the way for future data collection methods in the required.Toscaleuph and mocapsystemsto data collectionin pursuit of human-level robot dexterity. everydaytasks and environments for robotlearning,asuitable I. INTRODUCTION system should ideally be portable and robust for long capture Buildingroboticsystemstoper for meverydaymanipulation sessions, provide accurate finger and wrist poses, as well as tasks is a long-standing challenge. Our living environments 3 D environment information. Most hand mocap systems are and daily objects are designed with human hand functionality not portable and rely on well-calibrated third-view cameras. in mind, posing a substantial challenge for developing future Whileelectromagneticfield(EMF)glovesovercome this issue, home robots. Recent breakthroughs in robotic dexterity, espe- they cannot track the 6-Do F wrist pose in the world frame, ciallyin the controlofmulti-fingeredmechanicalh and switha which is important for end-effectors policy learning. Devices high degree of freedom, have shown remarkable potential [1\u2013 like IMU-based whole-body suits can",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 307,
      "paper_id": "dexcap",
      "text": "Whileelectromagneticfield(EMF)glovesovercome this issue, home robots. Recent breakthroughs in robotic dexterity, espe- they cannot track the 6-Do F wrist pose in the world frame, ciallyin the controlofmulti-fingeredmechanicalh and switha which is important for end-effectors policy learning. Devices high degree of freedom, have shown remarkable potential [1\u2013 like IMU-based whole-body suits can monitor wrist position 3]. However, enabling robotic hands to emulate human-level but are prone to drift over time. dexterity in manipulation tasks remains unsolved, due to both In addition to hardw are challenges, there are also algorith- hardw are and algorithmic challenges. mic challenges to use motion capture data for robot imitation Imitation Learning (IL) [4, 5] has recently made con- learning. While dexterous robot hands enable the possibility siderable strides toward this goal [6, 7], especially through of learning directly from human hand data, the inherent dif- 4202 lu J 4 ]OR.sc[ 2 v 88770.3042:vi Xra ferences in size, proportion, and kinematic structure between II. RELATEDWORKS the robot hand and human hand call for innovative algorithms A. Dexterous manipulation to overcome these embodiment gaps. Towards solving these challenges,ourworksimultaneouslyintroducesa new portable Dexterous manipulation has been a long-standing research hand mocap system, DEXCAP, and an imitation algorithm, area in robotics [15\u201319], posing significant challenges to DEXIL, that allows the robot to learn dexterous manipulation planning and control due to the high degrees-of-freedom. The policies directly from the human hand mocap data. traditional optimal control methods [17\u201319] often necessitate simplification of the contacts, which is usually not tenable DEXCAP (Fig. 1) is a portable hand mocap system that in more complex tasks. Recently, rein for cement learning has tracks the 6-Do F poses of the wrist and the finger motions in been explored to learn dexterous policies in simulation with real-time (60 Hz). The system includes a mocap glove to track minimalassumptionsaboutthetaskor the environment[2,20\u2013 finger joints, a camera mounted on top of each glove to track 29]. The learned policies can solve complex tasks, including the 6-Do F poses of the wrists with SLAM, and an RGB-D in-hand object re-orientatation [2, 20, 23\u201325, 28], bimanual Li DAR camera on the chest to observe the 3 D environments. manipulation[26,30],andlong-horizonmanipulation[22,27]. Besides the hardw are challenges, research efforts on de- However, due to the sim-to-real gap, deploying the learned veloping algorithms to utilize mocap data for robot learning policy on a real-world robot remains challenging. Imitation have been missing due to the lack of such a data collection learning, on the other hand, focuses on learning directly system and collected data. Prior algorithms that learn from fromreal-worlddemonstration data,whichisobtainedthrough human motion focus on learning the rewards [8, 9], high- either teleportation [1, 6, 31, 32] or human videos [3, 33, 34]. levelplans[10,11],andvisualrepresentations[12,13],which DIME [31] uses VR to teleoperate a dexterous hand for data oftenrequireadditionalrobot data and can notbedirectlyused collection; Qin et al. [35] uses an RGB camera to track hand for low-level control. In this work, we argue that the main pose for teleoperation; Dex Transfer [36] uses human mocap challenge of learning low-level control from human motion is data to guide dexterous grasping; Dex MV [33], Dex",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 308,
      "paper_id": "dexcap",
      "text": "collection; Qin et al. [35] uses an RGB camera to track hand for low-level control. In this work, we argue that the main pose for teleoperation; Dex Transfer [36] uses human mocap challenge of learning low-level control from human motion is data to guide dexterous grasping; Dex MV [33], Dex VIP [34] that the data is missing precise 3 D information of the hand and Video Dex [3] leverages human video data for learning motion (e.g., 6-Do F hand pose, 3 D finger positioning), which the motion priors but often require additional training in are exactly what DEXCAP can provide. simulation or real robot teleoperation data. Our work focuses To leverage data collected by DEXCAP for learning dex- on dexterous imitation learning, which relies on DEXCAP to terous robot policies, we propose imitation learning from collect high-quality hand mocap data grounded in 3 D point mocap data, DEXIL, which consists of two major steps \u2014 cloud observation, which can be directly used to train low- dat are targeting and traininggenerative-basedbehaviorcloning level positional control on robots with single or dual hands. policy with pointcloudinputs,withanoptionalhuman-in-the- B. Hand motion capture system loop motion correction step. For retargeting, we use inverse kinematics (IK) to retarget the robotic hand\u2019s fingertips to the Human hand mocap is an important technique for appli- same 3 D location as the human\u2019s fingertips. The 6-Do F pose cations in computer vision and graphics. Most previous sys- ofthewristisusedtoinitialize the IKtoensure the samewrist tems are camera-based, IMU-based, or electromagnet(EMF)- motion between the human and the robots. Then we convert based. Camera-based systems utilize monocular camera [37\u2013 RGB-Dobservationstopointcloud-basedrepresentations.We 39], RGB-D camera [40\u201342], VR headset [43], or multi-view thenuseapointcloud-basedbehaviorcloningalgorithmbased camera with markers [44, 45]. However, the quality of hand on Diffusion Policy [14]. In more challenging tasks when IK motion tracking quickly deteriorates in scenarios involving is insufficient to fulfill the embodiment gap between human heavy occlusions, which happen frequently in hand-object and robot hands, we propose a human-in-the-loop motion interactions. Some of these systems also require third-view correction mechanism. During policy rollouts, humans can calibrated cameras which are not portable or scalable. More wear the DEXCAP and interrupt the robot\u2019s motion when recently, Inertia Measurement Unit (IMU) has been used for unexpected behavior occurs, and such interruption data can in-the-wildhumanmocap[46\u201350].Never the less,mostofthem be further used for policy fine tuning. focus on whole-body motion capture and miss fine-grained In summary, the main contributions of this work include: finger motions. EMF-based mocap gloves are designed for capturing finger motion, which is widely used for dexterous \u2022 DEXCAP: a novel portable human hand mocap system, teleoperation [51\u201353]. However, the glove does not track the enablingreal-timetrackingofwrist and fingermovements 6-Do F palm poses grounded in the environment and misses for dexterous manipulation tasks. visual observations for training robot policies. DEXCAP is a \u2022 DEXIL:animitationlearningframeworkleveragingh and mocapglovesystem that isdesignedtocollect data for training mocap data for directly learning dexterous manipulation visuomotor manipulation policies. Through novel engineering skills from human hand motions. designs, our system stays robust to occlusions, captures fine- \u2022 Human-in-the-Loop Correction:ahuman-in-the-loopcor- grained finger motion, tracks palm poses using SLAM,",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 309,
      "paper_id": "dexcap",
      "text": "DEXCAP is a \u2022 DEXIL:animitationlearningframeworkleveragingh and mocapglovesystem that isdesignedtocollect data for training mocap data for directly learning dexterous manipulation visuomotor manipulation policies. Through novel engineering skills from human hand motions. designs, our system stays robust to occlusions, captures fine- \u2022 Human-in-the-Loop Correction:ahuman-in-the-loopcor- grained finger motion, tracks palm poses using SLAM, and rection mechanism with DEXCAP, signifi can tly enhanc- records RGB-D images to reconstruct the scene with a wear- ing robot per for mance in complex tasks. able camera vest. Calibrationphase Datacollectionphase Intel NUC Power bank 1 2 (a)Dex Capfrontview (b)Dex Capbackview (c)Detailsof the camerasetup Fig. 2: Details of the human system. (a) Our setup includes a 3 D-printed rack on a chest harness, featuring a Realsense L 515 Li DAR camera on top and three Realsense T 265 tracking cameras below. (b) An Intel NUC and power bank in a backpack power the system for approximately 40 minutes of data collection. (c) The T 265 cameras, initially in a known pose for calibration, are relocated to hand mounts during data collection to monitor palm positions, ensuring consistency through a click-in design. Finger motions are captured by Rokoko gloves, accurately tracking the finger joint positions. C. Robot learning with human demonstration designed and used for the parallel-gripper data collection process, while in this work we aim to collect multi-finger Imitation Learning (IL) has enabled robots to successfully handmotion data for dexterousmanipulationtasks(e.g.,using perform various manipulation tasks [4, 54\u201360]. Traditional scissors and unscrewing bottle caps). IL algorithms such as DMP and Pr MP [61\u201364] enjoy high learning sample efficiency but are limited in their ability to III. HARDW ARE SYSTEM:DEXCAP handle high-dimensional observations. In contrast, recent IL methods built upon deep neural networks can learn policies Inthissection,weintroduce the systemdesignincluding(1) with raw image observation inputs [65, 66], even for high- a portable human hand motion capture system DEXCAP that degree robot systems with bimanual arms [67, 68]. Despite is used for data collection (Sec. III-A) and (2) a bimanual their effectiveness, one key challenge for imitation learning robot system equipped with dexterous hands for testing the is how to scale up the training data. Prior works focus on policies learned from the collected data (Sec. III-B). teleoperation data [66, 69\u201377] which is expensive to collect A. Dex Cap due to the requirement of the robot hardw are. More recently, learning from human motion data has started to receive To capture the fine-grained hand motion data suitable to more attention because it allows collecting data without robot train dexterous robot policies, DEXCAP is designed with four hardw are [78]. By leveraging human videos [11, 79], hand key objectives in mind: (1) detailed finger motion tracking, trajectories [10, 80\u201382], promising results have been shown (2) accurate 6-Do F wrist pose estimation, (3) aligned 3 D ob- to train policies with less manual human effort. However, servations recording in a unified coordinate frame with hands, these human motions are in 2 D image space [80, 83, 84], and (4) outst and ing portability for data collection in various which fails to directly train 6-Do F manipulation policies in",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 310,
      "paper_id": "dexcap",
      "text": "train policies with less manual human effort. However, servations recording in a unified coordinate frame with hands, these human motions are in 2 D image space [80, 83, 84], and (4) outst and ing portability for data collection in various which fails to directly train 6-Do F manipulation policies in real-world environments. We achieved these objectives with 3 Denvironments and usuallyrequiresadditionalteleoperation zero compromise on scalability\u2014DEXCAP must be simple to data to bridge the gap [10, 11, 79]. Recently, human-in- calibrate, inexpensive to build, and robust for data collection the-loop correction algorithms have also shown promising of daily activities in the wild. results in robot learning [85\u201387]. Our DEXCAP provides Tracking finger motions. Our system uses electromag- tracking of 6-Do F hand poses together with finger motions netic field (EMF) gloves, offering a significant advantage grounded in 3 D point cloud observations, which is portable over vision-based finger tracking systems, particularly in the for data collection with outarobot.Basedon the datacollected robustness to visual occlusions that frequently occur in hand- with DEXCAP, we introduce DEXIL which is a point cloud- object interactions. In our system, finger motions are tracked based imitation learning algorithm for learning fine-grained using Rokokomotioncaptureglovesasillustratedin Figure 2. dexterous manipulation policies, with an optional human-in- Each glove\u2019s fingertip is embedded with a tiny magnetic the-loop correction step for more challenging tasks. sensor, while a signal receiver hub is placed on the glove\u2019s dorsal side. The 3 D location of each fingertip is measured D. Portable data collection systems for manipulation as the relative 3 D translation from the hub to the sensors. Recentlyadvancementsinlow-costh and-heldgrippers have In appendix we included a qualitative comparison between shown promising results in collecting robot manipulation data our EMF glove system and state-of-the-art vision-based hand- without robot hardw are [88\u201394]. All of these systems are tracking methods across different manipulation scenarios. Front view Side view (a)Retargeting with fingertip IK (b)Bimanualdexterousrobotsetup (c)Human-in-the-loopcorrectionsetup Fig. 3: Details of the robot system. Mirroring the human system, the robot system reuses the same chest cameras and mount. (a) Once the motion is captured by Dex Cap, it\u2019s retargeted to LEAP hand through discarding pinky finger and IK to match fingertiplocation.(c)Anoptionalhuman-in-the-loopcorrectionstep can beper for medtofurtherrefine the motionstransferred. Specifically, the human will provide the delta input in real time when the robot system is carrying out the task. Note the hand T 265 is only used at correction time, as the robot arm already knows the exact location of fingers. Tracking 6-Do F wrist pose. Beyond finger motion, know- a constant trans for mation between the camera frames. Then, ing the precise positioning of a robot\u2019s end-effector in a we take off the tracking cameras from the rack and insert 3 D space is crucial for robot manipulation. This necessitates them into the camera slot attached to each glove. In this way, DEXCAPtoestimate and record the 6-Do Fposetrajectoriesof we caneasilytransform the handposetrackingresultsinto the human hands during data collection. While camera-based and observationframeofthechestcamera with the constantinitial IMU-based methods are commonly used, each has its limita- trans for mation. The full calibration process is demonstrated tions. Camera-based",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 311,
      "paper_id": "dexcap",
      "text": "attached to each glove. In this way, DEXCAPtoestimate and record the 6-Do Fposetrajectoriesof we caneasilytransform the handposetrackingresultsinto the human hands during data collection. While camera-based and observationframeofthechestcamera with the constantinitial IMU-based methods are commonly used, each has its limita- trans for mation. The full calibration process is demonstrated tions. Camera-based systems, often non-portable and limited in Appendix Figure 13 and supplementary videos, which in their ability to estimate wrist orientation, are less suited takesaround 10 seconds.Tofur the rensurestableobservations for data collection in manipulation tasks. IMU-based systems, amidst human movement, another fisheye tracking camera although wearable, tend to suffer from position drifting when (markedredin Fig.2(c))ismountedunder the Li DARcamera, used for long recording sessions. To address these challenges, which provides a more robust SLAM per for mance than the we develop a 6-Do F wrist tracking system based on the Li DARcamera with itswidefieldofview.Wedefine the initial SLAM algorithm, as shown in Figure 2(c). This system uses pose frame of this tracking camera as the world frame for all an Intel Realsense T 265 camera, mounted on each glove\u2019s stream data. Figure 6 is the visualization of the collected data dorsal side. It combines images from two fisheye cameras by trans for ming the observations into colored point clouds in and IMU sensor signals to construct an environment map the world frame alongside the captured hand motions. using the SLAM algorithm, enabling consistent tracking of System Portability. Central to the portability of DEXCAP the wrist\u2019s 6-Do F pose. This design has three key advantages: is a compact mini-PC (Intel NUC 13 Pro), carried in a it is portable, allowing for wrist pose tracking without the backpack, which serves as the primary computation unit for need for hands to be visible in third-person camera frames; data recording. This PC is powered by a portable power bank SLAM can autonomously correct position drift with the built with a 40000 m Ah battery, enabling approximately 40 minutes map for long-time use; and the IMU sensor provides crucial of continuous data collection (Fig. 2(b)). The total weight of wrist orientation information to train the robot policy in the the backpack is 3.96 pounds. The supplementary video shows subsequent pipeline. that donning and calibrating DEXCAP is fast and simple, takinglessthan 10 seconds.Additionally,DEXCAP\u2019shardw are Recording 3 D observations and calibration. Capturing designismodular and inexpensivetobuild\u2014norestrictionto the data necessary for training robot policies requires not brandsor model sofcameras,motioncapturegloves,andmini- only the tracking of hand movement but also recording ob- PCs. We will open-source the code and instruction videos for servations of the 3 D environment as the policy input. As builders, along with a range of hardw are options. The overall depicted in Figure 2(a), we design a wearable camera vest cost of the DEXCAP is kept within a $4 k USD budget. for this purpose. It incorporates an Intel Realsense L 515 B. Bimanual dexterous robot RGB-D Li DAR camera, mounted on the top of the chest, to capture the observations during human data collection. The To validate the robot policy trained by the data from nextcriticalquestion the nbecomeshowtoeffectivelyintegrate DEXCAP, we establish a",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 312,
      "paper_id": "dexcap",
      "text": "this purpose. It incorporates an Intel Realsense L 515 B. Bimanual dexterous robot RGB-D Li DAR camera, mounted on the top of the chest, to capture the observations during human data collection. The To validate the robot policy trained by the data from nextcriticalquestion the nbecomeshowtoeffectivelyintegrate DEXCAP, we establish a bimanual dexterous robot setup. the tracked hand motion data with the 3 D observations. To This setup comprises two Franka Emika robot arms, each simplify the calibration process, we designed a 3 D-printed equippedwitha LEAPdexterousrobotich and(afour-fingered camera rack underneath the chest camera mount as illustrated hand with 16 joints) [95], as depicted in Figure 3(b). For in Figure 2(c). At the beginning of the data collection, all policy evaluation, the chest Li DAR camera used in human tracking cameras are placed in the rack slots, which secures data collection is detached from the vest and mounted on Timestep! Dat are targeting ! \" Policyrollouts Trans for mto Remove RGB-Dimage Pointcloud robotspace redundantpoints (in-the-wild) !' # \" \" \" \" Residual Humancorrection action Handmocap Fingertip IK Futuresteps inrobotspace Policy [!+#:!+%+#] ! Correction dataset Policy ! ! Original [\":\"$%] \u2026 \u2026 MSELoss 46-dim action space dataset Merge&fine tuning (a)Dex ILoverview (b)Human-in-the-loopcorrection Fig. 4: Algorithm overview. (a) DEXIL first retargets the DEXCAP data to the robot embodiment by first constructing 3 D point clouds from RGB-D observations and trans for ming it into robot operation space. Meanwhile, the hand motion capture data is retargeted to the dexterous hand and robot arm with fingertip IK. Based on the data, a robot policy is learned to output a sequence of future goal positions as the robot actions. (b). DEXCAP also offers an optional human-in-the-loop correction mechanism,wherehumansapplydelt are sidualactionto the policy-generatedactionstocorrectrobotbehavior.Thecorrections are stored in a new dataset and uni for mly sampled with the original dataset for fine-tuning the robot policy. a stand positioned between the robot arms. To simplify the A. Data re-targeting process of switching the camera system between the human Actionre-targeting.Asillustratedin Figure 3(a),anotable and robot, a quick-release buckle has been integrated into the challengeemergesduetothesizedisparitybetween the human back of the camera rack, allowing for swift camera swaps \u2013 hand and the LEAPh and,with the latterabout 50%larger[95]. in less than 5 seconds. In this way, the robot utilizes the same Thissizedifferencemakesithardtodirectlytransfer the finger observation camera employed during human data collection. motionsto the robotichardw are.Thefirststepistoretarget the Note that, for robot setups, only the Li DAR camera is used human hand motion capture data into the robot embodiment, and wrist cameras are not needed. Both the robot arms and which requires mapping the finger position and 6-Do F palm the LEAP hands operate at a control frequency of 20 Hz. We pose with inverse kinematics (IK). useend-effectorpositioncontrol for bothrobotarms and joint One critical finding in prior research is that fingertips position control for both LEAP hands. are the most frequently contacted areas on a hand when interacting with objects (as evidenced in studies like HO- IV. LEARNINGALGORITHM:DEXIL 3 D [41], GRAB [44], ARCTIC [45]). Motivated by this, Our goal is to use the human hand motion capture data we",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 313,
      "paper_id": "dexcap",
      "text": "fingertips position control for both LEAP hands. are the most frequently contacted areas on a hand when interacting with objects (as evidenced in studies like HO- IV. LEARNINGALGORITHM:DEXIL 3 D [41], GRAB [44], ARCTIC [45]). Motivated by this, Our goal is to use the human hand motion capture data we re-target finger motion by matching fingertip positions recorded by DEXCAP to train dexterous robot policies. There using inverse kinematics (IK). Specifically, we deploy an IK are several research questions along the way - (1) How can algorithm that generatessmooth and accuratefingertipmotion we re-target the human hand motion to the robotic hand? (2) in real time [96\u201398] to determine the 16-dimensional joint What algorithm can learn dexterous policies, especially when positions for the robotic hand. This ensures the alignment the action space is high-dimensional in the bimanual setup? between robot fingertips and the human fingertips in the (3) In addition, we would like to investigate the failure cases DEXCAP data. Considering the design of the LEAP hand, forlearningdirectly from humanmotioncapture data and their whichfeaturesf our fingers,weadapt our processbyexcluding potential solutions. littlefingerin for mationduring IKcomputations.Additionally, To tackle these challenges, we introduce DEXIL, a three- the 6-Do F wrist pose captured in the mocap data serves as an step framework to train dexterous robots using human hand initial reference for wrist pose in the IK algorithm. Figure 6 motioncapture data.Thefirststepistore-target the DEXCAP demonstrates the final result of re-targeting. The 6-Do F pose data into the action and observation spaces of the robot em- of the wrist p =[R |T ] and the finger joint positions J of t t t t bodiment (Sec. IV-A). Second step trains a point-cloud-based the LEAP hands are then used as the robot\u2019s proprioception diffusion policy using the re-targeted data (Sec. IV-B). The state s = (p ,J ). We use position control in our setup t t t final step involves an optional human-in-the-loop correction and the robot\u2019s action labels are defined as next future states mechanism, designed to address unexpected behaviors that a =s . t t+1 emerge during the policy execution (Sec. IV-C). Observation post-processing. Observation and state rep- resentation choice are critical for training robot policies. We where we empirically find it outperforms traditional MLP- convert the RGB-D images captured by the Li DAR camera based architecture for learning dexterous robot policies. in the DEXCAP data into point clouds using the camera parameters. This additional conversion offers two significant C. Human-in-the-loop correction benefits compared to RGB-D input. First, because DEXCAP allows the human torso to move naturally during data acqui- With the design presented above, DEXIL can learn chal- lengingdexterousmanipulationskills(e.g.,pick-and-place and sition, directly using RGB-D input would need to account for the moving camera frame. By trans for ming point cloud bimanual coordination) directly from DEXCAP data without the need for on-robot data. However, our simple retargeting observations into a consistent world frame\u2014defined as the method does not address all aspects of the human-robot coordinate frame of the main SLAM camera at the start of embodiment gap. For example, when using a pair of scissors, the",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 314,
      "paper_id": "dexcap",
      "text": "data without the need for on-robot data. However, our simple retargeting observations into a consistent world frame\u2014defined as the method does not address all aspects of the human-robot coordinate frame of the main SLAM camera at the start of embodiment gap. For example, when using a pair of scissors, the mocap (the main camera is marked in red in Fig. 2(c))\u2014 astableholdofscissorsrequiresinserting the fingersdeepinto we isolate and remove torso movements, resulting in a stable the handle. Due to the differences in finger length proportion, robot observation. Second, point clouds provide flexibility directly matching the fingertips and the joint motion does not in editing and alignment with the robot\u2019s operational space. guarantee the same force exerted on the scissors. Given that some motions captured in the wild may extend To address this issue, we offer a human-in-the-loop mo- beyond the robot\u2019s reachability, adjusting the placement of tion correction mechanism, which consists of two modes - point cloud observations and motion trajectories ensures their residualcorrection and teleoperation.Duringpolicyexecution, feasibilitywithin the robot\u2019soperationalrange.Basedonthese we allow humans to provide corrective actions to robots findings,all RGB-Dframes from the mocap data are processed into point clouds aligned with the robot\u2019s space, and the in real-time by wearing DEXCAP. In residual mode, DEX- task-irrelevant elements, such as the table surface points, are CAP measures the delta position changes of human hands (\u2206p H,\u2206JH) relative to hands\u2019 initial states (p H,JH) at excluded. This refined point cloud data thus becomes the t t 0 0 the beginning of the policy roll-out. The delta position is observation inputs o fed into the robot policy \u03c0. t applied as a residual action ar = (\u2206p H,\u2206JH) to the t t t robot policy action a = (p ,J ), scaled by \u03b1 and t t+1 t+1 B. Point cloud-based diffusion policy \u03b2. The corrected robot action can then be formalized as With the trans for med robot\u2019s state s t , action a t and cor- a\u2032 t = (p t+1 (cid:76) \u03b1\u00b7\u2206p H t ,J t+1 +\u03b2 \u00b7\u2206J t H). We empirically responding 3 D point cloud observation o , we formalize the find that setting \u03b2 with a small scale (< 0.1) offers the best t robot policy learning process as a trajectory generation task. user experience, which avoids fingers moving too fast. More specifically, a policy model \u03c0, processes the point In the case when a large position change is desired, a cloud observations o and the robot\u2019s current proprioception pressonthefootpedal will switch the systemtoteleoperation t state s into an action trajectory (a ,a ,...,a ) (as in mode. DEXCAP now ignores the policy rollout and applies t t t+1 t+d Fig. 4). Given point cloud observation with N points o human wrist delta directly to the robot wrist pose. The robot t in RN\u00d73, we uni for mly down-sample it into K points and fingertips are nowdirectlyfollowinghumanfingertips.Inother concatenate the RGB pixel color corresponding to each point words, the robot fingertip will track the human fingertip in into the final policy input in RK\u00d76. To bridge the visual gap their respective",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 315,
      "paper_id": "dexcap",
      "text": "robot t in RN\u00d73, we uni for mly down-sample it into K points and fingertips are nowdirectlyfollowinghumanfingertips.Inother concatenate the RGB pixel color corresponding to each point words, the robot fingertip will track the human fingertip in into the final policy input in RK\u00d76. To bridge the visual gap their respective wrist frame through IK. Users can also switch between human hands and the robot\u2019s hand, we use forward back to the residual mode after correcting the robot\u2019s mistake kinematics to transform the links of the robot model with by pressing the foot pedal again. the proprioception state s and merge the point clouds of the Since the robot has already learned an initial policy, typi- t trans for med links into the observation o . During training, we cally the correction happens in a small portion of the rollout, t alsouse data augmentationover the inputsbyapplyingrandom greatly reducing the human effort. The corrected actions and 2 D translations to the point clouds and motion trajectories observations are stored in a new dataset D\u2032. Training data within the robot\u2019s operational space. is sampled with equal probability from D\u2032 and the original One challenge of learning dexterous robot policies, espe- dataset D tofine-tune the policy model,similarto IWR[101]. cially for bimanual dexterous robots, is handling the large dimensional action outputs. In our setup, the action output V. EXPERIMENTS includes two 7-Do F robot arms and two 16-Do F dexterous We aim to answer the following research questions: hands for d steps, which forms a high-dimensional regression problem. Similar challenges have also been studied in image Q 1: What is the quality of DEXCAP data? generation tasks, which aim to regress all pixel values in a Q 2: Can DEXIL directlylearndexterousrobotpolicies from high-resolution frame. Recently, diffusion model [99, 100], DEXCAP data without any on-robot data? with its step-by-step diffusion process, has shown success in Q 3: What model architecture choices are critical to improv- modeling complex data distributions with high-dimensional ing the per for mance? data. For robotics, diffusion policy [14] follows the same idea Q 4: Can DEXIL learn from in-the-wild DEXCAP data? and formalizes the control problem into an action generation Q 5: How does human-in-the-loop correction help when task. Thus we use a diffusion policy as the action decoder, DEXCAP data is insufficient? gnikcipegnop S gnipiwetal P gnigakca P gnittucrossic S gnitcelloclla B a b c d e f gniraperpae T Fig. 5: Experiment Tasks. (a) Sponge Picking: Pick and lift the sponge. (b) Ball Collecting: Pick up a ball and drop it into a basket. (c) Plate Wiping: Use both hands to pick up a plate and sponge, then wipe the plate vertically twice. (d) Packaging: Place items into a box with one hand while using the other to either push or stabilize them, before securely closing the box lid. (e) Scissor Cutting: Secure paper with one hand and use scissors in the other to cut through the paper. (f) Tea Preparing: Grasp the tea bottle with one hand, use the other hand to uncap, then pick up tweezers to extract tea and",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 316,
      "paper_id": "dexcap",
      "text": "them, before securely closing the box lid. (e) Scissor Cutting: Secure paper with one hand and use scissors in the other to cut through the paper. (f) Tea Preparing: Grasp the tea bottle with one hand, use the other hand to uncap, then pick up tweezers to extract tea and pour it into the pot. Q 6: Can the whole framework handle extremely challeng- b) Data: We utilize two data types: (1) DEXCAP data ing bimanual dexterous manipulation tasks (e.g., using capturing human hand motion (In-the-wild data refers to a scissors and preparing tea)? mixture of data collected in more than 10 scenes) and (2) human-in-the-loop correction data for adjusting robot actions A. Experiment setups or enabling teleoperation to correct errors, collected using a a) Tasks: we evaluate DEXIL using six tasks of varying foot pedal. Data were initially recorded at 60 Hz and then difficulty to assess its per for mance with DEXCAP data. These downsampled to 20 Hz to match the robot\u2019s control speed, tasksrange from basic,suchas Spongepicking,Ballcollecting, except for correction data, which was collected directly at and Plate wiping, which test single-handed and dual-handed 20 Hz.For data collection,wega the red 30 minutesof DEXCAP coordination, to more complex ones like Packaging, which data across the first three tasks, resulting in 251, 179, and looksatbimanualtasks and generalizationusingbothfamiliar 102 demos respectively. An hour of in-the-wild DEXCAP and new objects. Scissor cutting focuses on the effectiveness data provided 104 demos for Packaging. Scissor Cutting and ofthehuman-in-the-loopcorrectionmechanisminprecisetool Tea Preparing tasks each received an hour of DEXCAP data, use,whereas Teapreparingchallenges the system with along- yielding 96 and 55 demos respectively. horizon task requiring intricate actions. To further analyze per for mance, we introduce the Subtask metric for multi-step c) Baselines: We evaluate multiple baselines to deter- tasks, indicating the completion of task subgoals, such as mine the model architecture with the best per for mance, fo- placing an object inside a box in Packaging, or picking up cusing on three key aspects using DEXCAP data: identifying scissors in Scissor Cutting. the best imitation learning framework for bimanual dexterous gnipiwetal P gnittucrossic S gnigakca P Raw Observation Right View Middle View Left View (Right View) Fig. 6: Data Retargeting for Tasks. DEXIL effectively retargets human mocap data for activities like plate wiping, scissor cutting, and packaging. The initial column displays the raw point cloud scene. Columns 2-7 offer three views\u2014right, middle, left\u2014withbluebackgroundcolumnsdepictinghuman data andyellow for roboth and retargeting.Thisside-by-sidearrangement highlights the precision of our fingertip IK in translating human to robot hand motions. tupn I Re Ma H sru O total trials) and 9 unseen objects (45 total trials). B. Results DEXCAP delivers high-quality 3 D mocap data (Q 1). Figure 6 showcases DEXCAP\u2019sabilitytocapturedetailedh and motionin 3 D,aligninghumanactions with objectpointclouds across all views, such as in Plate wiping and Scissor cutting tasks (blue columns). The retargeted robot hand motions, depictedin the yellowcolumns,demonstrateprecisealignment inthesame 3 Dspace.In Figure 7,wecomp are DEXCAP with the state-of-the-art vision-based hand pose estimation method Ha Me R [39], observing their per for mance from similar view- points. We find that the vision-based approach is",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 317,
      "paper_id": "dexcap",
      "text": "wiping and Scissor cutting tasks (blue columns). The retargeted robot hand motions, depictedin the yellowcolumns,demonstrateprecisealignment inthesame 3 Dspace.In Figure 7,wecomp are DEXCAP with the state-of-the-art vision-based hand pose estimation method Ha Me R [39], observing their per for mance from similar view- points. We find that the vision-based approach is vulnerable to self-occlusion, particularly when the fingers are obscured. As depicted in Figure 7, Ha Me R struggles in instances of significant occlusion, either failing to detect the hand (as seen Fig.7:Comp are withvision-basedmethod.Wedemonstrate in the second column) or inaccurately estimating fingertip that motion capture gloves provide more stable hand pose positions (noted in the first, third, and fourth columns). In estimation results compared to vision-based methods and are contrast, DEXCAP demonstrates good robustness under these not affected by visual occlusion. conditions. Beyond the challenge of occlusion, most vision- based methods rely on 2 D hand estimation, predicated on manipulation between BC-RNN [102] and diffusion policy learning from 2 Dimageprojectionlosses.Consequently,these (DP)[14], assessing the most effective observation type to methods are inherently limited in their ability to discern the bridge the visual gap between human and robot hands (com- precise 3 D hand positioning, as they are trained based on paringimageinputs[14,65]andapointcloudmethod[103]), presumed, fixed camera intrinsic parameters, which do not and determining the most suitable encoder for point cloud necessarily match the actual camera used for experiments. In inputs by comparing Point Net[104] and Perceiver [105, 106] Figure 8,weshowcase the datacollectionthroughputof DEX- encoders.Implementationdetails are includedin the appendix. CAP,whichisthreetimesfasterthantraditionalteleoperation. d) Metric: Each model variant is tested for 20 trials in DEXCAP data can directlytraindexterousrobotpolicies each task withr and omizedinitialplacements.The task success (Q 2).Table Iis the experimentresultoftrainingrobotpolicies rateisreportedin Table IIIIII.For the multi-object Packaging onlyusing DEXCAP data.Within 30-minuteh and motioncap- task, each object is tested with 5 trials - 6 trained objects (30 turedemonstrationscollectedby DEXCAP,thelearnedpolicies (a). Human motion (b). Dex Cap (c). Teleoperation Fig.8:Datacollectionthroughputcomparison. DEXCAP\u2019sdatacollectionspeedin the Ballcollecting task isclosetonatural human motion and is three times faster than traditional teleoperation. Fig. 9: Visualization of human-in-the-loop corrections. DEXCAP supports teleoperation and residual correction for human- in-the-loop adjustments. Teleoperation directly translates human hand movements to the robot end-effector actions, indicated bycolor-fadingtrajectories from bluetogreen(human)andredtoyellow(robot)over 20 timesteps.Residualcorrectionadjusts the robot\u2019s end-effector based on changes from the human hand\u2019s initial pose, enabling minimal movement but requiring more precise control. Users can switch between correction modes with a foot pedal. achieve up to 72% average task success rate in single-hand raw, DP-point, DP-prec), on the other hand, do not require pick-and-place(Spongepicking,Ballcollecting)andbimanual masking over observations and achieve more than 60% task coordination (Plate wiping) tasks. This result highlights the successrate.Thisresulthighlights the advantageofusingpoint effectiveness of DEXCAP data on training dexterous robot cloud inputs, which allow us to add robot hand points to the policies without on-robot data, which introduces a new way observation with outlosingthedetailsin the originalinputs.We for training robot dexterous manipulation. also observe that, even without adding robot hand points, DP- point-rawachievescloseper for manceto DP-point.Thismight Generative-based algorithm with point cloud inputs because the downsampling process of the point cloud inputs shows advantages (Q 3). In Table I, we comp are the per- lowers the appearance gap",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 318,
      "paper_id": "dexcap",
      "text": "for training robot dexterous manipulation. also observe that, even without adding robot hand points, DP- point-rawachievescloseper for manceto DP-point.Thismight Generative-based algorithm with point cloud inputs because the downsampling process of the point cloud inputs shows advantages (Q 3). In Table I, we comp are the per- lowers the appearance gap between human gloves and robot formance of multiple model architectures. We first observe hands. Fur the rmore, compared to the Point Net, the model that, due to the visual appearance gap between human and with Perceiver encoder has higher per for mance, especially in robothands,thepolicies with fullimageinputsfailcompletely bimanual tasks with multiple task objects (20% improvement (BC-RNN-img,DP-img).Wethentrymaskingouthuman and ontasksuccessratein Platewiping).Basedon the sefindings, robothands with whitecirclesintraining and evaluation.This we use DP-perc as the default model architecture for DEXIL. setting brings improvements, where DP-img-mask achieves more than 30% success rate in all tasks. Meanwhile, diffusion DEXIL canpurelylearnfromin-the-wild DEXCAP data policy works better than MLP-based BC-RNN policies (25% (Q 4). The first three columns of Table II are the results of higher in averaged task success rate). This result verifies training policies using in-the-wild DEXCAP data. We first our hypo the sis that generative-based policy is more suitable notice that image-input baselines (BC-RNN-img-mask, DP- for learning dexterous policies. Although getting promising img-mask)haveclosetozeroper for mancewhenlearning with results, masking out the end-effector loses details for in-hand in-the-wild data. This observation verifies our hypo the sis that manipulation. This hypo the sis is verified by the low success the viewpoint changes caused by human body movements rate in the Plate wiping task, which requires the robot to during in-the-wild data collection bring challenges to learn- use fine-grained finger motion to grab the plate from the ing image-based policies. Our DEXIL transforms the point edge. Our point cloud-based learning algorithms (DP-point- cloud inputs into a consistent world frame, resulting in stable DEXCAP Data Only Scissorcutting DEXCAP Data Only 30 humancorrections Spongepicking Ballcollecting Platewiping Overall Subtask All Subtask All BC-RNN-point[103] 0.00 0.00 0.10 0.00 BC-RNN-img 0.00 0.00 0.00 0.00 Ours 0.00 0.00 0.45 0.20 BC-RNN-img-mask[65] 0.25 0.10 0.10 0.15 BC-RNN-point[103] 0.45 0.30 0.25 0.33 BC-RNN-prec[105] 0.50 0.30 0.35 0.38 TABLE III: Quantitative results for the Scissor cutting task. DP-img 0.00 0.00 0.00 0.00 DP-img-mask[14] 0.55 0.40 0.30 0.42 DEXCAP Data Only 30 humancorrections Teapreparing DP-point-raw 0.70 0.70 0.40 0.60 Subtask All Subtask All DP-point 0.75 0.65 0.50 0.63 Ours(DP-perc) 0.85 0.60 0.70 0.72 Ours 0.30 0.00 0.65 0.25 TABLEI:Quantitativeresults for learning with DEXCAP data. TABLE IV: Quantitative results for the Tea preparing task. In-the-wild DEXCAP 30 humancorrections Packaging the same setup used for the evaluations. This result further Subtask All Unseen Subtask All Unseen supports our conclusion: image-based approaches are more BC-RNN-img-mask[65] 0.00 0.00 0.00 0.23 0.07 0.00 effective in learning with fixed third-view cameras compared BC-RNN-point[103] 0.33 0.23 0.16 0.40 0.27 0.22 DP-img-mask[14] 0.17 0.00 0.00 0.47 0.33 0.00 to the in-the-wild scenarios with moving cameras. Human Ours 0.70 0.47 0.40 0.83 0.57 0.42 corrections also result in a 10% improvement in our approach that utilizes point cloud inputs. However, we\u2019ve observed that TABLE II: Quantitative results for the Packaging",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 319,
      "paper_id": "dexcap",
      "text": "0.27 0.22 DP-img-mask[14] 0.17 0.00 0.00 0.47 0.33 0.00 to the in-the-wild scenarios with moving cameras. Human Ours 0.70 0.47 0.40 0.83 0.57 0.42 corrections also result in a 10% improvement in our approach that utilizes point cloud inputs. However, we\u2019ve observed that TABLE II: Quantitative results for the Packaging task. fine-tuning with human corrections has a minor effect on the results for unseenobjects,primarilydueto the limitedamount of correction data (30 trials in total). Ourwholeframework can handleextremelychallenging tasks (Q 6). DEXIL together with human-in-the-loop correc- tion is able to solve extremely challenging tasks such as Scissor cutting and Tea preparing. In Table III, we showcase that our system can achieve a 45% success rate on picking up Trainedobjects Unseenobjects the scissor from the container and 20% in cutting a piece of papertape.Inoursupplementaryvideo,wealsoshowcasehow Fig. 10: Objects used in the Packaging task the robot performs the long-horizon Tea preparing task which includes unscrewing a bottle cap and pouring tea into the pot. observations and thus getting better results (70% in Subtask Table IV presents the evaluation results of our approach (DP- and 47% in full task setup). Please refer to our video results perc) in the Tea preparing task. The subtask is defined as for more visualization of the stabilized input point clouds. By successfully unscrewing the cap of the tea bottle. We found training the policy with multiple task objects using in-the- thateven with humanmocap data only(DEXCAP Data Only), wild (Fig. 10), our model can already generalize to unseen our model can achieve a 30% success rate in uncapping. object instances, with a 40% success rate. During evaluation, Most of the failures occur during the task of picking up we identified two primary issues with the policy learned the tweezers, which requires high-precision control over the from in-the-wild DEXCAP data: firstly, the absence of force fingertip. In such cases, human-in-the-loop correction signifi- informationin DEXCAP datacauses the righth and tostruggle cantly improves per for mance. With 30 human corrections, we with stabilizing the box during box closure attempts by the achievea 35%improvementin the uncappingsuccessrate and left hand. Secondly, the box lid occasionally moves out of the attain a 25% success rate for the entire task. Please refer to chest camera\u2019s view due to human movements, hindering the our video submission for more qualitative results of this task. robot\u2019s ability to learn precise lid grasping. These challenges These tasks showcase the high potential of our framework in prompt us to seek improvement strategies. learning extremely challenging dexterous manipulation tasks. Human-in-the-loop correction greatly help when DEX- CAP data is insufficient (Q 5). Figure 9 illustrates two types VI. CONCLUSION AND LIMITATIONS of human-in-the-loop correction mode with DEXCAP. Users We present DEXCAP, a portable hand motion capture can switch between the two modes by stepping on the foot system, and DEXIL, an imitation algorithm enabling robots pedal and the whole trajectory is stored and used for fine- to learn dexterous manipulation directly from human mocap tuning the policy. The last three columns of Table II show- data.DEXCAP,designedtoovercomeocclusions,capturefine- case the effectiveness of using human-in-the-loop correction grained 3 D hand",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 320,
      "paper_id": "dexcap",
      "text": "system, and DEXIL, an imitation algorithm enabling robots pedal and the whole trajectory is stored and used for fine- to learn dexterous manipulation directly from human mocap tuning the policy. The last three columns of Table II show- data.DEXCAP,designedtoovercomeocclusions,capturefine- case the effectiveness of using human-in-the-loop correction grained 3 D hand motion, record RGB-D observations, and together with policy fine-tuning to improve the model perfor- allow data collection outside the lab. DEXIL applies this data mance.Withjust 30 humancorrectiontrialsduringpolicyroll- toteachrobotscomplexdexterousmanipulationtasks,withan out, the fine-tuned policy with image inputs (DP-img-mask) optional human-in-the-loop correction mechanism to further achieves a 33% improvement in the full task success rate for improve per for mance. Demonstrating proficiency in tasks like trained objects. This significant boost is mainly because the scissor cutting and tea preparation, DEXCAP and DEXIL human correction data is collected using a fixed camera - signifi can tlyadvanceroboticdexterity.Wehope DEXCAP can pave the path for future research on scaling up dexterous [6] Irmak Guzey, Ben Evans, Soumith Chintala, and Ler- manipulation data with portabledevices.Allhardw are designs rel Pinto. Dexterity from touch: Self-supervised pre- and code will be open-source. training of tactile representations with robotic play. While DEXCAP collects high-quality mocap data in-the- ar Xiv preprint ar Xiv:2303.12076, 2023. wild for learning challenging dexterous manipulation tasks, [7] Aravind Sivakumar,Kenneth Shaw,and Deepak Pathak. it has several limitations that need future research: (1) The Robotic telekinesis: Learning a robotic hand imita- system\u2019s power consumption currently restricts the collection tor by watching humans on youtube. ar Xiv preprint time to be at most 40 minutes. Future improvements will ar Xiv:2202.10448, 2022. focus on enhancing power efficiency to extend the collection [8] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter time. (2) Our learning algorithm DEXIL utilizes fingertip Abbeel, and Sergey Levine. Avid:Learning multi-stage inverse kinematics to retarget human hand motion to various tasks via pixel-level translation of human videos. ar Xiv robotic hands. However, the size difference between human preprint ar Xiv:1912.04443, 2019. and robotic hands (with some robotic fingers being thicker) [9] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. can make some tasks difficult to perform, such as playing the Human-to-robot imitation in the wild. ar Xiv preprint piano.Futuredevelopments will aimtointegrateadvancements ar Xiv:2207.09450, 2022. in robotic hand design to minimize these size differences and [10] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, fullydemonstrate the system\u2019spotential.(3)Current DEXCAP Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anand- collectsonly 3 Dobservations and motioncapture data,lacking kumar. Mimicplay: Long-horizon imitation learning by force sensing. One promising direction we plan to explore watchinghumanplay.ar Xivpreprintar Xiv:2302.12422, involves the use of con for mal tactile textiles, as introduced in 2023. [107], to gather tactile information during data collection. [11] Homanga Bharadhwaj, Abhinav Gupta, Vikash Kumar, and Shubham Tulsiani. Towardsgeneralizablezero-shot ACKNOWLEDGMENTS manipulation via translating human interaction plans. This research was supported by National Science Founda- ar Xiv preprint ar Xiv:2312.00775, 2023. tion NSF-FRR-2153854 and Stanford Institute for Human- [12] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Centered Artificial Intelligence,SUHAI.Thisworkispartially Chelsea Finn, and Abhinav Gupta. R 3 m: A universal supported by ONR MURI N 00014-21-1-2801. We would visual representation for robot manipulation.",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 321,
      "paper_id": "dexcap",
      "text": "by National Science Founda- ar Xiv preprint ar Xiv:2312.00775, 2023. tion NSF-FRR-2153854 and Stanford Institute for Human- [12] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Centered Artificial Intelligence,SUHAI.Thisworkispartially Chelsea Finn, and Abhinav Gupta. R 3 m: A universal supported by ONR MURI N 00014-21-1-2801. We would visual representation for robot manipulation. ar Xiv like to thank Yunfan Jiang, Albert Wu, Paul de La Sayette, preprint ar Xiv:2203.12601, 2022. Ruocheng Wang, Sirui Chen, Josiah Wong, Wenlong Huang, [13] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jiten- Yanjie Ze,Christopher Agia,Jingyun Yang and the SVLPAIR dra Malik.Maskedvisualpre-training for motorcontrol. group for providinghelp and feedback.Wealsothank Zhenjia ar Xiv preprint ar Xiv:2203.06173, 2022. Xu, Cheng Chi, Yifeng Zhu for their suggestions on the [14] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric robot controller. We especially thank Kenneth Shaw, Ananye Cousineau, Benjamin Burchfiel, and Shuran Song. Dif- Agrawal, Deepak Pathak for open-sourcing the LEAP Hand. fusion policy: Visuomotor policy learning via action diffusion. ar Xiv preprint ar Xiv:2303.04137, 2023. REFERENCES [15] J Kenneth Salisbury and John J Craig. Articulated [1] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, hands: Force control and kinematic issues. The Inter- Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan national journal of Robotics research, 1(1):4\u201317, 1982. Ratliff, and Dieter Fox. Dexpilot: Vision-based tele- [16] Matthew T Mason and J Kenneth Salisbury Jr. Robot operation of dexterous robotic hand-arm system. In hands and the mechanics of manipulation. 1985. 2020 IEEE International Conference on Robotics and [17] Igor Mordatch, Zoran Popovic\u00b4, and Emanuel Todorov. Automation (ICRA), pages 9164\u20139170. IEEE, 2020. Contact-invariant optimization for hand manipulation. [2] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, In Proceedings of the ACM SIGGRAPH/Eurographics Edward Adelson, and Pulkit Agrawal. Visual dexterity: symposium on computer animation, pages 137\u2013144, In-hand dexterous manipulation from depth. ar Xiv 2012. preprint ar Xiv:2211.11744, 2022. [18] Yunfei Bai and C Karen Liu. Dexterous manipulation [3] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. usingbothpalm and fingers.In 2014 IEEEInternational Videodex: Learning dexterity from internet videos. Conference on Robotics and Automation (ICRA), pages Co RL, 2022. 1560\u20131565. IEEE, 2014. [4] Stefan Schaal. Is imitation learning the route to hu- [19] Vikash Kumar, Yuval Tassa, Tom Erez, and Emanuel manoid robots? Trends in cognitive sciences, 3(6):233\u2013 Todorov. Real-time behavi our syn the sis for dynamic 242, 1999. hand-manipulation. In 2014 IEEEInternational Confer- [5] Ahmed Hussein,Mohamed Medhat Gaber,Eyad Elyan, ence on Robotics and Automation (ICRA), pages 6808\u2013 and Chrisina Jayne. Imitation learning: A survey of 6815. IEEE, 2014. learning methods. ACM Computing Surveys (CSUR), [20] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, 50(2):1\u201335, 2017. Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, terity with immersive mixed reality. In 2023 IEEE Balakumar Sundaralingam, et al. Dextreme: Transfer International Conference on Robotics and Automation ofagilein-handmanipulation from simulationtoreality. (ICRA), pages 5962\u20135969. IEEE, 2023. ar Xiv preprint ar Xiv:2210.13702, 2022. [33] Yuzhe Qin,Yueh-Hua Wu,Shaowei Liu,Hanwen Jiang, [21] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv: general in-hand object re-orientation. Conference on Imitation learning for dexterous manipulation from hu- Robot",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 322,
      "paper_id": "dexcap",
      "text": "(ICRA), pages 5962\u20135969. IEEE, 2023. ar Xiv preprint ar Xiv:2210.13702, 2022. [33] Yuzhe Qin,Yueh-Hua Wu,Shaowei Liu,Hanwen Jiang, [21] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv: general in-hand object re-orientation. Conference on Imitation learning for dexterous manipulation from hu- Robot Learning, 2021. man videos. In European Conference on Computer [22] Abhishek Gupta, Justin Yu, Tony Z. Zhao, Vikash Ku- Vision, pages 570\u2013587. Springer, 2022. mar, Aaron Rovinsky, Kelvin Xu, Thomas Devlin, and [34] Priyanka Mandikal and Kristen Grauman. Dexvip: Sergey Levine. Reset-free rein for cement learning via Learning dexterous grasping with human hand pose multi-task learning: Learning dexterous manipulation priors from video. In Conference on Robot Learning, behaviors without human intervention. In ICRA, pages pages 651\u2013661. PMLR, 2022. 6664\u20136671. IEEE, 2021. [35] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one [23] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng handtomultiplehands:Imitationlearning for dexterous Chen, and Xiaolong Wang. Rotating without seeing: manipulation from single-camera teleoperation. IEEE Towardsin-handdexteritythroughtouch.ar Xivpreprint Robotics and Automation Letters, 7(4):10873\u201310881, ar Xiv:2303.10880, 2023. 2022. [24] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, [36] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao, Wei and Jitendra Malik. In-Hand Object Rotation via Rapid Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter Motor Adaptation. In Conference on Robot Learning Fox. Dextransfer: Real world multi-fingered dexterous (Co RL), 2022. grasping with minimal human demonstrations. ar Xiv [25] Gagan Khandate, Siqi Shang, Eric T Chang, Tristan L preprint ar Xiv:2209.14284, 2022. Saidi, Johnson Adams, and Matei Ciocarlie. Sampling- [37] Christian Zimmermann, Duygu Ceylan, Jimei Yang, based Exploration for Rein for cement Learning of Dex- Bryan Russell, Max Argus, and Thomas Brox. Frei- terous Manipulation. In Proceedings of Robotics: Sci- hand: A dataset for markerless capture of hand pose ence and Systems,Daegu,Republicof Korea,July 2023. and shape from single rgb images. In Proceedings of doi: 10.15607/RSS.2023.XIX.020. the IEEE/CVF International Conference on Computer [26] Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Vision, pages 813\u2013822, 2019. Qin, Yaodong Yang, Nikolay Atanasov, and Xiaolong [38] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shira- Wang. Dynamic handover: Throw and catch with bi- tori, and Kyoung Mu Lee. Interh and 2. 6 m: A dataset manual hands. ar Xiv preprint ar Xiv:2309.05655, 2023. and baseline for 3 d interacting hand pose estimation [27] Yuanpei Chen, Chen Wang, Li Fei-Fei, and C Karen from a single rgb image. In Computer Vision\u2013ECCV Liu. Sequential dexterity: Chaining dexterous poli- 2020:16 th European Conference,Glasgow,UK,August cies for long-horizon manipulation. ar Xiv preprint 23\u201328, 2020, Proceedings, Part XX 16, pages 548\u2013564. ar Xiv:2309.00987, 2023. Springer, 2020. [28] Johannes Pitz, Lennart Ro\u00a8stel, Leon Sievers, and [39] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Berthold Ba\u00a8uml. Dextrous tactile in-hand manipulation Angjoo Kanazawa, David Fouhey, and Jitendra Malik. using a modular rein for cement learning architecture. Reconstructing hands in 3 d with trans for mers. ar Xiv ar Xiv preprint ar Xiv:2303.04705, 2023. preprint ar Xiv:2312.05251, 2023. [29] Kelvin Xu, Zheyuan Hu, Ria Doshi, Aaron Rovinsky, [40] Tanner Schmidt,Richard ANewcombe,and Dieter Fox. Vikash Kumar, Abhishek Gupta, and Sergey Levine. Dart: Dense articulated real-time tracking.",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 323,
      "paper_id": "dexcap",
      "text": "learning architecture. Reconstructing hands in 3 d with trans for mers. ar Xiv ar Xiv preprint ar Xiv:2303.04705, 2023. preprint ar Xiv:2312.05251, 2023. [29] Kelvin Xu, Zheyuan Hu, Ria Doshi, Aaron Rovinsky, [40] Tanner Schmidt,Richard ANewcombe,and Dieter Fox. Vikash Kumar, Abhishek Gupta, and Sergey Levine. Dart: Dense articulated real-time tracking. In Robotics: Dexterous manipulation from images: Autonomous Science and systems, volume 2, pages 1\u20139. Berkeley, real-world rl via substep guidance. In 2023 IEEE CA, 2014. International Conference on Robotics and Automation [41] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and (ICRA), pages 5938\u20135945. IEEE, 2023. Vincent Lepetit.Honnotate:Amethod for 3 dannotation [30] Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, of hand and object poses. In Proceedings of the and Jitendra Malik. Twisting lids off with two hands. IEEE/CVF conference on computer vision and pattern ar Xiv:2403.02338, 2024. recognition, pages 3196\u20133206, 2020. [31] Sridhar Pandian Arunachalam, Sneha Silwal, Ben [42] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Evans, and Lerrel Pinto. Dexterous imitation made Ankur Handa, Jonathan Tremblay, Yashraj S Narang, easy: A learning-based framework for efficient dexter- Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. ousmanipulation. In 2023 ieeeinternationalconference Dexycb: A benchmark for capturing hand grasping of on robotics and automation (icra), pages 5954\u20135961. objects.In Proceedingsof the IEEE/CVFConferenceon IEEE, 2023. Computer Vision and Pattern Recognition,pages 9044\u2013 [32] Sridhar Pandian Arunachalam, Irmak Gu\u00a8zey, Soumith 9053, 2021. Chintala, and Lerrel Pinto. Holo-dex: Teaching dex- [43] Shangchen Han, Beibei Liu, Randi Cabezas, Christo- pher D Twigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho operation for legged robot loco-manipulation using Yu, Chun-Jung Tai, Muzaffer Akbay, Zheng Wang, wearable imu-based motion capture. ar Xiv preprint et al. Megatrack: monochrome egocentric articulated ar Xiv:2209.10314, 2022. hand-tracking for virtual reality. ACM Transactions on [54] Sylvain Calinon, Florent D\u2019halluin, Eric L Sauser, Dar- Graphics (To G), 39(4):87\u20131, 2020. win G Caldwell, and Aude G Billard. Learning and [44] Omid Taheri, Nima Ghorbani, Michael J Black, and reproduction of gestures by imitation. IEEE Robotics Dimitrios Tzionas. Grab: A dataset of whole-body & Automation Magazine, 17(2):44\u201354, 2010. human grasping of objects. In Computer Vision\u2013ECCV [55] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Move- 2020:16 th European Conference,Glasgow,UK,August ment imitation with nonlinear dynamical systems in 23\u201328, 2020, Proceedings, Part IV 16, pages 581\u2013600. humanoid robots. In Proceedings 2002 IEEE Interna- Springer, 2020. tional Conference on Robotics and Automation (Cat. [45] Zicong Fan, Omid Taheri, Dimitrios Tzionas, No.02 CH 37292), volume 2, pages 1398\u20131403 vol.2, Muhammed Kocabas, Manuel Kaufmann, Michael J 2002. doi: 10.1109/ROBOT.2002.1014739. Black, and Otmar Hilliges. Arctic: A dataset [56] Jens Kober and Jan Peters. Imitation and rein for cement for dexterous bimanual hand-object manipulation. In learning. IEEE Robotics & Automation Magazine, 17 Proceedingsof the IEEE/CVFConferenceon Computer (2):55\u201362, 2010. Vision and Pattern Recognition, pages 12943\u201312954, [57] Peter Englert and Marc Toussaint. Learning manipu- 2023. lation skills from a single demonstration. The Inter- [46] Yinghao Huang, Manuel Kaufmann, Emre Aksan, national Journal of Robotics Research, 37(1):137\u2013154, Michael JBlack,Otmar Hilliges,and Gerard Pons-Moll. 2018. Deepinertialposer:Learningtoreconstructhumanpose [58] Chelsea Finn,Tianhe Yu,Tianhao Zhang,Pieter Abbeel, from sparse inertial measurements in real time. ACM and Sergey Levine. One-shot visual imitation learning",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 324,
      "paper_id": "dexcap",
      "text": "2023. lation skills from a single demonstration. The Inter- [46] Yinghao Huang, Manuel Kaufmann, Emre Aksan, national Journal of Robotics Research, 37(1):137\u2013154, Michael JBlack,Otmar Hilliges,and Gerard Pons-Moll. 2018. Deepinertialposer:Learningtoreconstructhumanpose [58] Chelsea Finn,Tianhe Yu,Tianhao Zhang,Pieter Abbeel, from sparse inertial measurements in real time. ACM and Sergey Levine. One-shot visual imitation learning Transactions on Graphics (TOG), 37(6):1\u201315, 2018. via meta-learning. In Conference on robot learning, [47] Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam pages 357\u2013368. PMLR, 2017. Won, Alexander W Winkler, and C Karen Liu. Trans- [59] Aude Billard,Sylvain Calinon,Ruediger Dillmann,and former inertial poser: Real-time human motion recon- Stefan Schaal. Robot programming by demonstration. struction from sparse imus with simultaneous terrain In Springer handbook of robotics, pages 1371\u20131394. generation. In SIGGRAPH Asia 2022 Conference Pa- Springer, 2008. pers, pages 1\u20139, 2022. [60] Brenna D Argall, Sonia Chernova, Manuela Veloso, [48] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shi- and Brett Browning. A survey of robot learning from mada,Vladislav Golyanik,Christian Theobalt,and Feng demonstration. Robotics and autonomous systems, 57 Xu. Physical inertial poser (pip): Physics-aware real- (5):469\u2013483, 2009. time human motion tracking from sparse inertial sen- [61] Stefan Schaal. Dynamic movement primitives-a frame- sors. In Proceedings of the IEEE/CVF Conference work for motor control in humans and humanoid on Computer Vision and Pattern Recognition, pages robotics. In Adaptive motion of animals and machines, 13167\u201313178, 2022. pages 261\u2013280. Springer, 2006. [49] Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, [62] Jens Kober and Jan Peters. Learning motor primitives Scott Delp, and C Karen Liu. Diffusion inertial poser: for robotics. In 2009 IEEE International Conference Humanmotionreconstruction from arbitrarysparseimu on Robotics and Automation, pages 2112\u20132118. IEEE, configurations. ar Xiv preprint ar Xiv:2308.16682, 2023. 2009. [50] Fabian C Weigend, Xiao Liu, and Heni Ben Amor. [63] Alex and ros Paraschos, Christian Daniel, Jan R Pe- Probabilistic differentiable filters enable ubiquitous ters, and Gerhard Neumann. Probabilistic move- robot control with smartwatches. ar Xiv preprint ment primitives. In C.J. Burges, L. Bottou, ar Xiv:2309.06606, 2023. M. Welling, Z. Ghahramani, and K.Q. Weinberger, [51] Lars Fritsche, Felix Unverzag, Jan Peters, and Roberto editors, Advances in Neural Information Process- Calandra. First-person tele-operation of a humanoid ing Systems, volume 26. Curran Associates, Inc., robot.In 2015 IEEE-RAS 15 th International Conference 2013. URL https://proceedings.neurips.cc/paper/2013/ on Humanoid Robots (Humanoids), pages 997\u20131002. file/e 53 a 0 a 2978 c 28872 a 4505 bdb 51 db 06 dc-Paper.pdf. IEEE, 2015. [64] Alex and ros Paraschos, Christian Daniel, Jan Peters, [52] Bin Fang,Di Guo,Fuchun Sun,Huaping Liu,and Yupei and Gerhard Neumann. Using probabilistic movement Wu. A robotic hand-arm teleoperation system using primitives in robotics. Autonomous Robots, 42(3):529\u2013 humanarm/hand with anovel data glove. In 2015 IEEE 551, 2018. International Conference on Robotics and Biomimetics [65] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush (ROBIO), pages 2483\u20132488. IEEE, 2015. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, [53] Chengxu Zhou, Christopher Peers, Yuhui Wan, Robert Silvio Savarese,Yuke Zhu,and Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n. Richardson, and Dimitrios Kanoulas. Teleman: Tele- Whatmattersinlearning from offlinehum and emonstra- tions for robot manipulation. In 5 th Annual Conference [76] Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, and on Robot Learning, 2021. URL https://openreview.net/",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 325,
      "paper_id": "dexcap",
      "text": "Fei-Fei, [53] Chengxu Zhou, Christopher Peers, Yuhui Wan, Robert Silvio Savarese,Yuke Zhu,and Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n. Richardson, and Dimitrios Kanoulas. Teleman: Tele- Whatmattersinlearning from offlinehum and emonstra- tions for robot manipulation. In 5 th Annual Conference [76] Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, and on Robot Learning, 2021. URL https://openreview.net/ Dorsa Sadigh. Efficient data collection for robotic forum?id=Jrsf BJt DFd I. manipulation via compositional generalization. ar Xiv [66] Peter Florence, Lucas Manuelli, and Russ Tedrake. preprint ar Xiv:2403.05110, 2024. Self-supervised correspondence in visuomotor policy [77] Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent learning. IEEE Robotics and Automation Letters, 5(2): Yi, Sergey Levine, and Jitendra Malik. Learn- 492\u2013499, 2019. ing visuotactile skills with two multifingered hands. [67] Tony Z Zhao, Vikash Kumar, Sergey Levine, and ar Xiv:2404.16823, 2024. Chelsea Finn. Learning fine-grained bimanual ma- [78] Jiafei Duan, Yi Ru Wang, Mohit Shridhar, Dieter Fox, nipulation with low-cost hardw are. ar Xiv preprint and Ranjay Krishna. Ar 2-d 2: Training a robot without ar Xiv:2304.13705, 2023. a robot. ar Xiv preprint ar Xiv:2306.13818, 2023. [68] Jennifer Grannen, Yilin Wu, Brandon Vu, and Dorsa [79] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Sadigh. Stabilize to act: Learning to coordinate for and Shuran Song. Xskill: Cross embodiment skill bimanualmanipulation. In Conferenceon Robot Learn- discovery. In Conference on Robot Learning, pages ing, pages 563\u2013576. PMLR, 2023. 3536\u20133555. PMLR, 2023. [69] Tianhao Zhang,Zoe Mc Carthy,Owen Jow,Dennis Lee, [80] Jingyun Yang, Junwu Zhang, Connor Settle, Akshara Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep Rai, Rika Antonova, and Jeannette Bohg. Learning imitation learning for complex manipulation tasks from periodic tasks from human demonstrations. In 2022 virtualrealityteleoperation. In 2018 IEEEinternational International Conference on Robotics and Automation conference on robotics and automation (ICRA), pages (ICRA), pages 8658\u20138665. IEEE, 2022. 5628\u20135635. IEEE, 2018. [81] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, [70] Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Savarese, and Li Fei-Fei. Scaling robot supervision to Xu, et al. Rt-trajectory: Robotic task generaliza- hundreds of hours with roboturk: Robotic manipulation tion via hindsight trajectory sketches. ar Xiv preprint dataset through human reasoning and dexterity. In ar Xiv:2311.01977, 2023. 2019 IEEE/RSJ International Conference on Intelligent [82] Haoyu Xiong, Haoyuan Fu, Jieyi Zhang, Chen Bao, Robots and Systems (IROS), pages 1048\u20131055. IEEE, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh 2019. Garg, and Cewu Lu. Robotube: Learning household [71] Liyiming Ke, Ajinkya Kamat, Jingqiang Wang, Tapo- manipulation from human videos with simulated twin mayukh Bhattacharjee, Christoforos Mavrogiannis, and environments. In Conference on Robot Learning, pages Siddhartha S Srinivasa. Telemanipulation with chop- 1\u201310. PMLR, 2023. sticks:Analyzinghumanfactorsinuserdemonstrations. [83] Dima Damen, Hazel Doughty, Giovanni Maria In 2020 IEEE/RSJ International Conference on Intelli- Farinella, Sanja Fidler, Antonino Furnari, Evangelos gent Robots and Systems (IROS), pages 11539\u201311546. Kazakos, Davide Moltisanti, Jonathan Munro, Toby IEEE, 2020. Perrett, Will Price, et al. Scaling egocentric vision: [72] Chen Wang, Rui Wang, Ajay Mandlekar, Li Fei- The epic-kitchens dataset. In Proceedings of the Fei, Silvio Savarese, and Danfei Xu.",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 326,
      "paper_id": "dexcap",
      "text": "Sanja Fidler, Antonino Furnari, Evangelos gent Robots and Systems (IROS), pages 11539\u201311546. Kazakos, Davide Moltisanti, Jonathan Munro, Toby IEEE, 2020. Perrett, Will Price, et al. Scaling egocentric vision: [72] Chen Wang, Rui Wang, Ajay Mandlekar, Li Fei- The epic-kitchens dataset. In Proceedings of the Fei, Silvio Savarese, and Danfei Xu. Generaliza- European conference on computer vision (ECCV), tion through hand-eye coordination: An action space pages 720\u2013736, 2018. for learning spatially-invariant visuomotor control. In [84] Kristen Grauman, Andrew Westbury, Eugene Byrne, 2021 IEEE/RSJ International Conference on Intelligent Zachary Chavis,Antonino Furnari,Rohit Girdhar,Jack- Robots and Systems (IROS), pages 8913\u20138920. IEEE, son Hamburger,Hao Jiang,Miao Liu,Xingyu Liu,etal. 2021. Ego 4 d: Around the world in 3,000 hours of egocentric [73] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke video. In Proceedings of the IEEE/CVF Conference Zhu. Viola: Imitation learning for vision-based ma- on Computer Vision and Pattern Recognition, pages nipulation with object proposal priors. ar Xiv preprint 18995\u201319012, 2022. ar Xiv:2210.11339, 2022. [85] Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao [74] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- Bao,and Yuke Zhu. Robotlearningon the job:Human- gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana in-the-loop autonomy and learning during deployment. Gopalakrishnan,Karol Hausman,Alex Herzog,Jasmine ar Xiv preprint ar Xiv:2211.08416, 2022. Hsu, et al. Rt-1: Robotics trans for mer for real-world [86] Zhenghao Peng, Wenjie Mo, Chenda Duan, Quanyi controlat scale.ar Xivpreprintar Xiv:2212.06817,2022. Li, and Bolei Zhou. Learning from active human in- [75] Philipp Wu,Yide Shentu,Zhongke Yi,Xingyu Lin,and volvement through proxy value propagation. In Thirty- Pieter Abbeel. Gello: A general, low-cost, and intuitive seventh Conference on Neural Information Processing teleoperation framework for robot manipulators. ar Xiv Systems, 2023. preprint ar Xiv:2309.13037, 2023. [87] Jonathan Spencer, Sanjiban Choudhury, Matthew Barnes, Matthew Schmittle, Mung Chiang, Peter Ra- Michael Gleicher. Rangedik: An optimization-based madge, and Siddhartha Srinivasa. Learning from in- robot motion generation method for ranged-goal tasks. terventions. In Robotics: Science and Systems (RSS), pages 9700\u20139706, 2023. 2020. [99] Jascha Sohl-Dickstein, Eric Weiss, Niru [88] Shuran Song, Andy Zeng, Johnny Lee, and Thomas Maheswaranathan, and Surya Ganguli. Deep Funkhouser. Grasping in the wild: Learning 6 dof unsupervised learning using nonequilibrium closed-loop grasping from low-cost demonstrations. thermodynamics. In International conference on IEEE Robotics and Automation Letters, 5(3):4978\u2013 machine learning, pages 2256\u20132265. PMLR, 2015. 4985, 2020. [100] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising [89] Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Ab- diffusion probabilistic models. Advances in neural hinav Gupta, Pieter Abbeel, and Lerrel Pinto. Visual information processing systems, 33:6840\u20136851, 2020. imitationmadeeasy. In Conferenceon Robot Learning, [101] Ajay Mandlekar, Danfei Xu, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, pages 1992\u20132005. PMLR, 2021. Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in- [90] Kiran Doshi, Yijiang Huang, and Stelian Coros. On the-loop imitation learning using remote teleoperation. hand-held grippers and the morphological gap in ar Xiv preprint ar Xiv:2012.06733, 2020. human manipulation demonstration. ar Xiv preprint [102] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush ar Xiv:2311.01832, 2023. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, [91] Felipe Sanches, Geng Gao, Nathan Elangovan, Ri- Silvio Savarese,Yuke Zhu,and Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n. cardo V Godoy, Jayden Chapman, Ke Wang, Patrick What matters in learning from offline human demon-",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 327,
      "paper_id": "dexcap",
      "text": "ar Xiv preprint [102] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush ar Xiv:2311.01832, 2023. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, [91] Felipe Sanches, Geng Gao, Nathan Elangovan, Ri- Silvio Savarese,Yuke Zhu,and Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n. cardo V Godoy, Jayden Chapman, Ke Wang, Patrick What matters in learning from offline human demon- Jarvis, and Minas Liarokapis. Scalable. intuitive hu- strations for robot manipulation. ar Xiv preprint man to robot skill transfer with wearable human ma- ar Xiv:2108.03298, 2021. chine interfaces: On complex, dexterous tasks. In [103] Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Hao Su, 2023 IEEE/RSJ International Conference on Intelligent and Xiaolong Wang. Dexpoint: Generalizable point Robots and Systems (IROS), pages 6318\u20136325. IEEE, cloud rein for cement learning for sim-to-real dexterous 2023. manipulation. In Conference on Robot Learning, pages [92] Hongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren, 594\u2013605. PMLR, 2023. Jingjing Chen, Ruo Zhang, Weiming Wang, and Cewu [104] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Lu. Low-cost exoskeletons for learning whole-arm ma- Guibas. Pointnet: Deep learning on point sets for 3 d nipulationin the wild.ar Xivpreprintar Xiv:2309.14975, classification and segmentation. ar Xiv preprint ar Xiv: 2023. Arxiv-1612.00593, 2016. [93] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja [105] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, Zisserman, Oriol Vinyals, and Joao Carreira. Per- and Lerrel Pinto. On bringing robots home. ar Xiv ceiver:Generalperception with iterativeattention.ar Xiv preprint ar Xiv:2311.16098, 2023. preprint ar Xiv: Arxiv-2103.03206, 2021. [94] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, [106] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Ko- Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and siorek, Seungjin Choi, and Yee Whye Teh. Set trans- Shuran Song. Universal manipulation interface: In-the- former: A framework for attention-based permutation- wild robot teaching without in-the-wild robots. ar Xiv invariant neural networks. ar Xiv preprint ar Xiv: Arxiv- preprint ar Xiv:2402.10329, 2024. 1810.00825, 2018. [95] Kenneth Shaw, Ananye Agarwal, and Deepak Pathak. [107] Yiyue Luo, Yunzhu Li, Pratyusha Sharma, Wan Shou, LEAP Hand: Low-Cost, Efficient, and Anthropomor- Kui Wu, Michael Foshey, Beichen Li, Toma\u00b4s Palacios, phic Hand for Robot Learning. In Proceedings of Antonio Torralba, and Wojciech Matusik. Learning Robotics: Science and Systems, Daegu, Republic of human\u2013environment interactions using con for mal tac- Korea, July 2023. doi: 10.15607/RSS.2023.XIX.089. tile textiles. Nature Electronics, 4(3):193\u2013201, 2021. [96] Daniel Rakita, Bilge Mutlu, and Michael Gleicher. Re- [108] Oussama Khatib. A unified approach for motion and laxed IK: Real-time Syn the sis of Accurate and Feasible force control of robot manipulators: The operational Robot Arm Motion. In Proceedings of Robotics: Sci- space formulation. IEEE Journal on Robotics and ence and Systems, Pittsburgh, Pennsylvania, June 2018. Automation, 3(1):43\u201353, 1987. doi: 10.15607/RSS.2018.XIV.043. [109] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian [97] Daniel Rakita, Haochen Shi, Bilge Mutlu, and Michael Sun. Deep residual learning for image recognition. In Gleicher. Collisionik: A per-instant pose optimization Proceedingsof the IEEEconferenceoncomputervision method for generating robot motions with environment and pattern recognition, pages 770\u2013778, 2016. collisionavoidance.In 2021 IEEEInternational Confer- [110] Jiaming Song, Chenlin Meng, and Stefano Ermon. ence on Robotics and Automation (ICRA), pages",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 328,
      "paper_id": "dexcap",
      "text": "Michael Sun. Deep residual learning for image recognition. In Gleicher. Collisionik: A per-instant pose optimization Proceedingsof the IEEEconferenceoncomputervision method for generating robot motions with environment and pattern recognition, pages 770\u2013778, 2016. collisionavoidance.In 2021 IEEEInternational Confer- [110] Jiaming Song, Chenlin Meng, and Stefano Ermon. ence on Robotics and Automation (ICRA), pages 9995\u2013 Denoising diffusion implicit models. ar Xiv preprint 10001. IEEE, 2021. ar Xiv:2010.02502, 2020. [98] Yeping Wang, Pragathi Praveena, Daniel Rakita, and APPENDIXA IMPLEMENTATIONDETAILS A. DEXCAP hardw are implementations Figure 11 illustrates the hardw are design of DEXCAP. All models are 3 D-printed with PLA material. The chest camera mount is equipped with four slots for cameras: at the top, an L 515 RGB-D Li DAR camera, followed by three T 265 fisheye SLAM tracking cameras. The Li DAR camera and the uppermost T 265 camera are securely fixed to the camera Fig. 11: Detailed view of chest mount and glove mount rack, while the two lower T 265 cameras are designed to be The glove mount follows the cont our of the hump on the detachable and can be affixed to the glove\u2019s back for hand 6- top of the Rokoko glove, and an opening is added to route Do F pose tracking. The design features of the camera mounts the USB-C cable to the glove. The angle of the camera is on both the chest and gloves include a locking mechanism set to 45 degrees facing upwards so that the camera view is to prevent the cameras from accidentally slipping out. On the less obstructed from the back of the hand. The slide guide glove, the camera mount is positioned over the magnetic hub has an indentation matching the position of the back plate onitsdorsalside,ensuringafirmattachmentbetween the hub to ensure the same insertion position across experiments. The and the mount. For powering and data storage, the user wears chest mount houses 3 identical slots following the cont our of a backpack containing a 40000 m Ah portable power bank and the T 265. An additional slot is added to fit in the slide plate a mini-PC with 64 GB RAM and 2 TB SSD. The system\u2019s of the T 265. total weight is 3.96 pounds, optimized for ease of mobility, supporting up to 40 minutes of continuous data collection. The power bank\u2019s rapid recharge capability, requiring only 30 10 minutes of each session prioritized for high-quality data minutes for a full charge, enables extensive data collection capture. After collection, transferring the data from RAM to sessions over several hours. SSD is efficiently completed within 3-5 minutes using multi- threading. B. Data collection details Inthisstudy,weprimarilyinvestigatetwotypesof DEXCAP Figure 13 andthesupplementaryvideoillustrate the begin- data:(1)datacapturedin the robotspace and(2)datacollected ning steps of a data collection session. Initially, all cameras inthewild.For the firstcategory,weposition the chestcamera are mounted on the chest. Upon initiating the program, the setup on a stand between two robot arms. The robots are then participant moves within the environment for several sec- adjusted to a resting position, clearing the operational space onds, allowing the SLAM algorithm to build the map of for human interaction.",
      "start_pos": 10164,
      "end_pos": 10676
    },
    {
      "chunk_id": 329,
      "paper_id": "dexcap",
      "text": "on the chest. Upon initiating the program, the setup on a stand between two robot arms. The robots are then participant moves within the environment for several sec- adjusted to a resting position, clearing the operational space onds, allowing the SLAM algorithm to build the map of for human interaction. This arrangement allows for the direct the surroundings. Subsequently, the bottom T 265 cameras are use of DEXCAP to collect data within the robot\u2019s operational relocated to the glove mounts, initiating the data collection area. Such data underpins basic experiments for tasks like phase. This preparatory phase is completed in approximately Sponge picking, Ball collecting, and Plate wiping, alongside 15 seconds, as demonstrated in the video submission. more complex challenges, including Scissor cutting and Tea The data collection encompasses four data types, recorded preparing. For the second category, individuals don DEXCAP at 60 frames per second: (1) the 6-Do F pose of the chest- togather data outside the labsetting,focusingon the system\u2019s mounted Li DAR camera, as tracked by the top T 265 camera; zero-shotlearningper for mancewithin-the-wild DEXCAP data (2) the 6-Do F wrist poses, as captured by the two lower T 265 and its ability to generalize to unseen objects, particularly in cameras attached to the gloves; (3) the positions of finger the Packaging task. joints within each glove\u2019s reference frame, detected by the motion capture gloves; and (4) RGB-D image frames from C. Data retargeting details the Li DAR camera. The initial pose of the top T 265 camera To adapt the collected raw DEXCAP data for training robot establishes the world frame for all data, allowing for the policies (commonly known as retargeting). This involves two integration of all streamed data\u2014RGB-D point clouds, hand key steps: (1) retargeting the observations and (2) retargeting 6-Do F poses, and finger joint locations\u2014into a unified world the actions. frame. This configuration permits unrestricted movement by For observation retargeting, the initial step is to convert the the participant, enabling easy isolation and removal of body RGB-D inputs into 3 D point clouds, ensuring each pixel\u2019s movements from the dataset. colorispreserved.Thesepointclouds are the naligned with the Data are initially buffered in the mini-PC\u2019s RAM, support- worldframe,definedbytheinitialposeof the main T 265 cam- ing a 15-minute collection at peak frame rate (60 fps). Once era. Subsequently, a point cloud visualization UI is launched, the RAM is full, data capture slows to 20 fps due to storage displaying the aligned input point clouds alongside the robot shifting to the SSD. We empirically find that this reduction operation space\u2019s point clouds within a unified coordinate in frame rate may affect SLAM tracking accuracy, potentially frame. Through this UI, users can adjust the point cloud\u2019s leading to jumping tracking results. Thus, we use the first position with intherobotoperationspaceusing the keyboard\u2019s directionalkeys.Thisadjustmentprocessisrequiredonlyonce Hyperparameter Default for all data collected in the same location and is completed in Batch Size 16 under a minute. After aligning the point clouds with the robot Learning Rate(LR) 1 e-4 Num Epoch 3000 space, points below the robot\u2019s table surface are eliminated, LRDecay None refining the observation",
      "start_pos": 10626,
      "end_pos": 11138
    },
    {
      "chunk_id": 330,
      "paper_id": "dexcap",
      "text": "directionalkeys.Thisadjustmentprocessisrequiredonlyonce Hyperparameter Default for all data collected in the same location and is completed in Batch Size 16 under a minute. After aligning the point clouds with the robot Learning Rate(LR) 1 e-4 Num Epoch 3000 space, points below the robot\u2019s table surface are eliminated, LRDecay None refining the observation data for policy development. Image Encoder Res Net-18 Action retargeting begins with applying a consistent trans- Image Feature Dim 64 RNNType LSTM formation between the T 265 cameras on the chest mount to RNNHorizon 3 translate the hand joint locations into the world frame. Then, GMM None we use the previously calculated point cloud trans for mation TABLE V: Hyperparameters - BC-RNN-img matrix to transform the hand joints to the robot operation space.Theresultsof this process are visualizedin Figure 12 by Hyperparameter Default depicting the trans for med hand joints together with the point Batch Size 16 cloudasaskeletalmodelof the hand.Thefinalphaseemploys Learning Rate(LR) 1 e-4 inverse kinematics to map the fingertip positions between the Num Epoch 3000 LRDecay None robot hand (LEAP hand) and the human hand. We use the Point Cloud Encoder Point Net hand\u2019s 6-Do F pose to initialize the LEAP hand\u2019s orientation Point Cloud Downsample 1000 for IKcalculation.Figure 12 illustrates the IKresults,showing Pooling Type Max Pooling UNet Embed Dim 256 the robot hand model integrated with the observational point UNet Downdims [256,512,1024] clouds,therebygenerating the actionsrequired for training the UNet Kernel Size 5 Diffusion Type DDIM robot policy. Diffusion Num Train 100 All of the point cloud observations are downsampled uni- Diffusion Num Infer 10 Input Horizon 3 formly to 5000 points and stored together with robot propri- oception states and actions into an hdf 5 file. We manually TABLE VI: Hyperparameters - DP-point annotate the start and end frames of each task demonstration from the entire recording session (10 minutes each). The motion for resetting the task environment is not included in the inputs is set to three. For pointcloud-based methods, the the training dataset. input point cloud is uni for mly downsampled to 1000 points. We list the hyperparameters for each architecture in Table V, D. Robot controller details VI, VII. Position control is employed throughout our experiments, F. Task implementations structured hierarchically: (1) At the high level, the learned policy generates the goal position for the next step, which In this section, we introduce the details of each task design encompasses the 6-Do F pose of the end-effector for both \u2022 Sponge Picking: A sponge is randomly placed on the robotarmsanda 16-dimensionalfingerjointposition for both table within a 40\u00d770 centimeter area. The objective is hands. (2) At the low level, an Operational Space Controller to grasp the sponge and lift it upwards by more than 30 (OSC) [108], continuously interpolates the arm\u2019s trajectory centimeters. towards the high-levelspecifiedgoalposition and relaysinter- \u2022 Ball Collecting: A ball is randomly positioned on the polated OSC actions to the robot for execution. Meanwhile, right side of the table within a 40\u00d730 centimeter area, finger movements are directly managed by a joint impedance while a basket is similarly placed randomly on the left controller. Following each",
      "start_pos": 11088,
      "end_pos": 11600
    },
    {
      "chunk_id": 331,
      "paper_id": "dexcap",
      "text": "Ball Collecting: A ball is randomly positioned on the polated OSC actions to the robot for execution. Meanwhile, right side of the table within a 40\u00d730 centimeter area, finger movements are directly managed by a joint impedance while a basket is similarly placed randomly on the left controller. Following each robot action, we calculate the dis- side within the same dimensions. The task is completed tancebetween the robot\u2019scurrentproprioception and the target whentheballisgrasped and thendroppedinto the basket. pose.Ifthedistancebetween the missmallerthanathreshold, \u2022 Plate Wiping: In a setup akin to the Ball Collecting task, we regard that the robot has reached the goal position and aplateandaspongearer and omlyplacedon the right and will query the policy for the next action. To prevent the robot left sides of the table, respectively, each within a 40\u00d730 from becoming idle, if it fails to reach the goal pose within centimeter area. The goal involves using both hands to h steps, the policy is queried anew for the subsequent action. pick up the plate and sponge separately, then utilizing We designate h=10 in our experiments. We empirically find the sponge to wipe the plate twice. This task demands that for tasks that consist of physical contact with objects or coordinationbetween the twohands,positioning the plate applyingforce,thissituationhappensmoreoften and asmaller in the table\u2019s middle area to facilitate the wiping action. h will have a smoother robot motion. \u2022 Packaging: An empty paper box and a target object are randomly positioned on the table, with the object within E. Policy model and training details a 40\u00d730 centimeter area on the right and the box within For all image-input methods, we use Res Net-18 [109] as a 10\u00d710 centimeter area on the left. This task aims to the image encoder. For models based on diffusion policy, we assess the model\u2019s ability to generalize across various use Denoising Diffusion Implicit Models (DDIM) [110] for objects, including unseen ones not present in the training the denoising iterations. For all baselines, the time horizon of dataset. Success involves using one hand to pick up the \ud835\udc61 gnipiw etal P gnittuc rossic S gnigakca P Human Robot Human Robot Human Robot Fig.12:Visualizationofcollectedhuman data and retargetedrobot data.DEXILsuccess full yadaptshumanmotioncapture data for tasks such as plate wiping, scissor cutting, and packaging. We demonstrate the entire workflow of executing these tasks. Hyperparameter Default \u2022 Scissor Cutting: A container is fixed at the table\u2019s center, Batch Size 16 with scissors on the left and a strip of paper tape on Learning Rate(LR) 1 e-4 the right. The task begins with the left hand function- Num Epoch 3000 LRDecay None ally grasping the scissors\u2014inserting the thumb into one Point Cloud Encoder Perceiver handle and the index and middle fingers into the other. Point Cloud Downsample 1000 Pooling Type Max Pooling Simultaneously,therighth and grasps the papertape.Both UNet Embed Dim 256 scissors and tape are then lifted and moved towards the UNet Downdims [256,512,1024] UNet Kernel Size 5 center, with the left hand operating the scissors to cut Diffusion Type DDIM the tape. A cut exceeding 3 millimeters deems the task Diffusion Num Train 100 Diffusion",
      "start_pos": 11550,
      "end_pos": 12062
    },
    {
      "chunk_id": 332,
      "paper_id": "dexcap",
      "text": "papertape.Both UNet Embed Dim 256 scissors and tape are then lifted and moved towards the UNet Downdims [256,512,1024] UNet Kernel Size 5 center, with the left hand operating the scissors to cut Diffusion Type DDIM the tape. A cut exceeding 3 millimeters deems the task Diffusion Num Train 100 Diffusion Num Infer 10 successful. Input Horizon 3 \u2022 Tea Preparing:Ateatableiscentrallyplaced with afixed TABLE VII: Hyperparameters - Ours (DP-prec) orientation, accompanied by a tea bottle, tweezers, and a teapot. The robot must first grasp the tea bottle with the left hand and unscrew the cap with the right hand, completing two rotations. The cap is then taken off and object and theothertomovetheboxto the table\u2019scenter. placedontherightsideof the teatable.Subsequently,the The object is then placed into the box, followed by righth and picksupthetweezers from the toprightcorner stabilizing the box with one hand while the other closes oftheteatable.Therobot the nattemptstop our tea from it by grasping and moving the lid. Fig. 13: Prepration of data collection in the wild. The first row illustrates data collection conducted in a laboratory setting, and the second row depicts in-the-wild data collection. (a) Initially, the human data collector moves around in the environment totrack 6-Do Fwristposes with SLAM.(b)-(d)Subsequently,the data collectordetachesthetwocameras from the chestmount and secures them onto the glove mount. (e) With this setup, the human is prepared to begin data collection. Fig.14:Switching DEXCAP fromthehumanto the robot.Weillustrate,frombothfirst-person and frontviews,theseamless transition of DEXCAP from a human data collector to a bimanual dexterous robot system. This process involves effortlessly detaching the cameras from the chest mount and inserting them into a stationary mount on the robot\u2019s table. the bottle into the teapot with the left hand, while the additionalcorrection data,whichisusedinfur the rrefining the right hand uses the tweezers to aid the pouring process. policyforenhanced task per for mance.Detaileddescriptionsof Finally, the robot returns the tweezers and the tea bottle these algorithms and their implementation are provided in the to their corresponding positions on the table. The task is main paper. In the human-in-the-loop process, we employ the deemedsuccessfulifteamakesitinto the teapot and both mini-PC to live stream data from all T 265 tracking cameras. theteabottle and tweezers are returnedto the irrespective Thistrackingin for mationis the ntransmittedtoa Redisserver places.For the tasktobeconsidered full ysuccessful,the configured on the local network. Concurrently, the robot, teabottlemust becompletelyreleased from the lefth and. operating the learned policy on a workstation, receives delta movements of the human hands from the Redis server. These G. Human-in-the-loop implementations deltasserveasresidualcorrections and areintegratedintoeach DEXCAP incorporates two human-in-the-loop correction robot action. The RGB-D Li DAR camera, positioned on the methodologies: teleoperation and residual correction. Both centralbarbetween the robotarms,connectsto the workstation methods can be utilized during policy rollouts to gather desab-UMI )sru O( UMI-MALS Trajectory overview Zoom-in result Start End Start & End Fig. 15: Comp are with IMU-based mocap system. We disable the SLAMmapping and pose-correctionfeaturesof the T 265 tracking camera, forcing it to rely on IMU information totrack the pose.Thehumanoperatorheld the camera,started from a fixed location, moved it along a predefined trajectory, and then returned to the starting position. IMU-based method (first row) fails to match the",
      "start_pos": 12012,
      "end_pos": 12524
    },
    {
      "chunk_id": 333,
      "paper_id": "dexcap",
      "text": "mocap system. We disable the SLAMmapping and pose-correctionfeaturesof the T 265 tracking camera, forcing it to rely on IMU information totrack the pose.Thehumanoperatorheld the camera,started from a fixed location, moved it along a predefined trajectory, and then returned to the starting position. IMU-based method (first row) fails to match the endpoint with the start point, which indicates that there is pose drift during tracking. Our SLAM-IMU method (second row) doesn\u2019t drift and captures smooth trajectory during the tracking. Drifting error (cm) Trajectory 1 Trajectory 2 IMU-based 8.0\u00b13.1 11.3\u00b14.7 SLAM-IMU (Ours) 0.4\u00b10.2 0.8\u00b10.3 TABLE VIII: Drifting error of different tracking methods. to capture observation data. Instead of recording the robot\u2019s actual positional changes, we log the action commands dis- patchedto the robotcontroller.Thisdesigniscrucial for tasks involving physical contact with the environment and objects. APPENDIXB SUPPLEMENTARYEXPERIMENTRESULTS A. Tracking accuracy Figure 15 and Table VIIIpresentqualitative and quantitative results, respectively. We observe that the IMU-based method suffers from pose drifting during tracking, while our SLAM- IMU approach more accurately tracks hand poses, with an average error of 0.8 cm compared to the 11.3 cm error of the IMU-based method.",
      "start_pos": 12474,
      "end_pos": 12658
    },
    {
      "chunk_id": 334,
      "paper_id": "inthewild",
      "text": "Learning Generalizable Robotic Reward Functions from \u201cIn-The-Wild\u201d Human Videos Annie S. Chen, Suraj Nair, Chelsea Finn Stanford University Abstract\u2014We are motivated by the goal of generalist robots \u201dIn-the-wild\u201d Human Videos Robot Videos that can complete a wide range of tasks across many en- Many Environments, Many Tasks One Environment, Few Tasks vironments. Critical to this is the robot\u2019s ability to acquire some metric of task success or reward, which is necessary for rein for cement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a DVD Reward pathtowardssuchgeneralizationincomputervision and natural Function language,collectinghighquality data setsofroboticinteractionat Training scaleremainsan open challenge.Incontrast,\u201cin-the-wild\u201dvideos ofhumans(e.g.You Tube)containanextensivecollectionofpeople Unseen Environment Human Demo Testing doing interesting tasks across a diverse range of settings. In this Unseen Task work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multi task reward functions DVD Reward by training a discriminator to classify whether two videos are Function per for ming the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to Task Completion unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real Widow X 200 robot in an unseen environment from a single human demo. I. INTRODUCTION Despite recent progress in robotic learning on tasks ranging Figure 1: Reward Learning and Planning from In-The-Wild from grasping [24] to in-hand manipulation [31], the long- Human Videos. During training (top), the agent learns a reward function from a small set of robot videos in one environment, and standing goal of the \u201cgeneralist robot\u201d that can complete many a large set of in-the-wild human videos spanning many tasks and tasks across environments and objects has remained out of environments. At test time (bottom), the learned reward function is reach. While there are numerous challenges to overcome in conditioned upon a task specification (a human video of the desired achieving this goal, one critical aspect of learning general task),andproduces are wardfunctionwhich the robot can usetoplan purpose robotic policies is the ability to learn general purpose actionsorlearnapolicy.Byvirtueoftrainingondiversehum and ata, this reward function generalizes to unseen environments and tasks. reward functions. Such reward functions are necessary for the robottodetermineitsownproficiencyat the specified task from its on-board sensor observations (e.g. RGB camera images). robots at a large scale remains challenging for a number of Moreover, unless these reward functions can generalize across reasons,suchasneedingtobalance data quality with scalability, varying environments and tasks, an agent cannot hope to use and maintaining safety without relying heavily on human them to learn generalizable multi-task policies. supervision and resets. Alternatively, You Tube and similar While prior works in computer vision and",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 335,
      "paper_id": "inthewild",
      "text": "these reward functions can generalize across reasons,suchasneedingtobalance data quality with scalability, varying environments and tasks, an agent cannot hope to use and maintaining safety without relying heavily on human them to learn generalizable multi-task policies. supervision and resets. Alternatively, You Tube and similar While prior works in computer vision and NLP [11, 12, 4] sources contain enormous amounts of \u201cin-the-wild\u201d visual data haveshownnotablegeneralizationvialarge and diverse data sets, of humans interacting in diverse environments. Robots that can translating these successes to robotic learning has remained learn reward functions from such data have the potential to be challenging, partially due to the dearth of broad, high-quality able to generalize broadly due to the breadth of experience in robotic interaction data. Motivated by this, a number of recent this widely available data source. works have taken important steps towards the collection of Of course, using such \u201cin-the-wild\u201d data of humans for large and diverse data setsofroboticinteraction[28,22,10,53] robotic learning comes with a myriad of challenges. First, such and have shown some promise in enabling generalization [10]. data often will have tremendous domain shift from the robot\u2019s At the same time, collecting such interaction data on real observationspace,inboththemorphologyof the agent and the 1202 ra M 13 ]OR.sc[ 1 v 71861.3012:vi Xra visualappearanceof the scene(e.g.see Figure 1).Fur the rmore, which study single task problems in a single environment, the the human\u2019s action space in these \u201cin-the-wild\u201d videos is often focusof this workisinlearninggeneralizablemulti-taskreward quite different from the robot\u2019s action space, and as a result functions for visual robotic manipulation that can produce there may not always be a clear mapping between human and rewards for different tasks by conditioning on a single video robot behavior. Lastly, in practice these videos will often be of a human completing the task. low quality, noisy, and may have an extremely diverse set of B. Robotic Learning from Human Videos viewpoints or backgrounds. Critically however, this data is plentiful and already exists, and is easily accessible through A number of works have studied learning robotic behavior websites like You Tube or in pre-collected academic datasets fromhumanvideos.Oneapproachistoexplicitlyper for msome like the Something-Something data set [21], allowing them form of object or hand tracking in human videos, which can to be incorporated into the robot learning process with little then be translated into a sequence of robot actions or motion additional supervision cost or collection overhead. primitives for task execution [26, 52, 30, 25, 36]. Unlike these Given the above challenges, how might one actually learn works,whichh and-design the mapping from ahumansequence reward functions from these videos? The key idea behind our to robot behaviors, we aim to learn the functional similarity approach is to train a classifier to predict whether two videos between human and robot videos through data. are completing the same task or not. By leveraging the activity More recently, a range of techniques have been proposed for labels that come with many human video datasets, along with end-to-end learning from human videos. One such approach a modest amount of robot demos, this model can capture the is to learn to translate",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 336,
      "paper_id": "inthewild",
      "text": "or not. By leveraging the activity More recently, a range of techniques have been proposed for labels that come with many human video datasets, along with end-to-end learning from human videos. One such approach a modest amount of robot demos, this model can capture the is to learn to translate human demos or goals to the robot functional similarity between videos from drastically different perspective directly through pixel based translation with paired visual domains. This approach, which we call a Domain- [27, 44] or unpaired [47] data. Other works attempt to infer agnostic Video Discriminator (DVD), is simple and therefore actions, rewards, or state-values of human videos and use them can be readily scaled to large and diverse datasets, including for learning predictive models [40] or RL [14, 39]. Learning heterogeneous data sets with bothpeopleandrobots and without keypoint [51, 8] or object/task centric representations from any dependence on a close one-to-one mapping between the videos [42, 38, 34] is another promising strategy to learning robot and human data. Once trained, DVD conditions on a rewards and representations between domains. Simulation has human video as a demonstration, and the robot\u2019s behavior also been leveragedassupervisiontolearnsuchrepresentations as the other video, and outputs a score which is an effective [32] or to produce human data with domain randomization [3]. measure of task success or reward. Finally,meta-learning[54]andsub task discovery[41,20]have The core contribution of this work is a simple technique also been exploredastechniques for acquiringrobotrewardsor for learning multi-task reward functions from a mix of robot demos from human videos. In contrast to the majority of these and in-the-wild human videos, which measures the functional works, which usually study a small set of human videos in a similarity between the robot\u2019s behavior and that of a human similar domain as the robot, we explicitly focus on leveraging demonstrator. We find that this method is able to handle the \u201cin-the-wild\u201d human videos, specifically large and diverse sets diversity of human videos found in the Something-Something- of crowd-sourced videos from the real world from an existing V 2 [21] dataset, and can be used in conjunction with visual dataset, which contains many different individuals, viewpoints, model predictive control (VMPC) to solve tasks. Most notably, backgrounds, objects, and tasks. we find that by training on diverse human videos (even from Our approach is certainly not the first to study using such unrelated tasks), our learned reward function is able to more in-the-wildhumanvideos.Works that haveusedobjecttrackers effectively generalize to unseen environments and unseen tasks [52], simulation [32], and sub-task discovery [20] have also than when only using robot data, yielding a 15-20% absolute been applied on in-the-wild video datasets like You Cook [9], improvement in downstream task success. Lastly, we evaluate Something-Something [21], and Activity Net [15]. Learning ourmethodonareal Widow X 200 robot,andfind that itenables from such videos has also shown promise for navigation generalizationtoanunseen task inanunseenenvironmentgiven problems [6]. Most related to this work is Concept 2 Robot only a single human demonstration video. [43], which learns robotic reward functions using videos from the Something-Something dataset [21] by using a pretrained II. RELATEDWORK video",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 337,
      "paper_id": "inthewild",
      "text": "itenables from such videos has also shown promise for navigation generalizationtoanunseen task inanunseenenvironmentgiven problems [6]. Most related to this work is Concept 2 Robot only a single human demonstration video. [43], which learns robotic reward functions using videos from the Something-Something dataset [21] by using a pretrained II. RELATEDWORK video classifier. Unlike Concept 2 Robot, our method learns a A. Reward Learning reward function that is conditioned on a human video demo, The problem of learning reward functions from demonstra- andthus can beusedtogeneralizeto new tasks.Fur the rmore,in tions of tasks, also known as inverse rein for cement learning Section IV-D, we empirically find that our proposed approach or inverse optimal control [1], has a rich literature of prior provides are ward that generalizestounseenenvironments with work [35, 58, 50, 17, 18]. A number of recent works have much greater success than the Concept 2 Robot classifier. generalized this setting beyond full demonstrations to the case C. Robotic Learning from Large Datasets wherehumansprovideonlydesiredoutcomesorgoals[19,45]. Fur the rmore, both techniques have been shown to be effective Much like our work, a number of prior works have studied for learning manipulation tasks on real robots in challenging how learning from broad datasets can enhance generalization high dimensional settings [17, 45, 57]. Unlike these works, in robot learning [16, 33, 56, 13, 22, 24, 10, 5]. These works DVD DVD DVD Label = 1 Label = 0 Label = 1 \u201cClosing Something\u201d = \u201cPushing Something Away\u201d \u2260 \u201cPushing Something Away\u201d = \u201cClosing Something\u201d \u201cClosing Something\u201d \u201cPushing Something Away\u201d Figure 2: Training DVD. DVD is trained to predict if two videos are completing the same task or not. By leveraging task labels from in-the-wild human video datasets and a small number of robot demos, DVD is trained comp are a video of a human to that of a robot (left, middle) and to comp are pairs of human videos which may have significant visual differences, but may still be doing the same task (right). By training on these visually diverse examples, DVD is forced to learn the functional similarity between the videos. havelargelystudied the problemofcollectinglarge and diverse for the task of \u201cmove two objects apart\u201d, the reward depends robotic datasets in scalable ways [28, 22, 10, 53, 7] as well not only on the current state, but on how close together the as techniques for learning general purpose policies from this objects were initially.Weinsteadassume that the rewardattime style of data in an offline [13, 5] or online [33, 29, 24] fashion. t is only dependent on the last H <T timesteps, specifically While our motivation of achieving generalization by learning states s . Our goal then is to learn a parametric model t\u2212H:t from diverse data heavily overlaps with the above works, our which estimates the underlying reward function for each task, approach fundamentally differs in that it aims to sidestep the conditioned on a task-specifying video. That is, given (1) a challenges associated with collecting diverse robotic data by sequence of H states s and (2) a video demonstration 1:H instead leveraging existing human data sources. d =s\u2217",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 338,
      "paper_id": "inthewild",
      "text": "for each task, approach fundamentally differs in that it aims to sidestep the conditioned on a task-specifying video. That is, given (1) a challenges associated with collecting diverse robotic data by sequence of H states s and (2) a video demonstration 1:H instead leveraging existing human data sources. d =s\u2217 of variable length for each task T , we aim to learn i 1:tdi i arewardfunction R (s ,d )thatapproximates R (s )for \u03b8 1:H i i 1:H III. LEARNINGGENERALIZABLEREWARDFUNCTIONS each i. Such a non-Markovian reward can then be optimized WITHDOMAIN-AGNOSTICVIDEODISCRIMINATORS using a number of strategies, ranging from open-loop planners In this section, we describe our problem setting and intro- to policies with memory or frame stacking. duce Domain-agnostic Video Discriminators (DVD), a simple For training the reward function R , we assume access to a \u03b8 approach for learningrewardfunctions that leveragein-the-wild dataset Dh = {Dh }N of videos of humans doing N < K Ti i=1 human videos to generalize to unseen environments and tasks. tasks{T }N .There are novisualconstraintson the viewpoints, i i=1 backgrounds or quality of this dataset, and the dataset does not A. Problem Statement needtobebalancedby task.Wearealsogivenalimited data set In our problem setting, we consider a robot that aims to Dr = {D T r i }M i=1 of videos of robot doing M tasks {T i }M i=1 complete K tasks{T i }K i=1 ,eachofwhichhassomeunderlying where {T i }M i=1 \u2282 {T i }N i=1 , and so M \u2264 N. Both datasets task reward function R . As a result, for any given task i, are partitioned by task. Since human data is widely available, i our robotic agent operates in a fixed horizon Markov decision we have many more human video demonstrations than robot process (MDP) Mr, consisting of the tuple (S,Ar,pr,R ,T) video demonstrations per task and often many more tasks i i where S isthestatespace(inourcase RGBimages),Ar isthe that have human videos but not robot videos, in which case robot\u2019sactionspace,pr(s |s ,ar)istherobotenvironment\u2019s M << N. Importantly, the reward is inferred only through t+1 t t stochastic dynamics, R indicates the reward for task T , and visual observations and does not assume any access to actions i i T is the episode horizon. Additionally, for each task T , we or low dimensional states from either the human or robot data, i consider a human operating in an MDP Mh, consisting of the and we do not make any assumptions on the visual similarity i tuple(S,Ah,ph,R ,T)where Ah isthehuman\u2019sactionspace between the human and robot data. As a result, there can be a i and ph(s |s ,ah) is the human environment\u2019s stochastic large domain shift between the two datasets. t+1 t t dynamics. Note that the human and robot MDPs for task i During evaluation, the robot is tasked with inferring the share a state space S, reward function R , and horizon T, but reward R based on a new demo d specifying a task T . i \u03b8 i i may have different action spaces and transition dynamics. The goal is for",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 339,
      "paper_id": "inthewild",
      "text": "evaluation, the robot is tasked with inferring the share a state space S, reward function R , and horizon T, but reward R based on a new demo d specifying a task T . i \u03b8 i i may have different action spaces and transition dynamics. The goal is for this reward to be effective for solving a task Weassume that the taskrewardfunctions R areunobserved, T . Fur the rmore, we aim to learn R in a way such that it i i \u03b8 andneedtobeinferredthroughavideoof the task.Note that for can generalize to unseen tasks T (cid:54)\u2208 {T }N given a task new i i=1 many tasks these rewards will not be Markovian\u2013for example demonstration d . new Video 1: Moving sthup Video 2: Moving sthup 20-40 x 120 x 120 20-40 x 120 x 120 (D x W x H) (D x W x H) 3 D Conv 1 32 Pretrained 3 D Conv 2 Video Encoder 64 3 D Conv 3 3 x 128 3 x 3 D Conv 4 256 3 x 3 D Conv 4 256 Average features Concatenate embeddings 6 CF 215 7 CF 652 8 CF 821 9 CF 46 01 CF 23 11 CF 2 xamtfo S ytiralimi S eroc S Algorithm 1 DOMAIN-AGNOSTICVIDEODISCRIMINATOR(DVD) 1: //Training DVD 2: Require:Dh hum and emonstration data for N tasks{Tn} 3: Require:Dr robotdemonstration data for M tasks{Tm}\u2286{Tn} 4: Require:Pre-trainedvideoencoderfenc 5: Randomlyinitialize\u03b8 6: whiletrainingdo 7: Sampleanchorvideodi\u2208Dh\u222aDr 8: Samplepositivevideod(cid:48) i \u2208{D T h i }\u222a{D T r i }\\di 9: Samplenegativevideodj \u2208{D T h j }\u222a{D T r j }\u2200j(cid:54)=i 3 D C 32 onv 1 1 1 1 0 : : //P U la p n d n a i t n e g R C \u03b8 on w di i t t i h on d e i d ,d o (cid:48) i n ,d V j id a e c o co D rd e i m ng o to Eq.1 3 D Conv 2 12: Require:Trainedrewardfunction R \u03b8 &videopredictionmodelp \u03c6 64 13: Require:Humanvideodemodi fortask Ti 3 x 3 D Conv 3 14: fortrials 1,...,ndo 3 x 3 D 1 C 2 o 8 nv 4 1 1 6 5 : : S S t a e m p p a le \u2217 1: { H a 1 1 w : : G H hi } ch & m g a e x t im pr i e z d es ic R tio \u03b8 n ( s s\u02dc { g 1: s\u02dc H g 1: , H d } i) \u223c{p \u03c6 (s 0,ag 1:H )} 256 3 x 3 D Conv 4 256 the robot videos and associate it with actions in human videos. Average features C. DVD Implementation We implement our reward function R as \u03b8 R (d ,d )=f (f (d ),f (d );\u03b8) (2) \u03b8 i j sim enc i enc j where h = f is a pretrained video encoder enc and f (h ,h ;\u03b8) is a fully connected neural network sim i j parametrized by \u03b8 trained to predict if video",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 340,
      "paper_id": "inthewild",
      "text": "R (d ,d )=f (f (d ),f (d );\u03b8) (2) \u03b8 i j sim enc i enc j where h = f is a pretrained video encoder enc and f (h ,h ;\u03b8) is a fully connected neural network sim i j parametrized by \u03b8 trained to predict if video encodings h i and h are completing the same task. Specifically, we encode j Figure 3: DVD Architecture. We use the same video encoder each video using a neural network video encoder f into architecture as [43]. For each 3 D convolution layer, the number of enc a latent space, and then train f as a binary classifier filters is denoted, and all kernels are 3\u00d73\u00d73 except for the first, sim which is 3\u00d75\u00d75. All conv layers have stride 1 in the temporal trained according to Equation 1. See Figure 3 for the detailed dimension, and conv layers 1, 3, 6, 9 and 11 have stride 2 in the architecture.f ispretrainedon the entire Sth Sth V 2 dataset enc spatial dimensions, the others having stride 1. All conv layers are and fixed during training (as in [43]), while f is randomly sim followed by a Batch Norm 3 D layer and all layers except the last FC initialized. While the training dataset contains many more are followed by a Re LU activation. human videos than robotvideos, we sample the batches sothat B. Domain-Agnostic Video Discriminators they are roughly balanced between robot and human videos; specifically, each of (d ,d(cid:48),d ) are selected to be a robot i i j How exactly do we go about learning R \u03b8 ? Our key idea demonstration with 0.5 probability. is to learn R that captures functional similarity by training \u03b8 a classifier which takes as input two videos d from T and D. Using DVD for Task Execution i i d j from T j and predicts if i=j. Both videos can come from Once we\u2019ve trained the reward function R \u03b8 , how do we use either Dh or Dr, and labels can be acquired since we know ittoselectactions that willsuccess full ycompletea task?While which demos d i correspond to which tasks T i (See Figure 2). in principle, this reward function can be combined with either To train R \u03b8 , we sample batches of videos (d i ,d(cid:48) i ,d j ) from model-free or model based rein for cement learning approaches, Dh\u222aDr, where d i and d(cid:48) i are both labelled as completing the we choose to use visual model predictive control (VMPC) same task T i , and d j is completing a different task T j . The [49, 16, 13, 23], which uses a learned visual dynamics model output of R \u03b8 represents a \u201csimilarity score\u201d that indicates how to plan a sequence of actions. We condition R \u03b8 on a human similar task-wise the two input videos are. More formally, R \u03b8 demonstration video d i of the desired task T i and then use the is trained to minimize the following objective, which is the predicted",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 341,
      "paper_id": "inthewild",
      "text": "how to plan a sequence of actions. We condition R \u03b8 on a human similar task-wise the two input videos are. More formally, R \u03b8 demonstration video d i of the desired task T i and then use the is trained to minimize the following objective, which is the predicted similarity as a reward for optimizing actions with a average cross-entropy loss over video pairs in the distribution learned visual dynamics model (See Figure 4). of the training data: Concretely, we first train an action-conditioned video pre- diction model p (s |s ,a ) using the SV 2 P model J(\u03b8)=E [log(R (d ,d(cid:48)))+log(1\u2212R (d ,d ))]. (1) \u03c6 t+1:t+H t t:t+H Dh\u222aDr \u03b8 i i \u03b8 i j [2]. We then uses the cross-entropy method (CEM) [37] with Since in-the-wild human videos are so diverse and visually this dynamics model p to choose actions that maximize \u03c6 different from the robot environment, a large challenge lies similarity with the given demonstration. More specifically, for in bridging the domain gap between the range of human each iteration of CEM, for an input image s , we sample G t video environments and the robot environment. In optimizing action trajectories of length H and roll out G corresponding Equation 1, R must learn to identify functional behavior in predictedtrajectories{s }g usingp .Wethenfeedeach \u03b8 t+1:t+H \u03c6 Human Demo: Close the drawer Final Task Execution DVD \u00e00.5 DVD \u00e00.1 DVD \u00e00.9 Sampled Future Trajectories Visual Dynamics Model Sampled Actions Current State Figure 4: Planning with DVD. To use DVD to select actions, we perform visual model predictive control (VMPC) with a learned visual dynamics model. Specifically, we sample many action sequences from an action distribution and feed each through our visual dynamics model to get many \u201cimagined\u201d future trajectories. For each trajectory, we feed the predicted visual sequence into DVD along with the human provideddemonstrationvideo,whichspecifies the task.DVDscoreseachtrajectorybyitsfunctionalsimilarityto the hum and emovideo,and steps the highest scored action sequence in the environment to complete the task. predicted trajectory and demonstration d i into R \u03b8 , resulting SS S iimm im Suu uillmaa la ttui t ioo iloanntn i o EE Ennn n vv Ev ssnsvs WWWW iididdo iood wwwo XXXw 222 X 0002 0000 EEE 0 nnn v Evvssnsvs in G similarity scores corresponding to the task-similarity between d and each predicted image trajectory. The action trajectoryc i orrespondingtotheimagesequence with the highest T T Tr r ar a ai i Tn n inr a E E Ein nnnv v v Env predicted probability is then executed to complete the task. Train Env Train Env Rearranged TTr Traarianini n EE Ennnvvv TTr Traariainnin EE Ennnvvv RR Reeeaaarrrrraraannngggeeeddd The full algorithm with all stages is described in Algorithm 1. Te Tsets Et n Evnv IV. EXPERIMENTS Tes Tt e Esntv Env In our experiments, we aim to study how effectively our Test Env 1 Test Env 2 Test Env 3 method DVD can leverage diverse human data, and to what Te TTs eet s st Et E n En vn v 1 v 1 1 TTTeeesssttt E EEnnnvvv",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 342,
      "paper_id": "inthewild",
      "text": "e Esntv Env In our experiments, we aim to study how effectively our Test Env 1 Test Env 2 Test Env 3 method DVD can leverage diverse human data, and to what Te TTs eet s st Et E n En vn v 1 v 1 1 TTTeeesssttt E EEnnnvvv 222 TTTeeessstt t EE Ennnvvv 33 3 extent doing so enables generalization to unseen environments Figure 5: Environment Domains. We consider various simulated tabletop environments that have a drawer, a faucet, a coffee cup, and and tasks. Concretely, we study the following questions: a coffee machine, as well as a real robot environment with a tissue 1) By leveraging human videos is DVD able to more box, stuffed animal, and either a file cabinet or a toy kitchen set. In effectively generalize to new environments? the simulation experiments, half of the robot demonstrations that are 2) By leveraging human videos is DVD able to more used for training come from the train env and the other half from the rearranged train env. effectively generalize to new tasks? 3) Does DVD enable robots to generalize from a single environment generalization, each of which is progressively hum and emonstrationmoreeffectivelythanpriorwork? more difficult, shown in Figure 5. These include an original 4) Can DVD infer rewards from a human video on a real variant (Train Env), from which we have task demos, as well robot? as a variant with changed colors (Test Env 1), changed colors In the following sections, we first describe our experimental and viewpoint (Test Env 2), and changed colors, viewpoint, setup and then investigate the above questions. For videos and object arrangement (Test Env 3). please see https://sites.google.com/view/dvd-human-videos. b) Tasks: Weevaluate our methodonthreetarget task sin simulation, specifically, (1) closing an open drawer, (2) turning A. Simulated Experimental Set-Up thefaucetright,and(3)pushingthecupaway from the camera a) Environments: For our first 3 experimental questions, to the coffee machine. Each task is specified by an unseen we utilize a Mu Jo Co [48] simulated tabletop environment in-the-wild human video completing the task (See Figure 6). adapted from Meta-World [55] that consists of a Sawyer robot c) Training Data: For human demonstration data, we arminteracting with adrawer,afaucet,andacoffeecup/coffee use the Something-Something-V 2 dataset [21], which contains machine. We use 4 variants of this environment to study 220,837 total videos and 174 total classes, each with humans Move faucet to the right in Test Env 3 High-ranked Human video Trajectories: Score = .88 Low-ranked Trajectory: Score < 0.01 Figure 6: Example Rankings During Planning. Examplesofpredicted trajectories that are rankedhigh and lowfor the taskofmoving the faucet to the right in the test env 3 with the similarity scores that were outputted by DVD. DVD associates high functional similarity with trajectories that complete the same task as specified in the human video and low scores to trajectories that do not, despite the large visual domain shift between the given videos and the simulation environments. per for ming a different basic action with a wide variety of different objects in various environments. Depending on the experiment, we choose videos from up to 15",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 343,
      "paper_id": "inthewild",
      "text": "and low scores to trajectories that do not, despite the large visual domain shift between the given videos and the simulation environments. per for ming a different basic action with a wide variety of different objects in various environments. Depending on the experiment, we choose videos from up to 15 different human tasks for training DVD, where each task has from 853-3170 videos (See Appendix for details). For our simulated robot demonstration data, we assume 120 video demonstrations of 3 tasks in the training environment only (See Figure 5). We ablate the number of robot demos needed in Section IV-F. B. Experiment 1: Environment Generalization In our first experiment, we aim to study how varying the amount of human data used for training impacts the reward function\u2019s ability to generalize across environments. To do so, we train DVD on robot videos of the 3 target tasks from the training environment, as well as varying amounts of human data,andmeasure task per for manceacrossunseenenvironments. One of our core hypo the ses is that the use of diverse human data can improve the reward function\u2019s ability to generalize to new environments. To test this hypo the sis, we comp are Figure 7: Effectof Human Dataon Environment Generalization. training DVD on only the robot videos (Robot Only), to We comp are DVD\u2019s per for mance on seen and unseen environments when trained on only robot videos compared to varying number training DVD on a mix of the robot videos and human videos of human videos. We see that training with human videos provides from K tasks (Robot + K Human Tasks). Note that the signifi can tlyimprovedper for manceoveronlytrainingonrobotvideos, first 3 human tasks included are for the same 3 target tasks and that DVD is generally robust to the number of different human in the robot videos, and thus K > 3 implies using human video tasks used. Each bar shows the average success rate over all videos for completely unrelated tasks to the target tasks. To 3 target tasks, computed over 3 seeds of 100 trials, with error bars denoting standard error. evaluate the learned reward functions, we report the success rate from running visual MPC with respect to the inferred generally robust to the number of human tasks included, even reward, where we train the visual dynamics model on data ifthesetasks are unrelated tothetargettasks.Evenwhenusing that is autonomously collected in the test environment. (See 9 completely unrelated tasks, per for mance greatly exceeds not Appendix A for details). All methods infer the reward from using any humanvideos. Qualitatively, in Figure 6, we observe a single human video. However, to provide an even stronger that DVD gives high similarity scores to trajectories that are comparison,wealsoevaluate the Robot Only DVDmodel with completing the task specified by the human video demo and a robot demo at test time Robot Only (Robot Demo), since low scores to trajectories that have less relevant behavior. this model has only been trained with robot data. C. Experiment 2: Task Generalization In Figure 7, we report the success rate using each reward function,",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 344,
      "paper_id": "inthewild",
      "text": "human video demo and a robot demo at test time Robot Only (Robot Demo), since low scores to trajectories that have less relevant behavior. this model has only been trained with robot data. C. Experiment 2: Task Generalization In Figure 7, we report the success rate using each reward function, computed over 3 randomized sets of 100 trials. In our second experiment, we study how including human Our first key observation is that training with human videos data for training affects the reward function\u2019s ability to signifi can tly improves environment generalization per for mance generalize to new tasks. In this case, we do not train on any over using only robot videos (20% on average), even when (human or robot) data from the target tasks, and instead train the robot only comparison gets the privileged information of DVD on robot videos of the 3 different tasks from the training a robot demonstration. Additionally, we observe that DVD is environment, namely (1) opening the drawer, (2) moving something from right to left, (3) not moving any objects, as well as varying amounts of human data. To again test how human videos affect generalization, we comp are the same methods as in the previous experiment. Since we are testing taskgeneralization,allevaluationisin the trainingenvironment. In the bottom section of Table I, we report the success rate using DVD with varying amounts of human data, computed over 3 randomizedsetsof 100 trials.Similarto the conclusions of the environment generalization experiment, first we find that training with human videos signifi can tly improves task generalization per for mance over using only robot videos (by roughly 10%onaverage),even with the robotonlycomparison conditioned on a robot demonstration. Given a human video demonstration, Robot Only does well at closing the drawer, but is completely unable to move the faucet to the right, suggesting that it is by default moving to the same area of the environment and is unable to actually distinguish tasks. This is unsurprising considering the reward function is not Figure 8: Environment Generalization Prior Work Comparison. trained on any human videos. Second, we observe that on Compared to Concept 2 Robot, the most relevant work leveraging \u201cin- average, including human videos for 6 unrelated human tasks the-wild\u201d human videos, as well as a demo-conditioned behavioral cloningpolicyandar and ompolicy,DVDper for mssignifi can tlybetter can signifi can tly improve per for mance, leading to more than across all environments, and over 20% better on average. Each bar a 20% gap over just training with robot videos, suggesting shows the average success rate over all 3 target tasks, computed over that training with human videos from more unrelated tasks is 3 seeds of 100 trials, with error bars denoting standard error. particularly helpful for task generalization. V 2 dataset,thereisnonaturalmethod for testinggeneralization D. Experiment 3: Prior Work Comparison to an unseen task specified by a human video. We see that DVD outperforms both other baselines by over 30%. Inthisexperiment,westudyhoweffective DVDiscompared First, DVD\u2019s significant improvement over Concept 2 Robot too the rtechniques for learningfromin-the-wildhumanvideos. The most related work is Concept 2 Robot [43], which uses",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 345,
      "paper_id": "inthewild",
      "text": "3: Prior Work Comparison to an unseen task specified by a human video. We see that DVD outperforms both other baselines by over 30%. Inthisexperiment,westudyhoweffective DVDiscompared First, DVD\u2019s significant improvement over Concept 2 Robot too the rtechniques for learningfromin-the-wildhumanvideos. The most related work is Concept 2 Robot [43], which uses a suggests that in learning reward functions which address the human-domain robot gap, using some robot data, even in small pretrained 174-way video classifier on only the Sth Sth V 2 quantities,isimportantforgoodper for mance.Second,methods dataset (no robot videos) as a reward. Since this method is like demo-conditioned behavior cloning likely require many not naturally conducive to one-shot imitation from a video morerobotdemonstrationstolearngoodpolicies,aspriorwork demonstration, during planning we follow the method used in in demo-conditioned behavior cloning often use on the order theoriginalpaper and taketheclassificationscore for the target of thousands of demonstrations [46]. DVD on the other hand, task from the predicted robot video as the reward (instead of uses the demos only to learn a reward function and offloads conditioningonahumanvideo).Unlike the open-looptrajectory the behavior learning to visual MPC. Lastly, when examining generator used in the original paper, we use the same visual the per for mance of demo-conditioned behavioral cloning on MPCapproachforselectingactions for afaircomparisonof the each individual task, we see the policy learns to ignore the learned reward function; we expect the relative per for mance of conditioning demo and mimics one trajectory for one of the the reward functions to be agnostic to this choice. In addition, we also comp are to a demo-conditioned behavioral cloning targettasks,doingwell for only that taskbutcompletelyfailing at other tasks, suggesting that the policy struggles to infer the method,similartowhathas been usedinpriorwork[54,3,46]. task from the visually diverse human videos. Wetrain this approachusingbehaviorcloningon the 120 robot demonstrations and their actions for 3 tasks conditioned on a E. Experiment 4: Real Robot Efficacy video demo of the task from either a robot or a human. See Appendix A for more details on this comparison. We also To answer our last experimental question, we study how include a comparison to a random policy. DVD with human data enables better environment and task In Figure 8 we comp are DVD with 6 human videos to these generalization on a real Widow X 200 robot. We consider a prior methods on the environment generalization experiment similar setup as described in Sections IV-B and IV-C, where presented in Section IV-B. Across all environments, DVD DVDisnowtrainedon 80 robotdemos from eachof 2 training performs signifi can tly better than all three comparisons on the tasksinatrainingenvironment and humanvideos.Thenduring targettasks,and 20%betteronaveragethan the best-per for ming testing, DVD is used as reward for visual MPC in an unseen othermethod.In Table I,wemake the samecomparison,nowon environment, per for ming both a seen and unseen task. theexperimentof task generalizationpresentedin Section IV-C. Specifically,inourrealrobotsetup,thetrainingenvironment Since Concept 2 Robot is not demo-conditioned and is already consists of a file cabinet, and in the testing environment, it is trained on all 174 possible human video tasks in the Sth Sth replaced with a toy kitchen set (See Figure 5). The training Method Close drawer Move faucet to",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 346,
      "paper_id": "inthewild",
      "text": "Concept 2 Robot is not demo-conditioned and is already consists of a file cabinet, and in the testing environment, it is trained on all 174 possible human video tasks in the Sth Sth replaced with a toy kitchen set (See Figure 5). The training Method Close drawer Move faucet to right Push cup away from the camera Average Random 20.00 (3.00) 9.00 (1.73) 32.33 (8.08) 20.44 (2.78) Behavioral Cloning Policy 0.00 (0.00) 45.33 (38.84) 1.00 (0.00) 15.44 (12.95) Concept 2 Robot 174-way classifier n/a n/a n/a n/a DVD, Robot Only (Human Demo) 67.33 (4.51) 1.00 (1.00) 29.67 (0.58) 32.67 (1.53) DVD, Robot Only (Robot Demo) 29.33 (14.99) 23.67 (1.53) 28.33 (0.58) 27.11 (5.23) DVD, Robot + 3 Human Tasks 66.33 (6.03) 19.33 (0.58) 40.00 (6.93) 41.89 (3.10) DVD, Robot + 6 Human Tasks 59.00 (5.29) 17.00 (7.94) 56.33 (11.06) 44.11 (1.39) DVD, Robot + 9 Human Tasks 57.67 (0.58) 52.67 (1.15) 55.00 (5.57) 55.11 (2.04) DVD, Robot + 12 Human Tasks 31.67 (9.02) 49.00 (6.24) 57.33 (2.08) 46.00 (2.60) Table I: Task generalization results in the original environment. DVD trained with human videos performs signifi can tly better on average than with only robot videos, a baseline behavioral cloning policy, and random. We report the average success rate for all 3 target tasks, computed over 3 seeds of 100 trials, as well as the standard deviation in paren the ses. Test Env + Method (Out of 20 Trials) Test Env Unseen Task 80 Random 5 5 Concept 2 Robot 174-way classifier 4 n/a 70 DVD, Robot Only (Human Demo) 5 6 60 DVD, Robot Only (Robot Demo) 5 8 DVD, Robot + 2 Human Tasks 7 7 50 DVD, Robot + 6 Human Tasks 13 14 DVD, Robot + 9 Human Tasks 9 11 40 DVD, Robot + 12 Human Tasks 10 9 30 Table II: Env and task generalization results on a real robot. We report successes out of 20 trials on a Widow X 200 in an unseen 20 environment on two different tasks, one on closing a toy kitchen door and another on moving a tissue box to the left. On both, DVD 10 performs signifi can tly better when trained with human videos than 0 with only robot demonstrations. Train env Test env 1 Test env 2 Test env 3 Average Environment tasks are \u201cClosing Something\u201d and \u201cPushing something left to right\u201d and the test tasks are \u201cClosing Something\u201d (seen) and \u201cPushing something right to left\u201d (unseen). We comp are DVD with varying amounts of human data to only robot data and baselines in Table II, where we report the success rate out of 20 trials when used with visual MPC conditioned on a human demo of the task. DVD trained with humanvideoshasabouttwice the successratewhenleveraging the diverse human dataset than when relying only on robot videos.Inparticular,DVDtrained with 6 tasksworthofhuman videos succeeds over 65-70% of the time whereas robot only succeeds at most 40%. We also observe that in general using human videos from unrelated tasks improves over only using humanvideos for the trainingtasks.Finally,weseequalitatively in Figure 14 in Appendix",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 347,
      "paper_id": "inthewild",
      "text": "dataset than when relying only on robot videos.Inparticular,DVDtrained with 6 tasksworthofhuman videos succeeds over 65-70% of the time whereas robot only succeeds at most 40%. We also observe that in general using human videos from unrelated tasks improves over only using humanvideos for the trainingtasks.Finally,weseequalitatively in Figure 14 in Appendix C that DVD captures the functional task being specified, in this case closing the door. F. Ablation on Amount of Robot Data for Training In our previous simulation experiments, we use 120 robot demonstrations per task. While this is a manageable number of robot demonstrations, it would be better to rely on fewer demonstrations. Hence, we ablate on the number of robot demonstrations used during training and evaluate environment generalization. In Figure 9, we see that the per for mance of DVD decreases by only a small margin when using as few as 20 robot demonstrations per task. This suggests that by leveraging the diversity in the human data, DVD can perform well even with very little robot data. )001 fo tuo( sesseccu S DVD With Varying Amounts of Robot Data Robot (20) + 6 Human Tasks Robot (40) + 6 Human Tasks Robot (120) + 6 Human Tasks Figure 9: Ablation on Amount of Robot Data Used for Training. While using 120 robot demonstrations per task slightly benefits per for manceoverusingonly 20 or 40,DVDstillper for mscomparably with fewer robot demos. V. LIMITATIONS AND FUTUREWORK Wepresentedanapproach,domain-agnosticvideodiscrimina- tor (DVD), that leverages the diversity of \u201cin-the-wild\u201d human videos to learn generalizable robotic reward functions. Our experiments find that training with a large, diverse dataset of human videos can signifi can tly improve the reward function\u2019s ability to generalize to unseen tasks and environments, and can be combined with visual MPC to solve tasks. There are multiple limitations and directions for future work. First, our method focuses only on learning reward functions that generalize and does not learn a generalizable policy or visual dynamics model directly. This is a necessary next step to achieve agents that broadly generalize and is an exciting direction for future work. Second, while limited in quantity, our work assumes access to some robot demonstrations and task labels for these demos and for all of the human videos. Techniques that can sidestep the need for this supervision would further enhance the scalability of DVD. Lastly, so far we have only tested DVD on coarse tasks that don\u2019t require fine-grained manipulation. Designing more powerful visual models and testing DVD with them on harder, more precise tasks is another exciting direction for future work. ACKNOWLEDGMENTS multi-robot learning. In Conference on Robot Learning, 2019. The authors would like to thank Ashvin Nair as well as [11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and members of the IRIS lab for valuable discussions. This work L. Fei-Fei. Image Net: A Large-Scale Hierarchical Image was supported in part by Schmidt Futures, by ONR grant Data base. In CVPR 09, 2009. N 00014-20-1-2675, and by an NSF GRFP. Chelsea Finn is a [12] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina CIFAR Fellow in the Learning in",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 348,
      "paper_id": "inthewild",
      "text": "discussions. This work L. Fei-Fei. Image Net: A Large-Scale Hierarchical Image was supported in part by Schmidt Futures, by ONR grant Data base. In CVPR 09, 2009. N 00014-20-1-2675, and by an NSF GRFP. Chelsea Finn is a [12] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina CIFAR Fellow in the Learning in Machines & Brains program. Toutanova. BERT: Pre-training of deep bidirectional REFERENCES trans for mers for language underst and ing. In Conference [1] Pieter Abbeel and Andrew Y. Ng. In Proceedings of of the North Ameri can Chapter of the Association for the Twenty-First International Conference on Machine Computational Linguistics: Human Language Technolo- Learning, ICML \u201904, page 1, 2004. gies (NAACL-HLT), Minneapolis, Minnesota, June 2019. [2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Association for Computational Linguistics. Roy H. Campbell, and Sergey Levine. Stochastic varia- [13] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie tional video prediction. In International Conference on Xie, Alex Lee, and Sergey Levine. Visual foresight: Learning Representations, 2018. Model-baseddeepreinforcementlearning for vision-based [3] Alessandro Bonardi, Stephen James, and Andrew J robotic control. ar Xiv:1812.00568, 2018. Davison. Learning one-shot imitation from humans [14] Ashley DEdwards and Charles LIsbell. Perceptualvalues without humans. IEEE Robotics and Automation Letters, fromobservation. ar Xivpreprintar Xiv:1905.07861,2019. 2020. [15] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia [4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie and Juan Carlos Niebles. Activitynet: A large-scale Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- video benchmark for human activity underst and ing. In lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Proceedings of the IEEE Conference on Computer Vision Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, and Pattern Recognition, pages 961\u2013970, 2015. Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. [16] Chelsea Finn and Sergey Levine. Deepvisual for esight for Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, planning robot motion. In IEEE International Conference Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, on Robotics and Automation (ICRA), 2017. Benjamin Chess, Jack Clark, Christopher Berner, Sam [17] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Mc Candlish, Alec Radford, Ilya Sutskever, and Dario cost learning: Deep inverse optimal control via policy Amodei. Language models are few-shot learners. optimization. In International conference on machine ar Xiv:2005.14165, 2020. learning, pages 49\u201358. PMLR, 2016. [5] Serkan Cabi, Sergio G\u00f3mez Colmenarejo, Alexander [18] Justin Fu, Katie Luo, and Sergey Levine. Learning robust Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, rewards with adverserial inverse rein for cement learning. Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, In International Conference on Learning Representations, et al. Scaling data-driven robotics with reward sketching 2018. and batch rein for cement learning. ar Xiv:1909.12200, [19] Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and 2019. Sergey Levine. Variational inverse control with events: [6] Matthew Chang, Arjun Gupta, and Saurabh Gupta. Se- A general framework for data-driven reward definition. mantic visual navigation by watching youtube videos. In In Advances in Neural Information Processing Systems, Neur IPS, 2020. 2018. [7] Annie S. Chen, Hyun Ji Nam, Suraj Nair, and Chelsea [20] W. Goo and S. Niekum. One-shot learning of multi-step Finn.Batchexplorationwi the xamples for scalablerobotic tasks from observationviaactivitylocalizationinauxiliary rein for",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 349,
      "paper_id": "inthewild",
      "text": "mantic visual navigation by watching youtube videos. In In Advances in Neural Information Processing Systems, Neur IPS, 2020. 2018. [7] Annie S. Chen, Hyun Ji Nam, Suraj Nair, and Chelsea [20] W. Goo and S. Niekum. One-shot learning of multi-step Finn.Batchexplorationwi the xamples for scalablerobotic tasks from observationviaactivitylocalizationinauxiliary rein for cement learning. IEEE Robotics and Automation video. In 2019 International Conference on Robotics and Letters, 2021. Automation (ICRA), 2019. [8] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayara- [21] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal- man, Akshara Rai, and Franziska Meier. Model-based ski, Joanna Materzynska, Susanne Westphal, Heuna Kim, inverserein for cementlearning from visualdemonstrations, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz 2021. Mueller-Freitag, et al. The\" something something\" video [9] Pradipto Das,Chenliang Xu,Richard FDoell,and Jason J data base for learning and evaluatingvisualcommonsense. Corso. A thous and frames in just a few words: Lingual In Proceedings of the IEEE International Conference on description of videos through latent topics and sparse Computer Vision, pages 5842\u20135850, 2017. object stitching. In Proceedings of the IEEE conference [22] Abhinav Gupta,Adithyavairavan Murali,Dhiraj Prakashc- on computer vision and pattern recognition, pages 2634\u2013 hand Gandhi, and Lerrel Pinto. Robot learning in homes: 2641, 2013. Improving generalization and reducing dataset bias. In [10] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Advances in Neural Information Processing Systems, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, 2018. Sergey Levine, and Chelsea Finn. Robonet: Large-scale [23] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. [35] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Learning latent dynamics for planning from pixels. In Zinkevich. Maximummarginplanning. In Proceedingsof International Conference on Machine Learning, pages the 23 rd International Conference on Machine Learning, 2555\u20132565. PMLR, 2019. ICML \u201906, page 729\u2013736, 2006. [24] Dmitry Kalashnikov,Alex Irpan,Peter Pastor,Julian Ibarz, [36] Jonas Rothfuss, Fabio Ferreira, Eren Erdal Aksoy, You Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Zhou, and Tamim Asfour. Deep episodic memory: Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Encoding, recalling, and predicting episodic experiences Scalable deep rein for cement learning for vision-based forrobotactionexecution. IEEERobotics and Automation robotic manipulation. In Conference on Robot Learning, Letters, 3(4):4007\u20134014, 2018. pages 651\u2013673. PMLR, 2018. [37] Reuven Y Rubinstein and Dirk P Kroese. The cross- [25] Jangwon Lee and Michael S Ryoo. Learning robot activ- entropy method: a unified approach to combinatorial op- ities from first-person human videos using convolutional timization,Monte-Carlosimulation and machinelearning. future regression. In Proceedings of the IEEE Conference Springer Science & Business Media, 2013. on Computer Vision and Pattern Recognition Workshops, [38] Rosario Scalise, Jesse Thomason, Yonatan Bisk, and pages 1\u20132, 2017. Siddhartha Srinivasa. Improving robot success detection [26] Kyuhwa Lee, Yanyu Su, Tae-Kyun Kim, and Yiannis using static object data. In Proceedings of the 2019 Demiris. A syntactic approach to robot imitation learning IEEE/RSJ International Conference on Intelligent Robots using probabilistic activity grammars. Robotics and and Systems, 2019. Autonomous Systems, 61(12):1323\u20131334, 2013. [39] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, [27] Yu Xuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Sergey Levine, and Chelsea Finn. Rein for cement learn- Levine. Imitation from observation:",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 350,
      "paper_id": "inthewild",
      "text": "learning IEEE/RSJ International Conference on Intelligent Robots using probabilistic activity grammars. Robotics and and Systems, 2019. Autonomous Systems, 61(12):1323\u20131334, 2013. [39] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, [27] Yu Xuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Sergey Levine, and Chelsea Finn. Rein for cement learn- Levine. Imitation from observation: Learning to imitate ing with videos: Combining offline observations with behaviors from raw video via context translation. In interaction. In Co RL, 2020. 2018 IEEE International Conference on Robotics and [40] Karl Schmeckpeper, Annie Xie, Oleh Rybkin, Stephen Automation (ICRA), pages 1118\u20131125. IEEE, 2018. Tian, Kostas Daniilidis, Sergey Levine, and Chelsea [28] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Finn. Learning predictive models from observation and Booher, Max Spero, Albert Tung, Julian Gao, John interaction. In ECCV, 2020. Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, [41] Pierre Sermanet, Kelvin Xu, and Sergey Levine. Un- and Li Fei-Fei. Roboturk: A crowds our cing platform for supervised perceptual rewards for imitation learning. robotic skill learning through imitation. In Conference Proceedings of Robotics: Science and Systems (RSS), on Robot Learning, 2018. 2017. [29] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar [42] Pierre Sermanet,Corey Lynch,Yevgen Chebotar,Jasmine Bahl,Steven Lin,and Sergey Levine.Visualrein for cement Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time- learning with imagined goals. In Advances in Neural contrastivenetworks:Self-supervisedlearning from video. Information Processing Systems, 2018. Proceedings of International Conference in Robotics and [30] Anh Nguyen, Dimitrios Kanoulas, Luca Muratore, Dar- Automation (ICRA), 2018. win G Caldwell, and Nikos G Tsagarakis. Translating [43] Lin Shao,Toki Migimatsu,Qiang Zhang,Karen Yang,and videos to commands for robotic manipulation with deep Jeannette Bohg. Concept 2 robot: Learning manipulation recurrent neural networks. In 2018 IEEE International concepts from instructions and human demonstrations. Conference on Robotics and Automation (ICRA), pages In Proceedings of Robotics: Science and Systems (RSS), 3782\u20133788. IEEE, 2018. 2020. [31] Open AI, Marcin Andrychowicz, Bowen Baker, Maciek [44] P. Sharma, Deepak Pathak, and Abhinav Gupta. Third- Chociej,Rafal Jozefowicz,Bob Mc Grew,Jakub Pachocki, personvisualimitationlearningviadecoupledhierarchical Arthur Petron, Matthias Plappert, Glenn Powell, Alex controller. In Neur IPS, 2019. Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter [45] Avi Singh, Larry Yang, Chelsea Finn, and Sergey Levine. Welinder, Lilian Weng, and Wojciech Zaremba. Learning End-to-endroboticrein for cementlearning with outreward dexterous in-hand manipulation, 2019. engineering. In Proceedings of Robotics: Science and [32] Vladim\u00edr Petr\u00edk, Makar and Tapaswi, Ivan Laptev, and Systems, Freiburgim Breisgau, Germany, June 2019. Josef Sivic. Learning object manipulation skills via [46] Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, approximate state estimation from real videos, 2020. Murtaza Dalal, Sergey Levinev, Mohi Khansari, and [33] Lerrel Pinto and Abhinav Gupta. Supersizing self- Chelsea Finn. Scalable multi-task imitation learning with supervision: Learning to grasp from 50 k tries and 700 autonomous improvement. In 2020 IEEE International robothours. In IEEEinternationalconferenceonrobotics Conference on Robotics and Automation (ICRA), pages and automation (ICRA), 2016. 2167\u20132173. IEEE, 2020. [34] S\u00f6ren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, [47] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter and Pierre Sermanet. Online object representations with Abbeel, and Sergey Levine. AVID: Learning Multi-Stage contrastive learning, 2019. Tasks via Pixel-Level Translation of Human Videos.",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 351,
      "paper_id": "inthewild",
      "text": "(ICRA), pages and automation (ICRA), 2016. 2167\u20132173. IEEE, 2020. [34] S\u00f6ren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, [47] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter and Pierre Sermanet. Online object representations with Abbeel, and Sergey Levine. AVID: Learning Multi-Stage contrastive learning, 2019. Tasks via Pixel-Level Translation of Human Videos. In Proceedings of Robotics: Science and Systems, Corvalis, Oregon, USA, July 2020. [48] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, 2012. [49] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: a locally linear latent dynamics model for control from raw images. In Proceedings of the 28 th International Conference on Neural Information Processing Systems- Volume 2, pages 2746\u20132754, 2015. [50] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse rein for cement learning, 2016. [51] Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj,Samarth Sinha,and Animesh Garg. Learning by watching: Physical imitation of manipulation skills from human videos, 2021. [52] Yezhou Yang, Yi Li, Cornelia Ferm\u00fcller, and Yiannis Aloimonos. Robot learning manipulation action plans by \"watching\" unconstrained videos from the world wide web. In AAAI, pages 3686\u20133693, 2015. [53] Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav Gupta, Pieter Abbeel, and Lerrel Pinto. Visual imitation made easy. In Co RL, 2020. [54] Tianhe Yu,Chelsea Finn,Sudeep Dasari,Annie Xie,Tian- hao Zhang, Pieter Abbeel, and Sergey Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018. [55] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta- world: A benchmark and evaluation for multi-task and meta rein for cement learning. In Conference on Robot Learning, 2020. [56] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Learn- ing synergies between pushing and grasping with self- supervised deep rein for cement learning. Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS), 2018. [57] Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients of real world robotic rein for cement learning. In International Conference on Learning Representations, 2020. [58] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse rein for cement learning. In Proc. AAAI, pages 1433\u20131438, 2008. APPENDIX A. Training Details a) dataset Details: Depending on the experiment, we choose videos from the following 15 different human tasks in the Something-Something-V 2 dataset for training DVD, where each task has from 853-3170 trainingvideos:1)Closingsth,2) Moving sth away from camera, 3) Moving sth towards camera, 4)Openingsth,5)Pushingsthlefttoright,6)Pushingsthright to left, 7) Poking sth so lightly it doesn\u2019t move, 8) Moving sth down, 9) Moving sth up, 10) Pulling sth from left to right, Figure 10: SV 2 P Architecture. We use video prediction models 11) Pulling sth from right to left, 12) Pushing sth with sth, trained via SV 2 P with the reward from DVD in order to complete 13) Moving sth closer to sth, 14) Plugging",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 352,
      "paper_id": "inthewild",
      "text": "sth from left to right, Figure 10: SV 2 P Architecture. We use video prediction models 11) Pulling sth from right to left, 12) Pushing sth with sth, trained via SV 2 P with the reward from DVD in order to complete 13) Moving sth closer to sth, 14) Plugging sth into sth, and tasksspecifiedbyagivenhumanvideo.Figuretaken from the original 15) Pushing sth so that it slightly moves. We used these tasks paper [2]. because they are appropriate for a single-arm setting and cover thetrainingclip.Additionally,duringtraining,eachinputvideo a diverse range of various actions. The first seven of those is first randomly rotated between -15 and 15 degrees, scaled so tasks were chosen as they are relevant to tasks possible in the that the heighthassize 120,and the nrandomlycroppedto have simulation environments, but the other tasks were not chosen size 120\u00d7120\u00d73. At planning time, the video demonstration for any particular reason, i.e. could have been replaced by a is spliced so that it is between 30 and 40 frames, rescaled to different set of 8 other appropriate tasks. For an experiment, have a height of 120 pixels, and then center-cropped to have if human videos for a task are used, we use all of the human size 120\u00d7120\u00d73. The demo-conditioned behavioral cloning videos available in the Sth Sth V 2 training set for that task to baseline uses the same hyperparameters and training details train DVD. except that it uses weight decay 0.0001. For the simulation experiments, DVD is also trained on c) SV 2 P Training: To evaluate DVD\u2019s per for mance on 120 robot video demonstrations of 3 tasks, half of which are potentially unseen tasks with a given human video, we collected in the original training environment and the other employ visual model predictive control with SV 2 P visual half in the rearranged training environment. These videos are prediction models trained on datasets autonomously collected collected via model predictive control with random shooting, a in each environment. SV 2 P learns an action-conditioned video groundtruthvideo-prediction model,andaground-truthshaped prediction model bysamplingalatentvariable and subsequently reward particular to each task. For the real robot experiments generating an image prediction with that sample. We use the on the Widow X 200, in addition to varying amounts of human same architecture, which is shown in Figure 10, and default videos, DVD is trained on 80 robot video demonstrations of 2 hyperparameters as the original paper [2]. tasks, which are collected in the original training environment. For each of the four simulation environments in which we Thesedemonstrations are collectedviaahard-codedscript with evaluate DVD, we collect 10,000 random episodes, each with uniform noise between -0.02 and 0.02 added to each action. 60 totalframes,oftheagentinteractingin that environment and To evaluate DVD\u2019s training progress, we use a validation set train SV 2 P for 200,000 epochs on all of the data. The models consisting of all of the human videos available in the Sth Sth are trained to predict the next fifteen frames given an input of V 2 validationset for the chosen task saswellas 48 robotvideo five frames. To evaluate DVD in the robot test",
      "start_pos": 8316,
      "end_pos": 8828
    },
    {
      "chunk_id": 353,
      "paper_id": "inthewild",
      "text": "on all of the data. The models consisting of all of the human videos available in the Sth Sth are trained to predict the next fifteen frames given an input of V 2 validationset for the chosen task saswellas 48 robotvideo five frames. To evaluate DVD in the robot test environment, demonstrations for the same 3 tasks with robot demos in the we train SV 2 P for 160,000 epochs on 58,500 frames worth of training set,withhalfofthesecoming from the originaltraining autonomously collected robot interaction in the original train environment and the other half from the rearranged. For the environment,andthenwefinetune the model for another 60,000 Widow X 200 experiments,weadd 8 robotvideodemonstrations epochson 21,000 framesworthofautonomouslycollected data for each of the 2 tasks into the validation set. inthetestenvironment that has the toykitchendoor.Collecting b) Hyperparameters: For DVD, the similarity discrimi- this data on the Widow X 200 took a total of roughly 75 hours nator is trained with a learning rate of 0.01 using stochastic but was entirely autonomous. gradient descent (SGD) with momentum 0.9 and weight decay d) Additional DVD Details: Here we expand on some of 0.00001. We use a batch size of 24, where each element of the DVDimplementationdetailstoucheduponin Section III-C, the batch consists of a triplet with two videos having the same particularly the way that batches are sampled during training. task label and the third having a different label. Each version Each batch consists of triplets (d ,d(cid:48),d ), where d is labeled i i j j of DVD in the experiments is trained for 120 epochs, where as a different task as d and d(cid:48), which are labeled as the same i i one epoch consists of 200 optimizer steps. For each epoch, task.Ineachtriplet,d isr and omlysampled with 0.5 probability i the video clips fed into DVD for training are sequences of of being a robot demonstration. Then, if d is from a task with i consecutive frames with random length between 20 and 40 only human data, d(cid:48) will be chosen from the remaining human i frames taken from the original video. If the original video has data for that task; otherwise it is chosen to be a robot video fewer than the randomly selected amount of frames, the last from that task with 0.5 probability. Finally, d is randomly j frame is repeated to achieve the desired number of frames for sampled repeatedly (usually just once) with 0.5 probability of being a robot demonstration until a video with a different task tasks: 1) Closing the drawer, which is defined as the last frame label from d and d(cid:48) is sampled. in the 60-frame trajectory having the drawer pushed in to be i i e) Comparisons: For the Concept 2 Robot comparison, less than 0.05, where it starts open at 0.07, 2) Turning the we use the same 174-way classifier that the paper used and faucet to the right more than 0.01 distance, where it starts at 0, do not alter it. For the demo-conditioned behavioral cloning and 3) Moves cup to be less than 0.07 distance to the",
      "start_pos": 8778,
      "end_pos": 9290
    },
    {
      "chunk_id": 354,
      "paper_id": "inthewild",
      "text": "at 0.07, 2) Turning the we use the same 174-way classifier that the paper used and faucet to the right more than 0.01 distance, where it starts at 0, do not alter it. For the demo-conditioned behavioral cloning and 3) Moves cup to be less than 0.07 distance to the coffee comparison,we useamethodsimilarto[3],[46],and[54].We machine, where the cup starts out at least 0.1 away. We run train a model that takes in as input the concatenated encodings 100 trials for 3 different seeds for each task for every method of a conditioning video and the image state from one of the in all experiments. robot demonstrations in the training set and outputs an action c) Real Robot Experiments: On the Widow X 200, for all that aims to lead the agent from the given image state to experiments,ineachtrialweplan 1 trajectoryoflength 10.For completing the same task as shown in the conditioning demo. this trajectory, we run 2 iterations of the cross-entropy method Duringtraining,the model istrainedonbatchesof(conditioning (CEM), sampling 100 action sequences and refitting to the video,robotdemonstration)pairs,where the conditioningvideo top 20 repeatedly. We then choose one of the top 5 predicted is randomly taken from the combined human and robot dataset trajectories with the highest functional similarity score given andarobotdemonstration with the same task labelisr and omly by DVD to execute in the environment. We evaluate on the chosen.Because the rearemanymorehumanvideosthanrobot following two target tasks: 1) Closing the toy kitchen door, demonstrations, the conditioning video is chosen to be a robot where a success is recorded for any trial where the robot arm demonstration with 50% probability, which is analogous to completely closes the door, and 2) Pushing the tissue box to the balancing of batches used in DVD. Note that this method the left, where the robot arm must clearly push the tissue box cannot naturally use human videos from tasks for which there left of its original starting position. We run 20 trials for each are no robot demonstrations. task for each method. The behavior cloning model uses the same pretrained C. Additional Experimental Results video encoder as DVD to encode the conditioning demo as well as a pretrained Res Net 18 for the image state. The In our simulation environment generalization experiments, resulting features are concatenated and passed into an MLP we evaluate on the three tasks of 1) Closing the drawer, 2) that takes an input of size [1512] and has fully connected Turning the faucet to the right, and 3) Pushing the cup away layers [512,256,128,64,32,a], where each layer except the from the camera. In Section IV, we reported the average last is followed by a Re LU activation and a corresponds to the per for mance across all the three tasks. In Figure 11, we numberofactiondimensions.The model istrainedtominimize present the individual task results for DVDtrained with varying mean squared error between the output action and the true amounts of human data. The conclusions of these experiments action. are the same as those in Section IV, in that leveraging diverse human videos in DVD allows for more effective generalization B. Experimental",
      "start_pos": 9240,
      "end_pos": 9752
    },
    {
      "chunk_id": 355,
      "paper_id": "inthewild",
      "text": "individual task results for DVDtrained with varying mean squared error between the output action and the true amounts of human data. The conclusions of these experiments action. are the same as those in Section IV, in that leveraging diverse human videos in DVD allows for more effective generalization B. Experimental Details across new environments rather than relying only on robot a) Domains: For the simulation domains, we use a videos. Mujoco simulation built off the Meta-World environments In Figure 12, we present results on the individual tasks [55]. In simulation, the state space is the space of RGB image across all four environments for DVD trained with 6 tasks observations with size [180, 120, 3]. We use a continuous worth of human videos compared with our three comparisons: action space over the linear and angular velocity of the robot\u2019s Concept 2 Robot [43], a demo-conditioned behavior cloning gripper and a discrete action space over the gripper open/close policy, and a random policy. On average over all of the action, for a total of five dimensions. For the robot domain, we environments, DVD performs over 40% better on the drawer considerareal Widow X 200 robotinteracting with afilecabinet, task and 30% better on the faucet task than the next best a tissue box, a stuffed animal, and a toy kitchen set. The state per for ming method. It also performs reasonably on the cup space is the space of RGB image observations with size [120, task;Concept 2 Robotjustper for msparticularlywellon that task 120, 3], and the action space consists of the continuous linear since it often chooses to push the cup away no matter which velocity of the robot\u2019s gripper in the x and z directions as well task is specified. The behavioral cloning policy has somewhat as the gripper\u2019s y-position, for a total of three dimensions. erratic behavior, mimicking the trajectory for one of the target b) Simulation Experiments: For all environment and task tasks in each environment and doing well on that task but not generalization experiments, in each trial we plan 3 trajectories ontheo the rtasks.Hence,wesee that both Concept 2 Robot and of length 20. For each trajectory, we sample 100 action the behavioral cloning policy are not able to provide effective sequencesuni for mlyrandomlyandr and omlychooseoneof the multi-task reward signals for each environment. top 5 predictedtrajectories with the highestfunctionalsimilarity Additionally, in Figure 13, we show the accuracy curves score given by DVD to execute in the environment. For on the training and validation sets while training DVD. We Concept 2 Robot, we take one of the top 5 predicted trajectories see unsurprisingly that the model trained only on three with the highest classification score for the specified task, and tasks of robot demonstrations (Robot Only) has the highest for the behavioral cloning policy, we simply take the predicted validation accuracy at 99%. However, while adding human action at each state. We evaluate on the following three target videos signifi can tly increases the difficulty of the optimization, Figure 12:Environment Generalization Prior Work Comparison. Wecomp are DVD\u2019sper for manceto Concept 2",
      "start_pos": 9702,
      "end_pos": 10214
    },
    {
      "chunk_id": 356,
      "paper_id": "inthewild",
      "text": "behavioral cloning policy, we simply take the predicted validation accuracy at 99%. However, while adding human action at each state. We evaluate on the following three target videos signifi can tly increases the difficulty of the optimization, Figure 12:Environment Generalization Prior Work Comparison. Wecomp are DVD\u2019sper for manceto Concept 2 Robot,themostrelevant work, a demo-conditioned behavioral cloning policy, and a random policy. On average across environments, DVD performs around or over 30% better than the next-best per for ming method on two of the three tasks. Each bar shows the average success rate over all 3 target tasks, computed over 3 seeds of 100 trials, with error bars denoting standard error. the model sremaingenerallyrobust,with DVDtrainedonrobot Figure 11:Effectof Human Dataon Environment Generalization. We comp are DVD\u2019s per for mance on seen and unseen environments data and 12 tasks worth of human videos still obtaining 89% when trained on only robot videos compared to varying number of validation accuracy. We find in our experiments in Section IV humanvideos.Acrossallthreetasks,wesee that training with human that this trade-offindiscriminatoraccuracy from addinghuman videosprovidessignifi can tlyimprovedper for manceoveronlytraining videos to the training set results in much greater ability to on robot videos, and that DVD is generally robust to the number of generalize to unseen environments and tasks. differenthumanvideo task sused.Eachbarshows the averagesuccess rate over all 3 target tasks, computed over 3 seeds of 100 trials, with Finally, in Figure 14, we include examples on the real error bars denoting standard error. Widowx 200 of predicted trajectories and their similarity scores with a human video demonstration given by DVD. We see that Figure 13: Accuracy Curves During DVD Training. We plot both training and validation accuracies over the course of training DVD for 150 epochs with varying amounts of human data. The accuracies gradually decrease as more human videos are added, but we find that this trade-off is worthwhile for greater generalization capabilities. Closing the toy kitchen door in Test Env 1 High-ranked Human video Trajectories: Score = 0.79 Low-ranked Trajectory: Score = 0.43 Figure 14: Rankings on the real robot. Examples of predicted trajectories on the Widow X 200 that are ranked high and low for the task of closing an unseen toy kitchen door. DVD gives the predicted trajectory where the door is closed a high similarity score and the predicted trajectory where the door stays open a low similarity score. DVD highly ranks trajectories that are completing the same task as demonstrated in the given human video.",
      "start_pos": 10164,
      "end_pos": 10575
    },
    {
      "chunk_id": 357,
      "paper_id": "humansin4d",
      "text": "Humans in 4 D: Reconstructing and Tracking Humans with Trans for mers Shubham Goel Georgios Pavlakos Jathushan Rajasegaran Angjoo Kanazawa Jitendra Malik \u2217 \u2217 shubham-goel, pavlakos, jathushan, kanazawa @berkeley.edu, malik@eecs.berkeley.edu { } Universityof Cali for nia,Berkeley Figure 1: A \u201ctrans for merized\u201d view of Human Mesh Recovery. We describe HMR 2.0, a fully trans for mer-based approach for 3 D humanpose and shapereconstruction from asingleimage.Besidesimpressiveper for manceacrossawidevarietyofposes and viewpoints, HMR 2.0 alsoactsas the backboneofanimprovedsystem for jointlyreconstructing and tracking Humansin 4 D(4 DHumans). Here,we seeoutputreconstructions from HMR 2.0 foreach 2 Ddetectionin the leftimage. Abstract 1.Introduction In this paper, we present a fully trans for mer-based ap- Wepresentanapproachtoreconstructhumans and track proach for recovering 3 Dmeshesofhumanbodies from sin- them over time. At the core of our approach, we propose gleimages,andtracking the movertimeinvideo.Weobtain a fully \u201ctrans for merized\u201d version of a network for human unprecedentedaccuracyin our single-image 3 Dreconstruc- meshrecovery. Thisnetwork,HMR 2.0,advances the state tions(see Figure 1)even for unusualposeswhereprevious oftheart and shows the capabilitytoanalyzeunusualposes approachesstruggle.Invideo,welink the sereconstructions that have in the past been difficult to reconstruct from sin- overtimeby 3 Dtracking,intheprocessbridginggapsdue gle images. To analyze video, we use 3 D reconstructions toocclusionordetectionfailures.These 4 Dreconstructions from HMR 2.0 as input to a tracking system that operates can beseenon the projectwebpage. in 3 D. This enables us to deal with multiple people and Ourproblem for mulation and approach can beconceived maintainidentitiesthroughocclusionevents. Ourcomplete as the \u201ctrans for merization\u201d of previous work on human approach, 4 DHumans, achieves state-of-the-art results for mesh recovery, HMR [30] and 3 D tracking, PHALP [65]. tracking people from monocular video. Fur the rmore, we Since the pioneering Vi Tpaper[15],theprocessof\u201ctrans- demonstrate the effectiveness of HMR 2.0 on the down- formerization\u201d, i.e., converting models from CNNs or stream task ofactionrecognition,achievingsignifi can tim- LSTMs to trans for mer backbones, has advanced rapidly provements over previous pose-based action recognition across multiple computer vision tasks, e.g., [8, 16, 24, 40, approaches. Our code and models are available on the 61,77]. Specifically for 2 Dpose(2 Dbodykeypoints)this project website: https://shubham-goel.github. has already been done by Vi TPose [81]. We take that as a io/4 dhumans/. startingpoint and wedevelopa new versionof HMR,which wecall HMR 2.0 toacknowledgeitsantecedent. 3202 gu A 13 ]VC.sc[ 3 v 19002.5032:vi Xra We use HMR 2.0 to build a system that can simultane- many improvements have been proposed for the original ously reconstruct and track humans from videos. We rely method. Notably, many works have proposed alternative on the recent 3 D tracking system, PHALP [65], which we methods for pseudo-groundtruthgeneration, includingus- simplify and improveusing our poserecovery. Thissystem ingtemporalin for mation[3],multipleviews[39],oritera- canreconstruct Humansin 4 D,whichgives the nameto our tive optimization [35, 29, 57]. SPIN [35] proposed an in- method, 4 DHumans. 4 DHumans can be deployed on any the-loopoptimization that incorporated SMPLify[7]inthe video and can jointlytrack and reconstructpeopleinvideo. HMR training. Here, we also rely on pseudo-ground truth Thefunctionalityofcreatingatrackingentity for everyper- fits for training,andwe use[37]for the offlinefitting. son is fundamental towards analyzing and underst and ing Morerecently,there have beenworks that proposemore humansinvideo. Besidesachievingstate-of-the-artresults specializeddesigns for the HMRarchitecture. Py MAF[89, for tracking on the Pose Track dataset [1], we also apply",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 358,
      "paper_id": "humansin4d",
      "text": "training. Here, we also rely on pseudo-ground truth Thefunctionalityofcreatingatrackingentity for everyper- fits for training,andwe use[37]for the offlinefitting. son is fundamental towards analyzing and underst and ing Morerecently,there have beenworks that proposemore humansinvideo. Besidesachievingstate-of-the-artresults specializeddesigns for the HMRarchitecture. Py MAF[89, for tracking on the Pose Track dataset [1], we also apply 88] incorporates a mesh alignment module for the regres- HMR 2.0 onthedownstreamapplicationofactionrecogni- sion of the SMPL parameters. PARE [34] proposes a tion. Wefollow the systemdesignofrecentwork,[63],and body-part-guidedattentionmechanism for betterocclusion we show that the use of HMR 2.0 can achieve impressive handling. HKMR [20] performs a prediction that is in- improvements upon the state of the art on action recogni- formedby the knownhierarchicalstructureof SMPL.Holo- tionon the AVAv 2.2 dataset. Pose [23] proposes a pooling strategy that follows the 2 D This paper is unabashedly a systems paper. We make locations of each body joints. Instead, we follow a design design choices that lead to the best systems for 3 D human withoutanydomain-specificdecisions and weshowthatit reconstruction and trackingin the wild. Our model ispub- outper for msallpreviousapproaches. liclyavailableon the projectwebpage. Thereisanemerg- Many related approaches are making non-parametric ing trend, in computer vision as in natural language pro- predictions, i.e., instead of estimating the parameters of cessing, of large pretrained models which find widespread the SMPL model,theyexplicitlyregress the verticesof the downstream applications and thus justify the scaling ef- mesh. Graph CMR[36]usesagraphneuralnetwork for the fort. HMR 2.0 is such a large pre-trained model which prediction,METRO[42]and Fast METRO[10]useatrans- couldpotentiallybeusefulnotjustincomputervision,but former, while Mesh Graphormer [43] adopts a hybrid be- also in robotics [54, 62, 73], computer graphics [76], bio- tween the two. Since we regress the SMPL model param- mechanics [60], and other fields where analysis of the hu- eters, instead of the locations of mesh vertices, we are not man figure and its movement from images or videos is directly comparable to these. However, we show how we needed. canuseafully\u201ctrans for merized\u201ddesign for HMR. Ourcontributions can besummarizedasfollows: Human Mesh & Motion Recovery from Video. To ex- 1. Weproposeanend-to-end\u201ctrans for merized\u201darchitec- tend Human Mesh Recovery over time, most methods use ture for humanmeshrecovery,HMR 2.0. Withoutre- the basic backbone of HMR [30] and propose designs for lying on domain-specific designs, we outperform ex- the temporal encoder that fuses the per-frame features. istingapproaches for 3 Dbodyposereconstruction. HMMR [31] uses a convolutional encoder on features ex- tracted from HMR [30]. VIBE [33], MEVA [48] and 2. Buildingon HMR 2.0,wedesign 4 DHumans that can TCMR [11] use a recurrent temporal encoder. DSD [71] jointlyreconstruct and trackhumansinvideo,achiev- combines convolutional and self-attention layers, while ingstate-of-the-artresults for tracking. MAED[75]andt-HMMR[57]employatrans for mer-based temporal encoder. Baradel et al. [5, 4] also used a trans- 3. Weshow that better 3 Dposes from HMR 2.0 resultin former for temporal pose prediction, while operating di- better per for mance on the downstream task of action rectly on SMPL poses. One key limitation of these ap- recognition, finally contributing to the state-of-the-art proachesis that the yoftenoperateinscenarioswheretrack- result(42.3 m AP)onthe AVAbenchmark. ing is simple [31, 90], e.g., videos with a single person or minimal occlusions. In contrast to that, our complete 2.Related Work 4",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 359,
      "paper_id": "humansin4d",
      "text": "task of action rectly on SMPL poses. One key limitation of these ap- recognition, finally contributing to the state-of-the-art proachesis that the yoftenoperateinscenarioswheretrack- result(42.3 m AP)onthe AVAbenchmark. ing is simple [31, 90], e.g., videos with a single person or minimal occlusions. In contrast to that, our complete 2.Related Work 4 DHumansapproachisalsosolving the trackingproblem. Human Mesh Recoveryfroma Single Image. Although, Tracking People in Video. Recently, there have been ap- there have been many approaches that estimate 3 D human proaches that demonstrate state-of-the-art per for mance for pose and shaperelyingoniterativeoptimization,e.g.,SM- trackingbyrelyingon 3 Dhumanreconstruction from HMR PLify [7] and variants [22, 38, 56, 66, 72, 85], for this models, i.e., T 3 DP [64] and PHALP [65]. In these meth- analysis we will focus on approaches that directly regress ods, every person detection is lifted to 3 D using an HMR the body shape from a single image input. In this case, network [57] and then tracking is per for med using the 3 D the canonical example is HMR [30], which uses a CNN representations from lifting[64]andprediction[65]totrack to regress SMPL [45] parameters. Since its introduction, peopleinvideo. Empiricalresultsshow that PHALPworks HMR 2.0 Humans 4 D Frame t Frame t+1 Patchify Vision Trans for mer HMR 2.0 HMR 2.0 Input Image Pose SMPL Trans for mer MLP Shape Query w/ Cross Attn Token Camera Associate using HMR 2.0 Pose SMPL Trans for mer Query w/ Cross Attn MLP Shape Token Camera SMPL Query Token Vi T Multi-head MLP Cross Attention Pose Shape Camera remrofsnar T noisi V Tracking Frame t Frame t+1 HMR 2.0 HMR 2.0 Input Image Associate using pose, location, appearance Figure 2:Overviewof our approach.Left:HMR 2.0 isafully\u201ctrans for merized\u201dversionofanetwork for Human Mesh Recovery.Right: we use HMR 2.0 asthebackboneof our 4 DHumanssystem,thatbuildson PHALP[65],tojointlyreconstruct and trackhumansin 4 D. very well on multiple tracking benchmarks (the main re- space (e.g., joints X) can be projected to the image as quirementis that the images have enoughspatialresolution x=\u03c0(X)=\u03a0(K(RX+t)),where\u03a0isaperspectivepro- to permit lifting of the people to 3 D). We use these track- jection with cameraintrinsics K.Since\u03b8alreadyincludesa ingpipelines,andparticularly PHALP,asa task toevaluate globalorientation,inpracticeweassume Rasidentity and methods for humanmeshrecovery. onlypredictcameratranslationt. Action Recognition. Action recognition is typically per- HMR.Thegoalof the humanmeshreconstruction(HMR) formed using appearance features from raw video input. task is to learn a predictor f(I) that given a single im- Canonicalexamplesin this categoryinclude Slow Fast[18] age I, reconstructs the person in the image by predicting and MVi T[16]. Simultaneously, there are approaches that their 3 D pose and shape parameters. Following the typi- usefeaturesextracted from bodyposein for mation,e.g.,Po- cal parametric approaches [30, 35], we model f to predict Tion[12]and JMRN[68]. Arecentapproach,LART[63], \u0398 = [\u03b8,\u03b2,\u03c0] = f(I) where \u03b8 and \u03b2 are the SMPL pose demonstratesstate-of-the-artperformance for actionrecog- andshapeparametersand\u03c0isthecameratranslation. nitionbyfusingvideo-basedfeatures with features from 3 D humanposeestimates. we use the pipelineof this approach 3.2.Architecture andemployactionrecognitionasadownstream task toeval- We re-imagine HMR [30] as an end-to-end trans for mer uatehumanmeshrecoverymethods. architecture that uses no domain specific design choices. Yet, it outperforms all existing approaches that have heav- 3.Reconstructing People ilycustomizedarchitectures and elaboratedesigndecisions. Asshownin Figure 2,we use(i)a Vi T[15]toextractimage 3.1.Preliminaries tokens, and (ii) a standard trans for",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 360,
      "paper_id": "humansin4d",
      "text": "andemployactionrecognitionasadownstream task toeval- We re-imagine HMR [30] as an end-to-end trans for mer uatehumanmeshrecoverymethods. architecture that uses no domain specific design choices. Yet, it outperforms all existing approaches that have heav- 3.Reconstructing People ilycustomizedarchitectures and elaboratedesigndecisions. Asshownin Figure 2,we use(i)a Vi T[15]toextractimage 3.1.Preliminaries tokens, and (ii) a standard trans for mer decoder that cross- Body Model. The SMPL model[46]isalow-dimensional attendstoimagetokenstooutput\u0398. parametricmodelof the humanbody. Giveninputparame- Vi T. The Vision Trans for mer, or Vi T [15] is a trans- ters for pose(\u03b8 R 24\u00d73\u00d73)andshape(\u03b2 R 10),itoutputs former [74] that has been modified to operate on an im- a mesh M R\u2208 3\u00d7N with N = 6890 ve \u2208 rtices. The body age. The input image is first patchified into input tokens joints X \u2208R 3\u00d7k are defined as a linear combination of and passed through the trans for mer to get output tokens. \u2208 thevertices and can becomputedas X = MW withfixed The output tokens are then passed to the trans for mer de- weights W RN\u00d7k. Notethatposeparameters\u03b8 include coder. we usea Vi T-H/16,the\u201cHuge\u201dvariant with 16 16 thebodypos \u2208 eparameters\u03b8 b R 23\u00d73\u00d73 and the globalori- inputpatchsize. Pleasesee Sup Mat for moredetails. \u00d7 entation\u03b8 g R 3\u00d73. \u2208 Trans for mer decoder. We use a standard trans for mer de- \u2208 Camera. We use a perspective camera model with fixed coder[74]withmulti-headself-attention.Itprocessesasin- focal length and intrinsics K. Each camera \u03c0 = (R,t) gle(zero)inputtokenbycross-attendingto the outputimage consists of a global orientation R R 3\u00d73 and transla- tokensandendswithalinearreadoutof\u0398. Wefollow[35] tion t R 3. Given these parameters \u2208 , points in the SMPL andregress 3 Drotationsusing the representationof[91]. \u2208 3.3.Losses Followingbestpracticesin the HMRliterature[30,35], we train our predictor f with a combination of 2 D losses, 3 Dlosses, andadiscriminator. Sincewetrain with amix- Pose Predictor tureof data sets,eachhavingdifferentkindsofannotations, weemployasubsetof the selosses for eachimageinamini- batch. We use the same losses even with pseudo-ground Mask Mask Mask truth annotations. Given an input image I, the model pre- Token Token Token dicts \u0398 = [\u03b8,\u03b2,\u03c0] = f(I). Whenever we have access to the ground-truth SMPL pose parameters \u03b8\u2217 and shape pa- rameters \u03b2\u2217, we bootstrap the model predictions using an Past Future MSEloss: Figure 3: Pose prediction: We train a BERT-style [13] trans- former model onover 1 milliontracksobtained from [63].Thisal- L smpl = || \u03b8 \u2212 \u03b8\u2217 || 2 2 + || \u03b2 \u2212 \u03b2\u2217 || 2 2 . lowustomakefuturepredictions and amodalcompletionofmiss- ingdetectionsusing the same model.Topredictfutureposes(t+1, When the imagehasaccurateground-truth 3 Dkeypointan- t+2,...),wequery the model with amask-tokenusingcorrespond- notations X\u2217, we additionally supervise the predicted 3 D ingpositionalembeddings. Similarly for amodalcompletion,we keypoints X withan L 1 loss: replacemissingdetections with amaskedtoken. = X X\u2217 . L kp 3 D || \u2212 || 1 \u201clift\u201d them to 3 D, extracting their 3 D pose, location in 3 D When the image has 2 D keypoints annotations x\u2217, we su- space(derived from the estimatedcamera),and 3 Dappear- ance (derived from the texture map). A tracklet represen- pervise projections of predicted 3 D keypoints \u03c0(X) using tation is incrementally built up for each individual person an L 1 loss: = \u03c0(X) x\u2217 . over time. The recursion step is",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 361,
      "paper_id": "humansin4d",
      "text": "x\u2217, we su- space(derived from the estimatedcamera),and 3 Dappear- ance (derived from the texture map). A tracklet represen- pervise projections of predicted 3 D keypoints \u03c0(X) using tation is incrementally built up for each individual person an L 1 loss: = \u03c0(X) x\u2217 . over time. The recursion step is to predict for each track- kp 2 D 1 L || \u2212 || let, the pose, location and appearance of the person in the Fur the rmore, we want to ensure that our model predicts next frame, all in 3 D, and then find best matches between valid 3 Dposes and use the adversarialpriorin HMR[30].It thesetop-downpredictions and the bottom-updetectionsof factorizes the modelparametersinto: (i)bodyposeparam- peoplein that frameafterlifting the mto 3 D.Thestaterep- eters \u03b8 b , (ii) shape parameters \u03b2, and (iii) per-part relative resented by each tracklet is then updated by the incoming rotations \u03b8 i , which is one 3 D rotation for each of the 23 observation, and the process is iterated. It is possible to jointsof the SMPL model. Wetrainadiscriminator D k for trackthroughocclusionsbecause the 3 Drepresentationofa eachfactorof the body model,and the generatorloss can be trackletcontinuestobeupdated base donpas this tory. expressedas: Webelieve that arobustposepredictorshouldalsoper- = (cid:88) (D (\u03b8 ,\u03b2) 1)2. formwell,whenevaluatedon this downstream task oftrack- adv k b L \u2212 ing,sowe use the trackingmetricsasaproxytoevaluate the k qualityof 3 Dreconstructions. Butfirstweneededtomod- 3.4.Pseudo-Ground Truthfitting ify the PHALP framework to allow for fair comparison of differentposepredictionmodels. Originally, PHALPused We scale to unlabelled datasets (i.e., Insta Variety [31], posefeatures base don the lastlayerof the HMRnetwork, AVA [21], AI Challenger [78]) by computing pseudo- i.e., a 2048-dimensional embedding space. This limits the ground truth annotations. Given any image, we first use ability of PHALP to be used with different pose models anoff-the-shelfdetector[40]andabodykeypointsestima- (e.g., HMR 2.0, PARE, Py MAF etc.). To create a more tor[81]togetboundingboxes and corresponding 2 Dkey- genericversionof PHALP,weperform the modificationof points. We then fit a SMPL mesh to these 2 D keypoints representing pose in terms of SMPL pose parameters, and using Pro HMR [37] to get pseudo-ground truth SMPL pa- weaccordinglyoptimize the PHALPcostfunctiontoutilize rameters\u03b8\u2217and\u03b2\u2217withcamera\u03c0\u2217. the new pose distance. Similarly, we adapt the pose pre- dictor to operate on the space of SMPL parameters. More 4.Tracking People specifically, we train a vanilla trans for mer model [74] by Invideos with multiplepeople,weneed the abilitytoas- masking random pose tokens as shown in the Fig 3. This sociate people across time, i.e., perform tracking. For this allowsustopredictfutureposesintime,aswellasamodal webuildupon PHALP[65],astate-of-the-arttrackerbased completionofmissingdetections.With the semodifications, on features derived from HMR-style 3 D reconstructions. we canpluginanymeshrecoverymethods and run the mon Thebasicideaistodetectpeopleinindividualframes,and anyvideos. Wecall this modifiedversion PHALP\u2032. 4 DHumans. Ourfinaltrackingsystem,4 DHumans,usesa 3 DPW Human 3.6 M sampling-basedparameter-freeappearancehead and anew Method MPJPEPA-MPJPEMPJPEPA-MPJPE posepredictor(Figure 3). Tomodelappearance,wetexture visiblepointsonthemeshbyprojectingthemonto the input image and samplingcolor from the correspondingpixels. Totrackpeopleinvideos,previousapproachesreliedon off-the-shelf tracking approaches and used their output to reconstructhumansinvideos(e.g.,take the boundingboxes fromtrackingoutput and reconstructpeople). Forexample, PHD[90],HMMR[31]canrunonvideos with onlysingle person in the scene. In this work, we combine reconstruc- tionandtrackingintoasinglesystem and show that better posereconstructionsresultinbettertracking and thiscom- binedsystem can nowrunonanyvideosin the wild. 5.Experiments Inthissection,weevaluate our reconstruction and track- ingsystemqualitatively and quantitatively. First, weshow that HMR 2.0 outperforms previous",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 362,
      "paper_id": "humansin4d",
      "text": "to reconstructhumansinvideos(e.g.,take the boundingboxes fromtrackingoutput and reconstructpeople). Forexample, PHD[90],HMMR[31]canrunonvideos with onlysingle person in the scene. In this work, we combine reconstruc- tionandtrackingintoasinglesystem and show that better posereconstructionsresultinbettertracking and thiscom- binedsystem can nowrunonanyvideosin the wild. 5.Experiments Inthissection,weevaluate our reconstruction and track- ingsystemqualitatively and quantitatively. First, weshow that HMR 2.0 outperforms previous methods on standard 2 Dand 3 Dposeaccuracymetrics(Section 5.2).Second,we show 4 DHumans is a versatile tracker, achieving state-of- the-artper for mance(Section 5.3).Third,wefur the rdemon- strate the robustness and accuracy of our recovered poses viasuperiorper for manceon the downstreamapplicationof actionrecognition(Section 5.4). Finally,wediscusstheex- perimentalinvestigationwhendesigning HMR 2.0 andab- lateaseriesofdesignchoices(Section 5.5). 5.1.Setup Datasets. Following previous work, we use the typi- cal datasets for training, i.e., Human 3.6 M [27], MPI-INF- 3 DHP[49],COCO[44]and MPII[2]. Additionally,we use Insta Variety[31],AVA[21]and AIChallenger[78]asextra datawherewegeneratepseudo-groundtruthfits. Baselines. Wereport per for manceonbenchmarks that we can comp are with many previous works (Section 5.2), but we also perform a more detailed comparison with recent state-of-the-art methods, i.e., Py MAF [89], CLIFF [41], HMAR[65],PARE[34],and Py MAF-X[88]. Forfairness, weonlyevaluate the body-onlyper for manceof Py MAF-X. 5.2.Pose Accuracy 3 D Metrics. For 3 D pose accuracy, we follow the typical protocols of prior work, e.g., [35], and we present results on the 3 DPW test split and on the Human 3.6 M val split, reporting MPJPE, and PA-MPJPE in Table 1. Please no- tice that weonlycomp are withmethods that donotuse the trainingsetof 3 DPWfor training,similartous.Weobserve that with our HMR 2.0 a model, which trains only on the typical datasets, we can outperform all previous baselines across all metrics. However, we believe that these bench- marks are very saturated and these smaller differences in pose metrics tend to not be very significant. In fact, we laropme T Kanazawaetal.[31] 116.5 72.6 - 56.9 Doerschetal.[14] - 74.7 - - Arnabetal.[3] - 72.2 77.8 54.3 DSD[71] - 69.5 59.1 42.4 VIBE[33] 93.5 56.5 65.9 41.5 desab-emar F Pavlakosetal.[59] - - - 75.9 HMR[30] 130.0 76.7 88.0 56.8 NBF[53] - - 59.9 Graph CMR[36] - 70.2 - 50.1 Holo Pose[23] - - 60.3 46.5 Dense Ra C[82] - - 76.8 48.0 SPIN[35] 96.9 59.2 62.5 41.1 Deco MR[86] - 61.7\u2020 - 39.3\u2020 Da Net[87] - 56.9 61.5 48.6 Songetal.[69] - 55.9 - 56.4 I 2 L-Mesh Net[51] 100.0 60.0 55.7\u2020 41.1\u2020 HKMR[20] - - 59.6 43.2 Py MAF[89] 92.8 58.9 57.7 40.5 PARE[34] 82.0 50.9 76.8 50.6 Py MAF-X[88] 78.0 47.1 54.2 37.2 HMR 2.0 a 70.0 44.5 44.8 33.6 HMR 2.0 b 81.3 54.3 50.0 32.4 Table 1:Reconstructionsevaluatedin 3 D:Reconstructionerrors (in mm) on the 3 DPW and Human 3.6 M datasets. \u2020 denotes the numbers evaluated on non-parametric results. Lower is better. \u2193 Pleasesee the text for details. LSP-Extended COCO Pose Track Method @0.05 @0.1 @0.05@0.1 @0.05@0.1 Py MAF[89] - - 0.68 0.86 0.77 0.92 CLIFF[41] 0.32 0.66 0.64 0.88 0.75 0.92 PARE[34] 0.27 0.60 0.72 0.91 0.79 0.93 Py MAF-X[88] - - 0.79 0.93 0.85 0.95 HMR 2.0 a 0.38 0.72 0.79 0.95 0.86 0.97 HMR 2.0 b 0.53 0.82 0.86 0.96 0.90 0.98 Table 2: Reconstructions evaluated in 2 D. PCK scores of pro- jected keypoints at different thresholds on the",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 363,
      "paper_id": "humansin4d",
      "text": "0.27 0.60 0.72 0.91 0.79 0.93 Py MAF-X[88] - - 0.79 0.93 0.85 0.95 HMR 2.0 a 0.38 0.72 0.79 0.95 0.86 0.97 HMR 2.0 b 0.53 0.82 0.86 0.96 0.90 0.98 Table 2: Reconstructions evaluated in 2 D. PCK scores of pro- jected keypoints at different thresholds on the LSP-Extended, COCO,and Pose Track data sets.Higher isbetter. \u2191 observe that byasmallcompromiseof the per for manceon 3 DPW, our HMR 2.0 b model, which trains for longer on more data (AVA [21], AI Challenger [78], and Insta Vari- ety [31]), achieves results that perform better on more un- usual poses than what can be found in Human 3.6 M and 3 DPW.Weobserve this qualitatively and fromper for mance evaluatedon 2 Dposereprojection(Table 2). Fur the rmore, weobserve that HMR 2.0 bisamorerobust model and use itforevaluationintherestof the paper. 2 D Metrics. We evaluate 2 D imagealignment of thegen- erated poses by reporting PCK of reprojected keypoints at different thresholds on LSP-Extended [28], COCO val- idation set [44], and Posetrack validation set [1]. Since Py MAF(-X)[89,88]weretrainedusing LSP-Extended,we Posetrack do not report numbers for that part of the table. Notice in Tracker Pose Engine HOTA IDs MOTA IDF 1 Table 2, that HMR 2.0 b consistently outperforms all pre- \u2191 \u2193 \u2191 \u2191 PARE[34] 53.6 510 59.4 76.8 vious approaches. On LSP-Extended, which contains un- Py MAF-X[88] 53.7 472 59.2 76.9 usualposes,HMR 2.0 bachieves PCK@0.05 of 0.53,which CLIFF[41] 53.5 551 58.7 76.5 is 1.6 betterthan the secondbest(CLIFF)with 0.32. For PHALP\u2032 \u00d7 Py MAF[89] 53.0 623 58.6 76.1 PCK@0.05 on easier datasets like COCO and Pose Track HMAR[65] 53.6 482 59.3 77.1 with less extreme poses, HMR 2.0 b still outperforms the HMR 2.0 54.1 456 59.4 77.4 second-bestapproachesbutbynarrowermarginsof 9%and 6%respectively. HMR 2.0 aalsooutper for msall base lines, butisworsethan HMR 2.0 b,especiallyonharderposesin Table 3: Tracking with different 3 D pose estimators. With LSP-Extended. themodificationsof PHALP\u2032,wehaveaversatiletrackerthatal- Qualitative Results. We show qualitative results of lowsdifferent 3 Dposeestimatorstobepluggedintoit.HMR 2.0, HMR 2.0 in Figure 4. We are robust to extreme poses PARE,and Py MAF-Xperform the bestin this setting. andpartialocclusions.Ourreconstructions are well-aligned with the image and arevalidwhenseen from anovelview. Moreover,wecomp are with our closestcompetitorsin Fig- Posetrack Method ure 5. We observe that Py MAF-X and particularly PARE HOTA IDs MOTA IDF 1 \u2191 \u2193 \u2191 \u2191 oftenstruggle with moreunusualposes,while HMR 2.0 re- Track for mer[50] 46.7 1263 33.7 64.0 turnsmorefaithfulreconstructions. Tracktor[6] 38.5 702 42.4 65.2 5.3.Tracking Alpha Pose[17] 37.6 2220 36.9 66.9 Pose Flow[79] 38.0 1047 15.4 64.2 For tracking, we first demonstrate the versatility of T 3 DP[64] 50.6 655 55.8 73.4 the modifications introduced by PHALP\u2032, which allow us PHALP[65] 52.9 541 58.9 76.4 to evaluate 3 D pose estimators on the downstream task 4 DHumans 54.3 421 59.8 77.9 of tracking. Then, we evaluate our complete system, 4 DHumans+Vi TDet 57.8 382 61.4 79.1 4 DHumans,withrespecttothestateof the art. Evaluation Setting. Following previous work [64, 65], wereportresults base don IDs(IDswitches), MOTA[32], Table 4:Comparisonof 4 DHumans with thestateof the arton IDF 1 [67], and HOTA [47] on the Posetrack validation set the Posetrack data set. 4 DHumansachievestate-of-the-arttrack- using the protocol of [65],",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 364,
      "paper_id": "humansin4d",
      "text": "57.8 382 61.4 79.1 4 DHumans,withrespecttothestateof the art. Evaluation Setting. Following previous work [64, 65], wereportresults base don IDs(IDswitches), MOTA[32], Table 4:Comparisonof 4 DHumans with thestateof the arton IDF 1 [67], and HOTA [47] on the Posetrack validation set the Posetrack data set. 4 DHumansachievestate-of-the-arttrack- using the protocol of [65], with detections from Mask R- ingperformance for allmetrics. Incorporatingabetterdetection CNN[25]. system[40]leadstofur the rper for manceimprovements. Versatility of PHALP\u2032. With the modifications of PHALP\u2032, we abandon the model-specific latent space mightstillbelessaccurate(asobserved from Table 2). See of [65] and instead, we operate in the SMPL space, which also Figure 5 and Sup Mat for morequalitativecomparisons. is shared across most mesh recovery systems. This makes PHALP\u2032 more versatile and allows us to plug in different 4 DHumans. Table 4 evaluates tracking per for mance 3 D pose estimators and comp are them based on their per- of our complete system, 4 DHumans, on the Pose Track formanceon the downstream task oftracking. Weperform dataset. Using the sameboundingboxdetectoras[64,65], this comparison in Table 3 where we use pose and loca- 4 DHumansoutper for msexistingapproachesonallmetrics, tion cues from state-of-the-art 3 D pose estimators (while improving ID Switches by 22%. Using the improved Vi T- stillusingappearance from HMAR[65]). Weobserve that Detdetector[40]canimproveper for mancefurther.Asaby- HMR 2.0 performs the best and PARE [34], HMAR [65], productof our temporalprediction model(Figure 3),we can and Py MAF-X[88]closelyfollowon the Posetrack data set, per for mamodalcompletion and attributeaposetomissing withminordifferencesbetweenthem. Note that trackingis detections. Weshowexamplesofthisin the Sup Mat. often most susceptible to errors in predicted 3 D locations 5.4.Action Recognition withbodyposehavingasmallereffectinper for mance[65]. This means that good tracking per for mance can indicate Evaluationsetting. Theapproachof[63]isthestateof the robustness to occlusions, so it is helpful to consider this art for actionrecognitioninvideos. Givenavideoasinput, metric, butitislesshelpfultodistinguishfine-graineddif- the authors propose using per-frame 3 D pose and location ferences in pose. As a result, the competitive results of estimates(usingoff-the-shelf HMRmodels[65])asanad- PARE[34], HMAR[65], and Py MAF-X[88]indicate that ditionalfeature for predictingactionlabels. Theyalsoshow theyh and leocclusionsgracefully,but the irposeestimation resultsfora\u201cpose-only\u201dbaseline that predictsactionlabels Input Front view Side view Top view Input Front view Side view Top view Figure 4: Qualitativeevaluationof HMR 2.0. Foreachexampleweshow: a)theinputimage,b)thereconstructionoverlay,c)aside view,d)thetopview. Todemonstrate the robustnessof HMR 2.0,wevisualizeresults for avarietyofsettings-forunusualposes(rows 1-4),forunusualviewpoints(row 5)and for images with poorvisibility,extremetruncations and extremeocclusions(rows 6-8). using only 3 D pose and location estimates. We use this Comparisons. Comparing results in Table 5, we observe setting to comp are our model with baselines on the down- that HMR 2.0 outperforms baselines on the different class stream task of action recognition on the AVA dataset [21]. categories (OM, PI, PM) and overall. It achieves an m AP In [63], the authors train a trans for mer that takes SMPL of 22.3 on the AVA test set, which is 14% better than the poses as input and predicts action labels. Following their second-best baseline. Since accurate action recognition setup, we train a separate action classification trans for mer fromposesneedsfine-grainedposeestimation,thisisstrong foreach base line. evidence that HMR 2.0 predicts more accurate poses than Full Image Input Py MAF-X PARE HMR 2.0 Figure 5:Qualitativecomparisonofstate-of-the-artmeshrecoverymethods.HMR 2.0 returnsmorefaithfulreconstructions for unusual posescomp are dto the closestcompetitors,Py MAF-X[88]and PARE[34]. 0 240 1304 1312",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 365,
      "paper_id": "humansin4d",
      "text": "action recognition setup, we train a separate action classification trans for mer fromposesneedsfine-grainedposeestimation,thisisstrong foreach base line. evidence that HMR 2.0 predicts more accurate poses than Full Image Input Py MAF-X PARE HMR 2.0 Figure 5:Qualitativecomparisonofstate-of-the-artmeshrecoverymethods.HMR 2.0 returnsmorefaithfulreconstructions for unusual posescomp are dto the closestcompetitors,Py MAF-X[88]and PARE[34]. 0 240 1304 1312 1325 0 19 51 65 122 0 44 55 63 170 0 Figure 6: Qualitativetrackingresultsof 4 DHumans. we useheadmasks(framenumberison the topleft). Firstrow: Wetrackpeople skatingonice with challengingposes and heavyocclusions,inaminutelongvideo with outswitchingidentities. Secondrow: Themain personistrackedthroughmultipleinteractionswitho the rplayers.Thirdrow:Thepersonofinterestistrackedthroughlongocclusions. existing approaches. In fact, when combined with appear- Action Pose OM PI PM m AP ance features, [63] shows that HMR 2.0 achieves the state Model Engine oftheartof 42.3 m APon AVAactionrecognition,whichis Py MAF[89] 7.3 16.9 34.7 15.4 7%betterthan the second-bestof 39.5 m AP. CLIFF[41] 9.2 20.0 40.3 18.6 HMAR[65] 8.7 20.1 40.3 18.3 [63] 5.5.HMR 2.0 Model Design PARE[34] 9.2 20.7 41.5 19.1 Py MAF-X[88] 10.2 21.4 40.8 19.6 In the process of developing HMR 2.0, we investigated HMR 2.0 11.9 24.6 45.8 22.3 aseriesofdesigndecisions. Figure 7 brieflyillustrates this exploration. We experimented with over 100 settings and we visualize the per for mance of 100 checkpoints for each Table 5: Action recognition results on the AVA dataset. We run. For the visualization,we use the per for manceofeach benchmark different mesh recovery methods on the downstream checkpointon the 3 DPWand the LSP-Extended data set. taskofpose-basedactionrecognition. Here,OM:Object Manip- Our investigation focused on some specific aspects of ulation,PI:Person Interactions,and PM:Person Movement. themodel,whichwedocumen the reasaseriesof\u201clessons learnt\u201dforfutureresearch. Inthefollowingparagraphs,we willregularlyreferto Table 6,whichevaluates the seaspects K @0.05 0.6 Models MPJPE 3 D PA PW -MPJPEMP H JP u E m P a A n 3 - . M 6 M PJPEPCK L @ SP 0 - .0 E 5 xt P e C nd K e @ d 0.1 C P-extended - P 0 0 . . 4 5 B H B 2 1 MR 2.0 b 8 8 7 1 5 9 . . . 3 2 7 5 5 5 4 6 3 . . . 3 8 4 5 5 5 0 8 1 . . . 0 9 4 3 4 3 2 1 4 . . . 4 4 4 0 0 0 . . . 5 3 4 3 5 8 0 0 0 . . . 8 6 8 2 6 1 S L 46 48 50 52 54 56 D 1 84.1 54.8 54.5 35.1 0.45 0.79 3 DPW - PA-MPJPE (in mm) D 2 80.2 53.3 52.4 34.9 0.46 0.79 P 1 98.9 61.7 89.9 58.7 0.24 0.52 Figure 7: Extensive model search. Witheachdot, wevisualize P 2 82.7 55.6 49.3 32.4 0.52 0.81 the per for mance of a checkpoint when evaluated on 3 DPW and LSP-Extended. Colorsindicatedifferentruns. Weexploremore Table 6:Ablations:Evaluation for different model designson the than 100 settings,andvisualize 100 checkpoints from eachrun. 3 DPW,Human 3.6 M,and LSP-Extended data sets. \u223c outlowqualitypseudo-groundtruthfits(highfittingerror) on 3 Dand 2 Dmetrics,using the 3 DPW,Human 3.6 M,and andpruneimages with low-confidence 2 Ddetections. LSP-Extended data sets. Effect of backbone. Unlike the majority of the previous 6.Conclusion work on Human Mesh",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 366,
      "paper_id": "humansin4d",
      "text": "designson the than 100 settings,andvisualize 100 checkpoints from eachrun. 3 DPW,Human 3.6 M,and LSP-Extended data sets. \u223c outlowqualitypseudo-groundtruthfits(highfittingerror) on 3 Dand 2 Dmetrics,using the 3 DPW,Human 3.6 M,and andpruneimages with low-confidence 2 Ddetections. LSP-Extended data sets. Effect of backbone. Unlike the majority of the previous 6.Conclusion work on Human Mesh Recovery that uses a Res Net back- bone, our HMR 2.0 methodreliesona Vi Tbackbone. For Westudy the problemofreconstructing and trackinghu- a direct comparison of the effect of the backbone, Model mans from images and video. First,wepropose HMR 2.0, B 1 implements HMR with a Res Net-50 backbone and an afully\u201ctrans for merized\u201dversionofanetwork for the prob- MLP-based head implementing IEF (Iterative Error Feed- lem of Human Mesh Recovery [30]. HMR 2.0 achieves back [9, 30]). In contrast, Model B 2 uses a trans for mer strongper for manceon the usual 2 D/3 Dposemetrics,while backbone(Vi T-H)whilekeepingtheo the rdesigndecisions alsoactingas the backbone for ourimprovedvideotracker. the same. By updating the backbone, we observe a signif- The full system,4 DHumans,jointlyreconstructs and tracks icant improvement across the 3 D and 2 D metrics, which people in video and achieves state-of-the-art results for justifies the\u201ctrans for merization\u201dstep. tracking. To further illustrate the benefit of our 3 D pose Effect of training data. Besides the architecture, we also estimator,HMR 2.0,weapplyitto the taskofactionrecog- investigated the effect of training data. Model D 1 trains nition, where we demonstrate strong improvements upon on the typical datasets (H 3.6 M, MPII, COCO, MPI-INF) previouspose-based base lines. that most of the previous works are leveraging. In com- Ourworkpushestheboundaryof the videos that can be parison, model D 2 adds AVA in the training set, follow- analyzed with techniques for 3 Dhumanreconstruction. At ing[21]. Eventually,wealsotrainusing AI-Challenger and the same time, the improved results also demonstrate the Insta-Variety(model B 2),tofurtherexp and the trainingset. type of limitations that need to be addressed in the future. Aswe cansee,addingmoretraining data leadstoimprove- Forexample, theuseof the SMPL model[45]createscer- mentsacrosstheboard for the reportedmetrics,but the ben- tainlimitations,andleveragingimproved model swouldal- efitissmallercomp are dto the backboneupdate. low us to model hand pose and facial expressions [56], or Vi T pretraining. Another factor that had significant ef- even capture greater age variation, e.g., infants [26] and fect on the per for mance of our model was the pretraining kids[55,70]. Moreover,sinceweconsidereachpersonin- of the Vi T backbone. Starting with randomly initialized dependently,ourreconstructions are lesssuccessfulatcap- weights (model P 1) results in slow convergence and poor turing the fine-grained nature of people in close proxim- per for mance. Resultsimproveif our backboneispretrained ity,e.g.,contact[19,52]. Besides this,ourreconstructions with MAE [24] on Imagenet (P 2). Eventually, our model \u201clive\u201d in the camera frame, so for proper underst and ing of ofchoice(HMR 2.0 b), whichisfirstpretrained with MAE the action in a video, we need to consider everyone in a on Image Net and then on the task of 2 D keypoint predic- common world coordinate frame, by reasoning about the tion[81],achieves the bestper for mance. camera motion too [58, 83, 84]. Finally, lower input reso- SMPLhead. Wealsoinvestigatetheeffectof the architec- lution can affect the quality of our reconstructions, which ture for the head that predicts the SMPL parameters.",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 367,
      "paper_id": "humansin4d",
      "text": "of 2 D keypoint predic- common world coordinate frame, by reasoning about the tion[81],achieves the bestper for mance. camera motion too [58, 83, 84]. Finally, lower input reso- SMPLhead. Wealsoinvestigatetheeffectof the architec- lution can affect the quality of our reconstructions, which ture for the head that predicts the SMPL parameters. Our couldbeaddressedbyresolutionaugmentations[80]. proposed trans for mer decoder (HMR 2.0 b) improves per- Acknowledgements Wethankmembersof the BAIRcom- formancewhenitcomesto the image-modelalignment(i.e., munity for helpful discussions and Stability AI for their 2 D metrics) compared to the traditional MLP-based head generous compute grant. This work was supported by with IEFsteps(B 2). BAIR/BDD sponsors, ONR MURI (N 00014-21-1-2801), Datasetquality. Similartopreviouswork,e.g.,[35],itwas andthe DARPAMCSprogram. crucialtokeepthequalityof the training data high;wefilter References [17] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. RMPE: Regional multi-person pose estimation. In ICCV, [1] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, 2017. Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt [18] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Schiele. Pose Track: Abenchmark for humanposeestima- Kaiming He. Slowfast networks for video recognition. In tion and tracking. In CVPR,2018. ICCV,2019. [2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and [19] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Bernt Schiele. 2 Dhumanposeestimation: Newbenchmark Popa, Vlad Olaru, and Cristian Sminchisescu. Three- andstateof the artanalysis. In CVPR,2014. dimensionalreconstructionofhumaninteractions.In CVPR, [3] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Ex- 2020. ploitingtemporalcontext for 3 Dhumanposeestimationin [20] Georgios Georgakis,Ren Li,Srikrishna Karanam,Terrence thewild. In CVPR,2019. Chen,Jana Kos\u02c7ecka\u00b4,and Ziyan Wu. Hierarchicalkinematic [4] Fabien Baradel,Romain Bre\u00b4gier,Thibault Groueix,Philippe humanmeshrecovery. In ECCV,2020. Weinzaepfel,Yannis Kalantidis,and Gre\u00b4gory Rogez. Pose- [21] Chunhui Gu, Chen Sun, David A Ross, Carl Von- BERT: A generic trans for mer module for temporal 3 D hu- drick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya- man model ing. PAMI,2022. narasimhan, George Toderici, Susanna Ricco, Rahul Suk- [5] Fabien Baradel, Thibault Groueix, Philippe Weinzaepfel, thankar,Cordelia Schmid,and Jitendra Malik.AVA:Avideo Romain Bre\u00b4gier, Yannis Kalantidis, and Gre\u00b4gory Rogez. datasetofspatio-temporallylocalizedatomicvisualactions. Leveraging Mo Capdata for humanmeshrecovery. In 3 DV, In CVPR,2018. 2021. [22] Peng Guan, Alexander Weiss, Alexandru O Ba\u02d8lan, and [6] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Michael J Black. Estimating human shape and pose from Tracking with outbells and whistles. In ICCV,2019. asingleimage. In ICCV,2009. [7] Federica Bogo,Angjoo Kanazawa,Christoph Lassner,Peter [23] Riza Alp Guler and Iasonas Kokkinos. Holo Pose: Holistic Gehler,Javier Romero,and Michael JBlack.Keepit SMPL: 3 Dhumanreconstructionin-the-wild. In CVPR,2019. Automatic estimation of 3 D human pose and shape from a [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr singleimage. In ECCV,2016. Dolla\u00b4r,and Ross Girshick.Maskedautoencoders are scalable [8] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas visionlearners. In CVPR,2022. Usunier,Alexander Kirillov,and Sergey Zagoruyko.End-to- [25] Kaiming He,Georgia Gkioxari,Piotr Dolla\u00b4r,and Ross Gir- endobjectdetection with trans for mers. In ECCV,2020. shick. Mask R-CNN. In ICCV,2017. [9] Joao Carreira,Pulkit Agrawal,Katerina Fragkiadaki,and Ji- [26] Nikolas Hesse, Sergi Pujades, Michael J Black, Michael tendra Malik. Human pose estimation with iterative error Arens, Ulrich G Hofmann, and A Sebastian Schroeder. feedback. In CVPR,2016. Learning and tracking the 3 Dbodyshapeoffreelymoving [10] Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Cross- infants from RGB-Dsequences. PAMI,2019. attentionofdisentangledmodalities for 3 Dhumanmeshre- [27] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian covery with trans for mers. In",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 368,
      "paper_id": "humansin4d",
      "text": "error Arens, Ulrich G Hofmann, and A Sebastian Schroeder. feedback. In CVPR,2016. Learning and tracking the 3 Dbodyshapeoffreelymoving [10] Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Cross- infants from RGB-Dsequences. PAMI,2019. attentionofdisentangledmodalities for 3 Dhumanmeshre- [27] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian covery with trans for mers. In ECCV,2022. Sminchisescu.Human 3.6 M:Large scale data sets and predic- [11] Hongsuk Choi,Gyeongsik Moon,Ju Yong Chang,and Ky- tivemethods for 3 Dhumansensinginnaturalenvironments. oung Mu Lee. Beyond static features for temporally con- PAMI,2013. sistent 3 Dhumanpose and shape from avideo. In CVPR, [28] Sam Johnson and Mark Everingham. Learningeffectivehu- 2021. manposeestimation from inaccurateannotation. In CVPR, [12] Vasileios Choutas, Philippe Weinzaepfel, Je\u00b4ro\u02c6me Revaud, 2011. and Cordelia Schmid. Po Tion: Posemotionrepresentation [29] Hanbyul Joo,Natalia Neverova,and Andrea Vedaldi.Exem- foractionrecognition. In CVPR,2018. plarfine-tuning for 3 Dhuman model fittingtowardsin-the- [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina wild 3 Dhumanposeestimation. In 3 DV,2021. Toutanova. Bert: Pre-training of deep bidirectional [30] Angjoo Kanazawa, Michael J Black, David W Jacobs, and trans for mers for language underst and ing. ar Xiv preprint Jitendra Malik. End-to-end recovery of human shape and ar Xiv:1810.04805,2018. pose. In CVPR,2018. [14] Carl Doersch and Andrew Zisserman. Sim 2 real transfer [31] Angjoo Kanazawa,Jason YZhang,Panna Felsen,and Jiten- learning for 3 Dhumanposeestimation: Motionto the res- dra Malik. Learning 3 D human dynamics from video. In cue. Neur IPS,2019. CVPR,2019. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [32] Rangachar Kasturi, Dmitry Goldgof, Padmanabhan Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Soundararajan, Vasant Manohar, John Garofolo, Rachel Mostafa Dehghani,Matthias Minderer,Georg Heigold,Syl- Bowers, Matthew Boonstra, Valentina Korzhova, and Jing vain Gelly,Jakob Uszkoreit,and Neil Houlsby. Animageis Zhang. Frameworkforper for manceevaluationofface,text, worth 16 x 16 words: Transformers for imagerecognitionat andvehicledetection and trackinginvideo: Data, metrics, scale. ICLR,2021. andprotocol. PAMI,2008. [16] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, [33] Muhammed Kocabas, Nikos Athanasiou, and Michael J Zhicheng Yan,Jitendra Malik,and Christoph Feichtenhofer. Black. VIBE: Video inference for human body pose and Multi scale visiontrans for mers. In ICCV,2021. shapeestimation. In CVPR,2020. [34] Muhammed Kocabas, Chun-Hao PHuang, Otmar Hilliges, [51] Gyeongsik Moon and Kyoung Mu Lee. I 2 L-Mesh Net: and Michael JBlack. PARE:Partattentionregressor for 3 D Image-to-lixel prediction network for accurate 3 D human humanbodyestimation. In ICCV,2021. pose and mesh estimation from a single RGB image. In [35] Nikos Kolotouros,Georgios Pavlakos,Michael JBlack,and ECCV,2020. Kostas Daniilidis. Learningto reconstruct 3 Dhuman pose [52] Lea Mu\u00a8ller, Vickie Ye, Georgios Pavlakos, Michael Black, andshapevia model-fittingin the loop. In ICCV,2019. and Angjoo Kanazawa. Generative proxemics: A prior [36] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani- for 3 D social interaction from images. ar Xiv preprint ilidis. Convolutional mesh regression for single-image hu- ar Xiv:2306.09337,2023. manshapereconstruction. In CVPR,2019. [53] Mohamed Omran,Christoph Lassner,Gerard Pons-Moll,Pe- [37] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, ter Gehler,and Bernt Schiele. Neuralbodyfitting:Unifying and Kostas Daniilidis. Probabilistic modeling for human deeplearningand model basedhumanpose and shapeesti- meshrecovery. In ICCV,2021. mation. In 3 DV,2018. [38] Christoph Lassner, Javier Romero, Martin Kiefel, Federica [54] Austin Patel,Andrew Wang,Ilija Radosavovic,and Jitendra Bogo,Michael JBlack,and Peter VGehler. Unite the peo- Malik. Learningtoimitateobjectinteractions from internet ple:Closing the loopbetween 3 Dand 2 Dhumanrepresenta- videos. ar Xivpreprintar Xiv:2211.13225,2022. tions. In",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 369,
      "paper_id": "humansin4d",
      "text": "deeplearningand model basedhumanpose and shapeesti- meshrecovery. In ICCV,2021. mation. In 3 DV,2018. [38] Christoph Lassner, Javier Romero, Martin Kiefel, Federica [54] Austin Patel,Andrew Wang,Ilija Radosavovic,and Jitendra Bogo,Michael JBlack,and Peter VGehler. Unite the peo- Malik. Learningtoimitateobjectinteractions from internet ple:Closing the loopbetween 3 Dand 2 Dhumanrepresenta- videos. ar Xivpreprintar Xiv:2211.13225,2022. tions. In CVPR,2017. [55] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch, [39] Vincent Leroy, Philippe Weinzaepfel, Romain Bre\u00b4gier, David THoffmann,Shashank Tripathi,and Michael JBlack. Hadrien Combaluzier,and Gre\u00b4gory Rogez. SMPLybench- AGORA: Avatars in geography optimized for regression marking 3 D human pose estimation in the wild. In 3 DV, analysis. In CVPR,2021. 2020. [56] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, [40] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Exploringplainvisiontransformerbackbones for objectde- Michael JBlack. Expressivebodycapture: 3 Dhands,face, tection. In ECCV,2022. andbody from asingleimage. In CVPR,2019. [41] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, [57] Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. and Youliang Yan. CLIFF:Carryinglocationin for mationin Humanmeshrecovery from multipleshots. In CVPR,2022. fullframesintohumanpose and shapeestimation.In ECCV, [58] Georgios Pavlakos, Ethan Weber, Matthew Tancik, and 2022. Angjoo Kanazawa. The one where they reconstructed 3 D [42] Kevin Lin,Lijuan Wang,and Zicheng Liu. End-to-endhu- humans and environmentsin TVshows. In ECCV,2022. man pose and mesh reconstruction with trans for mers. In [59] Georgios Pavlakos,Luyang Zhu,Xiaowei Zhou,and Kostas CVPR,2021. Daniilidis. Learningtoestimate 3 Dhumanpose and shape [43] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh fromasinglecolorimage. In CVPR,2018. graphormer. In ICCV,2021. [60] Owen Pearl,Soyong Shin,Ashwin Godura,Sarah Bergbre- [44] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, iter,and Eni Halilaj.Fusionofvideo and inertialsensing data Pietro Perona,Deva Ramanan,Piotr Dolla\u00b4r,and CLawrence viadynamicoptimizationofabiomechanical model.Journal Zitnick. Microsoft COCO:Commonobjectsincontext. In of Biomechanics,155:111617,2023. ECCV,2014. [61] William Peebles and Saining Xie. Scalablediffusionmodels [45] Matthew Loper,Naureen Mahmood,Javier Romero,Gerard withtrans for mers. ar Xivpreprintar Xiv:2212.09748,2022. Pons-Moll,and Michael JBlack. SMPL:Askinnedmulti- [62] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter personlinear model. ACMtransactionsongraphics(TOG), Abbeel,and Sergey Levine. Sfv: Rein for cementlearningof 34(6):1\u201316,2015. physicalskills from videos.ACMTransactions On Graphics [46] Matthew Loper, Naureen Mahmood, Javier Romero, Ger- (TOG),37(6):1\u201314,2018. ard Pons-Moll, and Michael J. Black. SMPL: A skinned [63] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo multi-person linear model. ACM Trans. Graphics (Proc. Kanazawa, Christoph Feichtenhofer, and Jitendra Malik. SIGGRAPHAsia),34(6):248:1\u2013248:16,Oct.2015. On the benefits of 3 D tracking and pose for human action [47] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip recognition. In CVPR,2023. Torr,Andreas Geiger,Laura Leal-Taixe\u00b4,and Bastian Leibe. [64] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo HOTA: A higher order metric for evaluating multi-object Kanazawa, and Jitendra Malik. Tracking people with 3 D tracking. IJCV,2021. representations. In Neur IPS,2021. [48] Zhengyi Luo,SAlireza Golestaneh,and Kris MKitani. 3 D [65] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo human motion estimation via motion compression and re- Kanazawa, and Jitendra Malik. Tracking people by pre- finement. In ACCV,2020. dicting 3 D appearance, location and pose. In CVPR, [49] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal 2022. Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian [66] Davis Rempe,Tolga Birdal,Aaron Hertzmann,Jimei Yang, Theobalt. Monocular 3 Dhumanposeestimationin the wild Srinath Sridhar,and Leonidas JGuibas.Hu Mo R:3 Dhuman usingimproved CNNsupervision. In 3 DV,2017. motion model for robustposeestimation. In ICCV,2021. [50] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and [67]",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 370,
      "paper_id": "humansin4d",
      "text": "Dan Casas, Pascal 2022. Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian [66] Davis Rempe,Tolga Birdal,Aaron Hertzmann,Jimei Yang, Theobalt. Monocular 3 Dhumanposeestimationin the wild Srinath Sridhar,and Leonidas JGuibas.Hu Mo R:3 Dhuman usingimproved CNNsupervision. In 3 DV,2017. motion model for robustposeestimation. In ICCV,2021. [50] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and [67] Ergys Ristani,Francesco Solera,Roger Zou,Rita Cucchiara, Christoph Feichtenhofer. Track Former: Multi-objecttrack- and Carlo Tomasi. Per for mancemeasures and adataset for ing with trans for mers. In CVPR,2022. multi-target,multi-cameratracking. In ECCV,2016. [68] Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng [85] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis- Chen,Rama Chellappa,and Abhinav Shrivastava. Pose and escu. Monocular 3 Dpose and shapeestimationofmultiple joint-awareactionrecognition. In WACV,2022. peopleinnaturalscenes-Theimportanceofmultiplescene [69] Jie Song,Xu Chen,and Otmar Hilliges. Humanbody model constraints. In CVPR,2018. fittingbylearnedgradientdescent. In ECCV,2020. [86] Wang Zeng,Wanli Ouyang,Ping Luo,Wentao Liu,and Xi- [70] Yu Sun,Wu Liu,Qian Bao,Yili Fu,Tao Mei,and Michael J aogang Wang. 3 Dhumanmeshregression with densecorre- Black. Puttingpeoplein the irplace: Monocularregression spondence. In CVPR,2020. of 3 Dpeopleindepth. In CVPR,2022. [87] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and [71] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Zhenan Sun. Da Nnet: Decompose-and-aggregate network Mei. Human mesh recovery from monocular images via a for 3 D human shape and pose estimation. In Proceedings skeleton-disentangledrepresentation. In ICCV,2019. of the 27 th ACM International Conference on Multimedia, [72] Garvita Tiwari,Dimitrije Antic\u00b4,Jan Eric Lenssen,Nikolaos 2019. Sarafianos,Tony Tung,and Gerard Pons-Moll. Pose-NDF: [88] Hongwen Zhang,Yating Tian,Yuxiang Zhang,Mengcheng Modelinghumanposemanifolds with neuraldistancefields. Li,Liang An,Zhenan Sun,and Yebin Liu. Py MAF-X:To- In ECCV,2022. wardswell-aligned full-body model regression from monoc- [73] Vasileios Vasilopoulos,Georgios Pavlakos,Sean LBowman, ularimages. PAMI,2023. J Diego Caporale, Kostas Daniilidis, George J Pappas, and [89] Hongwen Zhang,Yating Tian,Xinchi Zhou,Wanli Ouyang, Daniel EKoditschek. Reactivesemanticplanninginunex- Yebin Liu,Limin Wang,and Zhenan Sun. Py MAF:3 Dhu- plored semantic environments using deep perceptual feed- manpose and shaperegression with pyramidalmeshalign- back. IEEE Robotics and Automation Letters, 5(3):4455\u2013 mentfeedbackloop. In ICCV,2021. 4462,2020. [90] Jason YZhang,Panna Felsen,Angjoo Kanazawa,and Jiten- [74] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko- dra Malik. Predicting 3 Dhum and ynamics from video. In reit,Llion Jones,Aidan NGomez,\u0141ukasz Kaiser,and Illia ICCV,2019. Polosukhin. Attentionisallyouneed. In NIPS,2017. [91] Yi Zhou,Connelly Barnes,Jingwan Lu,Jimei Yang,and Hao [75] Ziniu Wan, Zhengjia Li, Maoqing Tian, Jianbo Liu, Shuai Li. On the continuity of rotation representations in neural Yi, and Hongsheng Li. Encoder-decoder with multi-level networks. In CVPR,2019. attention for 3 Dhumanshape and poseestimation.In ICCV, 2021. [76] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan TBarron,and Ira Kemelmacher-Shlizerman. Hu- man Ne RF:Free-viewpointrenderingofmovingpeople from monocularvideo. In CVPR,2022. [77] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview compres- sivecoding for 3 Dreconstruction. In CVPR,2023. [78] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang,Wenjia Wang,Shipei Zhou,Guosen Lin,Yanwei Fu,Yizhou Wang,and Yonggang Wang. AIChallenger: A large-scale data set for goingdeeperinimageunderst and ing. ar Xivpreprintar Xiv:1711.06475,2017. [79] Yuliang Xiu,Jiefeng Li,Haoyu Wang,Yinghong Fang,and Cewu Lu. Pose Flow: Efficient online pose tracking. In BMVC,2018. [80] Xiangyu Xu,Hao Chen,Francesc Moreno-Noguer,Laszlo A Jeni,and Fernando Dela Torre. 3 Dhumanpose,shape and texture from low-resolutionimages and videos.PAMI,2021. [81] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vi TPose: Simple vision trans for mer baselines for",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 371,
      "paper_id": "humansin4d",
      "text": "Li,Haoyu Wang,Yinghong Fang,and Cewu Lu. Pose Flow: Efficient online pose tracking. In BMVC,2018. [80] Xiangyu Xu,Hao Chen,Francesc Moreno-Noguer,Laszlo A Jeni,and Fernando Dela Torre. 3 Dhumanpose,shape and texture from low-resolutionimages and videos.PAMI,2021. [81] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vi TPose: Simple vision trans for mer baselines for human poseestimation. In Neur IPS,2022. [82] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. Dense Ra C: Joint 3 D pose and shape estimation by dense render-and- comp are. In ICCV,2019. [83] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videosin the wild. In CVPR,2023. [84] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. GLAMR:Globalocclusion-awarehumanmesh recovery with dynamiccameras. In CVPR,2022. Supplementary Material for: \u201cHumans in 4 D: Reconstructing and Tracking Humans with Trans for mers\u201d Shubham Goel Georgios Pavlakos Jathushan Rajasegaran Angjoo Kanazawa Jitendra Malik \u2217 \u2217 shubham-goel, pavlakos, jathushan, kanazawa @berkeley.edu, malik@eecs.berkeley.edu { } Universityof Cali for nia,Berkeley Weprovidemoredetailsabout HMR 2.0,i.e.,thearchi- pointdetection, whilefittinghappensusing Pro HMR[10]. tecturewe use(Section S.1),thedata(Section S.2)andthe We discard detections with very few 2 D detected key- training pipeline (Section S.3). Fur the rmore, we describe points(lessthanfive)andlowdetectionconfidence(thresh- the aspect of pose prediction (Section S.4) and we discuss old 0.5). We also discard fits with unnatural body shapes the metrics we use for evaluation (Section S.5). Then, we (i.e., body shape parameters outside [ 3,3]), unnatural \u2212 discuss the experimentalsettings for tracking(Section S.6), bodyposes(computedusingaper-join this togramofposes and action recognition (Section S.7). Finally, we provide on AMASS [17]), and large fitting errors (i.e., which indi- additionalqualitativeresults(Section S.8). cates that the reconstructionwasnotsuccessful). Fortrain- ingour HMR 2.0 bmodel,wesample with differentproba- S.1.HMR 2.0 architecturedetails bilities from each data set,i.e.,Human 3.6 M:0.1,MPII:0.1, MPI-INF-3 DHP: 0.1, AVA: 0.15, AI Challenger: 0.15, In- The architecture of our HMR 2.0 model is based on a sta Variety: 0.2,COCO:0.2. Vi T image encoder and a trans for mer decoder. We use a Vi T-H/16 (\u201chuge\u201d) pre-trained on the task of 2 D key- S.3.Trainingdetails pointlocalization[25]. Ithas 50 trans for merlayers,takesa 256 192 sizedimageasinput,andoutputs 16 12 image Wetrain our main model using 8 A 100 GPUs with anef- \u00d7 \u00d7 tokens,eachofdimension 1280.Ourtrans for merdecoderis fective batch size of 8 48 = 384. We use an Adam W \u00d7 ast and ardtrans for merdecoderarchitecture[23]with 6 lay- optimizer [15] with a learning rate of 1 e-5, \u03b2 = 0.9, 1 ers, each containing multi-head self-attention, multi-head \u03b2 = 0.999,andaweightdecayof 1 e-4. Traininglasts for 2 cross-attention, and feed-forward blocks, with layer nor- 1 Miterations, whichtakesroughlysixdays. For our main malization[2]. Ithasa 2048 hiddendimension,8(64-dim) model HMR 2.0 b, we train the network end-to-end. How- heads for self-andcross-attention,andahiddendimension ever, for the HMR 2.0 a variant, the Vi T encoder remains of 1024 inthefeed-forward MLPblock.Itoperatesonasin- frozen,allowingalargereffectivebatchsizeof 8 512 = \u00d7 glelearnable 2048-dimensional SMPLquerytokenasinput 4096,learningrateof 1 e-4,andfewertrainingiterationsof and cross-attends to the 16 12 image tokens. Finally, a 100 K(i.e.,roughlyequivalentnumberofepochs). \u00d7 linearreadoutontheoutputtoken from the trans for merde- While training, we weigh the different losses. , kp 3 D L codergivespose\u03b8,shape\u03b2,andcamera\u03c0. , and have weights 0.05, 0.01, and 0.0005 re- kp 2 D adv L L spectively. The terms within are",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 372,
      "paper_id": "humansin4d",
      "text": "to the 16 12 image tokens. Finally, a 100 K(i.e.,roughlyequivalentnumberofepochs). \u00d7 linearreadoutontheoutputtoken from the trans for merde- While training, we weigh the different losses. , kp 3 D L codergivespose\u03b8,shape\u03b2,andcamera\u03c0. , and have weights 0.05, 0.01, and 0.0005 re- kp 2 D adv L L spectively. The terms within are also weighed dif- smpl L S.2.Datadetails ferently,the\u03b8and\u03b2 termsweigh 0.001 and 0.0005 respec- tively. Inourtraining,weadopt the training data conventionsof previous works [10], using images from Human 3.6 M [4], S.4.Poseprediction COCO[13],MPII[1]and MPI-INF-3 DHP[18].Thisforms thetrainingset for the version were fertoas HMR 2.0 ain For the pose prediction model, we train a vanilla trans- the main manuscript. For the eventual HMR 2.0 b version, former model[23]from the trackletsobtainedby[19].Each we additionally generate pseudo-ground truth SMPL [14] tracklet at every time instance contains 3 D pose and 3 D fits for images from AVA[3],Insta Variety[6]and AIChal- location information, where the pose is parameterized by lenger[24]. Since AVAand Insta Varietyincludevideos,we the SMPL model[14]and the locationisrepresentedas the collect frames by sampling at 1 fps and 5 fps respectively. translationin the cameraframe. Thetrans for merhas 6 lay- For pseudo-ground truth generation, we use Vi TDet [11] ers and 8 self-attention heads with a hidden dimension of for bounding box detection and Vi TPose [25] for key- 256. Eachoutputtokenregresses the 3 Dpose and 3 Dloca- tion of the person at the specified time-step. We train this ofthemainmanuscript). Tomodelappearance,wetexture model by randomly masking input pose tokens and apply- visiblepointsonthemeshbyprojectingthemonto the input ing the loss on the masked tokens. During inference, to image and samplingcolor from the correspondingpixels. predict a future 3 D pose, we query the model by reading out from afuturetime-step, usingalearnedmask-tokenas S.7.Actionrecognition input to that time-step. Similarly for amodal completion, we replace the missing detections with the learned mask- As an alternative way to assess the quality of 3 D hu- token and read out from the output at the corresponding man reconstruction, we evaluate various human mesh re- time-step. The model istrained with abatchsizeof 64 se- covery systems on the downstream task of action recogni- quences and a sequence length of 128 tokens. We use the tion on AVA (please refer to [19] for more details on the Adam W optimizer [15] with a learning rate of 0.001 and task definition). More specifically, we take the tracklets \u03b2 =0.9,\u03b2 =0.95. from [19], which were generated by running PHALP [21] 1 2 on the Kinetics [8] and AVA [3] datasets. Then, we re- S.5.Metrics place the poses from varioushumanmeshrecoverymodels (i.e.,Py MAF[28],Py MAF-X[27],PARE[9],CLIFF[12], For our evaluation,we use the metrics that are common HMAR[21], HMR 2.0)andevaluate the irper for manceon intheliterature: the action recognition task. In this pose-only setting, the 3 DPose:Wefollow[5]andwe use MPJPE and PA-MPJPE. action recognition model has access only to the 3 D poses MPJPErefersto Mean Per Joint Position Error and itis the (inthe SMPL for mat)and 3 Dlocation and istrainedtopre- average L 2 erroracrossalljoint,afteraligning with the root dict the action of each person. For a fair comparison and node. PA-MPJPE is similar but is computed after aligning toachieve the bestperformance for each 3 Dposeregressor, the predicted pose with",
      "start_pos": 6930,
      "end_pos": 7442
    },
    {
      "chunk_id": 373,
      "paper_id": "humansin4d",
      "text": "Error and itis the (inthe SMPL for mat)and 3 Dlocation and istrainedtopre- average L 2 erroracrossalljoint,afteraligning with the root dict the action of each person. For a fair comparison and node. PA-MPJPE is similar but is computed after aligning toachieve the bestperformance for each 3 Dposeregressor, the predicted pose with the ground-truth pose using Pro- weretrain the actionrecognition model specifically for each crustes Alignment. 3 Dposemethod. 2 D Pose: We use PCK as defined in [26]. This is the Per- centageof Correctlylocalized Keypoints,whereakeypoint S.8.Additionalqualitativeresults is considered as correctly localized if its L 2 distance from theground-truthkeypointislessthanathresholdt. Were- We have already provided a lot of qualitative results of portresultsusingdifferentthresholds(@0.05 and@0.1 of HMR 2.0, both in the main manuscript and in videos on imagesize). the project webpage. Here, we provide additional results, Tracking: Following [20, 21], we use standard tracking including comparisons with our closest competitors (Fig- metrics. This includes ID switches (IDs), MOTA [7], ure S.1), and a demonstration of our results in a variety of IDF 1[22],and HOTA[16]. challengingcases,includingsuccesses(Figure S.2)andfail- Action Recognition: Wereportresultsusingm APmetrics urecases(Figure S.3). as defined in the AVA dataset [3]. We further provided a morefine-grainedanalysisreportingresultsondifferentac- References tion categories: actions that involve Object Manipulation [1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and (OM),actions that involve Person Interactions(PI),andac- Bernt Schiele. 2 Dhumanposeestimation: Newbenchmark tions that involve Person Movement (PM). The results in andstateof the artanalysis. In CVPR,2014. thesecategories are alsoreportedusingm AP. [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. Layernormalization. ar Xivpreprintar Xiv:1607.06450, S.6.Tracking with PHALP \u2032 2016. In the main manuscript, we comp are different human [3] Chunhui Gu, Chen Sun, David A Ross, Carl Von- drick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya- mesh recovery systems on the downstream problem of narasimhan, George Toderici, Susanna Ricco, Rahul Suk- tracking (Table 3 of the main manuscript). For this, we thankar,Cordelia Schmid,and Jitendra Malik.AVA:Avideo modify the PHALPapproach[21], sothatposedistanceis datasetofspatio-temporallylocalizedatomicvisualactions. computedon the SMPLspace that all the modelssh are. To In CVPR,2018. make this comparison fair, we keep other variables simi- [4] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian lar to the original PHALP (e.g., same appearance embed- Sminchisescu.Human 3.6 M:Large scale data sets and predic- ding). Note that this comparison is generous to baselines tivemethods for 3 Dhumansensinginnaturalenvironments. that do not model appearance themselves. Eventually, our PAMI,2013. final 4 DHumanssystemusesasampling-basedappearance [5] Angjoo Kanazawa, Michael J Black, David W Jacobs, and head and our new poseprediction, whichleadto the state- Jitendra Malik. End-to-end recovery of human shape and of-the-art per for mance for tracking on Pose Track (Table 4 pose. In CVPR,2018. Input Py MAF-X PARE CLIFF HMR 2.0 Figure S.1:Qualitativecomparisonof our approach with state-of-the-artmethods.Wecomp are HMR 2.0 with our closestcompetitors, Py MAF-X[27], PARE[9]and CLIFF[12]. Foreachexample, weshow the inputimage, andresults from eachmethod(including the frontal and a side view). HMR 2.0 is signifi can tly more robust in a variety of settings, including images with unusual poses, unusual viewpoints and heavyperson-personoverlap. Input Front view Side view Top view Input Front view Side view Top view Figure S.2:Qualitativeresultsof our approachonchallengingexamples.Foreachexampleweshow the inputimage,thereconstruction overlay, a side view and the top view. The examples",
      "start_pos": 7392,
      "end_pos": 7904
    },
    {
      "chunk_id": 374,
      "paper_id": "humansin4d",
      "text": "is signifi can tly more robust in a variety of settings, including images with unusual poses, unusual viewpoints and heavyperson-personoverlap. Input Front view Side view Top view Input Front view Side view Top view Figure S.2:Qualitativeresultsof our approachonchallengingexamples.Foreachexampleweshow the inputimage,thereconstruction overlay, a side view and the top view. The examples include unusual poses, unusual viewpoints, people in close interaction, extreme truncations and occlusions,aswellasblurryimages. Input Front view Side view Top view Input Front view Side view Top view Figure S.3: Failuresofsingleframe 3 Dhumanreconstruction with HMR 2.0. Despite the increasedrobustnessof our method, we observe that HMR 2.0 occasionallyrecoverserroneousreconstructionsincases with veryunusualarticulation(firstrow),heavyperson- personinteraction(secondrow),andverychallengingdepthordering for the differentbodyparts(thirdrow). [6] Angjoo Kanazawa,Jason YZhang,Panna Felsen,and Jiten- [15] Ilya Loshchilov and Frank Hutter. Decoupledweightdecay dra Malik. Learning 3 D human dynamics from video. In regularization. ar Xivpreprintar Xiv:1711.05101,2017. CVPR,2019. [16] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip [7] Rangachar Kasturi, Dmitry Goldgof, Padmanabhan Torr,Andreas Geiger,Laura Leal-Taixe\u00b4,and Bastian Leibe. Soundararajan, Vasant Manohar, John Garofolo, Rachel HOTA: A higher order metric for evaluating multi-object Bowers, Matthew Boonstra, Valentina Korzhova, and Jing tracking. IJCV,2021. Zhang. Frameworkforper for manceevaluationofface,text, [17] Naureen Mahmood,Nima Ghorbani,Nikolaus FTroje,Ger- andvehicledetection and trackinginvideo: Data, metrics, ard Pons-Moll, and Michael JBlack. AMASS:Archiveof andprotocol. PAMI,2008. motioncaptureassurfaceshapes. In ICCV,2019. [8] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, [18] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Theobalt. Monocular 3 Dhumanposeestimationin the wild and Andrew Zisserman. The kinetics human action video usingimproved CNNsupervision. In 3 DV,2017. dataset. ar Xivpreprintar Xiv:1705.06950,2017. [19] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo [9] Muhammed Kocabas, Chun-Hao PHuang, Otmar Hilliges, Kanazawa, Christoph Feichtenhofer, and Jitendra Malik. and Michael JBlack. PARE:Partattentionregressor for 3 D On the benefits of 3 D tracking and pose for human action humanbodyestimation. In ICCV,2021. recognition. In CVPR,2023. [10] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, [20] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo and Kostas Daniilidis. Probabilistic modeling for human Kanazawa, and Jitendra Malik. Tracking people with 3 D meshrecovery. In ICCV,2021. representations. In Neur IPS,2021. [11] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. [21] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Exploringplainvisiontransformerbackbones for objectde- Kanazawa, and Jitendra Malik. Tracking people by pre- tection. In ECCV,2022. dicting 3 D appearance, location and pose. In CVPR, [12] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, 2022. and Youliang Yan. CLIFF:Carryinglocationin for mationin [22] Ergys Ristani,Francesco Solera,Roger Zou,Rita Cucchiara, fullframesintohumanpose and shapeestimation.In ECCV, and Carlo Tomasi. Per for mancemeasures and adataset for 2022. multi-target,multi-cameratracking. In ECCV,2016. [13] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, [23] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko- Pietro Perona,Deva Ramanan,Piotr Dolla\u00b4r,and CLawrence reit,Llion Jones,Aidan NGomez,\u0141ukasz Kaiser,and Illia Zitnick. Microsoft COCO:Commonobjectsincontext. In Polosukhin. Attentionisallyouneed. In NIPS,2017. ECCV,2014. [24] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, [14] Matthew Loper,Naureen Mahmood,Javier Romero,Gerard Rui Liang,Wenjia Wang,Shipei Zhou,Guosen Lin,Yanwei Pons-Moll,and Michael JBlack. SMPL:Askinnedmulti- Fu,Yizhou Wang,and Yonggang Wang. AIChallenger: A personlinear model. ACMtransactionsongraphics(TOG), large-scale data set for goingdeeperinimageunderst and ing. 34(6):1\u201316,2015. ar Xivpreprintar Xiv:1711.06475,2017. [25] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vi TPose: Simple vision trans for mer baselines for human",
      "start_pos": 7854,
      "end_pos": 8366
    },
    {
      "chunk_id": 375,
      "paper_id": "humansin4d",
      "text": "Romero,Gerard Rui Liang,Wenjia Wang,Shipei Zhou,Guosen Lin,Yanwei Pons-Moll,and Michael JBlack. SMPL:Askinnedmulti- Fu,Yizhou Wang,and Yonggang Wang. AIChallenger: A personlinear model. ACMtransactionsongraphics(TOG), large-scale data set for goingdeeperinimageunderst and ing. 34(6):1\u201316,2015. ar Xivpreprintar Xiv:1711.06475,2017. [25] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vi TPose: Simple vision trans for mer baselines for human poseestimation. In Neur IPS,2022. [26] Yi Yang and Deva Ramanan. Articulated human detection withflexiblemixturesofparts. PAMI,2012. [27] Hongwen Zhang,Yating Tian,Yuxiang Zhang,Mengcheng Li,Liang An,Zhenan Sun,and Yebin Liu. Py MAF-X:To- wardswell-aligned full-body model regression from monoc- ularimages. PAMI,2023. [28] Hongwen Zhang,Yating Tian,Xinchi Zhou,Wanli Ouyang, Yebin Liu,Limin Wang,and Zhenan Sun. Py MAF:3 Dhu- manpose and shaperegression with pyramidalmeshalign- mentfeedbackloop. In ICCV,2021.",
      "start_pos": 8316,
      "end_pos": 8423
    },
    {
      "chunk_id": 376,
      "paper_id": "habitat",
      "text": "Habitat: A Platform for Embodied AI Research Manolis Savva 1,4*,Abhishek Kadian 1*,Oleksandr Maksymets 1*,Yili Zhao 1, Erik Wijmans 1,2,3,Bhavana Jain 1,Julian Straub 2,Jia Liu 1,Vladlen Koltun 5, Jitendra Malik 1,6,Devi Parikh 1,3,Dhruv Batra 1,3 1 Facebook AIResearch,2 Facebook Reality Labs,3 Georgia Instituteof Technology, 4 Simon Fraser University,5 Intel Labs,6 UCBerkeley https://aihabitat.org 1.Introduction Abstract Theembodimenthypothesisis the idea that intelligenceemerges intheinteractionofanagent with anenvironment and asaresult Wepresent Habitat,aplatform for researchinembodied ofsensorimotoractivity. artificialintelligence(AI).Habitatenablestrainingembod- Smith and Gasser[26] iedagents(virtualrobots)inhighlyefficientphotorealistic 3 Dsimulation. Specifically,Habitatconsistsof: Imagi new alkinguptoahomerobot and asking\u2018Hey\u2013 (i) Habitat-Sim: a flexible, high-per for mance 3 D sim- canyougocheckifmylaptopisonmydesk?Andifso,bring ulator with configurable agents, sensors, and generic 3 D ittome.\u2019 Inordertobesuccessful,sucharobotwouldneed dataseth and ling. Habitat-Simisfast\u2013whenrendering arangeofskills\u2013visualperception(torecognizescenes and a scene from Matterport 3 D, it achieves several thous and objects),languageunderst and ing(totranslatequestions and framespersecond(fps)runningsingle-threaded,andcan instructionsintoactions),andnavigationincomplexenviron- reachover 10,000 fpsmulti-processonasingle GPU. ments(tomove and findthingsinachangingenvironment). (ii)Habitat-API:amodularhigh-levellibrary for end-to- While there has been significant progress in the vision enddevelopmentofembodied AIalgorithms\u2013definingtasks andlanguagecommunitiesthankstorecentadvancesindeep (e.g.navigation,instructionfollowing,questionanswering), representations [14, 11], much of this progress has been configuring,training,andbenchmarkingembodiedagents. on\u2018internet AI\u2019ratherthanembodied AI.Thefocusof the formerispatternrecognitioninimages,videos,andtexton Theselarge-scaleengineeringcontributionsenableusto datasetstypicallycurated from the internet[10,18,4]. The answerscientificquestionsrequiringexperiments that were focusof the latteristoenableactionbyanembodiedagent tillnowimpracticableor\u2018merely\u2019impractical. Specifically, (e.g.arobot)inanenvironment.Thisbringsto the foreactive in the context of point-goal navigation: (1) we revisit the perception, long-termplanning, learning from interaction, comparisonbetweenlearning and SLAMapproaches from andholdingadialoggroundedinanenvironment. tworecentworks[20,16]andfindevidence for the oppo- siteconclusion\u2013thatlearningoutperforms SLAMifscaled Astraight for wardproposalistotrainagentsdirectlyin to an order of magnitude more experience than previous thephysicalworld\u2013exposing the mtoallitsrichness. This investigations, and (2) we conduct the first cross-dataset isvaluable and willcontinuetoplayanimportantrolein the generalizationexperiments{train,test}\u00d7{Matterport 3 D, developmentof AI.However,wealsorecognize that train- Gibson}formultiplesensors{blind,RGB,RGBD,D}and ingrobotsin the realworldisslow(therealworldrunsno find that onlyagents with depth(D)sensorsgeneralizeacross fasterthanrealtime and can notbeparallelized),dangerous datasets. Wehope that ouropen-sourceplatform and these (poorly-trainedagents can unwittinglyinjure the mselves,the findings will advanceresearchinembodied AI. environment,orothers),res our ceintensive(therobot(s)and theenvironment(s)inwhich the yexecutedem and res our ces andtime),difficulttocontrol(itishardtotestcorner-case scenarios as these are, by definition, infrequent and chal- lengingtorecreate),andnoteasilyreproducible(replicating conditionsacrossexperiments and institutionsisdifficult). *Denotesequalcontribution. Weaimtosupportacomplementaryresearchprogram: 9102 vo N 52 ]VC.sc[ 2 v 10210.4091:vi Xra Habitat Platform Tasks Habitat API Embodied QA Language grounding Interactive QA Vision-Language Navigation Visual Navigation (Das et al., 2018) (Hill et al., 2017) (Gordon et al., 2018) (Anderson et al., 2018) (Zhu et al., 2017, Gupta et al., 2017) Simulators Habitat Sim House 3 D AI 2-THOR MINOS Gibson CHALET (Wu et al., 2017) (Kolveet al., 2017) (Savva et al., 2017) (Zamir et al., 2018) (Yan et al., 2018) Generic dataset Datasets Support Replica(Straubet al., 2019) Matterport 3 D(Chang et al., 2017) 2 D-3 D-S(Armeniet al., 2017) Figure 1: The\u2018softw are stack\u2019for trainingembodiedagentsinvolves(1)datasetsproviding 3 Dassets with semanti can notations, (2) simulators that render the seassets and withinwhichanembodiedagentmaybesimulated,and(3)tasks that defineevaluatableproblems that enableustobenchmarkscientificprogress.Priorwork(highlightedinblueboxes)hascontributedavarietyof data sets,simulationsoftw are, and task definitions. Weproposeaunifiedembodiedagentstack with the Habitatplatform,includinggeneric data setsupport,ahighly per for mantsimulator(Habitat-Sim),andaflexible API(Habitat-API)allowing the definition and evaluationofabroadsetoftasks. trainingembodiedagents(e.g.virtualrobots)inrichrealistic question answering), configuring and training embodied simulators and thentransferring the learnedskillstoreality. agents(viaimitationorrein for cementlearning,orviaclassic Simulations have a long and rich history in science and SLAM),andbenchmarkingusingst and ardmetrics[2]. engineering(fromaerospacetozoology). Inthecontextof The Habitat architecture and implementation combine embodied AI,simulatorshelpovercome the aforementioned modularity and highper for mance. Whenrenderingascene challenges\u2013they can runordersofmagnitudefasterthan from the Matterport 3 D dataset, Habitat-Sim achieves real-time and can be parallelized over a cluster; training several thous and frames per second (fps) running single- in",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 377,
      "paper_id": "habitat",
      "text": "and ardmetrics[2]. engineering(fromaerospacetozoology). Inthecontextof The Habitat architecture and implementation combine embodied AI,simulatorshelpovercome the aforementioned modularity and highper for mance. Whenrenderingascene challenges\u2013they can runordersofmagnitudefasterthan from the Matterport 3 D dataset, Habitat-Sim achieves real-time and can be parallelized over a cluster; training several thous and frames per second (fps) running single- in simulation is safe, cheap, and enables fair comparison threaded, and can reach over 10,000 fps multi-process on andbenchmarkingofprogressinaconcertedcommunity- asingle GPU,whichisordersofmagnitudefasterthan the wideeffort. Onceapromisingapproachhas been developed closest simulator. Habitat-API allows us to train and and tested in simulation, it can be transferred to physical benchmarkembodiedagents with differentclassesofmeth- plat for msthatoperatein the realworld[6,15]. ods and indifferent 3 Dscene data sets. Datasets have beenakeydriverofprogressincomputer Theselarge-scaleengineeringcontributionsenableusto vision, NLP, and other areas of AI [10, 18, 4, 1]. As the answerscientificquestionsrequiringexperiments that were communitytransitionstoembodied AI,webelieve that sim- tillnowimpracticableor\u2018merely\u2019impractical. Specifically, ulators will assume the roleplayedpreviouslyby data sets. in the context of point-goal navigation [2], we make two Tosupport this transition,weaimtost and ardize the entire scientificcontributions: \u2018softw are stack\u2019 for training embodied agents (Figure 1): 1. We revisit the comparison between learning and scanning the world and creatingphotorealistic 3 Dassets,de- SLAMapproaches from tworecentworks[20,16]andfind veloping the nextgenerationofhighlyefficient and paralleliz- evidence for the opposite conclusion \u2013 that learning out- ablesimulators,specifyingembodied AItasks that enable performs SLAM if scaled to an order of magnitude more us to benchmark scientific progress, and releasing modu- experiencethanpreviousinvestigations. larhigh-levellibraries for training and deployingembodied 2. Weconduct the firstcross-datasetgeneralizationexper- agents. Specifically,Habitatconsistsof the following: iments{train,test}\u00d7{Matterport 3 D,Gibson}formultiple 1. Habitat-Sim: a flexible, high-per for mance 3 D sensors{Blind 1,RGB,RGBD,D}\u00d7{GPS+Compass}and simulator with configurable agents, multiple sensors, and find that onlyagents with depth(D)sensorsgeneralizewell generic 3 Ddataseth and ling(withbuilt-insupport for Mat- across data sets. terport 3 D,Gibson,and Replica data sets). Wehope that ouropen-sourceplatform and the sefindings 2. Habitat-API:amodularhigh-levellibrary for end- willadvance and guidefutureresearchinembodied AI. to-enddevelopmentofembodied AIalgorithms\u2013defining embodied AItasks(e.g.navigation, instructionfollowing, 1 Blindreferstoagents with novisualsensoryinputs. 2.Related Work Realityissomethingy our iseabove. Liza Minnelli Theavailabilityoflarge-scale 3 Dscene data sets[5,27,8] andcommunityinterestinactivevision task sledto are cent surgeofwork that resultedin the developmentofavariety ofsimulationplatforms for indoorenvironments[17,7,13, 24,29,3,30,31,23]. Theseplat for msvary with respectto the 3 Dscene data the yuse,theembodiedagent task sthey Figure 2:Examplerenderedsensorobservations for threesensors (colorcamera,depthsensor,semanticinstancemask)intwodiffer- address,andtheevaluationprotocols the yimplement. entenvironment data sets. AMatterport 3 D[8]environmentisin Thissurgeofactivityisboththrilling and alarming. On thetoprow,anda Replica[28]environmentin the bottomrow. theoneh and,itisclearlyasignof the interestinembodied AIacrossdiverseresearchcommunities(computervision, naturallanguageprocessing,robotics,machinelearning).On \u2013 thousands vs. one hundred frames per second \u2013 allows theo the rhand,theexistenceofmultipledifferingsimulation us to evaluate agents that have been trained with signifi- environments can causefragmentation,replicationofeffort, cantlylargeramountsofexperience(75 millionstepsvs.five anddifficultyinreproduction and community-wideprogress. million steps). The trends we observe demonstrate that Moreover,existingsimulatorsexhibitseveralshortcomings: learnedagents can begintomatch and outper for mclassical \u2013 Tightcouplingof task(e.g.navigation),simulationplat- approacheswhenprovided with largeamountsoftraining form(e.g.Gibson Env),and 3 Ddataset(e.g.Gibson). Ex- experience.Otherrecentworkby Koijima and Deng[16]has periments with multiple task sor data sets are impractical. alsocomparedh and-engineerednavigationagentsagainst \u2013 Hard-codedagentconfiguration(e.g.size,action-space). learnedagentsbut the irfocusisondefiningadditionalmet- Ablationsofagentparameters and sensortypes are not ricstocharacterize the per for manceofagents and toestablish supported,makingresultshardtocomp are. measuresofhardness for navigationepisodes. Toourknowl- \u2013 Suboptimalrendering and simulationper for mance. Most edge,ourexperiments are the firsttotrainnavigationagents existingindoorsimulatorsoperateatrelativelylowframe provided with multi-month experience in realistic indoor rates (10-100 fps), becoming a bottleneck in training environments and contrast the magainstclassicalmethods. agents and makinglarge-scalelearninginfeasible. Take- awaymessages from suchexperimentsbecomeunreliable 3.Habitat Platform \u2013hasthelearningconvergedtotrust the comparisons? \u2013 Limitedcontrolofenvironmentstate. Thestructureof the",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 378,
      "paper_id": "habitat",
      "text": "navigationepisodes. Toourknowl- \u2013 Suboptimalrendering and simulationper for mance. Most edge,ourexperiments are the firsttotrainnavigationagents existingindoorsimulatorsoperateatrelativelylowframe provided with multi-month experience in realistic indoor rates (10-100 fps), becoming a bottleneck in training environments and contrast the magainstclassicalmethods. agents and makinglarge-scalelearninginfeasible. Take- awaymessages from suchexperimentsbecomeunreliable 3.Habitat Platform \u2013hasthelearningconvergedtotrust the comparisons? \u2013 Limitedcontrolofenvironmentstate. Thestructureof the What Icannotcreate Idonotunderst and. 3 Dsceneintermsofpresentobjects can notbeprogram- Richard Feynman maticallymodified(e.g.totest the robustnessofagents). Mostcritically,workbuiltontopofanyof the existing plat for msishardtoreproduceindependently from the plat- Thedevelopmentof Habitatisalong-termef for ttoen- form, and thus hard to evaluate against work based on a able the formation of a common task framework [12] for differentplatform,evenincaseswhere the targettasks and researchintoembodiedagents,therebysupportingsystem- datasets are the same. Thisstatusquoisundesirableandmo- aticresearchprogressin this area. tivates the Habitateffort. Weaimtolearn from the successes Designrequirements. Theissuesdiscussedin the previous ofpreviousframeworks and developaunifyingplat for mthat sectionleadustoasetofrequirements that weseektofulfill. combines their desirable characteristics while addressing \u2013 Highly per for mant rendering engine: resource- their limitations. A common, unifying platform can sig- efficientrenderingengine that can producemultiplechan- nifi can tlyaccelerateresearchbyenablingcodere-useand nels of visual information (e.g. RGB, depth, semantic consistentexperimentalmethodology. Moreover,acommon instancesegmentation,surfacenormals,opticalflow)for plat for menablesustoeasilycarryoutexperimentstesting multipleconcurrentlyoperatingagents. agents base dondifferentparadigms(learnedvs. classical) \u2013 Scene data setingestion API:makes the plat for magnos- andgeneralizationofagentsbetween data sets. ticto 3 Dscene data sets and allowsuserstouse the irown The experiments we carry out contrasting learned and datasets. classicalapproachestonavigation are similarto the recent \u2013 Agent API: allows users to specify parameterized em- work of Mishkin et al. [20]. However, the per for mance bodiedagents with well-definedgeometry,physics,and of the Habitat stack relative to MINOS [24] used in [20] actuationcharacteristics. \u2013 Sensorsuite API:allowsspecificationofarbitrarynum- 1 process 5 processes bersofparameterizedsensors(e.g.RGB,depth,contact, Sensors/Resolution 128 256 512 128 256 512 GPS,compasssensors)attachedtoeachagent. RGB 4,093 1,987 848 10,592 3,574 2,629 \u2013 Scenario and task API: allows portable definition of RGB+depth 2,050 1,042 423 5,223 1,774 1,348 tasks and the irevaluationprotocols. \u2013 Implementation: C++ backend with Python API and Table 1: Per for mance of Habitat-Sim in frames per second interoperation with commonlearningframeworks,mini- foranexample Matterport 3 Dscene(id 17 DRP 5 sb 8 fy)onan Intel mizesentrythreshold. Xeon E 5-2690 v 4 CPUand Nvidia Titan Xp GPU,measuredat \u2013 Containerization:enablesdistributedtraininginclusters differentframeresolutions and withavaryingnumberofconcur- rentsimulatorprocessessharing the GPU.See the supplement for andremote-serverevaluationofuser-providedcode. additionalbenchmarkingresults. \u2013 Humans-as-agents: allowshumanstofunctionasagents insimulationinordertocollecthumanbehaviorandin- vestigatehuman-agentorhuman-humaninteractions. three data setsbysimplyspecifyingadifferentinputscene. \u2013 Environment state manipulation: programmatic con- Per for mance. Habitat-Sim achieves thousands of trolof the environmentconfigurationintermsoftheob- framespersecondpersimulatorthread and isordersofmag- jects that are present and the irrelativelayout. nitude faster than previous simulators for realistic indoor Designoverview. Theabovedesignrequirementscutacross environments(whichtypicallyoperateattensorhundredsof severallayersin the\u2018softw are stack\u2019in Figure 1. Amono- framespersecond)\u2013see Table 1 forasummary and the sup- lithicdesignisnotsuitable for addressingrequirementsat plement for moredetails. Bycomparison,AI 2-THOR[17] alllevels. We,therefore,structure the Habitatplat for mto and CHALET[31]runattensoffps,MINOS[24]and Gib- mirror this multi-layerabstraction. son[30]runataboutahundred,and House 3 D[29]runsat Atthelowestlevelis Habitat-Sim, aflexible, high- about 300 fps. Habitat-Simis 2-3 ordersofmagnitude per for mance 3 Dsimulator,responsible for loading 3 Dscenes faster. Byoperatingat 10,000 framespersecondweshift intoast and ardizedscene-graphrepresentation,configuring thebottleneck from simulationtooptimization for network agents with multiplesensors,simulatingagentmotion,and training. Basedon Tensor Flowbenchmarks,manypopular returning sensory data from an agent\u2019s sensor suite. The network architectures run at frame rates that are 10-100 x sensorabstractionin Habitatallowsadditionalsensorssuch loweronasingle GPU 3.Inpractice,wehaveobserved that as LIDAR and IMUtobeeasilyimplementedasplugins. itisoftenfastertogenerateimagesusing Habitat-Sim Generic 3 D dataset API using scene graphs. thantoloadimages from disk. Habitat-Sim employs",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 379,
      "paper_id": "habitat",
      "text": "with multiplesensors,simulatingagentmotion,and training. Basedon Tensor Flowbenchmarks,manypopular returning sensory data from an agent\u2019s sensor suite. The network architectures run at frame rates that are 10-100 x sensorabstractionin Habitatallowsadditionalsensorssuch loweronasingle GPU 3.Inpractice,wehaveobserved that as LIDAR and IMUtobeeasilyimplementedasplugins. itisoftenfastertogenerateimagesusing Habitat-Sim Generic 3 D dataset API using scene graphs. thantoloadimages from disk. Habitat-Sim employs a hierarchical scene graph Efficient GPU throughput. Currently, frames rendered torepresentallsupported 3 Denvironment data sets,whether by Habitat-Sim are exposedas Pythontensorsthrough syn the tic or based on real-world reconstructions. The shared memory. Future development will focus on even use of a uniform scene graph representation allows us to higher rendering efficiency by entirely avoiding GPU-to- abstract the detailsofspecific data sets,andtotreat the mina CPUmemorycopyoverheadthrough the useof CUDA-GL consistentfashion. Scenegraphsallowustocompose 3 D interoperationanddirectsharingofrenderbuffers and tex- environmentsthroughproceduralscenegeneration,editing, turesastensors. Ourpreliminaryinternaltestingsuggests orprogrammaticmanipulation. that this can leadtoaspeedupbyafactorof 2. Renderingengine. The Habitat-Simbackendmodule Above the simulationbackend,the Habitat-APIlayer isimplementedin C++andleverages the Magnumgraphics isamodularhigh-levellibrary for end-to-enddevelopment middlew are library 2 tosupportcross-plat for mdeployment inembodied AI.Settingupanembodied task involvesspeci- on a broad variety of hardw are configurations. The simu- fyingobservations that maybeusedby the agent(s),using latorbackendemploysanefficientrenderingpipeline that environmentin for mationprovidedby the simulator,andcon- implements visual sensor frame rendering using a multi- necting the information with atask-specificepisode data set. attachment\u2018uber-shader\u2019combiningoutputs for colorcam- \u2013 Task: this class extends the simulator\u2019s erasensors,depthsensors,andsemanticmasksensors. By Observations class and action space with task- allowingalloutputstobeproducedinasinglerenderpass, specific ones. The criteria of episode termination and weavoidadditionaloverheadwhensensorparameters are measures of success are provided by the Task. For shared and the samerenderpass can beused for alloutputs. example, in goal-driven navigation, Task provides Figure 2 showsexamplesofvisualsensorsrenderedinthree the goal and evaluation metric [2]. To support this different supported datasets. The same agent and sensor kind of functionality the Task has read-only access to configurationwasinstantiatedinascene from eachof the 3 https://www.tensorflow.org/guide/per for mance/ 2 https://magnum.graphics/ benchmarks Simulator and Episode-dataset. continuousstatespace 4 andmotion can producecollisions \u2013 Episode: aclass for episodespecification that includes resultinginpartial(orno)progressalong the directionin- theinitialposition and orientationofan Agent,sceneid, tended \u2013 simply put, it is possible for the agent to \u2018slide\u2019 goalposition,andoptionallytheshortestpathto the goal. alongawallorobstacle. Crucially,theagentmaychoose Anepisodeisadescriptionofaninstanceof the task. move_forward(0.25 m)andendupinalocationthatis \u2013 Environment: thefundamentalenvironmentconcept not 0.25 mforwardofwhereitstarted;thus,odometryisnot for Habitat, abstracting all the information needed for trivialevenin the absenceofactuationnoise. workingonembodiedtasks with asimulator. Goalspecification: staticordynamic? Oneconspicuous More details about the architecture of the Habitat plat- underspecificationin the Point Goal task[2]iswhe the rthe form,per for mancemeasurements,andexamplesof APIuse goalcoordinates are static(i.e.providedonceat the startof areprovidedin the supplement. theepisode)ordynamic(i.e.providedateverytimestep). The for merismorerealistic\u2013itisdifficulttoimagineareal taskwhereanoraclewouldprovideprecisedynamicgoalco- 4.Point Goal Navigationat Scale ordinates.However,intheabsenceofactuationnoise and col- lisions,everysteptakenby the agentresultsinaknownturn To demonstrate the utility of the Habitat platform de- ortranslation,and this combined with the initialgoalloca- sign,wecarryoutexperimentstotest for generalizationof tionisfunctionallyequivalenttodynamicgoalspecification. goal-directedvisualnavigationagentsbetween data setsof Wehypo the size that thisiswhyrecentworks[16,20,13] differentenvironments and tocomp are the per for manceof useddynamicgoalspecification. Wefollow and prescribe learning-basedagentsagainstclassicagentsas the amount thefollowingconceptualdelineation\u2013asatask,weadopt ofavailabletrainingexperienceisincreased. static Point Goalnavigation;asfor the sensorsuite,weequip Taskdefinition. we usethe Point Goal task(asdefinedby ouragents with anidealized GPS+Compasssensor. Thisori- Andersonetal.[2])asourexperimentaltestbed. Thistaskis entsustowards are alistic task(static Point Goalnavigation), ostensiblysimpletodefine\u2013anagentisinitialize data ran- disentanglessimulatordesign(actuationnoise,collisiondy- domstartingposition and orientationinanenvironment and namics)from the taskdefinition,andallowsustocomp are askedtonavigatetotargetcoordinates that are providedrela- techniques by sensors used (RGB, depth, GPS, compass, tiveto the agent\u2019sposition;noground-truthmapisavailable contactsensors). and the agent must only use its sensory input to navigate. Sensoryinput. Theagents are endowed with asinglecolor However, in the",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 380,
      "paper_id": "habitat",
      "text": "Point Goalnavigation), ostensiblysimpletodefine\u2013anagentisinitialize data ran- disentanglessimulatordesign(actuationnoise,collisiondy- domstartingposition and orientationinanenvironment and namics)from the taskdefinition,andallowsustocomp are askedtonavigatetotargetcoordinates that are providedrela- techniques by sensors used (RGB, depth, GPS, compass, tiveto the agent\u2019sposition;noground-truthmapisavailable contactsensors). and the agent must only use its sensory input to navigate. Sensoryinput. Theagents are endowed with asinglecolor However, in the course of experiments, we realized that visionsensorplace data heightof 1.5 mfrom the centerof this task leavesspace for subtlechoices that(a)canmakea theagent\u2019sbase and orientedtoface\u2018forward\u2019. Thissensor signifi can tdifferenceinexperimentaloutcomes and(b)are provides RGBframesat are solutionof 2562 pixels and with either not specified or inconsistent across papers, making afieldofviewof 90 degrees. Inaddition,anidealizeddepth comparisondifficult. Weattempttobeasdescriptiveaspos- sensorisavailable,inthesameposition and orientationas sibleabout the seseeminglylow-levelchoices;wehope the the color vision sensor. The field of view and resolution Habitatplat for mwillhelpironout the seinconsistencies. ofthedepthsensormatchthoseof the colorvisionsensor. Agentembodiment and actionspace. Theagentisphysi- We designate agents that make use of the color sensor by callyembodiedasacylindricalprimitiveshape with diame- RGB,agents that makeuseof the depthsensorby Depth, ter 0.2 mandheight 1.5 m. Theactionspaceconsistsoff our and agents that make use of both by RGBD. Agents that actions: turn_left, turn_right, move_forward, use neither sensor are denoted as Blind. All agents are and stop. These actions are mapped to idealized actua- equipped with an idealized GPS and compass \u2013 i.e., they tions that result in 10 degree turns for the turning actions haveaccessto the irlocationcoordinates,andimplicitlytheir andlineardisplacementof 0.25 mforthemove_forward orientationrelativeto the goalposition. action. Thestopactionallows the agenttosignalthatit Episode specification. We initialize the agent at a start- hasreached the goal. Habitatsupportsnoisyactuationsbut ingposition and orientation that are sampleduni for mlyat experiments in this paper are conducted in the noise-free random from allnavigablepositionsonthefloorof the envi- settingas our analysisfocusesono the rfactors. ronment. Thegoalpositionischosensuch that itlieson the Collisiondynamics. Somepreviousworks[3]useacoarse samefloor and thereexistsanavigablepath from the agent\u2019s irregularnavigationgraphwhereanagenteffectively\u2018tele- startingposition. During the episode,theagentisallowedto ports\u2019fromonelocationtoanother(1-2 mapart). Others[9] takeupto 500 actions. Thisthresholdsignifi can tlyexceeds useafine-grainedregulargrid(0.01 mresolution)where the thenumberofstepsanoptimalagentrequirestoreachall agentmovesonunoccupiedcells and the rearenocollisions goals (see the supplement). After each action, the agent or partial steps. In Habitat and our experiments, we use amorerealisticcollision model\u2013theagentnavigatesina 4 Uptomachineprecision. receivesasetofobservations from the activesensors. differenceofstaticgoal and dynamic GPScoordinates). Evaluation. Anavigationepisodeisconsideredsuccessful \u2013 Forwardonlyalwayscallsthemove_forwardaction, ifandonlyif the agentissuesastopactionwithin 0.2 mof andcalls the stopactionwhenwithin 0.2 mof the goal. thetargetcoordinates,asmeasuredbyageodesicdistance \u2013 Goalfollowermovestowards the goaldirection. Ifitis alongtheshortestpath from the agent\u2019spositionto the goal not facing the goal (more than 15 degrees off-axis), it position. Iftheagenttakes 500 actions with out the above performsturn_leftorturn_righttoalignitself; conditionbeingmet the episodeends and isconsideredun- otherwise,itcallsmove_forward. Theagentcalls the successful. Per for mance is measured using the \u2018Success stopactionwhenwithin 0.2 mof the goal. weightedby Path Length\u2019(SPL)metric[2]. Foranepisode \u2013 RL(PPO)isanagenttrained with rein for cementlearn- wherethegeodesicdistanceof the shortestpathisl and the ing,specificallyproximalpolicyoptimization[25]. We agenttraversesadistancep,SPLisdefinedas S\u00b7l/max(p,l), experiment with RLagentsequipped with differentvisual where S isabinaryindicatorofsuccess. sensors: no visual input (Blind), RGB input, Depth input,and RGB with depth(RGBD).The model consists Episode data setpreparation. Wecreate Point Goalnaviga- ofa CNN that producesanembedding for visualinput, tionepisode-datasets for Matterport 3 D[8]and Gibson[30] whichtogether with the relativegoalvectorisusedbyan scenes. For Matterport 3 Dwefollowed the publiclyavailable actor(GRU)andacritic(linearlayer). The CNNhas the train/val/testsplits. Note that asinrecentworks[9,20,16], following architecture: {Conv 8\u00d78, Re LU, Conv 4\u00d74, thereisnooverlapbetweentrain,val,andtestscenes. For Re LU,Conv 3\u00d73,Re LU,Linear,Re LU}(seesupplement Gibsonscenes,weobtainedtextured 3 Dsurfacemeshes from fordetails). Letr denote the rewardattimestept,d be the Gibsonauthors[30],manuallyannotatedeachsceneon t t the geodesic distance to goal at timestep",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 381,
      "paper_id": "habitat",
      "text": "For Matterport 3 Dwefollowed the publiclyavailable actor(GRU)andacritic(linearlayer). The CNNhas the train/val/testsplits. Note that asinrecentworks[9,20,16], following architecture: {Conv 8\u00d78, Re LU, Conv 4\u00d74, thereisnooverlapbetweentrain,val,andtestscenes. For Re LU,Conv 3\u00d73,Re LU,Linear,Re LU}(seesupplement Gibsonscenes,weobtainedtextured 3 Dsurfacemeshes from fordetails). Letr denote the rewardattimestept,d be the Gibsonauthors[30],manuallyannotatedeachsceneon t t the geodesic distance to goal at timestep t, s a success itsreconstructionquality(small/bigholes,floating/irregular rewardand\u03bbatimepenalty(toenc our ageefficiency). All surfaces,poortextures),andcuratedasubsetof 106 scenes models were trained with the followingrewardfunction: (outof 572);see the supplement for details.Anepisodeisde- finedbytheuniqueidof the scene,thestartingposition and (cid:40) orientationof the agent,and the goalposition. Additional r = s+d t\u22121 \u2212d t +\u03bb ifgoalisreached t meta data such as the geodesic distance along the shortest d \u2212d +\u03bb otherwise t\u22121 t path(GDSP)fromstartpositiontogoalpositionisalsoin- cluded. While generating episodes, we restrict the GDSP In our experiments s is set to 10 and \u03bb is set to \u22120.01. to be between 1 m and 30 m. An episode is trivial if there Note that rewards are onlyprovidedintrainingenviron- isanobstacle-freestraightlinebetween the start and goal ments;the task ischallengingas the agentmustgeneralize positions. A good measure of the navigation complexity tounseentestenvironments. of an episode is the ratio of GDSP to Euclidean distance \u2013 SLAM[20]isanagentimplementingaclassicrobotics betweenstart and goalpositions(notice that GDSP can only navigationpipeline(includingcomponents for localiza- be larger than or equal to the Euclidean distance). If the tion,mapping,andplanning),using RGB and depthsen- ratioisnearly 1,there are fewobstacles and the episodeis sors.we use the classicagentby Mishkinetal.[20]which easy;iftheratioismuchlargerthan 1,theepisodeisdifficult leverages the ORB-SLAM 2 [21] localization pipeline, becausestrategicnavigationisrequired. Tokeep the navi- withthesameparametersasreportedin the originalwork. gationcomplexityof the precomputedepisodesreasonably high,weperformrejectionsampling for episodes with the training procedure. Whentraininglearning-basedagents, aboveratiofallingin the range[1,1.1]. Following this,there wefirstdividethescenesin the trainingsetequallyamong isasignifi can tdecreasein the numberofnear-straight-line 8(Gibson),6(Matterport 3 D)concurrentlyrunningsimula- episodes (episodes with a ratio in [1,1.1]) \u2013 from 37% to torworkerthreads. Eachthreadestablishesblocksof 500 10%forthe Gibson data setgeneration. Thisstepwasnot trainingepisodes for eachsceneinitstrainingsetpartition per for medinanypreviousstudies. Wefind that with outthis andshufflestheorderingof the seblocks. Trainingcontinues filtering, all metrics appear inflated. Gibson scenes have throughshuffledcopiesof this array.Wedonothardcode the smallerphysicaldimensionscomp are dto the Matterport 3 D stopactiontoretaingenerality and allow for comparison scenes. Thisisreflectedin the resulting Point Goal data set\u2013 withfuturework that doesnotassume GPSinputs. Forthe average GDSPofepisodesin Gibsonscenesissmallerthan experimentsreportedhere,wetrainuntil 75 millionagent thatof Matterport 3 Dscenes. steps are accumulated across all worker threads. This is Baselines. Wecomp are the following base lines: 15 x larger than the experience used in previous investiga- \u2013 Random chooses an action randomly among tions[20,16]. Trainingagentsto 75 millionstepstook(in turn_left, turn_right, and move_forward sum over all three datasets): 320 GPU-hours for Blind, with uniform distribution. The agent calls the stop 566 GPU-hours for RGB,475 GPU-hours for Depth,and actionwhenwithin 0.2 mof the goal(computedusing the 906 GPU-hours for RGBD(overall 2267 GPU-hours). 1.0 0.8 0.6 0.4 0.2 0 10 20 30 40 50 60 70 Number of training steps taken (experience) in million LPS Per for mance on Gibson validation split 1.0 0.8 0.6 0.4 RGB Depth RGBD 0.2 Blind SLAM 0 10 20 30 40 50 60 70 Number of training steps taken (experience) in million LPS Per for mance on Matterport 3 D validation split RGB Depth RGBD Blind SLAM Figure 3: Average SPLofagentsonthevalsetover the courseoftraining. Previouswork[20,16]hasanalyzedper for manceat 5-10 millionsteps.Interestingtrendsemerge with moreexperience:i)Blindagentsinitiallyoutperform RGBand RGBDbutsaturatequickly; ii)Learning-based Depthagentsoutper for mclassic SLAM.Theshaded are",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 382,
      "paper_id": "habitat",
      "text": "20 30 40 50 60 70 Number of training steps taken (experience) in million LPS Per for mance on Matterport 3 D validation split RGB Depth RGBD Blind SLAM Figure 3: Average SPLofagentsonthevalsetover the courseoftraining. Previouswork[20,16]hasanalyzedper for manceat 5-10 millionsteps.Interestingtrendsemerge with moreexperience:i)Blindagentsinitiallyoutperform RGBand RGBDbutsaturatequickly; ii)Learning-based Depthagentsoutper for mclassic SLAM.Theshaded are asaroundcurvesshow the standarderrorof SPLoverfiveseeds. Gibson MP 3 D Matterport 3 D).All RL(PPO)agentsstartout with farworse SPL, but RL (PPO) Depth, in particular, improves dra- Sensors Baseline SPL Succ SPL Succ matically and matches the classic base lineatapproximately Random 0.02 0.03 0.01 0.01 10 Mframes(Gibson)or 30 Mframes(Matterport 3 D)ofex- Forwardonly 0.00 0.00 0.00 0.00 perience, continuing to improve thereafter. Notice that if Blind Goalfollower 0.23 0.23 0.12 0.12 weterminated the experimentat 5 Mframesasin[20]we RL(PPO) 0.42 0.62 0.25 0.35 wouldalsoconclude that SLAM[20]dominates. Interest- ingly, RGBagentsdonotsignifi can tlyoutperform Blind RGB RL(PPO) 0.46 0.64 0.30 0.42 agents;wehypo the sizebecauseboth are equipped with GPS Depth RL(PPO) 0.79 0.89 0.54 0.69 sensors. Indeed,qualitativeresults(Figure 4 andvideoin RL(PPO) 0.70 0.80 0.42 0.53 supplement) suggest that Blind agents \u2018hug\u2019 walls and RGBD SLAM[20] 0.51 0.62 0.39 0.47 implement\u2018wallfollowing\u2019heuristics. Incontrast,RGBsen- sorsprovideahigh-dimensionalcomplexsignal that maybe pronetooverfittingtotrainenvironmentsdueto the variety Table 2:Per for manceof base linemethodson the Point Goal task[2] testedon the Gibson[30]and MP 3 D[8]testsetsundermultiple acrossscenes(evenwithin the same data set). Wealsonotice sensorconfigurations.RLmodels have beentrained for 75 million in Figure 3 thatallmethodsper for mbetteron Gibsonthan steps.Wereportaveragerateofepisodesuccess and SPL[2]. Matterport 3 D.Thisisconsistent with ourpreviousanalysis that Gibsoncontainssmallerscenes and shorterepisodes. 5.Results and Findings Next, for each agent and dataset, we select the best- per for mingcheckpointonvalidation and reportresultson We seek to answer two questions: i) how do learning- testin Table 2.Weobserve that uni for mlyacross the datasets, based agents comp are to classic SLAM and hand-coded RL(PPO)Depthper for msbest,outper for ming RL(PPO) baselinesas the amountoftrainingexperienceincreases and RGBD(by 0.09-0.16 SPL),SLAM(by 0.15-0.28 SPL),and ii)howwelldolearnedagentsgeneralizeacross 3 Ddatasets. RGB(by 0.13-0.33 SPL)inthatorder(see the supplement for Itshould betacitlyunderstood,buttobeexplicit\u2013\u2018learn- additionalexperimentsinvolvingnoisydepth). Webelieve ing\u2019and\u2018SLAM\u2019arebroadfamiliesoftechniques(andnot Depthper for msbetterthan RGBDbecausei)the Point Goal a single method), are not necessarily mutually exclusive, navigation task requiresreasoningonlyaboutfreespace and and are not\u2018settled\u2019intheirdevelopment. Wecomp are rep- depth provides relevant information directly, and ii) RGB resentative instances of these families to gain insight into hassignifi can tlymoreentropy(differenthouseslookvery questionsofscaling and generalization,anddonotmakeany different),thusitiseasiertooverfitwhenusing RGB.Weran claimsaboutintrinsicsuperiorityofoneor the other. ourexperiments with 5 randomseedsperrun,toconfirm that Learning vs SLAM. To answer the first question we plot thesedifferences are statisticallysignificant. Thedifferences agentper for mance(SPL)onvalidation(i.e.unseen)episodes areaboutanorderofmagnitudelargerthan the standarddevi- over the courseoftrainingin Figure 3(top: Gibson,bottom: ationofaverage SPL for allcases(e.g.onthe Gibson data set Matterport 3 D). SLAM [20] does not require training and errors are,Depth: \u00b10.015,RGB:\u00b10.055,RGBD:\u00b10.028, thushasaconstantper for mance(0.59 on Gibson,0.42 on Blind: \u00b10.005). Random and forward-onlyagents have Gibson MP 3 D per for mance degradation, while the Blind agent is least Blind SPL=0.28 RGBSPL=0.57 Blind SPL=0.35 RGBSPL=0.88 affected(aswewouldexpect). Second, we find a potentially counter-intuitive trend \u2013 agentstrainedon Gibsonconsistentlyoutperform the ircoun- terpartstrainedon Matterport 3 D,evenwhenevaluatedon RGBDSPL=0.91 Depth SPL=0.98 RGBDSPL=0.90 Depth SPL=0.94 Matterport 3 D.Webelievethereasonis the previouslynoted observation that Gibsonscenes are smaller and episodes are shorter(lower GDSP)than Matterport 3 D.Gibsonagents are trainedon\u2018easier\u2019episodes and encounterpositivereward moreeasilyduringr and omexploration,thusbootstrapping learning. Consequently,forafixedcomputationbudget Gib- Figure 4:Navigationexamples for differentsensoryconfigurations of the RL (PPO) agent, visualizing trials from the Gibson and sonagents are strongeruniversally(notjuston Gibson).This",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 383,
      "paper_id": "habitat",
      "text": "RGBDSPL=0.90 Depth SPL=0.94 Matterport 3 D.Webelievethereasonis the previouslynoted observation that Gibsonscenes are smaller and episodes are shorter(lower GDSP)than Matterport 3 D.Gibsonagents are trainedon\u2018easier\u2019episodes and encounterpositivereward moreeasilyduringr and omexploration,thusbootstrapping learning. Consequently,forafixedcomputationbudget Gib- Figure 4:Navigationexamples for differentsensoryconfigurations of the RL (PPO) agent, visualizing trials from the Gibson and sonagents are strongeruniversally(notjuston Gibson).This MP 3 Dvalsets. Abluedot and reddotindicate the starting and findingsuggests that visualnavigationagentscouldbenefit goalpositions,and the bluearrowindicatesfinalagentposition. fromcurriculumlearning. Theblue-green-redlineis the agent\u2019strajectory.Colorshifts from Theseinsights are enabledby the engineeringof Habitat, bluetoredas the maximumnumberofagentstepsisapproached. whichmade the seexperimentsassimpleasachangein the See the supplementalmaterials for moreexampletrajectories. evaluation data setname. Gibson MP 3 D 6.Habitat Challenge Blind Gibson 0.42 0.34 MP 3 D 0.28 0.25 Nobattleplaneversurvivescontact with the enemy. RGB Gibson 0.46 0.40 MP 3 D 0.25 0.30 Helmuth Karl Bernhardvon Moltke Depth Gibson 0.79 0.68 MP 3 D 0.56 0.54 Challengesdriveprogress. Thehistoryof AIsub-fields RGBD Gibson 0.70 0.53 indicates that the formulation of the right questions, the MP 3 D 0.44 0.42 creationof the right data sets,and the coalescenceofcommu- Figure 5: Generalizationofagentsbetween data sets. Wereport nitiesaround the rightchallengesdrivesscientificprogress. average SPL for amodeltrainedon the source data setineachrow, Ourgoalistosupport this process for embodied AI.Habitat asevaluatedontestepisodes for the target data setineachcolumn. Challengeisanautonomousnavigationchallenge that aims to benchmark and advance efforts in goal-directed visual navigation. verylowper for mance,while the hand-codedgoalfollower Onedifficultyincreatingachallengearoundembodied AI and Blind base lineseemodestper for mance.See the sup- tasksis the transition from staticpredictions(asinpassive plement for additionalanalysisoftrainedagentbehavior. perception) to sequential decision making (as in sensori- In Figure 4 weplotexampletrajectories for the RL(PPO) motorcontrol). Intraditional\u2018internet AI\u2019challenges(e.g. agents,toqualitativelycontrasttheirbehaviorin the same Image Net[10],COCO[18],VQA[4]),itispossibletore- episode. Consistent with the aggregatestatistics,weobserve leaseastatictesting data set and askparticipantstosimply that Blindcollides with obstacles and followswalls,while upload the irpredictionson this set.Incontrast,embodied AI Depth is the most efficient. See the supplement and the taskstypicallyinvolvesequentialdecisionmaking and agent- video for moreexampletrajectories. drivencontrol,makingitinfeasibletopre-packageatesting Generalization across datasets. Our findings so far are dataset. Essentially,embodied AIchallengesrequirepartici- that RL(PPO)agentssignifi can tlyoutperform SLAM[20]. pantstouploadcodenotpredictions. Theuploadedagents This prompts our second question \u2013 are these findings can the nbeevaluatedinnovel(unseen)testenvironments. dataset specific or do learned agents generalize across Challenge infrastructure. We leverage the frontend and datasets? We report exhaustive comparisons in Figure 5 challengesubmissionprocessof the Eval AIplatform,and \u2013 specifically, average SPL for all combinations of {train, buildbackendinfrastructure our selves. Participantsin Habi- test} \u00d7 {Matterport 3 D, Gibson} for all agents {Blind, tat Challenge are asked to upload Docker containers [19] RGB,RGBD,Depth}. Rowsindicate(agent,trainset)pair, with the iragentsvia Eval AI.Thesubmittedagents are then columnsindicatetestset. Wefindanumberofinteresting evaluatedonalive AWSGPU-enabledinstance.Specifically, trends. First,nearlyallagentssufferadropinper for mance contestants are freetotraintheiragentshowever the ywish whentrainedonone data set and testedonanother,e.g.RGBD (any language, any framework, any infrastructure). In or- Gibson\u2192Gibson 0.70 vs RGBDGibson\u2192Matterport 3 D 0.53 dertoevaluate the seagents,participants are askedtoderive (drop of 0.17). RGB and RGBD agents suffer a significant froma base Habitat Dockercontainer and implementaspe- cificinterfaceto the irmodel\u2013agent\u2019sactiontakengivenan sensorsgeneralizewellbetweendifferent 3 Denvironment observation from the environmentateachstep. Thisdocker- datasetsincomparisontoagentsequipped with only RGB. izedinterfaceenablesrunning the participantcodeon new Feature roadmap. Our near-term development roadmap environments. willfocusonincorporatingphysicssimulation and enabling More details regarding the Habitat Challenge held at physics-based interaction between mobile agents and ob- CVPR 2019 areavailableat the https://aihabitat. jects in 3 D environments. Habitat-Sim\u2019s scene graph org/challenge/ website. In a future iteration of this representationiswell-suited for integration with physicsen- challengewe will introducethreemajordifferencesdesigned",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 384,
      "paper_id": "habitat",
      "text": "development roadmap environments. willfocusonincorporatingphysicssimulation and enabling More details regarding the Habitat Challenge held at physics-based interaction between mobile agents and ob- CVPR 2019 areavailableat the https://aihabitat. jects in 3 D environments. Habitat-Sim\u2019s scene graph org/challenge/ website. In a future iteration of this representationiswell-suited for integration with physicsen- challengewe will introducethreemajordifferencesdesigned gines,allowingustodirectlycontrol the stateofindividual tobothreduce the gapbetweensimulation and realityandto objects and agents with inascenegraph. Ano the rplanned increasethedifficultyof the task. avenueoffutureworkinvolvesproceduralgenerationof 3 D \u2013 Inthe 2019 challenge,therelativecoordinatesspecifying environmentsbyleveragingacombinationof 3 Dreconstruc- the goal were continuously updated during agent tion and virtualobject data sets. Bycombininghigh-quality movement\u2013essentiallysimulatinganagent with perfect reconstructions of large indoor spaces with separately re- localization and headingestimation(e.g. anagent with constructedor model ledobjects,we cantake full advantage an idealized GPS+Compass). However, high-precision ofourhierarchicalscenegraphrepresentationtointroduce localizationinindoorenvironments can notbeassumedin controlledvariationin the simulated 3 Denvironments. realisticsettings\u2013GPShaslowprecisionindoors,(visual) Lastly,weplantofocusondistributedsimulationsettings odometry may be noisy, SLAM-based localization can thatinvolvelargenumbersofagentspotentiallyinteracting fail,etc. Hence,wewillinvestiageonlyprovidingto the withoneano the rincompetitiveorcollaborativescenarios. agent a fixed relative coordinate for the goal position from the startlocation. Acknowledgments. Wethankthereviewers for the irhelp- fulsuggestions. The Habitatprojectwouldnot have been \u2013 Likewise, the 2019 Habitat Challenge modeled agent actions (e.g. forward, turn 10\u25e6 left,...) deter- possible with out the support and contributionsofmanyin- dividuals. Wearegratefulto Mandeep Baines,Angel Xuan ministically. However in real settings, agent intention Chang, Alexander Clegg, Devendra Singh Chaplot, Xin- (e.g.goforward 1 m)and the resultr are lymatchperfectly lei Chen,Wojciech Galuba,Georgia Gkioxari,Daniel Gor- \u2013actuationerror,differingsurfacematerials,andamyriad don,Leonidas Guibas,Saurabh Gupta,Jerry(Zhi-Yang)He, ofo the rsourcesoferrorintroducesignifi can tdriftovera Rishabh Jain,Or Litany,Joel Marcey,Dmytro Mishkin,Mar- longtrajectory. Tomodel this,weintroduceanoise model cus Rohrbach,Amanpreet Singh,Yuandong Tian,Yuxin Wu, acquired by benchmarking a real robotic platform [22]. Fei Xia,Deshraj Yadav,Amir Zamir,and Jiazhi Zhang for Visual sensing is an excellent means of combating this theirhelp. \u201cdead-reckoning\u201ddrift and thischangeallowsparticipants tostudymethodologies that are robustto and can correct Licenses for referenced data sets. for this noise. Gibson: https://storage.googleapis. \u2013 Finally,wewillintroducerealistic model sofsensornoise com/gibson_material/Agreement%20 GDS% for RGB and depthsensors\u2013narrowing the gapbetween 2006-04-18.pdf perceptualexperiencesagentswould have insimulation Matterport 3 D: http://kaldir.vc.in.tum.de/ andreality. matterport/MP_TOS.pdf. Welook for wardtosupporting the communityinestab- lishingabenchmarktoevaluate the state-of-the-artinmeth- ods for embodiednavigationagents. 7.Future Work Wedescribed the design and implementationof the Habi- tatplatform. Ourgoalistounifyexistingcommunityefforts andtoaccelerateresearchintoembodied AI.Thisisalong- termef for tthat will succeedonlyby full engagementof the broaderresearchcommunity. Experimentsenabledby the generic data setsupport and the high per for mance of the Habitat stack indicate that i)learning-basedagents can match and exceed the perfor- mance of classic visual navigation methods when trained forlongenoughandii)learnedagentsequipped with depth References [17] Eric Kolve,Roozbeh Mottaghi,Daniel Gordon,Yuke Zhu, Abhinav Gupta,and Ali Farhadi. AI 2-THOR:Aninteractive [1] Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana 3 Denvironment for visual AI. ar Xiv:1712.05474,2017. Ko\u0161eck\u00e1,and Alexander CBerg. Adataset for developing [18] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, andbenchmarkingactivevision. In ICRA,2017. Pietro Perona,Deva Ramanan,Piotr Doll\u00e1r,and C.Lawrence [2] Peter Anderson,Angel X.Chang,Devendra Singh Chaplot, Zitnick. Microsoft COCO:Commonobjectsincontext. In Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana ECCV,2014. Kosecka,Jitendra Malik,Roozbeh Mottaghi,Manolis Savva, [19] Dirk Merkel. Docker:Lightweight Linuxcontainers for con- and Amir Roshan Zamir. Onevaluationofembodiednaviga- sistentdevelopment and deployment. Linux Journal,2014. tionagents. ar Xiv:1807.06757,2018. [20] Dmytro Mishkin,Alexey Dosovitskiy,and Vladlen Koltun. [3] Peter Anderson,Qi Wu,Damien Teney,Jake Bruce,Mark Benchmarkingclassi can dlearnednavigationincomplex 3 D Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and environments. ar Xiv:1901.10915,2019. Antonvanden Hengel. Vision-and-languagenavigation:In- [21] Ra\u00fal Mur-Artal and Juan D. Tard\u00f3s. ORB-SLAM 2: An terpretingvisually-groundednavigationinstructionsinreal environments. In CVPR,2018.",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 385,
      "paper_id": "habitat",
      "text": "Linux Journal,2014. tionagents. ar Xiv:1807.06757,2018. [20] Dmytro Mishkin,Alexey Dosovitskiy,and Vladlen Koltun. [3] Peter Anderson,Qi Wu,Damien Teney,Jake Bruce,Mark Benchmarkingclassi can dlearnednavigationincomplex 3 D Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and environments. ar Xiv:1901.10915,2019. Antonvanden Hengel. Vision-and-languagenavigation:In- [21] Ra\u00fal Mur-Artal and Juan D. Tard\u00f3s. ORB-SLAM 2: An terpretingvisually-groundednavigationinstructionsinreal environments. In CVPR,2018. open-source SLAMsystem for monocular,stereo and RGB-D cameras. IEEETransactionson Robotics,33(5),2017. [4] Stanislaw Antol,Aishwarya Agrawal,Jiasen Lu,Margaret Mitchell,Dhruv Batra,C.Lawrence Zitnick,and Devi Parikh. [22] Adithyavairavan Murali,Tao Chen,Kalyan Vasudev Alwala, VQA:Visual Question Answering. In ICCV,2015. Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav [5] Iro Armeni,Ozan Sener,Amir R.Zamir,Helen Jiang,Ioannis Gupta. Pyrobot:Anopen-sourceroboticsframeworkforre- Brilakis,Martin Fischer,and Silvio Savarese. 3 Dsemantic search and benchmarking. ar Xivpreprintar Xiv:1906.08236, parsingoflarge-scaleindoorspaces. In CVPR,2016. 2019. [6] Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, [23] Xavier Puig,Kevin Ra,Marko Boben,Jiaman Li,Tingwu Richard Shen,Vinh-Dieu Lam,and Alex Kendall. Learning Wang,Sanja Fidler,and Antonio Torralba.Virtual Home:Sim- todrive from simulation with outrealworldlabels. In ICRA, ulatinghouseholdactivitiesviaprograms. In CVPR,2018. 2019. [24] Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, [7] Simon Brodeur,Ethan Perez,Ankesh Anand,Florian Golemo, Thomas Funkhouser, and Vladlen Koltun. MINOS: Mul- Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, timodalindoorsimulator for navigationincomplexenviron- and Aaron C.Courville. Ho ME:Ahouseholdmultimodal ments. ar Xiv:1712.03931,2017. environment. ar Xiv:1711.11017,2017. [25] John Schulman,Filip Wolski,Prafulla Dhariwal,Alec Rad- [8] Angel Chang,Angela Dai,Thomas Funkhouser,Maciej Hal- ford,and Oleg Klimov. Proximalpolicyoptimizationalgo- ber,Matthias Niessner,Manolis Savva,Shuran Song,Andy rithms. ar Xiv:1707.06347,2017. Zeng,and Yinda Zhang. Matterport 3 D:Learning from RGB- [26] Linda Smith and Michael Gasser. Thedevelopmentofem- Ddatainindoorenvironments. In International Conference bodiedcognition: Sixlessons from babies. Artificial Life, on 3 DVision(3 DV),2017. 11(1-2),2005. [9] Abhishek Das,Samyak Datta,Georgia Gkioxari,Stefan Lee, [27] Shuran Song,Fisher Yu,Andy Zeng,Angel XChang,Mano- Devi Parikh,and Dhruv Batra. Embodied Question Answer- lis Savva,and Thomas Funkhouser. Semanticscenecomple- ing. In CVPR,2018. tion from asingledepthimage. In CVPR,2017. [10] Jia Deng,Wei Dong,Richard Socher,Li-Jia Li,Kai Li,and [28] Julian Straub,Thomas Whelan,Lingni Ma,Yufan Chen,Erik Fei-Fei Li. Image Net: A large-scale hierarchical image Wijmans,Simon Green,Jakob J.Engel,Raul Mur-Artal,Carl data base. In CVPR,2009. Ren,Shobhit Verma,Anton Clarkson,Mingfei Yan,Brian [11] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina Budge,Yajie Yan,Xiaqing Pan,June Yon,Yuyang Zou,Kim- Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans- berly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, formers for language underst and ing. ar Xiv:1810.04805, Elias Mueggler,Luis Pesqueira,Manolis Savva,Dhruv Batra, 2018. Hauke M.Strasdat,Renzo De Nardi,Michael Goesele,Steven [12] David Donoho. 50 yearsof data science. In Tukey Centennial Lovegrove,and Richard Newcombe. The Replica data set:A Workshop,2015. digitalreplicaofindoorspaces. ar Xiv:1906.05797,2019. [13] Saurabh Gupta,James Davidson,Sergey Levine,Rahul Suk- [29] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. thankar,and Jitendra Malik. Cognitivemapping and planning Building generalizable agents with a realistic and rich 3 D forvisualnavigation. In CVPR,2017. environment. ar Xiv:1801.02209,2018. [14] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. [30] Fei Xia,Amir R.Zamir,Zhiyang He,Alexander Sax,Jiten- Deepresiduallearning for imagerecognition.In CVPR,2016. dra Malik, and Silvio Savarese. Gibson env: Real-world [15] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario perception for embodiedagents. In CVPR,2018. Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco [31] Claudia Yan,Dipendra Misra,Andrew Bennnett,Aaron Wals- Hutter. Learningagile and dynamicmotorskills for legged man,Yonatan Bisk,and Yoav Artzi.CHALET:Cornellhouse robots. Science Robotics,2019. agentlearningenvironment. ar Xiv:1801.07357,2018. [16] Noriyuki Kojima and Jia Deng. To learn or not to learn: Analyzing the roleoflearning for navigationinvirtualenvi- ronments. ar Xiv:1907.11770,2019. A.Habitat Platform Details \u2022 Flexible,structuredrepresentationof 3 Denvironments using Scene Graphs,allowing for programmaticma- Asdescribedin the mainpaper,Habitatconsistsof the",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 386,
      "paper_id": "habitat",
      "text": "and dynamicmotorskills for legged man,Yonatan Bisk,and Yoav Artzi.CHALET:Cornellhouse robots. Science Robotics,2019. agentlearningenvironment. ar Xiv:1801.07357,2018. [16] Noriyuki Kojima and Jia Deng. To learn or not to learn: Analyzing the roleoflearning for navigationinvirtualenvi- ronments. ar Xiv:1907.11770,2019. A.Habitat Platform Details \u2022 Flexible,structuredrepresentationof 3 Denvironments using Scene Graphs,allowing for programmaticma- Asdescribedin the mainpaper,Habitatconsistsof the nipulationofobjectstate,andcombinationofobjects followingcomponents: fromdifferentenvironments. \u2022 Habitat-Sim: a flexible, high-per for mance 3 D \u2022 High-efficiencyrenderingengine with multi-attachment simulator with configurable agents, multiple sensors, renderpasstoreduceoverhead for multiplesensors. and generic 3 D dataset handling (with built-in sup- \u2022 Arbitrary numbers of Agents and corresponding port for Matterport 3 D [8], Gibson [30], and other Sensors that can belinkedtoa 3 Denvironmentby datasets). Habitat-Simisfast\u2013whenrenderinga attachmenttoa Scene Graph. realistics can nedscene from the Matterport 3 Ddataset, Theper for manceof the simulationbackendsurpasses that Habitat-Simachievesseveralthous and framesper ofpriorworkoperatingonrealisticreconstruction data sets second (fps) running single-threaded, and can reach byalargemargin. Table 3 reportsper for mancestatisticson over 10,000 fpsmulti-processonasingle GPU. atestscene from the Matterport 3 Ddataset. Single-thread \u2022 Habitat-API:amodularhigh-levellibrary for end- per for mance reaches several thous and frames per second to-enddevelopmentofembodied AI\u2013definingembod- (fps),whilemulti-processoperation with severalsimulation ied AI tasks (e.g. navigation [2], instruction follow- backends can reach over 10,000 fps on a single GPU. In ing[3],questionanswering[9]),configuringembodied addition,byemploying Open GL-CUDAinteroperationwe agents(physicalform,sensors,capabilities),training enable direct sharing of rendered image frames with ML theseagents(viaimitationorrein for cementlearning, frameworkssuchas Py Torch with outameasurableimpact orviaclassic SLAM),andbenchmarking the irper for- on per for mance as the image resolution is increased (see manceon the defined task susingst and ardmetrics[2]. Figure 7). Habitat-APIcurrentlyuses Habitat-Simas the Habitat-API. The second layer of the Habitat platform coresimulator,butisdesigned with amodularabstrac- (Habitat-API) focuses on creating a general and flex- tion for the simulatorbackendtomaintaincompatibility ible API for definingembodiedagents,tasks that the ymay overmultiplesimulators. carryout,andevaluationmetrics for thosetasks. Whende- Key abstractions. The Habitat platform relies on a num- signingsuchan API,akeyconsiderationistoallow for easy berofkeyabstractions that model the domainofembodied extensibilityof the definedabstractions. Thisisparticularly agents and tasks that can becarriedoutinthree-dimensional importantsincemanyof the parametersofembodiedagent indoorenvironments. Hereweprovideabriefsummaryof tasks, specific agent configurations, and 3 D environment keyabstractions: setups can bevariedininterestingways. Futureresearchis \u2022 Agent: aphysicallyembodiedagent with asuiteof likelytopropose new tasks,newagentconfigurations,and Sensors.Canobserve the environment and iscapable new 3 Denvironments. oftakingactions that changeagentorenvironmentstate. The API allows for alternative simulator backends to \u2022 Sensor: associated with aspecific Agent, capable beused,beyond the Habitat-Simmodule that weimple- ofreturningobservation data from the environmentata mented.Thismodularityhas the advantageofallowingincor- specifiedfrequency. porationofexistingsimulatorbackendstoaidintransitioning \u2022 Scene Graph: ahierarchicalrepresentationofa 3 D fromexperiments that previousworkhasper for medusing environment that organizes the environment into re- legacyframeworks. Thearchitectureof Habitat-APIis gions and objectswhich can beprogrammaticallyma- illustratedin Figure 8,indicatingcore APIfunctionality and nipulated. functionalityimplementedasextensionsto the core. \u2022 Simulator: an instance of a simulator backend. Above the APIlevel,wedefineaconcreteembodied task Given actions for a set of configured Agents and suchasvisualnavigation. Thisinvolvesdefiningaspecific Scene Graphs,canupdate the stateof the Agents datasetconfiguration, specifying the structureofepisodes and Scene Graphs,andprovideobservations for all (e.g.numberofstepstaken,terminationconditions),training active Sensorspossessedby the Agents. curriculum(progressionofepisodes,difficultyramp),and These abstractions connect the different layers of the evaluationprocedure(e.g.testepisodesets and taskmetrics). platform.Theyalsoenablegeneri can dportablespecification Anexampleofloadingapre-configured task(Point Nav)and ofembodied AItasks. stepping through the environment with a random agent is Habitat-Sim.Thearchitectureof the Habitat-Simback- shownin the codebelow. end module is illustrated in Figure 6. The design of this 5 Note:Thesemanticsensorin Matterport 3 Drequiresusingadditional moduleensuresafewkeyproperties: 3 Dmeshes with signifi can tlymoregeometriccomplexity,leadingtore- \u2022 Memory-efficientmanagementof",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 387,
      "paper_id": "habitat",
      "text": "and taskmetrics). platform.Theyalsoenablegeneri can dportablespecification Anexampleofloadingapre-configured task(Point Nav)and ofembodied AItasks. stepping through the environment with a random agent is Habitat-Sim.Thearchitectureof the Habitat-Simback- shownin the codebelow. end module is illustrated in Figure 6. The design of this 5 Note:Thesemanticsensorin Matterport 3 Drequiresusingadditional moduleensuresafewkeyproperties: 3 Dmeshes with signifi can tlymoregeometriccomplexity,leadingtore- \u2022 Memory-efficientmanagementof 3 Denvironmentre- ducedper for mance. Weexpect this tobeaddressedinfutureversions, sources(trianglemeshgeometry,textures,shaders)en- leadingtospeedscomparableto RGB+depth. suringsharedres our ces are cached and reused. Resource Manager Simulator Agent Scene Manager Texture Material Shader Scene Graph Mesh Scene Node Sensor Figure 6:Architectureof Habitat-Simmainclasses.The Simulatordelegatesmanagementofallres our cesrelatedto 3 Denvironments toa Resource Manager that isresponsible for loading and caching 3 Denvironment data from avarietyofon-disk for mats.Theseres our ces areusedwithin Scene Graphsat the levelofindividual Scene Nodes that representdistinctobjectsorregionsinaparticular Scene.Agents andtheir Sensors are instantiatedbybeingattachedto Scene Nodesinaparticular Scene Graph. GPU\u2192CPU\u2192GPU GPU\u2192CPU GPU\u2192GPU Sensors/numberofprocesses 1 3 5 1 3 5 1 3 5 RGB 2,346 6,049 7,784 3,919 8,810 11,598 4,538 8,573 7,279 RGB+depth 1,260 3,025 3,730 1,777 4,307 5,522 2,151 3,557 3,486 RGB+depth+semantics 5 378 463 470 396 465 466 464 455 453 Table 3:Per for manceof Habitat-Siminframespersecond for anexample Matterport 3 Dscene(id 17 DRP 5 sb 8 fy)ona Xeon E 5-2690 v 4 CPUand Nvidia Titan Xp GPU,measure data frameresolutionof 128 x 128,underdifferentframememorytransferstrategies and witha varyingnumberofconcurrentsimulatorprocessessharing the GPU.\u2018GPU-CPU-GPU\u2019indicatespassingofrenderedframes from Open GL contextto CPUhostmemory and backto GPUdevicememory for useinoptimization,\u2018GPU-CPU\u2019onlyreportscopying from Open GL contextto CPUhostmemory,whereas\u2018GPU-GPU\u2019indicatesdirectsharingthrough Open GL-CUDAinteroperation. reporttheaveragegeodesicdistancealong the shortestpath (GDSP)betweenstartingpoint and goalposition. Asnoted inthemainpaper,Gibsonepisodes are signifi can tlyshorter than Matterport 3 D ones. Figure 9 visualizes the episode distributionsovergeodesicdistance(GDSP),Euclideandis- tancebetweenstart and goalposition, and the ratioof the two(anapproximatemeasureofcomplexity for the episode). Weagainnote that Gibsonepisodes have moreepisodes with shorterdistances,leadingto the datasetbeingoveralleasier than the Matterport 3 Ddataset. import habitat # Load embodied AI task (Point Nav) # and a pre-specified virtual robot config = habitat.get_config(config_file= \"pointnav.yaml\") Figure 7:Per for manceof Habitat-Simunderdifferentsensor env = habitat.Env(config) framememorytransferstrategies for increasingimageresolution. observations = env.reset() Wesee that\u2018GPU->GPU\u2019isunaffectedbyimageresolutionwhile otherstrategiesdegraderapidly. # Step through environment with random actions while not env.episode_over: observations = \\ B.Additional dataset Statistics env.step(env.action_space.sample()) In Table 5 wesummarize the train,validation and testsplit sizes for allthree data setsusedin our experiments. Wealso Sensor API RL Environment RL baselines Habitat-Sim Simulator API SLAM . . . Environment Embodied QA Imitation Task learning Navigation Episodes Episode Baselines dataset Gibson Point Nav Matterport 3 D Point Nav Replica Point Nav Matterport 3 D EQA Replica EQA use inherit core API extensions and implementations Figure 8:Architectureof Habitat-API.Thecorefunctionalitydefinesfundamentalbuildingblockssuchas the API for interacting with thesimulatorbackend and receivingobservationsthrough Sensors. Concretesimulationbackends,3 Ddatasets,andembodiedagent baselines are implementedasextensionsto the core API. dataset scenes(#) episodes(#) average GDSP(m) C.1.Analysisof Collisions Matterport 3 D 58/11/18 4.8 M/495/1008 11.5/11.1/13.2 To further characterize the behavior of learned agents Gibson 72/16/10 4.9 M/1000/1000 6.9/6.5/7.0 duringnavigationweplot the averagenumberofcollisions in Figure 10. We see that Blind incurs a much larger Table 4: Statistics of the Point Goal navigation datasets that we numberofcollisionsthano the ragents,providingevidence precompute for the Matterport 3 Dand Gibson data sets:totalnumber for\u2018wall-following\u2019behavior. Depth-equippedagents have ofscenes,totalnumberofepisodes,andaveragegeodesicdistance the lowest number of collisions, while RGB agents are in betweenstart and goalpositions.Eachcellreportstrain/val/test between. splitstatistics. C.2.Noisy Depth dataset Min Median Mean Max Toinvestigate the impactofnoisydepthmeasurementson Matterport 3 D 18 90.0 97.1 281 agentper for mance,were-evaluateddepthagents(without",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 388,
      "paper_id": "habitat",
      "text": "the ragents,providingevidence precompute for the Matterport 3 Dand Gibson data sets:totalnumber for\u2018wall-following\u2019behavior. Depth-equippedagents have ofscenes,totalnumberofepisodes,andaveragegeodesicdistance the lowest number of collisions, while RGB agents are in betweenstart and goalpositions.Eachcellreportstrain/val/test between. splitstatistics. C.2.Noisy Depth dataset Min Median Mean Max Toinvestigate the impactofnoisydepthmeasurementson Matterport 3 D 18 90.0 97.1 281 agentper for mance,were-evaluateddepthagents(without Gibson 15 60.0 63.3 207 re-training)onnoisydepthgeneratedusingasimplenoise Table 5: Statisticsofpathlength(inactions)foranoraclewhich model: iid Gaussiannoise(\u00b5 = 0,\u03c3 = 0.4)ateachpixel greedily fits actions to follow the negative of geodesic distance in inverse depth (larger depth = more noise). We observe gradienton the Point Goalnavigationvalidationsets.Thisprovides a drop of 0.13 and 0.02 SPL for depth-RL and SLAM on expectedhorizonlengths for anear-perfectagent and contextualizes Gibson-val(depth-RLstilloutperforms SLAM).Note that thedecision for amax-steplimitof 500. SLAM from [20] utilizes ORB-SLAM 2, which is quite robusttonoise,whiledepth-RLwastrained with outnoise. C.Additional Experimental Results If we increase \u03c3 to 0.1, depth-RL gets 0.12 SPL whereas SLAMsufferscatastrophicfailures. In order to confirm that the trends we observe for the experimentalresultspresentedin the paperhold for much D.Gibson dataset Curation largeramountsofexperience,wescaled our experimentsto 800 M steps. We found that (1) the ordering of the visual Wemanuallycurated the full data setof Gibson 3 Dtex- inputs stays Depth > RGBD > RGB > Blind; (2) RGB turedmeshes[30]toselectmeshes that donotexhibitsignif- is consistently better than Blind (by 0.06/0.03 SPL on icantreconstructionartifactssuchasholesortexturequality Gibson/Matterport 3 D),and(3)RGBDoutperforms SLAM issues. Akeyissue that wetriedtoavoidis the presenceof on Matterport 3 D(by 0.16 SPL). Figure 9:Statisticsof Point Goalnavigationepisodes.Fromleft:distributionover Euclideandistancebetweenstart and goal,distribution overgeodesicdistancealongshortestpathbetweenstart and goal,anddistributionover the ratioofgeodesicto Euclide and istance. Gibson Blind habitat-api/habitat_baselines. Below is the RGB shellscriptwe used for our RLexperiments: RGBD Depth MP 3 D Blind # Note: parameters in {} are experiment specific. RGB # Note: use 8, 6 processes for Gibson, MP 3 D RGBD # respectively. Depth 0 5 10 15 20 25 30 35 40 python habitat_baselines/train_ppo.py \\ Avg. Collisions --sensors {RGB_SENSOR,DEPTH_SENSOR} \\ --blind {0,1} --use-gae --lr 2.5 e-4 \\ Figure 10: Averagenumberofcollisionsduringsuccessfulnavi- --clip-param 0.1 --use-linear-lr-decay \\ gationepisodes for the differentsensoryconfigurationsof the RL --num-processes {8,6} --num-steps 128 \\ (PPO)baselineagentontestsetepisodes for the Gibson and Matter- --num-mini-batch 4 --num-updates 135000 \\ --use-linear-clip-decay \\ port 3 Ddatasets.The Blindagentexperiences the highestnumber ofcollisions,whileagentspossessingdepthsensors(Depth and For running SLAM please refer to habitat- RGBD)have the fewestcollisionsonaverage. api/habitat_baselines/slambased. holesorcracksinfloorsurfaces.Thisisparticularlyproblem- F.Example Navigation Episodes atic for navigation task sasitdividesseeminglyconnected navigable areasintonon-traversabledisconnectedcompo- Figure 12 visualizes additional example navigation nents. Wemanuallyannotated the scenes(using the 0 to 5 episodes for the differentsensoryconfigurationsof the RL quality scale shownin Figure 11)andonlyusesceneswitha (PPO) agents that we describe in the main paper. Blind ratingof 4 orhigher,i.e.,noholes,goodreconstruction,and agents have the lowestper for mance,collidingmuchmore negligibletextureissuestogenerate the datasetepisodes. frequently with the environment and adoptinga\u2018wallhug- ging\u2019 strategy for navigation. RGB agents are less prone E.Reproducing Experimental Results tocollisions but stillstruggle tonavigateto the goalposi- Our experimental results can be reproduced us- tionsuccess full yinsomecases. Incontrast,depth-equipped ing the Habitat-API (commit ec 9557 a) and agents are muchmoreefficient,exhibitingfewercollisions, Habitat-Sim (commit d 383 c 20) repositories. The andnavigatingtogoalsmoresuccessfully(asindicatedby code for running experiments is present under the folder theoverallhigher SPLvalues). 0:criticalreconstructionartifacts,holes,ortextureissues 1:bigholesorsignifi can ttextureissues and reconstructionartifacts 2:bigholesorsignifi can ttextureissues,butgoodreconstruction 3:smallholes,sometextureissues,goodreconstruction 4:noholes,sometextureissues,goodreconstruction 5:noholes,uni for mtextures,goodreconstruction Figure 11:Rating scale usedincurationof 3 Dtexturedmeshreconstructions from the Gibson data set.we useonlymeshes with ratingsof 4 orhigher for the Habitat Challenge",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 389,
      "paper_id": "habitat",
      "text": "c 20) repositories. The andnavigatingtogoalsmoresuccessfully(asindicatedby code for running experiments is present under the folder theoverallhigher SPLvalues). 0:criticalreconstructionartifacts,holes,ortextureissues 1:bigholesorsignifi can ttextureissues and reconstructionartifacts 2:bigholesorsignifi can ttextureissues,butgoodreconstruction 3:smallholes,sometextureissues,goodreconstruction 4:noholes,sometextureissues,goodreconstruction 5:noholes,uni for mtextures,goodreconstruction Figure 11:Rating scale usedincurationof 3 Dtexturedmeshreconstructions from the Gibson data set.we useonlymeshes with ratingsof 4 orhigher for the Habitat Challenge data set. Gibson Blind SPL=0.00 RGBSPL=0.45 RGBDSPL=0.82 Depth SPL=0.88 Blind SPL=0.00 RGBSPL=0.29 RGBDSPL=0.49 Depth SPL=0.96 Figure 12:Additionalnavigationexampleepisodes for the differentsensoryconfigurationsof the RL(PPO)agent,visualizingtrials from the Gibson and MP 3 Dvalsets.Abluedot and reddotindicate the starting and goalpositions,and the bluearrowindicatesfinalagent position.Theblue-green-redlineis the agent\u2019strajectory.Colorshifts from bluetoredas the maximumnumberofallowedagentstepsis approached. MP 3 D Blind SPL=0.00 RGBSPL=0.40 RGBDSPL=0.92 Depth SPL=0.98 Figure 12:Additionalnavigationexampleepisodes for the differentsensoryconfigurationsof the RL(PPO)agent,visualizingtrials from the Gibson and MP 3 Dvalsets.Abluedot and reddotindicate the starting and goalpositions,and the bluearrowindicatesfinalagent position.Theblue-green-redlineis the agent\u2019strajectory.Colorshifts from bluetoredas the maximumnumberofallowedagentstepsis approached.",
      "start_pos": 6006,
      "end_pos": 6140
    },
    {
      "chunk_id": 390,
      "paper_id": "OKAMI",
      "text": "OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation Jinhan Li 1\u2020 Yifeng Zhu 1\u2217 Yuqi Xie 1,2\u2217 Zhenyu Jiang 1,2\u2217 Mingyo Seo 1 Georgios Pavlakos 1 Yuke Zhu 1,2 UTAustin 1 NVIDI ARe search 2 Abstract:Westudy the problemofteachinghumanoidrobotsmanipulationskills byimitating from singlevideodemonstrations. Weintroduce OKAMI,amethod that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAMI achievesstronggeneralizationsacrossvaryingvisual and spatialconditions,out- per for ming the state-of-the-art baseline on open-world imitation from observa- tion. Fur the rmore,OKAMIrollouttrajectories are leveragedtotrainclosed-loop visuomotorpolicies,whichachieveanaveragesuccessrateof 79.2%without the need for labor-intensiveteleoperation. Morevideos can befoundon our website https://ut-austin-rpl.github.io/OKAMI/. Keywords:Humanoid Manipulation,Imitation From Videos,Motion Retargeting Single Video H D u e m m a o n Imitation H D u e m m a o n Human Human Demo Demo Figure 1: OKAMIenablesahumanusertoteach the humanoidrobothowtoper for manew task byproviding asinglevideodemonstration. 1 Introduction Deployinggeneralistrobotstoassist with everyday task srequires the mtooperateautonomouslyin natural environments. With recent advances in hardw are designs and increased commercial avail- ability,humanoidrobotsemergeasapromisingplat for mtodeployin our living and workingspaces. Despite the irgreatpotential,theystillstruggletooperateautonomously and deployrobustlyin the \u2020Thisworkwasdo new hile Jinhan Liwasavisitingresearcherat UTAustin. *Equalcontribution. 8 th Conferenceon Robot Learning(Co RL 2024),Munich,Germany. 4202 tc O 51 ]OR.sc[ 1 v 29711.0142:vi Xra unstructured world. A burgeoning line of work has resorted to deep imitation learning methods forhumanoidmanipulation[1\u20133]. However,theyrelyonlargeamountsofdemonstrationsthrough whole-body teleoperation, requiring domain expertise and strenuous efforts. In contrast, humans have the innate ability to watch their peers do a task once and mimic the behaviors. Equipping robots with theabilitytoimitate from visualobservations will moveuscloserto the goaloftraining roboticfoundationmodels from Internet-scalehumanactivityvideos. We explore teaching humanoid robots to manipulate objects by watching humans. We consider a problemsettingrecently for mulatedas\u201copen-worldimitation from observation,\u201dwherearobotim- itates a manipulation skill from a single video of human demonstration [4\u20136]. This setting would facilitateusersinef for tlesslydemonstratingtasks and enableahumanoidrobottoacquire new skills quickly. Enabling humanoids to imitate from single videos presents a significant challenge \u2014 the videodoesnot have actionlabels, butyet the robothastolearntoper for mtasksin new situations beyondwhat\u2019sdemonstratedin the video. Priorworksonone-shotvideolearning have attemptedto optimizerobotactionstoreconstruct the futureobjectmotiontrajectories[4,5]. However,they have been applied to single-arm manipulators and are computationally prohibitive for humanoid robots due to their high degrees of freedom and joint redundancy [7]. Meanwhile, the similar kinematic structuresh are dbyhumans and humanoidsmakesdirectlyretargetinghumanmotionstorobotsfea- sible[8,9]. None the less,existingretargetingtechniquesfocusonfree-spacebodymotions[10\u201314], lacking the contextual awareness of objects and interactions needed for manipulation. To address thisshortcoming, weintroduce the conceptof\u201cobject-awareretargeting\u201d. Byincorporatingobject contextual information into the retargeting process, the resulting humanoid motions can be effi- cientlyadaptedto the locationsofobjectsin open-endedenvironments. Tothisend,weintroduce OKAMI(Object-aware Kinematicret Argetingforhu Manoid Imitation), anobject-awareretargetingmethod that enablesabimanualhumanoid with twodexteroush and sto imitate manipulation behaviors from a single RGB-D video demonstration. OKAMI uses a two- stage process to retarget the human motions to the humanoid robot to accomplish the task across varyinginitialconditions. Thefirststageprocesses the videotogenerate are ferencemanipulation plan.Thesecondstageuses this plantosynthesize the humanoidmotionsthroughmotionretargeting thatadaptsto the objectlocationsintargetenvironments. OKAMI consists of two key designs. The first",
      "start_pos": 0,
      "end_pos": 512
    },
    {
      "chunk_id": 391,
      "paper_id": "OKAMI",
      "text": "manipulation behaviors from a single RGB-D video demonstration. OKAMI uses a two- stage process to retarget the human motions to the humanoid robot to accomplish the task across varyinginitialconditions. Thefirststageprocesses the videotogenerate are ferencemanipulation plan.Thesecondstageuses this plantosynthesize the humanoidmotionsthroughmotionretargeting thatadaptsto the objectlocationsintargetenvironments. OKAMI consists of two key designs. The first design is an open-world vision pipeline that iden- tifies task-relevantobjects,reconstructshumanmotions from the video,andlocalizes task-relevant objects during evaluation. Localizing objects at test time also enables motion retargeting to adapt todifferentbackgroundsor new objectinstancesof the samecategories. Theseconddesignis the factorized process for retargeting, where we retarget the body motions and hand poses separately. Wefirstretargetthebodymotions from thereferenceplanin the taskspace,andthenwarp the retar- getedtrajectorygiven the locationof task-relevantobjects. Thetrajectoryofbodyjointsisobtained throughinversekinematics. Thejointanglesoffingers are mapped from theplanonto the dexterous hands, reproducing hand-object interaction. With object-aware retargeting, OKAMI policies sys- tematicallygeneralizeacrossvariousspatiallayoutsofobjects and sceneclutters. Finally,wetrain visuomotorpolicieson the rollouttrajectories from OKAMI throughbehavioralcloningtoobtain vision-basedmanipulationskills. Weevaluate OKAMI onhumanvideodemonstrationsofdiversetasks that coverrichobjectinter- actions, suchaspicking, placing, pushing, andp our ing. Weshow that itsobject-awareretargeting achieves 71.7%tasksuccessratesaveragedacrossalltasks and outperforms the ORION[4]baseline by 58.3%. Wethentrainclosed-loopvisuomotorpolicieson the trajectoriesgeneratedby OKAMI, achievinganaveragesuccessrateof 79.2%. Ourcontributionsof OKAMI are three-fold: 1. OKAMI enables a humanoid robot to mimic human behaviors from a single video for dexterousmanipulation. Itsobject-awareretargetingprocessgeneratesfeasiblemotionsof thehumanoidrobotwhileadapting the motionstotargetobjectlocationsattesttime; 2. OKAMI uses vision foundation models [15, 16] to identify task-relevant objects with- outadditionalhumaninputs. Theircommon-sensereasoningabilityhelpsrecognize task- 2 relevantobjectsevenifthey are notdirectlyincontact with otherobjectsor the robothands, allowing our methodtoimitatemorediverse task sthanpriorwork; 3. Wevalidate OKAMI\u2019sstrongspatial and visualgeneralizationabilitiesonhumanoidhard- ware. OKAMIenablesreal-robotdeploymentinnaturalenvironments with unseenobject layouts,varyingvisualbackgrounds,and new objectinstances. 2 Related Work Humanoid Robot Control. Methods like motion planning and optimal control have been devel- oped for humanoid locomotion and manipulation [10, 12, 17]. These model-based approaches rely on precise physical modeling and expensive computation [11, 12, 18]. To mitigate the stringent requirements, researchers have explored policy training in simulation and sim-to-real transfer [10, 19]. However, thesemethods still require a significant amount of labor and expertise indesigningsimulationtasks and rewardfunctions,limiting the irsuccessestolocomotiondomains. In parallel to automated methods, a variety of human control mechanisms and devices have been developed for humanoid teleoperation using motion capture suits [9, 12, 20\u201324], telexistence cockpits[25\u201329],VRdevices[1,30,31],orvideos that trackhumanbodies[17,32]. Whilethese systems can control the robotstogeneratediversebehaviors,theyrequirereal-timehumaninput that posessignifi can tcognitive and physicalburdens. Incontrast,OKAMIonlyrequiressingle RGB-D humanvideostoteach the humanoidrobot new skills,signifi can tlyreducing the humancost. Imitation Learning for Robot Manipulation. Imitation Learning has signifi can tly advanced vision-basedrobotmanipulation with highsampleefficiency[33\u201344]. Priorworks have shown that robots can learnvisuomotorpoliciestocompletevarioustasks with justdozensofdemonstrations, ranging from long-horizon manipulation [34\u201336] to dexterous manipulation [37\u201339]. However, collecting demonstrations often requires domain expertise and high costs, creating challenges to scale. Another line of work focuses on one-shot imitation learning [40\u201344], yet they demand excessive data collection for meta-training tasks. Recently, researchers have looked into a new problem setting of imitating from a single video demonstration [4\u20136], referred to as \u201copen-world imitation from observation\u201d [4]. Unlike prior works that abstract away embodiment motions due to kinematic differences between the robot and the human, we exploit embodiment motion information owing to the kinematic similarity between humans and humanoids. Specifically, we introduceobject-awareretargeting that adaptshumanmotionstohumanoidrobots. Motion Retargeting. Motion retargeting has wide applications in computer graphics and 3 D vision[8],whereextensiveliteraturestudieshowtoadapthumanmotionstodigitalavatars[45\u201347]. This technique has been adopted in robotics for recreating human-like motions",
      "start_pos": 462,
      "end_pos": 974
    },
    {
      "chunk_id": 392,
      "paper_id": "OKAMI",
      "text": "between the robot and the human, we exploit embodiment motion information owing to the kinematic similarity between humans and humanoids. Specifically, we introduceobject-awareretargeting that adaptshumanmotionstohumanoidrobots. Motion Retargeting. Motion retargeting has wide applications in computer graphics and 3 D vision[8],whereextensiveliteraturestudieshowtoadapthumanmotionstodigitalavatars[45\u201347]. This technique has been adopted in robotics for recreating human-like motions on humanoid or anthropomorphic robots through various retargeting methods, including optimization-based ap- proaches[11,12,20,48],geometric-basedmethods[49],andlearning-basedtechniques[10,13,17]. However, in manipulation tasks, these retargeting methods have been used within teleoperation systems,lackingavisionpipeline for automaticadaptationtoobjectlocations. OKAMIintegrates theretargetingprocess with open-worldvision,endowingit with objectaw are nessso that the robot canmimichumanmotions from videodemonstrations and adapttoobjectlocationsattesttime. 3 OKAMI In this work, we introduce OKAMI, a two-staged method that tackles open-world imitation from observation for humanoidrobots. OKAMIfirstgenerates are ferenceplanusing the objectlocations andreconstructedhumanmotions from agiven RGB-Dvideo. Then,itretargets the humanmotion trajectories to the humanoid robot while adapting the trajectories based on new locations of the objects. Figure 2 illustrates the wholepipeline. Problem Formulation We formulate a humanoid manipulation task as a discrete-time Markov Decision Process defined by a tuple: M = (S,A,P,R,\u03b3,\u00b5), where S is the state space, A is the action space, P(\u00b7|s,a) is the transition probability, R(s) is the reward function, \u03b3 \u2208 [0,1) is the 3 GPT 4 V \u201cbottle\u201d \u201cbowl\u201d Identify Keyframes Through Returnalistof task- Trackobjectsacross Changepoint Detections relevantobjectnames thevideo Reference Plan Human RGB-DVideo Reconstruction Reference Plan Model l<latexit sha 1_base 64=\"L 8 w Fmirg Yiq Iex Nq 3 ulkz L 02 s EI=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cnfar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Av 0+Oq A==</latexit> 0 l<latexit sha 1_base 64=\"9 L/YXDXQXOLHv 1 NEo N 5 OIo Od PBM=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cm/ar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM",
      "start_pos": 924,
      "end_pos": 1436
    },
    {
      "chunk_id": 393,
      "paper_id": "OKAMI",
      "text": "6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cm/ar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Aw NSOq Q==</latexit> 1 l<latexit sha 1_base 64=\"j Afv Ax TKm Ay IZRV+Bm Wd/Jpc Kv I=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 mkq Mei F 09 Swb SFNp TNdtsu 3 Wz C 7 k Qoob/Biwd Fv Pq Dv Plv 3 LY 5 a Ou Dgcd 7 M 8 z MCx Mp DLrut 1 NYW 9/Y 3 Cpul 3 Z 29/YPyod HTROnmn Gfx TLW 7 ZAa Lo Xi Pgq Uv J 1 o Tq NQ 8 l Y 4 vp 35 r Seuj Yj VI 04 SHk R 0 q MRAMIp W 8 m Uvu 5/2 yh W 36 s 5 BVom Xkwrka PTKX 91+z NKIK 2 SSGt Px 3 ASDj Go UTPJpq Zsanl A 2 pk Pes VTRi Jsgmx 87 JWd W 6 ZNBr G 0 p JHP 190 RGI 2 Mm UWg 7 I 4 ojs+z Nx P+8 Toq D 6 y ATKkm RK 7 ZYNEglw Zj MPid 9 o Tl DOb GEMi 3 sr YSNq KYMb T 4 l G 4 K 3/PIqa V 5 Uvctq 7 a FWqd/kc RTh BE 7 h HDy 4 gjrc QQN 8 YCDg GV 7 hz VHOi/Puf Cxa C 04+cwx/4 Hz+AOzljs Y=</latexit> N Generation SMPL-Htrajectory Identifytarget/refobjects and generatereferenceplan Estimatetrans for mation Inverse Robot Observation Localizerelevantobjects betweenpointclouds Kinematics attesttime Reference Plan Hand Finger l<latexit sha 1_base 64=\"L 8 w Fmirg Yiq Iex Nq 3 ulkz L 02 s EI=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cnfar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6",
      "start_pos": 1386,
      "end_pos": 1898
    },
    {
      "chunk_id": 394,
      "paper_id": "OKAMI",
      "text": "1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Av 0+Oq A==</latexit> 0 l<latexit sha 1_base 64=\"9 L/YXDXQXOLHv 1 NEo N 5 OIo Od PBM=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cm/ar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Aw NSOq Q==</latexit> 1 l<latexit sha 1_base 64=\"j Afv Ax TKm Ay IZRV+Bm Wd/Jpc Kv I=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 mkq Mei F 09 Swb SFNp TNdtsu 3 Wz C 7 k Qoob/Biwd Fv Pq Dv Plv 3 LY 5 a Ou Dgcd 7 M 8 z MCx Mp DLrut 1 NYW 9/Y 3 Cpul 3 Z 29/YPyod HTROnmn Gfx TLW 7 ZAa Lo Xi Pgq Uv J 1 o Tq NQ 8 l Y 4 vp 35 r Seuj Yj VI 04 SHk R 0 q MRAMIp W 8 m Uvu 5/2 yh W 36 s 5 BVom Xkwrka PTKX 91+z NKIK 2 SSGt Px 3 ASDj Go UTPJpq Zsanl A 2 pk Pes VTRi Jsgmx 87 JWd W 6 ZNBr G 0 p JHP 190 RGI 2 Mm UWg 7 I 4 ojs+z Nx P+8 Toq D 6 y ATKkm RK 7 ZYNEglw Zj MPid 9 o Tl DOb GEMi 3 sr YSNq KYMb T 4 l G 4 K 3/PIqa V 5 Uvctq 7 a FWqd/kc RTh BE 7 h HDy 4 gjrc QQN 8 YCDg GV 7 hz VHOi/Puf Cxa C 04+cwx/4 Hz+AOzljs Y=</latexit> N Target and Mapping Send Joint referenceobjects Commands Object-Aware Retargeting SMPL-Htrajectory Retargetmotions segment Using SMPL-H Warpedmotions Robotexecution Figure 2: Overviewof OKAMI.OKAMIisatwo-stagedmethod that enablesahumanoidrobottoimitatea manipulation task from asinglehumanvideo.Inthefirststage,OKAMIgenerates are ferenceplanusing GPT- 4 Vandlargevisionmodels for subsequentmanipulation. Inthesecondstage,OKAMIfollows the reference plan,whereitretargetshumanmotionsonto the humanoid with objectaw are ness. Theretargetedmotions are convertedintoasequenceofrobotjointcomm and sfor the robottofollow. discountfactor,and\u00b5istheinitialstatedistribution. Inourcontext,S isthespaceofraw RGB-D observations that captureboth the robot and",
      "start_pos": 1848,
      "end_pos": 2360
    },
    {
      "chunk_id": 395,
      "paper_id": "OKAMI",
      "text": "Retargetmotions segment Using SMPL-H Warpedmotions Robotexecution Figure 2: Overviewof OKAMI.OKAMIisatwo-stagedmethod that enablesahumanoidrobottoimitatea manipulation task from asinglehumanvideo.Inthefirststage,OKAMIgenerates are ferenceplanusing GPT- 4 Vandlargevisionmodels for subsequentmanipulation. Inthesecondstage,OKAMIfollows the reference plan,whereitretargetshumanmotionsonto the humanoid with objectaw are ness. Theretargetedmotions are convertedintoasequenceofrobotjointcomm and sfor the robottofollow. discountfactor,and\u00b5istheinitialstatedistribution. Inourcontext,S isthespaceofraw RGB-D observations that captureboth the robot and objectstates, Aisthespaceof the motioncommands for the humanoidrobot,Ris the sparserewardfunction that returns 1 whena task iscomplete. The objectiveofsolvingataskistofindapolicy\u03c0thatmaximizestheexpectedtasksuccessratesfrom awiderangeofinitialconfigurationsdrawnfrom\u00b5attesttime. We consider the setting of \u201copen-world imitation from observation\u201d [4], where the robot system takes are corded RGB-Dhumanvideo, V asinput, andreturnsahumanoidmanipulationpolicy\u03c0 thatcompletes the taskasdemonstratedin V.Thissettingis\u201copen-world\u201dastherobotdoesnot have priorknowledgeorground-truthaccessto the categoriesorphysicalstatesofobjectsinvolvedin the task, and it is \u201cfrom observation\u201d in the sense that video V does not come with any ground-truth robotactions. Apolicyexecutionisconsideredsuccessfulifthestatematchesthestateof the final frame from V. Thesuccessconditionsofalltestedtasks are describedin Appendix B.1. Notably, two assumptions are made about V in this paper: all the image frames in V capture the human bodies,and the cameraviewofshooting V isstaticthroughout the recording. 3.1 Reference Plan Generation Toenableobject-awareretargeting,OKAMIfirstgenerates are ferenceplan for the humanoidrobot to follow. Plan generation involves underst and ing what task-relevant objects are and how humans manipulatethem. Identifying and Localizing Task-Relevant Objects. To imitate manipulation tasks from videos V,OKAMImustidentify the task-relevantobjectstointeract with.Whilepriormethodsrelyonun- supervisedapproaches with simplebackgroundsorrequireadditionalhumanannotations[50\u201353], OKAMIusesanoff-the-shelf Vision-Language Models(VLMs),GPT-4 V,toidentify task-relevant objects in V by leveraging the commonsense knowledge internalized in the model. Concretely, OKAMI obtains the names of task-relevant objects by sampling RGB frames from the video demonstration V and prompting GPT-4 V with the concatenation of these images (details in Appendix A.2). Using these object names, OKAMI employs Grounded-SAM [16] to segment the objects in the first frame and track their locations throughout the video using a Vidoe Object 4 Segmentation model,Cutie[54]. Thisprocessenables OKAMItolocalize task-relevantobjectsin V,forming the basis for subsequentsteps. Reconstructing Human Motions. To retarget human motions to the humanoid robot, OKAMI reconstructs human motions from V to obtain motion trajectories. We adopt an improved version of SLAHMR [55], an iterative optimization algorithm that reconstructs human motion sequences. While SLAHMR assumes flat hands, our extension optimizes the hand poses of the SMPL-H model [56], which are initialized using estimated hand poses from Ha Me R [57] (More details in Appendix A.1). This modification allows us to jointly optimize body and hand poses from monocularvideo. Theoutputisasequenceof SMPL-Hmodelscapturing full-bodyandh and poses, enabling OKAMI to retarget human motions to humanoids (See Section 3.2). Additionally, the SMPL-Hmodel can representhumanposesacrossdemographicdifferences,allowingeasymapping ofmotions from hum and emonstratorsto the humanoid. Generatinga Plan from Video. Havingidentified task-relevantobjects and reconstructedhuman motions,OKAMIgenerates are ferenceplan from V forrobotstocompleteeachsubgoal. OKAMI identifies subgoals by per for ming temporal segmentation on V with the following procedure: We first track keypoints using Co Tracker [58] and detect velocity changes of keypoints to determine keyframes, which correspond to subgoal states. For each subgoal, we identify a target object (in motion due to manipulation) and a reference object (serving as a spatial reference for the target object\u2019smovementsthroughei the rcontactornon-contactrelations). Thetargetobjectisdetermined basedon the averagedkeypointvelocitiesperobject,while the referenceobjectisidentifiedthrough geometric heuristics or semantic relations predicted by GPT-4 V (More implementation details of plangenerationin Appendix A.4). Withsubgoals and associatedobjectsdetermined,wegenerate are ferenceplanl ,l ,...,l ,where 0 1 N eachstepl correspondstoakeyframe and includesthepointcloudsof",
      "start_pos": 2310,
      "end_pos": 2822
    },
    {
      "chunk_id": 396,
      "paper_id": "OKAMI",
      "text": "reference object (serving as a spatial reference for the target object\u2019smovementsthroughei the rcontactornon-contactrelations). Thetargetobjectisdetermined basedon the averagedkeypointvelocitiesperobject,while the referenceobjectisidentifiedthrough geometric heuristics or semantic relations predicted by GPT-4 V (More implementation details of plangenerationin Appendix A.4). Withsubgoals and associatedobjectsdetermined,wegenerate are ferenceplanl ,l ,...,l ,where 0 1 N eachstepl correspondstoakeyframe and includesthepointcloudsof the targetobjecto ,the i target reference object o , and the SMPL-H trajectory segment \u03c4SMPL . If no reference object is reference ti:ti+1 required (e.g., grasping an object), o is null. Point clouds are obtained by back-projecting reference segmentedobjects from RGBimagesusingdepthimages[59]. 3.2 Object-Aware Retargeting Given are ferenceplan from the videodemonstration,OKAMIenables the humanoidrobottoimi- tate the taskin V. Therobotfollowseachstepl intheplanbylocalizing task-relevantobjects and i retargeting the SMPL-Htrajectorysegmentonto the humanoid. Theretargetedtrajectories are then convertedintojointcomm and sthroughinversekinematics. Thisprocessrepeatsuntilall the steps areexecuted,andsuccessisevaluated base don task-specificconditions(see Appendix B.1). Localizing Objectsat Test Time. Toexecutetheplanin the test-timeenvironment,OKAMImust localize the task-relevant objects in the robot\u2019s observations, extracting 3 D point clouds to track objectlocations. Byattendingto task-relevantobjects, OKAMI policiesgeneralizeacrossvarious visualconditions,includingdifferentbackgroundsor the presenceofnovelinstancesof task-relevant objects. Retargeting Human Motionsto the Humanoid. Thekeyaspectofobject-awarenessisadapting motions to new object locations. After localizing the objects, we employ a factorized retargeting process that syn the sizesarmandh and motionsseparately. OKAMIfirstadapts the armmotionsto theobjectlocationsso that thefingersoftheh and sareplacedwithin the object-centriccoordinate frame. Then OKAMI only needs to retarget fingers in the joint configuration to mimic how the demonstratorinteracts with objects with the irhands. Concretely,wefirstmaphumanbodymotionstothe task spaceof the humanoid,scaling and adjust- ingtrajectoriestoaccount for differencesinsize and proportion. OKAMIthenwarps the retargeted trajectoryso that the robot\u2019sarmreaches the newobjectlocations(Moredetailsin Appendix A.5). Weconsidertwocasesintrajectorywarping\u2014when the relationalstatebetweentarget and refer- enceobjectsisunchanged and whenitchanges,adjusting the warpingaccordingly. Inthefirstcase, 5 Sprinkle-salt Plush-toy-in-basket Close-the-laptop Close-the-drawer Place-snacks-on-plate Bagging Figure 3:Visualizationofinitialandfinalframesofbothhumandemonstrations and robotrollouts for alltasks. weonlywarpthetrajectory base don the targetobjectlocations;inthesecondcase,thetrajectoryis warped base don the referenceobjectlocation. After warping, we use inverse kinematics to compute a sequence of joint configurations for the arms while balancing the weights of position and rotation targets in inverse kinematics computa- tiontomaintainnaturalpostures. Simultaneously,weretargetthehumanh and posesto the robot\u2019s finger joints, allowing the robot to perform fine-grained manipulations (Implementation details in Appendix A.3). Intheend,weobtaina full-bodyjointconfigurationtrajectory for execution. Since armmotionretargetingisaffine,ourprocessnaturallyscales and adjustsmotions from demonstrators withvarieddemographiccharacteristics. Byadaptingarmtrajectoriestoobjectlocations and retar- getingh and posesindependently,OKAMIachievesgeneralizationacrossvariousspatiallayouts. 4 Experiments Our experiments are designed to answer the following research question: 1) Is OKAMI effective forahumanoidrobottoimitatediversemanipulationtasks from singlevideosofhum and emonstra- tion?2)Isitcriticalin OKAMItoretargetthebodymotionsofdemonstratorsto the humanoidrobot insteadofonlyretargeting base donobjectlocations? 3)Can OKAMIretainitsper for mancescon- sistentlyonvideosdemonstratedbyhumansofdiversedemographics?4)Can the rolloutsgenerated by OKAMIbeused for trainingclosed-loopvisuomotorpolicies? 4.1 Experimental Setup Task Designs.Wedescribe the six task swe usein the experiments:1)Plush-toy-in-basket: placing a plush toy in the basket; 2) Sprinkle-salt: sprinkling a bit of salt into the bowl; 3) Close-the-drawer: pushing the drawer in to close it; 4) Close-the-laptop: closing the lid of the laptop; 5) Place-snacks-on-plate: placing a bag of snacks on the plate. 6) Bagging: placing a chip bag into a shopping bag. We select these six tasks that cover a diverse rangeofmanipulationbehaviors: Plush-toy-in-basket and Place-snacks-on-plate requirepick-and-placebehaviorsofdailyobjects; Sprinkle-saltis the task that coversp our- ingbehavior;Close-the-drawer and Close-the-laptoprequire the humanoidtointeract with articulated objects, a prevalent type of interaction in daily environments; Bagging involves dexterous, bimanualmanipulation and includesmultiplesubgoals. Whilewemainlyfocusonreal 6 Successrate Missedgrasping Failed Completion Demonstrator 1 Demonstrator 2 Demonstrator 3 83.3% 83.3% 83.3% 75.0% 75.0% 75.0% 75.0% 75.0%",
      "start_pos": 2772,
      "end_pos": 3284
    },
    {
      "chunk_id": 397,
      "paper_id": "OKAMI",
      "text": "Place-snacks-on-plate requirepick-and-placebehaviorsofdailyobjects; Sprinkle-saltis the task that coversp our- ingbehavior;Close-the-drawer and Close-the-laptoprequire the humanoidtointeract with articulated objects, a prevalent type of interaction in daily environments; Bagging involves dexterous, bimanualmanipulation and includesmultiplesubgoals. Whilewemainlyfocusonreal 6 Successrate Missedgrasping Failed Completion Demonstrator 1 Demonstrator 2 Demonstrator 3 83.3% 83.3% 83.3% 75.0% 75.0% 75.0% 75.0% 75.0% 66.7% 66.7% 58.3% 58.3% 58.3% Sprinkle-salt Plush-toy-in- Close-the- Close-the- Place-snacks- Bagging Place-snacks- Close-the- basket laptop drawer on-plate on-plate laptop (a) (b) Figure 4: (a)Evaluationof OKAMI overallsixtasks, includingthesuccessrates and the quantificationof failedtrials,separatedbyfailuremode.(b)Evaluationof OKAMIusingvideos from differentdemonstrations. Demonstrator 1 isthemainpersonrecordingvideos for allevaluationsin(a). robot experiments, we also implement Sprinkle-salt and Close-the-drawer in simula- tionusing Robo Suite[60]foreasyreproducibilityof OKAMI.See Appendix B.4. Hardw are Setup. we usea Fourier GR 1 robotas our hardw are platform,equipped with two 6-Do F Inspiredexteroush and sanda D 435 i Intel Real Sensecamera for videorecording and test-timeobser- vation. Weimplementajointpositioncontroller that operatesat 400 Hz. Toavoidjerkymovements, wecomputejointpositioncomm and sat 40 Hzandinterpolate the comm and sto 400 Hztrajectories. Evaluation Protocol. We run 12 trials for each task. The locations of the objects are randomly initialized within the intersection of the robot camera\u2019s view and the humanoid arms\u2019 reachable range. The tasks are evaluated on a tabletop workspace with multiple objects, including both task-relevant objects and various other objects. Further, we test new object generalization on Place-snacks-on-plate,Plush-toy-in-basket,and Sprinkle-salttasks,chang- ing the involvedplate,snackbag,plushtoy,andbowltootherinstancesof the sametype. Baselines. Wecomp are ourresult with abaseline ORION [4]. Since ORION wasproposed for parallel-jawgrippers,itisnotdirectlyapplicablein our experiments and weadoptit with minimal modifications: we estimate the palm trajectory using the SMPL-H trajectories, and warp the tra- jectory conditioning on the new object locations. The warped trajectory is used in the subsequent inversekinematics for computingrobotjointconfigurations. 4.2 Quantitative Results Toanswerquestion(1), weevaluate the policiesof OKAMI acrossall the tasks, coveringdiverse behaviorssuchasdailypick-place,pouring,andmanipulationofarticulatedobjects. Theresults are presentedin Figure 4(a). Inourexperiment,wer and omlyinitialize the objectlocationsso that the robotneedstoadapttothelocationsof the objects. Thisresultshows the effectivenessof OKAMI ingeneralizingoverdifferentvisual and spatialconditions. To answer question (2), we comp are OKAMI against ORION on two representative tasks, Place-snacks-on-plate and Close-the-laptop. In the comparison experiment, OKAMI differs from ORION in that ORION does not condition on the human body poses. OKAMIachieves 75.0%and 83.3%successrates,respectively,while ORIONonlyachieves 0.0% and 41.2%,respectively. Additionally,wecomp are OKAMIagainst ORIONon the twosimulated versionsof Sprinkle-salt and Close-the-drawertasks. Insimulation,OKAMIachieves 82.0% and 84.0% success rates in two tasks while ORION only achieves 0.0% and 10.0%. Most failuresof ORIONpolicies are duetofailingtoapproachobjects with reliablegraspingposes(e.g., in Place-snacks-on-plate task, ORION tries to grasp the snack from the sides instead of thetop-downgraspinhumanvideo),andfailingtorotate the wrist full ytoachievebehaviorssuchas pouring. Thesebehaviorsoriginate from the fact that ORIONignores the embodimentin for mation, thusfallingshortinper for mancecomp are dto OKAMI.Thesuperiorper for manceof OKAMIsug- geststheimportanceofretargetingthebodymotionofthehum and emonstratorsonto the humanoid whenimitating from humanvideos. To answer question (3), we conduct a controlled experiment of recording videos of different demonstrators and test if OKAMI policies maintain strong per for mance across the video inputs. 7 Sameas the previousexperiment,weevaluate OKAMIon the Place-snacks-on-plate and Close-the-laptoptasks. Theresults are presentedin Figure 4(b). Weshow that for the task Close-the-laptop, there is no statistical significance in per for mance change. As for task Place-snacks-on-plate, while the evaluation maintains above 50%, the worst policy per- formanceis 16.7%worsethan the bestpolicyper for mance. Afterlookinginto the videorecording, wefind that the motionofdemonstrator",
      "start_pos": 3234,
      "end_pos": 3746
    },
    {
      "chunk_id": 398,
      "paper_id": "OKAMI",
      "text": "and Close-the-laptoptasks. Theresults are presentedin Figure 4(b). Weshow that for the task Close-the-laptop, there is no statistical significance in per for mance change. As for task Place-snacks-on-plate, while the evaluation maintains above 50%, the worst policy per- formanceis 16.7%worsethan the bestpolicyper for mance. Afterlookinginto the videorecording, wefind that the motionofdemonstrator 2 isrelativelyfasterthantheo the rtwodemonstrators,and fastermotionscreateanoisyestimationofmotionwhendoinghuman model reconstruction. Over- all,OKAMI can maintainreasonablygoodper for mancegivenvideos from differentdemonstrators, but the reisroom for improvementson our visionpipelinetoh and lesuchvariety. 4.3 Learning Visuomotor Policy With OKAMIRollout Data We address question (4) by training neural visuomotor policies on OKAMI rollouts. We 50 Trajectories 100 Trajectories first run OKAMI over randomly initialized 83.3% 75.0% object layouts to generate multiple rollouts 66.7% 58.3% and collect a dataset of successful trajectories while discarding the failed ones. We train neuralnetworkpolicieson this datasetthrough a behavioral cloning algorithm. Since smooth execution is critical for humanoid manipu- Sprinkle-salt Bagging lation, we implement the behavioral cloning Figure 5: Successratesoflearnedvisuomotorpolicies with ACT[61], whichpredictssmoothactions on Sprinkle-salt and Baggingusing 50 and 100 via its temporal ensemble design, a trajectory trajectories,respectively. smoothing component (more implementation details in Appendix B.5). We train visuomotor policies for Sprinkle-salt and Bagging. Figure 5 illustrates the success rates of these policies, demonstrating that OKAMI rollouts are effective data sources for training. Wealsoshow that the learnedpoliciesimproveasmorerollouts are collected. These results hold the promise of scaling up data collection for learning humanoid manipulationskills with outlaboriousteleoperation. 5 Conclusion This paper introduces OKAMI that enables a humanoid robot to imitate a single RGB-D human videodemonstration.Atthecoreof OKAMIisobject-awareretargeting,whichretargets the human motions onto the humanoid robot and adapts the motions to the target object locations. OKAMI consists of two stages to realize object-aware retargeting. The first stage is generating a reference plan for manipulation from the video. The second stage is used for retargeting, where OKAMI retargetsthearmmotionsinthe task space and thefingermotionsin the jointconfigurationspace. Ourexperimentsvalidate the designof OKAMI,showing the systematicgeneralizationof OKAMI policies. OKAMI enables efficient collection of trajectory data based on a single human video demonstration. OKAMI-based data collectionsignifi can tlyreduces the humancost for policytrain- ingcomp are dto that requiredbyteleoperation. Limitations and Future Work. The current focus of OKAMI is on the upper body motion retargetingofhumanoidrobots, particularly for manipulationtasks with intabletopworkspaces. A promising future direction is to include lower body retargeting that enables locomotion behaviors during video imitation. To enable full-body loco-manipulation, a whole-body motion controller needstobeimplementedasopposedto the jointpositioncontrollerusedin OKAMI.Additionally, we rely on RGB-D videos in OKAMI, which limits us from using in-the-wild Internet videos recorded in RGB. Extending OKAMI to use web videos will be another promising direction for futureworks. Atlast,thecurrentimplementationofretargetinghaslimitedrobustnessagainstlarge variationsinobjectshapes. Afutureimprovementwouldbeintegratingmorepowerfulfoundation models that endow the robot with ageneralunderst and ingofhowtointeract with aclassofobjects inspiteof the irlargeshapechanges. 8 Acknowledgments We would like to thank William Yue for providing the initial implementation of the behavioral cloningpolicies, Peter Stone for hisvaluablesupport with taskdesigns and demoshooting, Yuzhe Qin for sharing the dex-retargeting code base, and Zhenjia Xu for his advice on developing the humanoidrobotinfrastructure. References [1] M.Seo,S.Han,K.Sim,S.H.Bang,C.Gonzalez,L.Sentis,and Y.Zhu. Deepimitationlearn- ing for humanoidloco-manipulationthroughhumanteleoperation. In IEEE-RASInternational Conferenceon Humanoid Robots(Humanoids),2023. [2] Y.Matsuura,K.Kawaharazuka,N.Hiraoka,K.Kojima,K.Okada,and M.Inaba.Development ofawhole-bodyworkimitationlearningsystembyabipedandbi-armedhumanoid. In 2023 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems(IROS),pages 10374\u2013 10381.IEEE,2023. [3] T.Asfour,P.Azad,F.Gyarfas,and R.Dillmann. Imitationlearningofdual-armmanipulation tasksinhumanoidrobots.",
      "start_pos": 3696,
      "end_pos": 4208
    },
    {
      "chunk_id": 399,
      "paper_id": "OKAMI",
      "text": "Yuzhe Qin for sharing the dex-retargeting code base, and Zhenjia Xu for his advice on developing the humanoidrobotinfrastructure. References [1] M.Seo,S.Han,K.Sim,S.H.Bang,C.Gonzalez,L.Sentis,and Y.Zhu. Deepimitationlearn- ing for humanoidloco-manipulationthroughhumanteleoperation. In IEEE-RASInternational Conferenceon Humanoid Robots(Humanoids),2023. [2] Y.Matsuura,K.Kawaharazuka,N.Hiraoka,K.Kojima,K.Okada,and M.Inaba.Development ofawhole-bodyworkimitationlearningsystembyabipedandbi-armedhumanoid. In 2023 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems(IROS),pages 10374\u2013 10381.IEEE,2023. [3] T.Asfour,P.Azad,F.Gyarfas,and R.Dillmann. Imitationlearningofdual-armmanipulation tasksinhumanoidrobots. Internationalj our nalofhumanoidrobotics,5(02):183\u2013202,2008. [4] Y. Zhu, A. Lim, P. Stone, and Y. Zhu. Vision-based manipulation from single human video with open-worldobjectgraphs. ar Xivpreprintar Xiv:2405.20321,2024. [5] N.Heppert,M.Argus,T.Welschehold,T.Brox,and A.Valada.Ditto:Demonstrationimitation bytrajectorytrans for mation. ar Xivpreprintar Xiv:2403.15203,2024. [6] D. Guo. Learning multi-step manipulation tasks from a single human demonstration. ar Xiv preprintar Xiv:2312.15346,2023. [7] T.Asfour and R.Dillmann. Human-likemotionofahumanoidrobotarm base donaclosed- formsolutionof the inversekinematicsproblem. In Proceedings 2003 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems(IROS 2003)(Cat.No.03 CH 37453),volume 2, pages 1407\u20131412.IEEE,2003. [8] M.Gleicher.Retargettingmotionto new characters.Proceedingsof the 25 thannualconference on Computergraphics and interactivetechniques,1998. [9] K.Darvish,Y.Tirupachuri,G.Romualdi,L.Rapetti,D.Ferigo,F.J.A.Chavez,and D.Pucci. Whole-bodygeometricretargeting for humanoidrobots.In 2019 IEEE-RAS 19 th International Conferenceon Humanoid Robots(Humanoids),pages 679\u2013686.IEEE,2019. [10] X.Cheng,Y.Ji,J.Chen,R.Yang,G.Yang,and X.Wang. Expressivewhole-bodycontrol for humanoidrobots. ar Xivpreprintar Xiv:2402.16796,2024. [11] S.Nakaoka,A.Nakazawa,F.Kanehiro,K.Kaneko,M.Morisawa,and K.Ikeuchi.Task model oflowerbodymotion for abipedhumanoidrobottoimitatehum and ances. In 2005 IEEE/RSJ International Conferenceon Intelligent Robots and Systems,pages 3157\u20133162.IEEE,2005. [12] K.Hu,C.Ott,and D.Lee. Onlinehumanwalkingimitationin task and jointspace base don quadraticprogramming. In 2014 IEEEInternational Conferenceon Robotics and Automation (ICRA),pages 3458\u20133464.IEEE,2014. [13] S. Choi, M. K. Pan, and J. Kim. Nonparametric motion retargeting for humanoid robots on sharedlatentspace. In Robotics: Science and Systems,2020. [14] E.Demir can,T.Besier,S.Menon,and O.Khatib. Humanmotionreconstruction and syn the sis ofhumanskills. In Advancesin Robot Kinematics: Motionin Manand Machine: Motionin Manand Machine,pages 283\u2013292.Springer,2010. 9 [15] Open AI. Gpt-4 technicalreport,2023. [16] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,H.Su,J.Zhu,etal.Grounding dino: Marryingdino with groundedpre-training for open-setobjectdetection. ar Xivpreprint ar Xiv:2303.05499,2023. [17] T.He,Z.Luo,W.Xiao,C.Zhang,K.Kitani,C.Liu,and G.Shi.Learninghuman-to-humanoid real-timewhole-bodyteleoperation. Inar Xiv,2024. [18] A.Escande,N.Mansard,and P.-B.Wieber. Hierarchicalquadraticprogramming: Fastonline humanoid-robot motion generation. The International Journal of Robotics Research, 33(7): 1006\u20131028,2014. [19] Q. Liao, B. Zhang, X. Huang, X. Huang, Z. Li, and K. Sreenath. Berkeley humanoid: A researchplatform for learning-basedcontrol. ar Xivpreprintar Xiv:2407.21781,2024. [20] L.Penco,N.Scianca,V.Modugno,L.Lanari,G.Oriolo,and S.Ivaldi. Amultimodeteleop- erationframework for humanoidloco-manipulation: Anapplication for the icubrobot. IEEE Robotics&Automation Magazine,26(4):73\u201382,2019. [21] D. Kim, B.-J. You, and S.-R. Oh. Whole body motion control framework for arbitrarily and simultaneously assigned upper-body tasks and walking motion. Modeling, Simulation and Optimizationof Bipedal Walking,pages 87\u201398,2013. [22] A.Di Fava,K.Bouyarmane,K.Chappellet,E.Ruffaldi,and A.Kheddar.Multi-contactmotion retargeting from humantohumanoidrobot. In 2016 IEEE-RAS 16 thinternationalconference onhumanoidrobots(humanoids),pages 1081\u20131086.IEEE,2016. [23] M.Arduengo,A.Arduengo,A.Colome\u00b4,J.Lobo-Prat,and C.Torras. Humantorobotwhole- bodymotiontransfer. In 2020 IEEE-RAS 20 th International Conferenceon Humanoid Robots (Humanoids),pages 299\u2013305.IEEE,2021. [24] R.Cisneros,M.Benallegue,K.Kaneko,H.Kaminaga,G.Caron,A.Tanguy,R.Singh,L.Sun, A. Dallard, C. Fournier, et al. Team janus humanoid avatar: A cybernetic avatar to embody human telepresence. In Toward Robot Avatars: Perspectives on the ANA Avatar XPRIZE Competition,RSSWorkshop,volume 3,2022. [25] S. Tachi, K. Komoriya, K. Sawada, T. Nishiyama, T. Itoko, M. Kobayashi, and K. Inoue. Telexistencecockpit for humanoidrobotcontrol. Advanced Robotics,17(3):199\u2013217,2003. [26] J.Ramos and S.Kim. Humanoiddynamicsynchronizationthroughwhole-bodybilateralfeed- backteleoperation. IEEETransactionson Robotics,34(4):953\u2013965,2018. [27] Y.Ishiguro,T.Makabe,Y.Nagamatsu,Y.Kojio,K.Kojima,F.Sugai,Y.Kakiuchi,K.Okada, and M.Inaba. Bilateralhumanoidteleoperationsystemusingwhole-bodyexoskeletoncockpit tablis. IEEERobotics and Automation Letters,5(4):6419\u20136426,2020. [28] F.Abi-Farrajl,B.Henze,A.Werner,M.Panzirsch,C.Ott,and M.A.Roa.Humanoidteleoper- ationusing task-relevanthapticfeedback.In 2018 IEEE/RSJInternational Conferenceon Intel- ligent Robots and Systems(IROS),pages 5010\u20135017,2018. doi:10.1109/IROS.2018.8593521. [29] M. Schwarz, C. Lenz, A. Rochow, M. Schreiber, and S. Behnke. Nimbro avatar: Interactive immersivetelepresence with force-feedbacktelemanipulation.In 2021 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems(IROS),pages 5312\u20135319.IEEE,2021. [30] M. Hirschmanner, C. Tsiourti, T. Patten, and M. Vincze. Virtual reality teleoperation of a humanoid robot using markerless human upper body pose imitation.",
      "start_pos": 4158,
      "end_pos": 4670
    },
    {
      "chunk_id": 400,
      "paper_id": "OKAMI",
      "text": "[29] M. Schwarz, C. Lenz, A. Rochow, M. Schreiber, and S. Behnke. Nimbro avatar: Interactive immersivetelepresence with force-feedbacktelemanipulation.In 2021 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems(IROS),pages 5312\u20135319.IEEE,2021. [30] M. Hirschmanner, C. Tsiourti, T. Patten, and M. Vincze. Virtual reality teleoperation of a humanoid robot using markerless human upper body pose imitation. in 2019 ieee-ras 19 th internationalconferenceonhumanoidrobots(humanoids),2019. 10 [31] D.Lim,D.Kim,and J.Park.Onlinetelemanipulationframeworkonhumanoid for bothmanip- ulation and imitation. 202219 th International Conferenceon Ubiquitous Robots(UR),pages 8\u201315,2022. URLhttps://api.semanticscholar.org/Corpus ID:250577582. [32] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn. Humanplus: Humanoid shadowing and imitation from humans. ar Xivpreprintar Xiv:2406.10454,2024. [33] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. Mart\u00b4\u0131n-Mart\u00b4\u0131n. What matters in learning from offline human demonstrations forrobotmanipulation. ar Xivpreprintar Xiv:2108.03298,2021. [34] A. Mandlekar, D. Xu, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, S. Savarese, and L. Fei-Fei. Learning to general- izeacrosslong-horizontasks from hum and emonstrations. ar Xivpreprintar Xiv:2003.06085, 2020. [35] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu,Y.Zhu,and A.Anandkumar.Mimicplay: Long-horizonimitationlearningbywatchinghumanplay. ar Xivpreprintar Xiv:2302.12422, 2023. [36] Y.Zhu,A.Joshi,P.Stone,and Y.Zhu. Viola:Imitationlearning for vision-basedmanipulation withobjectproposalpriors. ar Xivpreprintar Xiv:2210.11339,2022. [37] C.Wang,H.Shi,W.Wang,R.Zhang,L.Fei-Fei,and C.K.Liu.Dexcap:Scalable and portable mocap data collection system for dexterous manipulation. ar Xiv preprint ar Xiv:2403.07788, 2024. [38] T.Lin,Y.Zhang,Q.Li,H.Qi,B.Yi,S.Levine,and J.Malik. Learningvisuotactileskills with twomultifingeredhands. ar Xivpreprintar Xiv:2404.16823,2024. [39] Y.Ze,G.Zhang,K.Zhang,C.Hu,M.Wang,and H.Xu. 3 ddiffusionpolicy. ar Xivpreprint ar Xiv:2403.03954,2024. [40] M.Chang and S.Gupta. One-shotvisualimitationviaattributedwaypoints and demonstration augmentation. ar Xivpreprintar Xiv:2302.04856,2023. [41] T. Yu, P. Abbeel, S. Levine, and C. Finn. One-shot composition of vision-based skills from demonstration. In 2019 IEEE/RSJInternational Conferenceon Intelligent Robots and Systems (IROS),pages 2643\u20132650.IEEE,2019. [42] E. Valassakis, G. Papagiannis, N. Di Palo, and E. Johns. Demonstrate once, imitate imme- diately (dome): Learning visual servoing for one-shot imitation learning. In 2022 IEEE/RSJ International Conferenceon Intelligent Robots and Systems(IROS),pages 8614\u20138621.IEEE, 2022. [43] E.Johns. Coarse-to-fineimitationlearning: Robotmanipulation from asingledemonstration. In 2021 IEEEinternationalconferenceonrobotics and automation(ICRA),pages 4613\u20134619. IEEE,2021. [44] N.Di Palo and E.Johns. Learningmulti-stagetasks with onedemonstrationviaself-replay. In Conferenceon Robot Learning,pages 1180\u20131189.PMLR,2022. [45] Z. Luo, J. Cao, K. Kitani, W. Xu, et al. Perpetual humanoid control for real-time simulated avatars. In Proceedingsof the IEEE/CVFInternational Conferenceon Computer Vision,pages 10895\u201310904,2023. [46] X.B.Peng,Z.Ma,P.Abbeel,S.Levine,and A.Kanazawa. Amp: Adversarialmotionpriors for stylized physics-based character control. ACM Transactions on Graphics (To G), 40(4): 1\u201320,2021. 11 [47] B.Jiang,X.Chen,W.Liu,J.Yu,G.Yu,and T.Chen. Motiongpt: Humanmotionasa for eign language. Advancesin Neural Information Processing Systems,36,2024. [48] S.Kuindersma,R.Deits,M.Fallon,A.Valenzuela,H.Dai,F.Permenter,T.Koolen,P.Marion, and R.Tedrake. Optimization-basedlocomotionplanning,estimation,andcontroldesign for theatlashumanoidrobot. Autonomousrobots,40:429\u2013455,2016. [49] Y. Liang, W. Li, Y. Wang, R. Xiong, Y. Mao, and J. Zhang. Dynamic movement primitive based motion retargeting for dual-arm sign language motions. In 2021 IEEE International Conferenceon Robotics and Automation(ICRA),pages 8195\u20138201.IEEE,2021. [50] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, and L. Van Gool. The 2019 davis challenge on vos: Unsupervised multi-object segmentation. ar Xiv preprint ar Xiv:1905.00737,2019. [51] Y. Huang, J. Yuan, C. Kim, P. Pradhan, B. Chen, L. Fuxin, and T. Hermans. Out of sight, still in mind: Reasoning and planning about unobserved objects with video tracking enabled memorymodels. ar Xivpreprintar Xiv:2309.15278,2023. [52] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu. Learning generalizable manipulation policies with object-centric 3 drepresentations. In 7 th Annual Conferenceon Robot Learning,2023. [53] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,S.Kirmani, B.Zitkovich,F.Xia,etal. Open-worldobjectmanipulationusingpre-trainedvision-language models. ar Xivpreprintar Xiv:2303.00905,2023. [54] H.K.Cheng,S.W.Oh,B.Price,J.-Y.Lee,and A.Schwing. Putting the objectbackintovideo object",
      "start_pos": 4620,
      "end_pos": 5132
    },
    {
      "chunk_id": 401,
      "paper_id": "OKAMI",
      "text": "unobserved objects with video tracking enabled memorymodels. ar Xivpreprintar Xiv:2309.15278,2023. [52] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu. Learning generalizable manipulation policies with object-centric 3 drepresentations. In 7 th Annual Conferenceon Robot Learning,2023. [53] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,S.Kirmani, B.Zitkovich,F.Xia,etal. Open-worldobjectmanipulationusingpre-trainedvision-language models. ar Xivpreprintar Xiv:2303.00905,2023. [54] H.K.Cheng,S.W.Oh,B.Price,J.-Y.Lee,and A.Schwing. Putting the objectbackintovideo object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 3151\u20133161,2024. [55] V.Ye,G.Pavlakos,J.Malik,and A.Kanazawa. Decouplinghuman and cameramotion from videosin the wild. In CVPR,2023. [56] J.Romero,D.Tzionas,and M.J.Black. Embodiedhands. ACMTransactionson Graphics,36 (6):1\u201317,2017. [57] G.Pavlakos,D.Shan,I.Radosavovic,A.Kanazawa,D.Fouhey,and J.Malik. Reconstructing handsin 3 Dwithtrans for mers. In CVPR,2024. [58] N.Karaev,I.Rocco,B.Graham,N.Neverova,A.Vedaldi,and C.Rupprecht. Cotracker: Itis bettertotracktogether. ar Xivpreprintar Xiv:2307.07635,2023. [59] Q.-Y.Zhou,J.Park,and V.Koltun. Open 3 d: Amodernlibrary for 3 ddataprocessing. ar Xiv preprintar Xiv:1801.09847,2018. [60] Y. Zhu, J. Wong, A. Mandlekar, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, A. Joshi, S. Nasiriany, and Y. Zhu. ro- bosuite: Amodularsimulationframework and benchmark for robotlearning. ar Xivpreprint ar Xiv:2009.12293,2020. [61] T.Z.Zhao, V.Kumar, S.Levine, and C.Finn. Learningfine-grainedbimanualmanipulation withlow-costhardw are. ar Xivpreprintar Xiv:2304.13705,2023. [62] S.Goel,G.Pavlakos,J.Rajasegaran,A.Kanazawa,and J.Malik. Humansin 4 D:Reconstruct- ing and trackinghumans with trans for mers. In ICCV,2023. [63] Y.Xu,J.Zhang,Q.Zhang,and D.Tao. Vi TPose++: Visiontransformer for genericbodypose estimation. IEEETransactionson Pattern Analysis and Machine Intelligence,2023. [64] S.Caron,Y.De Mont-Marin,R.Budhiraja,and S.H.Bang. Pink: Pythoninversekinematics basedon Pinocchio,2024. URLhttps://github.com/stephane-caron/pink. 12 [65] Y.Qin,W.Yang,B.Huang,K.Van Wyk,H.Su,X.Wang,Y.-W.Chao,and D.Fox.Anyteleop: Ageneralvision-baseddexterousrobotarm-handteleoperationsystem. In Robotics: Science and Systems,2023. [66] R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear computational cost. Journal of the Ameri can Statistical Association, 107(500):1590\u20131598, 2012. [67] X.Cheng,J.Li,S.Yang,G.Yang,and X.Wang. Open-television: Teleoperation with immer- siveactivevisualfeedback. ar Xivpreprintar Xiv:2407.01512,2024. [68] T.Darcet,M.Oquab,J.Mairal,and P.Bojanowski. Visiontrans for mersneedregisters. ar Xiv preprintar Xiv:2309.16588,2023. [69] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li, W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov 2: Learning robust visual features without supervision,2023. 13 A Implementation Details A.1 Human Reconstruction From Videos Method. Forthe 3 Dhumanreconstruction,westartbytrackingthepersonin the video and getting aninitialestimateoftheir 3 Dbodyposeusing 4 DHumans[62]. Thisbodyreconstruction can not capture the handposedetails(i.e.,theh and sareflat). Therefore,foreachdetectionof the personin thevideo, wedetect the twoh and susing Vi TPose[63], and for eachh and, weapply Ha Me R[57] togetanestimateof the 3 Dhandpose. However,theh and sreconstructedby Ha Me Rcan beincon- sistent with the arms from the body reconstruction (e.g., different wrist orientation and location). Toaddress this,weapplyanoptimizationrefinementtomakethebody and the handsconsistentin each frame, andenc our age that the holistic body and hands motion is smooth overtime. This op- timizationissimilarto SLAHMR[55],withthedifference that besides the bodypose and location ofthe SMPL+Hmodel[56],wealsooptimize the handposes. Weinitialize the procedureusing the 3 Dbodyposeestimate from 4 DHumans and the 3 Dhandposes from Ha Me R.Moreover, we use the 2 Dprojectionof the 3 Dhandspredictedby Ha Me Rtoconstrain the projectionof the 3 Dhand keypoints of the holistic model using a reprojection loss. Finally, we can jointly optimize all the parameters (body location, body pose, hand poses) over the duration of the video, as described in SLAHMR[55]. Our modified SLAHMR incorporates the SMPL-H model [56] to include hand poses in the hu- manmotionreconstruction. Weinitializeh and posesineachframeusing 3 Dhandestimates",
      "start_pos": 5082,
      "end_pos": 5594
    },
    {
      "chunk_id": 402,
      "paper_id": "OKAMI",
      "text": "model using a reprojection loss. Finally, we can jointly optimize all the parameters (body location, body pose, hand poses) over the duration of the video, as described in SLAHMR[55]. Our modified SLAHMR incorporates the SMPL-H model [56] to include hand poses in the hu- manmotionreconstruction. Weinitializeh and posesineachframeusing 3 Dhandestimates from Ha Me R [57]. The optimization process then jointly refines body locations, body poses, and hand posesover the videosequence. Thisjointoptimizationallows for accurate model ingofhowhands interact with objects,whichiscrucial for manipulationtasks. Theoptimizationminimizes the errorbetween the 2 Dprojectionsof the 3 Djoints from the SMPL-H model and the detected 2 Djointlocations from the video. we usestandardparameters and settings asdescribedin SLAHMR[55],adapting the mtoaccommodate the SMPL-Hmodel. Inference Requirements. The model ofhumanreconstructionwe useislarge and needstoberun onacomputer with sufficientlygoodcomputationspeed. Hereweprovidedetailsabout the runtime per for manceof the humanreconstruction model.we useadesktop that comeswitha GPURTX 3090 thathasthesizeof the memory 24 GB.Fora 10 secondsvideo with fps 30,itprocesses 10 minutes. A.2 Promptsof Using GPT 4 V In order to use GPT 4 V in OKAMI, we need GPT 4 V\u2019s output to be in a typed format so that the restof the programs can parse the result. Moreover,inorder for the promptstobegeneralacrossa diversesetoftasks,ourpromptdoesnotleakanytaskin for mationto the model. Herewedescribe thethreedifferentpromptsin OKAMI for using GPT 4 V. Identify Task-relevant Objects. OKAMI uses the followingprompttoinvoke GPT 4 Vsothatit canidentify the task-relevantobjects from aprovidedhumanvideo: Prompt: Youneedtoanalyzewhatthehumanisdoingin the images,thentellme: 1. Allthe objectsinfrontscene(mostlyon the table). Youshouldignore the backgroundobjects. 2. The objectsofinterest. Theyshould beasubsetofy our answerto the firstquestion. They are likely theobjectsmanipulatedbyhumanornearhuman. Note that the reareirrelevantobjectsin the scene,suchasobjects that doesnotmoveatall. Youshouldignore the irelevantobjects. Youroutput for matis: The human is xxx. All objects are xxx. The objects of interest are: \u2018\u2018\u2018json { 14 \"objects\": [\"OBJECT 1\", \"OBJECT 2\", ...], } \u2018\u2018\u2018 Ensure the response can beparsedby Python\u2018json.loads\u2019, e.g.: notrailingcommas, nosingle quotes, etc. You should output the names of objects of interest in a list [\u201cOBJECT 1\u201d, \u201cOB- JECT 2\u201d, ...] that can be easily parsed by Python. The name is a string, e.g., \u201capple\u201d, \u201cpen\u201d, \u201ckeyboard\u201d,etc. Identify Target Objects. OKAMI usesthefollowingprompttoidentify the targetobjectofeach stepin the referenceplan: Prompt:Thefollowingimagesshowsamanipulationmotion,where the humanismanipulating anobject. Your task istodetermi new hichobjectisbeingmanipulatedin the imagesbelow. Youneedto choose from the followingobjects: {alistof task-relevantobjects}. Tips:themanipulatedobjectistheobject that the humanisinteracting with,suchaspickingup, moving,orpressing,anditisincontact with the human\u2019s{themajormovingarmin this step} hand. Youroutput for matis: \u2018\u2018\u2018json { \"manipulate_object_name\": \"MANIPULATE_OBJECT_NAME\", } \u2018\u2018\u2018 Ensure the response can beparsedby Python\u2018json.loads\u2019, e.g.: notrailingcommas, nosingle quotes,etc. Identify Reference Objects. Hereis the prompt that asks GPT 4 Vtoidentify the referenceobject ofeachstepin the referenceplan: Prompt:Thefollowingimagesshowsamanipulationmotion,where the humanismanipulating theobject{manipulate object name}. Please identify the reference object in the image below, which could be an object on which toplace{manipulate object name}, oranobject that{manipulate object name}isinteracting with. Note that the remaynotnecessarily have anreferenceobject, assometimeshumanmay just playing with the object itself, like throwing it, or spinning it around. You need to first identify whether there is a reference object. If so, you need to output the reference object\u2019s namechosen from the followingobjects: {alistof task-relevantobjects}. Youroutput for matis: \u2018\u2018\u2018json { \"reference_object_name\": \"REFERENCE_OBJECT_NAME\" or \"None\", } \u2018\u2018\u2018 Ensure the response can beparsedby Python\u2018json.loads\u2019, e.g.: notrailingcommas, nosingle quotes,etc. 15 A.3 Detailson Factorized Process for Retargeting Body Motion Retarget. To retarget body motions from",
      "start_pos": 5544,
      "end_pos": 6056
    },
    {
      "chunk_id": 403,
      "paper_id": "OKAMI",
      "text": "so, you need to output the reference object\u2019s namechosen from the followingobjects: {alistof task-relevantobjects}. Youroutput for matis: \u2018\u2018\u2018json { \"reference_object_name\": \"REFERENCE_OBJECT_NAME\" or \"None\", } \u2018\u2018\u2018 Ensure the response can beparsedby Python\u2018json.loads\u2019, e.g.: notrailingcommas, nosingle quotes,etc. 15 A.3 Detailson Factorized Process for Retargeting Body Motion Retarget. To retarget body motions from the SMPL-H representation to the hu- manoid, we extract the shoulder, elbow, and wrist poses from the SMPL-H models. We then use inversekinematicstosolvethebodyjointson the humanoid,ensuring the yproducesimilarshoulder and elbow orientations and similar wrist poses. The inverse kinematics is implemented using an open-sourcedlibrary Pink[64]. The IKweightswe use for shoulderorientation,elboworientation, wristorientation,andwristposition are 0.04,0.04,0.08,and 1.0,respectively. Hand Pose Mapping. As we describe in the method section, we first retarget the hands from SMPL-Hmodelsto the humanoid\u2019sdexteroush and susingahybridimplementationofinversekine- matics and anglemapping. Here are the detailsofhow this mappingisper for med. Onceweobtain the SMPL-Hmodels from avideodemonstration,we canobtain the locationsof 3 Djoints from the handmeshmodels from SMPL-H.Subsequently,we cancompute the rotatinganglesofeachjoint thatcorrespondtocertainh and poses. Thenweapplythecomputedjointanglesto the handmeshes of a canonical SMPL-H model, which is pre-defined to have the same size as the humanoid robot hardw are. From this canonical SMPL-H model, we can get the 3 D keypoints of hand joints and useanexistingpackage,dex-retarget,anoff-the-shelfoptimizationpackagetodirectlycompute the handjointanglesof the robot[65]. Inverse Kinematics. Afterwarping the armtrajectory,we useinversekinematicstocompute the robot\u2019s joint configurations. We assign weights of 1.0 to hand position and 0.08 to hand rotation, prioritizingaccurateh and placementwhileallowing the armstomaintainnaturalpostures. For retargeting human hand poses to the robot, we map the human hand joint angles to the corre- spondingjointsin the robot\u2019shand. Thisenables the robottoreplicatefine-grainedmanipulations demonstrated by the human, such as grasping and object interaction. Our implementation ensures that the retargeted motions are physically feasible for the robot and that overall execution appears natural and effective for the taskath and. A.4 Additional Detailsof Plan Generation For temporal segmentation, we sample keypoints from the segmented objects in the first frame and track them across the video using Co Tracker [58]. We compute the average velocity of these keypointsateachframe and applyanunsupervisedchangepointdetectionalgorithm[66]todetect signifi can tchangesinmotion,identifyingkeyframes that correspondtosubgoalstates. To determine contact between objects, we compute the relative spatial locations and distances be- tween the pointcloudsofobjects. Ifthedistancebetweenobjectsfallsbelowapredefinedthreshold, we consider them to be in contact. For non-contact relations that are difficult to infer geometri- cally\u2014suchasacupinap our ing task\u2014we use GPT 4 Vtopredictsemanticrelations base don the visualcontext. GPT 4 Vcaninfer that thecupistherecipientinap our ingactionevenif the reisno directcontact. A.5 Trajectory Warping Here, we mathematically describe the process of trajectory warping. We denote the trajectory for robotas\u03c4robotretargetedfrom\u03c4SMPL inthegeneratedplan. Denote the startingpoint and endpoint ti:ti+1 of\u03c4robot asp ,p ,respectively. Note that allpointsalong the trajectory are representedin SE(3) start end space. Eachpointp ontheoriginalretargetdtrajectory can bedescribedby the followingfunction: t p =p +(\u03c4robot(t)\u2212p ) (1) t start start wheret\u2208{t ,...,t },\u03c4robot(t )=p ,\u03c4robot(t )=p . i i+1 i start i+1 end When warping the trajectory, we either only needs to adapt the trajectory to the new target object location, or adapt the trajectory to the new locations of both the target and the reference objects, 16 as described in Section 3.2. Without loss of generality, we denote the SE(3) trans for mation for the startingpoint is T , andthe SE(3)trans",
      "start_pos": 6006,
      "end_pos": 6518
    },
    {
      "chunk_id": 404,
      "paper_id": "OKAMI",
      "text": "adapt the trajectory to the new target object location, or adapt the trajectory to the new locations of both the target and the reference objects, 16 as described in Section 3.2. Without loss of generality, we denote the SE(3) trans for mation for the startingpoint is T , andthe SE(3)trans for mation forthe endpoint is T . Nowthe warped start end trajectory can bedescribedby the followingfunction: p =T \u00b7p +(\u03c4\u02c6robot(t)\u2212T \u00b7p ) (2) t start start start start where\u03c4\u02c6robot(t)= \u03c4robot(t)\u2212pstart(T \u00b7p \u2212T \u00b7p )+T \u00b7p ,\u2200t\u2208{t ,...,t }.Inthisway, pend\u2212pstart end end start start start start i i+1 wehave\u03c4\u02c6robot(t )=T \u00b7p ,\u03c4\u02c6robot(t )=T \u00b7p . Note that thistrajectorywarpingassumes i start start i+1 end end theendpointofatrajectoryisnotthesameas the startingpoint,whichisacommonassumption for mostof the manipulationbehaviors. B Additional Experimental Details B.1 Success Conditions Wedescribe the successconditionswe usetoevaluateifa task rolloutissuccessfulornot. \u2022 Sprinkle-salt: Thesaltbottlereachesapositionwhere the saltisp our edoutinto the bowl. \u2022 Plush-toy-in-basket:Theplushtoyisputinside the container,withmorethan 50% ofthetoyinside the container. \u2022 Close-the-laptop: Thedisplayislo were dtowardsthe base until the twopartsmeet atthehinge(aka the laptopisclosed). \u2022 Close-the-drawer: Thedrawerispushedbackto the containingregion, eitherit\u2019sa draweroralayerofacabinet. \u2022 Place-snacks-on-plate: The snack is placed on top of the plate, with more than 50%ofthesnackpackageon the plate. \u2022 Bagging: Thechipbagisputinto the shoppingbagwhichisinitiallyclosed. B.2 Implementationof Baseline We implement the baseline ORION [4] with minimal modifications to apply it to our humanoid setting.First,weestimate the palmtrajectory from SMPL-Htrajectoriesbyusing the centerpointof thereconstructedfingersas the palmpositionateachtimestep. Next,wewarp the palmtrajectory basedon the test-timeobjects\u2019locations. Finally,we useinversekinematicstosolve for the robot\u2019s bodyjoints,withthewarpedtrajectoryservingas the targetpalmposition. B.3 Detailson Different Demonstrators Figure 6 shows the videos of three different human demonstrators per for ming Place-snacks-on-plate and Close-the-laptop tasks. We calculate the success ratesofimitatingdifferentvideos,and the results are shownin Figure 4(b). B.4 Simulation Evaluation For easy reproducibility, we replicate two tasks, Sprinkle-salt and Close-the-drawer, insimulation(Figure 7). Weimplement the setasksusingrobosuite[60],whichrecentlyprovided cross-embodimentsupport, includinghumanoidmanipulation. we use\u201cGR 1 Fixed Lower Body\u201das therobotembodimentin the setwotasks. Note that for the policy of each task, we use the same human video as the ones used in real robot experiments. Wecomp are threemethodsinsimulation: OKAMI(w/vision),OKAMI(w/ovision), and ORION.OKAMI(w/vision)thesamemethodwe usein our realrobotexperiments. OKAMI (w/o vision) is the simplified version of OKAMI where we assume the model directly gets the 17 Figure 6: Theinitialandendframesofvideosper for medbydifferenthum and emonstrators. Thefirstrowis Place-snacks-on-plate task,and the secondrowis Close-the-laptop task. ground-truth poses of objects. The evaluation results are shown in Table 1, where each reported numberis the successrateaveragedover 50 rollouts. Wenotice that thesimulationresults are generallybetterthan the realrobotexperiments. Theper- formancedifferencecomes from the easyphysicalinteractionbetweendexteroushands and objects comp are dto the realrobothardw are. Also,OKAMI with outvision can achieveamuchhighersuc- cess rate than OKAMI with vision because the noise and uncertainty of perception are abstracted away. Specifically,alargeportionofuncertaintiescome from the partialobservationofobjectpoint clouds,andtheestimationoftheobjectlocationisoff the ground-truthlocationsofobjects,while the successof OKAMIhighlydependson the qualityoftrajectorywarping,whichisdependenton the correctestimationofobjectlocations. Thissimulationresultalsoindicates that the per for manceof OKAMIisexpectedtoimproveifmorepowerfulvisionmodels with higheraccuracy are available. Method Sprinkle-salt Close-the-drawer OKAMI(w/vision) 82% 84% OKAMI(w/ovision) 100% 100% ORION 0% 10% Table 1: The average success rates (%) across different methods in two tasks, Sprinkle-salt and Close-the-drawer B.5 Visuomotor Policy Details Wechoose ACT[61]inourexperiments for behavioralcloning,analgorithm that has been shown effective in learning humanoid manipulation policies [67]. Notably, we choose pretrained Di- no V 2 [68, 69] as the visual backbone of a policy. The policy takes a single",
      "start_pos": 6468,
      "end_pos": 6980
    },
    {
      "chunk_id": 405,
      "paper_id": "OKAMI",
      "text": "across different methods in two tasks, Sprinkle-salt and Close-the-drawer B.5 Visuomotor Policy Details Wechoose ACT[61]inourexperiments for behavioralcloning,analgorithm that has been shown effective in learning humanoid manipulation policies [67]. Notably, we choose pretrained Di- no V 2 [68, 69] as the visual backbone of a policy. The policy takes a single RGB image and 26- dimensionjointpositionsasinput and outputs the actionof the 26-dimensionabsolutejointposition for the robottoreach. In Table 2,weshow the hyperparametersused for behavioralcloning. 18 KLweight 10 chunksize 60 hiddendimension 512 batch size 45 feed for warddimension 3200 epochs 25000 learning rate 5 e-5 temporalweighting 0.01 Table 2:Thehyperparametersusedin ACT. Close-the-drawer Sprinkle-salt Figure 7:Thescreenshotsofthestarting and endingframesof the twosimulationtasks,Close-the-drawer and Sprinkle-salt. 19",
      "start_pos": 6930,
      "end_pos": 7037
    }
  ]
}