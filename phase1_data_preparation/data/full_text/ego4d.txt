 
 
 
 
 
 
 
 
 Ego 4 D: Around the World in 3,000 Hours of Egocentric Video 
 
 
 Kristen Grauman 1,2,Andrew Westbury 1,Eugene Byrne∗1,Zachary Chavis∗3,Antonino Furnari∗4,
 Rohit Girdhar∗1,Jackson Hamburger∗1,Hao Jiang∗5,Miao Liu∗6,Xingyu Liu∗7,Miguel Martin∗1,
 Tushar Nagarajan∗1,2,Ilija Radosavovic∗8,Santhosh Kumar Ramakrishnan∗1,2,Fiona Ryan∗6,
 Jayant Sharma∗3,Michael Wray∗9,Mengmeng Xu∗10,Eric Zhongcong Xu∗11,Chen Zhao∗10,
 Siddhant Bansal 17,Dhruv Batra 1,Vincent Cartillier 1,6,Sean Crane 7,Tien Do 3,Morrie Doulaty 13,
 Akshay Erapalli 13,Christoph Feichtenhofer 1,Adriano Fragomeni 9,Qichen Fu 7,
 
 Abrham Gebreselasie 12,Cristina Gonza´lez 14,James Hillis 5,Xuhua Huang 7,Yifei Huang 15,
 Wenqi Jia 6,Weslie Khoo 16,Ja´chym Kola´ˇr 13,Satwik Kottur 13,Anurag Kumar 5,Federico Landini 13,
 Chao Li 5,Yanghao Li 1,Zhenqiang Li 15,Karttikeya Mangalam 1,8,Raghava Modhugu 17,
 Jonathan Munro 9,Tullie Murrell 1,Takumi Nishiyasu 15,Will Price 9,Paola Ruiz Puentes 14,
 Merey Ramazanova 10,Leda Sari 5,Kiran Somasundaram 5,Audrey Sou the rland 6,Yusuke Sugano 15,
 Ruijie Tao 11,Minh Vo 5,Yuchen Wang 16,Xindi Wu 7,Takuma Yagi 15,Ziwei Zhao 16,Yunyi Zhu 11,
 Pablo Arbela´ez†14,David Crandall†16,Dima Damen†9,Giovanni Maria Farinella†4,
 Christian Fuegen†13,Bernard Ghanem†10,Vamsi Krishna Ithapu†5,C.V.Jawahar†17,Hanbyul Joo†1,
 Kris Kitani†7,Haizhou Li†11,Richard Newcombe†5,Aude Oliva†18,Hyun Soo Park†3,
 James M.Rehg†6,Yoichi Sato†15,Jianbo Shi†19,Mike Zheng Shou†11,Antonio Torralba†18,
 Lorenzo Torresani†1,20,Mingfei Yan†5,Jitendra Malik 1,8 
 
 
 1 Facebook AIResearch(FAIR),2 Universityof Texasat Austin,3 Universityof Minnesota,4 Universityof Catania,
 5 Facebook Reality Labs,6 Georgia Tech,7 Carnegie Mellon University,8 UCBerkeley,9 Universityof Bristol,
 10 King Abdullah Universityof Science and Technology,11 National Universityof Singapore,
 12 Carnegie Mellon University Africa,13 Facebook,14 Universidaddelos Andes,15 Universityof Tokyo,16 Indiana University,
 17 International Instituteof Information Technology,Hyderabad,18 MIT,19 Universityof Pennsylvania,20 Dartmouth
 
 
 Abstract episodicmemory),present(analyzingh and-objectmanipu-
 lation,audio-visualconversation,andsocialinteractions),
 andfuture(forecastingactivities). Bypubliclysharing this
 Weintroduce Ego 4 D,amassive-scaleegocentricvideo 
 massiveannotated data set and benchmarksuite,weaimto
 dataset and benchmarksuite. Itoffers 3,670 hoursofdaily- 
 push the frontieroffirst-personperception. Projectpage:
 lifeactivityvideospanninghundredsofscenarios(house- 
 https://ego 4 d-data.org/ 
 hold, outdoor, workplace, leisure, etc.) captured by 931 
 uniquecamerawe are rsfrom 74 worldwidelocations and 9 
 differentcountries. Theapproachtocollectionisdesigned 
 toupholdrigorousprivacyandethicsst and ards,withcon- 1.Introduction 
 sentingparticipants and robustde-identificationprocedures 
 whererelevant. Ego 4 Ddramaticallyexpands the volumeof Today’scomputervisionsystemsexcelatnamingobjects
 diverse egocentric video footage publicly available to the andactivitiesin Internetphotosorvideoclips. Theirtremen-
 researchcommunity. Portionsof the video are accompanied dousprogressover the lastdecadehas been fueledbymajor
 byaudio, 3 Dmeshesof the environment, eyegaze, stereo, dataset and benchmark efforts, which provide the annota-
 and/orsynchronizedvideos from multipleegocentriccam- tionsneededtotrain and evaluatealgorithmsonwell-defined
 erasat the sameevent. Fur the rmore,wepresentahostof tasks[49,60,61,92,108,143]. 
 newbenchmarkchallengescenteredaroundunderst and ing While this progressisexciting,current data sets and mod-
 thefirst-personvisualexperiencein the past(queryingan elsrepresentonlyalimiteddefinitionofvisualperception.
 1 
 2202 
 ra M 
 11 
 ]VC.sc[ 
 3 v 85070.0112:vi Xra 

 
 
 
 
 
 
 1 2 
 
 3 4 
 Doing laundry 
 Baking 
 Geographic diversity Multi-perspective 
 
 IMU 
 L R 
 Shopping 
 Sports 
 Reading 
 Human locomotion 
 Stereo vision 
 Gardening 
 Sewing / Knitting 
 3 D 
 Pets 
 
 
 Playing games 
 Social interaction Video + 3 D scans 
 Figure 1.Ego 4 Disamassive-scaleegocentricvideo data setofdailylifeactivityspanning 74 locationsworldwide.Hereweseeasnapshotof
 the data set(5%oftheclips,randomlysampled)highlightingitsdiversityingeographiclocation,activities,andmodalities.The data includes
 socialvideoswhereparticipantsconsentedtoremainunblurred.Seehttps://ego 4 d-data.org/fig 1.html for interactivefigure.
 First,today’sinfluential Internet data setscapturebrief,iso- theygoaboutdailyactivitiesin the home,workplace,leisure,
 latedmomentsintime from athird-person“spectactor”view. social settings, and commuting. Based on self-identified
 However,inbothrobotics and augmentedreality,theinput characteristics, the camera wearers are of varying back-
 isalong,fluidvideostream from the first-personor“ego- grounds,occupations,gender,andages—notsolelygraduate
 centric” point of view—where we see the world through students! Thevideo’srichgeographicdiversitysupports the
 theeyesofanagentactivelyengaged with itsenvironment. inclusionofobjects,activities,andpeoplefrequentlyabsent
 Second,whereas Internetphotos are intentionallycaptured fromexisting data sets. Sinceeachparticipantworeacamera
 byahumanphotographer,images from analways-onwear- for 1 to 10 hoursatattime,the data setofferslong-formvideo
 able egocentric camera lack this active curation. Finally, content that displays the fullarcofaperson’scomplexinter-
 first-personperceptionrequiresapersistent 3 Dunderst and- actions with the environment,objects,ando the rpeople. In
 ingof the camerawearer’sphysicalsurroundings,andmust additionto RGBvideo,portionsof the datasetalsoprovide
 interpretobjects and actionsinahumancontext—attentive audio,3 Dmeshes,gaze,stereo,and/orsynchronizedmulti-
 tohuman-objectinteractions and high-levelsocialbehaviors. camera views that allow seeing one event from multiple
 perspectives. Our data setdrawsinspiration from priorego-
 Motivated by these critical contrasts, we present the 
 centricvideo data efforts[43,44,129,138,179,201,205,210],
 Ego 4 D dataset and benchmark suite. Ego 4 D aims to cat- 
 butmakessignifi can tadvancesintermsof scale,diversity,
 alyze the nexteraofresearchinfirst-personvisualpercep- 
 andrealism. 
 tion. Ego is for egocentric, and 4 D is for 3 D spatial plus 
 temporalin for mation. 
 Equallyimportanttohaving the right data isto have the
 Ourfirstcontributionis the dataset: amassiveego-video rightresearchproblems. Oursecondcontributionisasuite
 collectionofunprecedented scale and diversity that captures offivebenchmark task sspanning the essentialcomponents
 dailylifeactivityaround the world. See Figure 1. Itconsists ofegocentricperception—indexingpastexperiences,ana-
 of 3,670 hoursofvideocollectedby 931 uniqueparticipants lyzingpresentinteractions,andanticipatingfutureactivity.
 from 74 worldwidelocationsin 9 differentcountries. The Toenableresearchon the sefronts,weprovidemillionsof
 vastmajorityof the footageisunscripted and“inthewild”, rich annotations that resulted from over 250,000 hours of
 representingthenaturalinteractionsof the camerawe are rsas annotatoreffort and range from temporal,spatial,andseman-
 2 

 
 
 
 
 
 
 tic labels, to dense textual narrations of activities, natural 
 languagequeries,andspeechtranscriptions. 
 Ego 4 Dis the culminationofanintensivetwo-yeareffort 
 by Facebook and 13 universitiesaround the worldwhocame 
 together for the commongoalofspurring new researchin 
 egocentricperception. Wearekickstarting that workwitha 
 formalbenchmarkchallengetobeheldin June 2022. Inthe 
 comingyears,webelieve our contribution can catalyze new 
 researchnotonlyinvision,butalsorobotics,augmentedreal- 
 ity,3 Dsensing,multimodallearning,speech,andlanguage. 
 These directions will stem not only from the benchmark 
 taskswepropose,butalsoalternativeones that the commu- Figure 2.Ego 4 Dcamerawe are rdemographics—age,gender,coun-
 nity will developleveraging our massive,publiclyavailable triesofresidence,andoccupations(self-reported).Fontsizereflects
 relativefrequencyof the occupation. 
 dataset. 
 2.Related Work 
 graduatestudentsascamerawearers[43,44,66,129,129,138,
 Large-scalethird-person data sets Inthelastdecade,an- 168,179,194,210],Ego 4 Dcamerawearers are ofamuch
 notated data sets have bothpresented new problemsincom- wider demographic, as detailed below. Aside from daily
 puter vision and ensured their solid evaluation. Existing lifeactivity,priorego data setsfocusonconversation[170],
 collectionslike Kinetics[108],AVA[92],UCF[207],Ac- inter-personinteractions[66,168,194,231],placelocaliza-
 tivity Net [61], How To 100 M [157], Image Net [49], and tion[183,208],multimodalsensor data[124,166,204],hu-
 COCO[143]focusonthird-person Webdata,which have man hands [16,134] human-object interaction [106,184],
 thebenefit and biasofahumanphotographer. Incontrast, andobjecttracking[56]. 
 Ego 4 Disfirst-person. Passivelycapturedwearablecamera Ego 4 Disanorderofmagnitudelargerthantoday’slargest
 video entails unusual viewpoints, motion blur, and lacks egocentric data setsbothintermsofh our sofvideo(3,670
 temporal curation. Notably, pre-training egocentric video hoursvs.100 in[43])anduniquecamerawearers(931 peo-
 models with third-person data[70,221,224,239]suffers from ple vs. 71 in [201]); it spans hundreds of environments
 thesizeabledomainmismatch[139,201]. (ratherthanoneordozens,asinexistingcollections);and
 itsvideocomes from 74 worldwidelocations and 9 coun-
 Egocentricvideounderst and ing Egocentricvideooffers 
 tries(vs.justoneorafewcities). The Ego 4 Dannotations
 a host of interesting challenges, such as human-object in- 
 are also of unprecedented scale and depth, with millions
 teractions[26,46,163],activityrecognition[110,139,243], 
 ofannotationssupportingmultiplecomplextasks. Assuch,
 anticipation[4,75,86,144,205],videosummarization[48, 
 Ego 4 Drepresentsastepchangein data setscale and diversity.
 129,131,147,148,232],detectinghands[16,134],parsing 
 We believe both factors are paramount to pursue the next
 socialinteractions[66,168,231],andinferring the camera 
 generationofperception for embodied AI. 
 wearer’s body pose [107]. Our dataset can facilitate new 
 workinall the seareas and more,and our proposedbench- 
 3.Ego 4 DDataset 
 marks(andannotations the reof)widen the tasksresearchers 
 canconsidermoving for ward. Wedeferdiscussionofhow Nextweoverview the dataset,whichwe are makingpub-
 priorworkrelatesto our benchmark task sto Sec.5. liclyavailableunderan Ego 4 Dlicense.
 Egocentric video datasets Multiple egocentric datasets 
 3.1.Collectionstrategy and camerawearers 
 have been developedover the lastdecade. Mostrelevantto 
 ourwork are thosecontainingunscripteddailylifeactivity, Notonlydowewishtoamassanego-videocollection that
 whichincludes EPIC-Kitchens[43,44],UTEgo[129,210], issubstantialin scale,butwealsowanttoensureitsdiversity
 Activities of Daily Living (ADL) [179], and the Disney ofpeople,places,objects,andactivities. Fur the rmore,for
 dataset[66]. Thepracticeofgivingcamerastoparticipants realism,weareinterestedinunscriptedfootagecapturedby
 totakeoutof the lab,firstexploredin[66,129,179],inspires peoplewearingacamera for longperiodsoftime.
 our approach. Others are (semi-)scripted, where camera To this end, we devised a distributed approach to data
 wearers are instructed to perform a certain activity, as in collection. The Ego 4 D project consists of 14 teams from
 Charades-Ego[201]and EGTEA[138]. Whereastoday’s universities and labs in 9 countries and 5 continents (see
 largestego data setsfocussolelyonkitchens[44,44,124,138], mapin Figure 1). Eachteamrecruitedparticipantstoweara
 Ego 4 Dspanshundredsofenvironmentsbothindoors and out- camera for 1 to 10 hoursatatime,foratotalof 931 unique
 doors. Fur the rmore,whileexisting data setsrelylargelyon camerawearers and 3,670 hoursofvideoin this first data set
 3 

 
 
 
 
 
 
 
 
 
 Carpenter > 7 hrsof videos Crafting> 12 hrsof videos Bike Mechanic> 5.5 hrsof videos
 
 
 
 
 
 Figure 3. Scenarios in Ego 4 D. Outer circle shows the 14 most 
 commonscenarios(70%ofthe data).Wordleshowsscenariosin 
 theremaining 30%.Innercircleiscolorcodedby the contributing 
 partner(seemapcolorlegendin Fig 1). Figure 4. Somevideos(bottom)havecoupled 3 Dmeshes(top)
 from Matterport 3 Dscanners,allowingonetorelate the dynamic
 videoto the static 3 Denvironment(middle). 
 release(Ego 4 D-3 K).Participantsin 74 totalcitieswerere- 
 cruitedbywordofmouth,ads,andpostingsoncommunity 
 bulletinboards.Someteamsrecruitedparticipants with occu- underst and ing [108]. In this way, we capture unscripted
 pations that haveinterestingvisualcontexts,suchasbakers, activitywhilebeingmindfulof the scenarios’coverage.
 carpenters,landscapers,ormechanics. The exception is for certain multi-person scenarios,
 Both the geographic spread of our team as well as our where,inordertoensuresufficient data for the audio-visual
 approachtorecruitingparticipants were criticaltoarriveat andsocialbenchmarks, weaskedparticipantsatfivesites
 adiversedemographiccomposition,asshownin Figure 2.1 whohadconsentedtosh are the irconversationaudioandun-
 Participantscoverawidevarietyofoccupations,spanmany blurredfacestotakepartinsocialactivities,suchasplaying
 agebrackets,with 96 ofthemover 50 yearsold,and 45% games. We leverage this portion of Ego 4 D for the audio-
 arefemale. Twoparticipantsidentifiedasnon-binary,and visual and socialinteractionbenchmarks(Sec.5.3 and 5.4).
 twopreferrednottosayagender. Figure 3 shows the widedistributionofscenarioscaptured
 inour data set. Note that with ineachgivenscenario the reare
 3.2.Scenarioscomposing the dataset 
 typicallydozensofactionstakingplace,e.g.,thecarpentry
 scenario includes hammering, drilling, moving wood, etc.
 What activities belong in an egocentric video dataset? 
 Overall,the 931 camerawe are rsbestow our data setwitha
 Ourresearchismotivatedbyproblemsinrobotics and aug- 
 glimpseofdailylifeactivityaround the world.
 mentedreality, wherevisionsystems will encounterdaily 
 lifescenarios. Hence,weconsultedasurvey from the U.S. 3.3.Cameras and modalities 
 Bureauof Labor Statistics 2 thatcaptureshowpeoplespend 
 thebulkoftheirtimein the home(e.g.,cleaning,cooking, To avoid models overfitting to a single capture device,
 yardwork),leisure(e.g.,crafting,games,attendingaparty), sevendifferenthead-mountedcameras were deployedacross
 transportation (e.g., biking, car), errands (e.g., shopping, the data set: Go Pro,Vuzix Blade,Pupil Labs,ZShades,OR-
 walkingdog,gettingcarfixed),andin the workplace(e.g, DRO EP 6, i Vue Rincon 1080, and Weeview. They offer
 talking with colleagues,makingcoffee). tradeoffs in the modalities available (RGB, stereo, gaze),
 Tomaximizecoverageofsuchscenarios,ourapproachis fieldofview, andbatterylife. Thefieldofview and cam-
 acompromisebetweendirectingcamerawearers and giving era mounting are particularly influential: while a Go Pro
 noguidanceatall: (1)werecruitedparticipantswhosecol- mounted on the head pointing down offers a high resolu-
 lectivedailylifeactivitywouldnaturallyencompassaspread tionviewof the handsmanipulatingobjects(Fig.5,right),
 ofthescenarios(asselectedfreelyby the participant),and a heads-up camera like the Vuzix shares the vantage of a
 (2)weaskedparticipantstowear the cameraatlength(at person’seyes,but will missinteractionscloseto the body
 leastaslongasthebatterylifeof the device)sothat the activ- (Fig.5,left). 
 itywouldunfoldnaturallyinalongercontext. Atypicalraw Inadditiontovideo,portionsof Ego 4 Dofferseveralother
 videoclipin our data setlasts 8 minutes—signifi can tlylonger 
 datamodalities:3 Dscans,audio,gaze 3,stereo,multiplesyn-
 than the 10 secondclipsoftenstudiedinthird-personvideo chronized wearable cameras, and textual narrations. See
 Table 1. Each can support new research challenges. For
 1 for 64%ofallparticipants;missingdemographics are duetoprotocols example, having Matterport 3 D scans of the environment
 orparticipantsoptingoutofansweringspecificquestions. 
 2 https://www.bls.gov/news.release/atus.nr 0.htm 3 Eyetrackers were deployedby Indiana U.and Georgia Techonly.
 4 

 
 
 
 
 
 
 Modality: RGBvideo Textnarrations Features Audio Faces 3 Dscans Stereo Gaze IMU Multi-cam
 #hours: 3,670 3,670 3,670 2,535 612 491 80 45 836 224 
 
 Table 1. Modalitiesofdatain Ego 4 Dand the iramounts. “Narrations”aredense,timestampeddescriptionsofcamerawe are ractivity
 (cf. Sec.4).“3 Dscans”aremeshes from Matterport 3 Dscanners for the full environmentinwhich the videowascaptured.“Faces”refersto
 videowhereparticipantsconsentedtoremainunblurred.“Multi-cam”referstosynchronizedvideocapturedat the sameeventbymultiple
 camerawearers.“Features”referstoprecomputed Slow Fast[70]videofeatures.Gazecollectedonlyby Indiana U.and Georgia Tech.
 
 coupled with ego-videoclips(Figure 4)offersauniqueop- 
 portunity for underst and ingdynamicactivitiesinapersistent 
 3 Dcontext, asweexploitin the Episodic Memorybench- 
 mark(see Sec.5.1). Multiplesynchronizedegocentricvideo 
 streams allow accounting for the first and second-person 
 viewinsocialinteractions. Audioallowsanalysisofconver- 
 sationandacousticscenes and events. 
 Figure 5.Examplenarrations.“C”referstocamerawearer.
 3.4.Privacy and ethics 
 From the onset,privacyandethicsst and ards were critical 
 will be at least subtle ways in which the language-based
 tothis data collectioneffort. Eachpartnerwasresponsible 
 narrations are biasedtowards the irlocalwordchoices.
 fordevelopingapolicy. Whilespecificsvarypersite,this 
 generallyentails: 3.6.Datasetaccessibility 
 • Comply with own institutional research policy, e.g., At 3,670 hours of video, we are mindful that Ego 4 D’s
 independentethicscommitteereviewwhererelevant scale can be an obstacle for accessibility for some re-
 searchers,dependingon the irstorage and computeres our ces.
 • Obtainin for medconsentofcamerawearers,whocan 
 Tomitigate this,wehavetakenseveralmeasures. First,we
 askquestions and withdrawatanytime,and are freeto 
 provide precomputed action features (Slow Fast 8 x 8 with
 review and redact the irownvideo 
 Res Net 101 backbonepretrained for Kinetics 400)with the
 • Respect rights of others in private spaces, and avoid 
 dataset,anoptionalstartingpoint for anydownstreamwork.
 captureofsensitive are asoractivities 
 Second,onlyportionsofthe data constitute the formalchal-
 • Follow de-identification requirements for personally lengetrain/testsets for eachbenchmark—notall 3,670 hours
 identifiablein for mation(PII) (see Appendix E).As Ego 4 Dannotationsincrease,wewill
 createst and ardizedmini-sets. Finally,weprovide the option
 Inshort,thesest and ardstypicallyrequire that the videobe 
 todownloadonly the datatargetinganindividualbenchmark
 capturedinacontrolledenvironmentwithin for medconsent 
 ormodalityofinterest. 
 by all participants, or else in public spaces where faces 
 andother PII are blurred. Appendix Kdiscussespotential 
 4.Narrationsof Camera Wearer Activity 
 negativesocietalimpact. 
 Before any other annotation occurs, we pass all video
 3.5.Possibles our cesofbias 
 throughanarrationprocedure. Inspiredby the pause-and-
 While Ego 4 D pushes the envelope on massive every- talknarrator[44],annotators are askedtowatcha 5 minute
 dayvideo from geographically and demographicallydiverse clipofvideo,summarizeit with afewsentences,andthen
 sources, we are aware of a few biases in our dataset. 74 re-watch,pausingrepeatedlytowriteasentenceabouteach
 locationsisstillalongway from completecoverageof the thing the camera wearer does. We record the timestamps
 globe. Inaddition,thecamerawearers are generallylocated and the associatedfree-formsentences. See Figure 5. Each
 inurbanorcollegetownareas. The COVID-19 pandemic video receives two independent narrations from different
 ledtoamplefootageinstay-at-homescenariossuchascook- annotators. Thenarrations are temporallydense: onaverage
 ing,cleaning,crafts,etc.andmorelimitedopportunitiesto wereceived 13.2 sentencesperminuteofvideo,foratotalof
 collectvideoatmajorsocialpublicevents. Inaddition,since 3.85 Msentences. Intotal the narrationsdescribe the Ego 4 D
 batterylifeprohibitsdaylongfilming,thevideos—though videousing 1,772 uniqueverbs(activities)and 4,336 unique
 unscripted—tendtocontainmoreactiveportionsofapartic- nouns(objects). See Appendix Dfordetails.
 ipant’sday. Finally,Ego 4 Dannotations are donebycrowd- The narrations allow us to (1) perform text mining for
 sourcedworkersintwositesin Africa. Thismeans that there data-driventaxonomyconstruction for actions and objects,
 5 

 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 6. The Ego 4 Dbenchmarksuitecentersaround the first-personvisualexperience—fromremembering the past,toanalyzing the
 present,toanticipating the future. 
 
 (2)sortthevideosbytheircontenttomap the mtorelevant 
 benchmarks,and(3)identifytemporalwindowswherecer- 
 tainannotationsshould beseeded. Beyond the seuses,the 
 narrations are themselvesacontributionof the dataset,po- 
 tentiallyvaluable for researchonvideo with weaklyaligned 
 naturallanguage.Toourknowledge,oursis the largestrepos- 
 itoryofalignedlanguage and video(e.g.,How To 100 M[157], 
 anexisting Internetrepository with narrations,containsnoisy 
 spokennarrations that onlysometimescommentontheac- 
 tivitiestakingplace). 
 Figure 7.Episodic Memory’sthreequerytypes
 5.Ego 4 DBenchmark Suite 
 First-personvisionhas the potentialtotrans for mmany 
 to France?”), to be distinguished from semantic memory
 applications in augmented reality and robotics. However, 
 (“what’sthecapitalof France?”). Anaugmentedrealityas-
 compared to mainstream video underst and ing, egocentric 
 sistant that processes the egocentricvideostreamcouldgive
 perceptionrequires new fundamentalresearchtoaccount for 
 ussuper-humanmemoryifitcouldappropriatelyindex our
 long-formvideo,attentioncues,person-objectinteractions, 
 visualexperience and answerqueries. 
 multi-sensory data,and the lackofmanualtemporalcuration 
 inherenttoapassivelyworncamera. Taskdefinition Givenanegocentricvideo and aquery,the
 Inspiredbyall the sefactors,weproposeasuiteofchal- Ego 4 D Episodic Memory task requires localizing where
 lengingbenchmarktasks. Thefivebenchmarkstackle the the answer can be seen within the user’s past video. We
 past,present,andfutureoffirst-personvideo. See Figure 6. consider three query types. (1) Natural language queries
 Thefollowingsectionsintroduceeach task and itsannota- (NLQ),inwhich the queryisexpressedintext(e.g.,“What
 tions. Thefirst data setreleasehasannotations for 48-1,000 did I put in the drawer?”), and the output response is the
 hoursof data perbenchmark,ontopof the 3,670 hoursof temporalwindowwhere the answerisvisibleordeducible.
 data that isnarrated. The Appendicesdescribehowwesam- (2)Visualqueries(VQ),inwhich the queryisastaticimage
 pledvideosperbenchmarktomaximizerelevanceto the task of an object, and the output response localizes the object
 whilemaintaininggeographicdiversity. thelasttimeitwasseenin the video,bothtemporally and
 Wedeveloped base line model sdrawingonstate-of-the- spatially. Thespatialresponseisa 2 Dboundingboxon the
 artcomponents from the literatureinordertotestdriveall object, and optionally a 3 D displacement vector from the
 Ego 4 Dbenchmarks. The Appendixpresents the baseline currentcamerapositionto the object’s 3 Dboundingbox.VQ
 models and quantitativeresults. Wearerunninga for mal captureshowausermightteach the systemanobject with
 Ego 4 Dcompetitionin June 2022 inviting the researchcom- animageexample, thenlaterask for itslocation(“Where
 munitytoimproveon the sebaselines. isthis[pictureofmykeys]?”). (3)Momentsqueries(MQ),
 in which the query is the name of a high-level activity or
 5.1.Episodic Memory “moment”,and the responseconsistsofalltemporalwindows
 where the activity occurs (e.g., “When did I read to my
 Motivation Egocentric video from a wearable camera 
 children?”). See Figure 7. 
 records the who/what/when/whereofanindividual’sdaily 
 lifeexperience. Thismakesitideal for what Tulvingcalled Annotations Forlanguagequeries,wedevisedasetof 13
 episodic memory [213]: specific first-person experiences template questions meant to span things a user might ask
 (“what did I eat and who did I sit by on my first flight to augment their memory, such as “what is the state of
 6 

 
 
 
 
 
 
 object X?”,e.g.,“did Ileave the window open?”.Annotators 
 express the queriesinfree-formnaturallanguage,andalso 
 provide the slot filling (e.g., X = window). For moments, 
 weestablishe data xonomyof 110 activitiesina data-driven, pre-condition PNR post-condition
 semi-automaticmannerbymining the narrationsummaries. 
 State-change:Plantremoved from ground
 Momentscapturehigh-levelactivitiesin the camerawearer’s 
 day,e.g.,setting the tableisamoment,whereaspickupis 
 anactionin our Forecastingbenchmark(Sec.5.5). 
 For NLQ and VQ, we ask annotators to generate lan- 
 guage/visual queries and couple them with the “response pre-condition PNR post-condition
 track”inthevideo. For MQ,weprovide the taxonomyof 
 State-change:Woodsmoothed 
 labels and askannotatorstolabelclipswi the ach and every 
 temporalsegmentcontainingamomentinstance. Intotal, Figure 8.Hands and Objects:Exampleobjectstatechangesdefined
 wehave 74 Ktotalqueriesspanning 800 hoursofvideo. bypre-condition,PNR,andpost-conditionframes.
 ∼ 
 Evaluationmetrics and baselines For NLQ,we usetop-k 
 recall at a certain temporal intersection over union (t Io U) manactionsbetter,aswellastotrainrobotstolearn from
 threshold. MQ adopts a popular metric used in temporal hum and emonstrationsinvideo.
 actiondetection:m APatmultiplet Io Uthresholds,aswellas 
 Taskdefinitions Weinterpretanobjectstatechangetoin-
 top-kxrecall. VQadoptstemporal and spatio-temporallocal- 
 cludevariousphysicalchanges,includingchangesinsize,
 izationmetricsaswellastimelinessmetrics that enc our age 
 shape,composition,andtexture. Objectstatechanges can be
 speedysearches. Appendix Fpresents the baselinemodels 
 viewedalongtemporal,spatial and semanticaxes,leadingto
 wedeveloped and reportsresults. 
 thesethreetasks: (1)Point-of-no-returntemporallocaliza-
 Relation to existing tasks Episodic Memory has some tion: givenashortvideoclipofastatechange,thegoalisto
 foundationsinexistingvisionproblems,butalsoadds new estimatethekeyframe that contains the point-of-no-return
 challenges. All three queries call for spatial reasoning in (PNR)(thetimeatwhichastatechangebegins);(2)State
 astaticenvironmentcoupled with dynamicvideoofaper- changeobjectdetection: giventhreetemporalframes(pre,
 son who moves and changes things; current work largely post,PNR),thegoalistoregress the boundingboxof the
 treats these two elements separately. The timeliness met- objectundergoingastatechange; (3)Objectstatechange
 ricsenc our ageworkonintelligentcontextualsearch. While classification: givenashortvideoclip,thegoalistoclassify
 currentliteratureonlanguage+visionfocusesoncaptioning whe the ranobjectstatechangehastakenplaceornot.
 and question answering for isolated instances of Internet 
 Annotations Weselect the datatoannotate base donactivi-
 data[12,35,119,228],NLQismotivatedbyqueriesabout 
 ties that are likelytoinvolveh and-objectinteractions(e.g.,
 thecamerawearer’sownvisualexperience and operatesover 
 knitting,carpentry,baking,etc.). Westartbylabelingeach
 long-termobservations. VQupgradesobjectinstancerecog- 
 narratedh and-objectinteraction. Foreach, welabelthree
 nition [23,85,126,155] to deal with video (frequent Fo V 
 momentsintime(pre,PNR,post)and the boundingboxes
 changes, objects entering/exiting the view) and to reason 
 for the hands,tools,andobjectsineachof the threeframes.
 aboutobjectsin the contextofa 3 Denvironment. Finally, 
 Wealsoannotate the statechangetypes(remove,burn,etc.,
 MQcan beseenasactivitydetection[141,229,237]butfor 
 see Fig.8),actionverbs,andnouns for the objects.
 theactivitiesof the camerawearer. 
 Evaluation metrics and baselines Object state change
 5.2.Hands and Objects temporallocalizationisevaluatedusingabsolutetemporal
 errormeasuredinseconds. Objectstatechangeclassifica-
 Motivation While Episodic Memory aims to make past 
 tion is evaluated by classification accuracy. State change
 video queryable, our next benchmark aims to underst and 
 objectdetectionisevaluatedbyaverageprecision(AP).Ap-
 the camera wearer’s present activity—in terms of inter- 
 pendix Gdetails the annotations and presents base line model
 actions with objects and other people. Specifically, the 
 results for the three Hands and Objectstasks. 
 Hands and Objectsbenchmark captures how the camera 
 wearer changes the state of an object by using or manip- Relation to existing tasks Limited prior work considers
 ulatingit—whichwecallanobjectstatechange. Though objectstatechangeinphotos[102,164]orvideo[8,68,242];
 cutting a piece of lumber in half can be achieved through Ego 4 Disthefirstvideobenchmarkdedicatedto the taskof
 manymethods(e.g.,varioustools,force,speed,grasps,end- underst and ingobjectstatechanges. The task issimilarto
 effectors),allshould berecognizedas the samestatechange. actionrecognition(e.g.,[100,110,139,221,243])becausein
 Thisgeneralizationability will enableustounderstandhu- somecasesaspecificaction can correspondtoaspecificstate
 7 

 
 
 
 
 
 
 rate (DER) [11] and word error rate (WER) [114] for di-
 arization and transcription,respectively. Wepresent AVD
 baselinemodels and resultsin Appendix H. 
 Relation to existing tasks The past few years have seen
 audiostudiedincomputervisiontasks[245]foractionclas-
 sification[110,226],objectcategorization[125,234],source
 localization and tracking[14,197,212]andembodiednavi-
 gation[33]. Meanwhile,visualin for mationisincreasingly
 usedinhistoricallyaudio-only task slikespeechtranscrip-
 tion,voicerecognition,audiospatialization[5,80,104,161],
 speakerdiarization[10,83],ands our ceseparation[57,78,82].
 Datasetslike Vox Celeb[39],AVASpeech[31],AVAactive
 Figure 9. Audio-Visual and Socialbenchmarkannotations 
 speaker[192],AVDIAR[83],and Easy Com[53]support this
 research. However,these data sets are mainlynon-egocentric.
 change. However,asinglestatechange(e.g.,cutting)can Unlike Ego 4 D,theydonotcapturenaturalconversational
 alsobeobservedinmanyforms(variousobject-tool-action characteristics involving a variety of noisy backgrounds,
 combinations). Itis our hope that the proposedbenchmarks overlapping,interruptingandun-intelligiblespeech,environ-
 will lead to the development of more explicit models of mentvariation,movingcamerawearers,andspeakersfacing
 objectstatechange,whileavoidingapproaches that simply away from the camerawearer. 
 overfittoactionorobjectobservations. 
 5.4.Social Interactions 
 5.3.Audio-Visual Diarization Motivation Anegocentricvideoprovidesauniquelens for
 studyingsocialinteractionsbecauseitcapturesutterances
 Motivation Ournexttwo task saimtounderst and the cam- 
 and nonverbal cues [115] from each participant’s unique
 erawearer’spresentinteractions with people. Peoplecom- 
 view and enablesembodiedapproachestosocialunderst and-
 municateusingspokenlanguage,making the captureofcon- 
 ing. Progressinegocentricsocialunderst and ingcouldlead
 versationalcontentinbusinessmeetings and socialsettings 
 tomorecapablevirtualassistants and socialrobots. Compu-
 aproblemofgreatscientifi can dpracticalinterest. While 
 tational model sofsocialinteractions can alsoprovide new
 diarizationhas been ast and ardproblemin the speechrecog- 
 tools for diagnosing and treatingdisordersofsocialization
 nition community, Ego 4 D brings in two new aspects (1) 
 andcommunicationsuchasautism[188],andcouldsupport
 simultaneouscaptureofvideo and audio(2)theegocentric 
 novelprosthetictechnologies for the hearing-impaired.
 perspectiveofaparticipantin the conversation. 
 Taskdefinition While the Ego 4 Ddataset can supportsuch
 Task definition and annotations The Audio-Visual Di- 
 along-termresearchagenda,ourinitial Socialbenchmark
 arization(AVD)benchmarkiscomposedoff our tasks(see 
 focusesonmultimodalunderst and ingofconversationalin-
 Figure 9): 
 teractionsviaattention and speech. Specifically,wefocuson
 • Localization and trackingof the participants(i.e.,candi- 
 identifyingcommunicativeacts that are directedtowards the
 datespeakers)inthevisualfieldofview(Fo V).Abound- 
 camera-wearer,asdistinguished from thosedirectedtoother
 ingboxisannotatedaroundeachparticipant‘sface. 
 socialpartners: (1)Lookingatme(LAM):givenavideoin
 • Activespeakerdetectionwhereeachtrackedspeakerisas- 
 which the facesofsocialpartners have beenlocalized and
 signedananonymouslabel,including the camerawearer 
 identified,classifywhe the reachvisiblefaceislookingat the
 whoneverappearsin the visual Fo V. 
 camerawearer;and(2)Talkingtome(TTM):givenavideo
 • Diarization of each speaker’s speech activity, where 
 and audio segment with the same tracked faces, classify
 we provide the time segments corresponding to each 
 whethereachvisiblefaceistalkingto the camerawearer.
 speaker’svoiceactivityin the clip. 
 • Transcriptionofeachspeaker’sspeechcontent(only En- Annotations Socialannotationsbuildonthose from AVdi-
 glishspeakers are considered for thisversion). arization(Sec.5.3). Given(1)faceboundingboxeslabeled
 withparticipant IDs and trackedacrossframes,and(2)asso-
 Evaluationmetrics and baselines we usest and ardizedob- ciatedactivespeakerannotations that identifyineachframe
 ject tracking (MOT) metrics [18,19] to evaluate speaker whether the socialpartnerswhosefacesarevisible are speak-
 localization and trackingin the visual Fo V.Speakerdetec- ing,annotatorsprovide the groundtruthlabels for LAMand
 tion with anonymouslabelsisevaluatedusing the speaker TTMasabinarylabel for eachfaceineachframe.For LAM,
 error rate, which measures the proportion of wrongly as- annotatorslabel the timesegment(start and endtime)ofa
 signedlabels. Weadopt the wellstudieddiarizationerror visiblepersonwhentheindividualislookingat the camera
 8 

 
 
 
 
 
 
 wearer. For TTM,we use the vocalactivityannotation from take 
 AVD, then identify the time segment when the speech is doughin 
 0.8 s take 
 directedat the camerawearer. See Figure 9. doughin 
 0.8 s 
 Evaluation metrics and baselines We use mean average Locomotion Movements Hands Movements Short-Term Anticipation
 precision(m AP)and Top-1 accuracytoquantify the classifi- prediction:kneaddough putdough packspice pourspice
 cationperformance for bothtasks. Unlike AVD,wemeasure 
 precisionateveryframe. Appendix Iprovidesdetails and 
 Input video Long-Term Anticipation 
 presents Social base linemodels and results. 
 Relationtoexistingtasks Comp are dto[67],Ego 4 Dcon- Figure 10.The Forecastingbenchmarkaimstopredictfutureloco-
 tainssubstantiallymoreparticipants,hoursofrecording,and motion,movementofhands,nextobjectinteractions,andsequences
 offutureactions. 
 varietyofsensors and socialcontexts.The LAM task ismost 
 closelyrelatedtopriorworkoneyecontactdetectioninego- 
 video[36,159],butaddressesmorediverse and challenging 
 Evaluationmetrics and baselines Weevaluatefutureloco-
 scenarios. Mutualgazeestimation[54,150–152,172,176] 
 motionmovementandh and movementpredictionusing L 2
 andgazefollowing[37,65,111,186]arealsorelevant. The 
 distance. Short-termobjectinteractionanticipationiseval-
 TTM task isrelatedtoaudio-visualspeakerdetection[7,193] 
 uatedusinga Top-5 mean Average Precisionmetricwhich
 andmeetingunderst and ing[21,132,154]. 
 discounts the Top-4 falsenegativepredictions.Long-termac-
 5.5.Forecasting tionanticipationisevaluatedusingeditdistance. Appendix J
 details the tasks,annotations,baselinemodels,andresults.
 Motivation Havingaddressed the past and presentof the 
 Relation to existing tasks Predicting future events from
 camera wearer’s visual experience, our last benchmark 
 egocentric vision has increasing interest [191]. Previous
 moves on to anticipating the future. Forecasting move- 
 workconsidersfuturelocalization[113,120,174,230],ac-
 ments and interactionsrequirescomprehending the camera 
 tionanticipation[76,77,86,118,127,219],nextactiveobject
 wearer’sintention. Ithasimmediateapplicationsin ARand 
 prediction[20,74],futureeventprediction[149,167],andfu-
 human-robotinteraction,suchasanticipativelyturningon 
 tureframeprediction[145,146,153,215,218,227]. Whereas
 appliancesormovingobjects for the human’sconvenience. 
 pastworkreliesondifferentbenchmarks and taskdefinitions,
 Thescientificmotivation can beseenbyanalogy with lan- 
 we propose a unified benchmark to assess progress in the
 guage model ssuchas GPT-3[24],whichimplicitlycapture 
 field. 
 knowledgeneededbymanyo the rtasks. Ratherthanpredict 
 thenextword,visual for ecastingmodels the dynamicsofan 
 6.Conclusion 
 agentactingin the physicalworld. 
 Taskdefinition The Forecastingbenchmarkincludesf our Ego 4 Disafirst-of-its-kind data set and benchmarksuite
 tasks (Fig. 10): (1) Locomotion prediction: predict a set aimed at advancing multimodal perception of egocentric
 of possiblefuture ground plane trajectoriesof the camera video. Comp are dtoexistingwork,our data setisordersof
 wearer. (2) Hand movement prediction: predict the hand magnitudelargerin scale and diversity. Thedata will allow
 positionsof the camerawe are rinfutureframes. (3)Short- AItolearn from dailylifeexperiencesaround the world—
 termobjectinteractionanticipation: detectasetofpossible seeingwhatwesee and hearingwhatwehear—while our
 futureinteractedobjectsinthemostrecentframeof the clip. benchmark suite provides solid footing for innovations in
 Toeachobject,assignaverbindicating the possiblefuture videounderst and ingthat are critical for augmentedreality,
 interactionanda“timetocontact”estimateofwhen the inter- robotics,andmanyo the rdomains. Welook for wardto the
 actionisgoingtobegin. (4)Long-termactionanticipation: research that willbuildon Ego 4 Din the yearsahead.
 predict the camerawearer’sfuturesequenceofactions. 
 Contributionstatement 
 Annotations Using the narrations, we identify the occur- 
 renceofeachobjectinteraction,assigningaverb and atarget Projectled and initiatedby Kristen Grauman. Program
 objectclass. Theverb and nountaxonomies are seeded from management and operationsledby Andrew Westbury. Scien-
 thenarrations and the nhand-refined. Foreachaction, we tificadvisingby Jitendra Malik. Authors with stars(∗)were
 identifyacontactframe and apre-conditionframeinwhich keydriversofimplementation,collection,and/orannotation
 we annotate bounding boxes around active objects. The developmentthroughout the project. Authors with daggers
 sameobjectsaswellash and sareannotatedinthreeframes (†)arefaculty PIs and workinggroupleadsin the project.
 preceding the pre-conditionframeby 0.5 s,1 sand 1.5 s. We Thebenchmarksbroughttoge the rmanyresearchers from all
 obtain ground truth ego-trajectories of the camera wearer institutionsincludingcross-institution base lineevaluations.
 usingstructure from motion. Appendices Fthrough Jdetail the contributionsofindividual
 9 

 
 
 
 
 
 
 authors for the variousbenchmarks. Thevideocollectedby 
 Facebook Reality Labsused Vuzix Blade®Smart Glasses 
 andwasdoneinaclosedenvironmentin Facebook’sbuild- 
 ingsbypaidparticipantswhosignedconsentstosh are their 
 data. Allo the rvideocollection and participantrecruitment 
 wasmanagedby the universitypartners. Appendix Apro- 
 videsdetailsabout the datacollectiondonepersiteandac- 
 knowledges the primarycontributors. Theannotationeffort 
 wasledby Facebook AI. 
 Acknowledgements 
 Wegrate full yacknowledge the followingcolleagues for 
 valuablediscussions and supportof our project: Aaron Ad- 
 cock, Andrew Allen, Behrouz Behmardi, Serge Belongie, 
 Antoine Bordes, Mark Broyles, Xiao Chu, Samuel Clapp, 
 Irene D’Ambra,Peter Dodds,Jacob Donley,Ruohan Gao, 
 Tal Hassner,Ethan Henderson,Jiabo Hu,Guillaume Jean- 
 neret,Sanjana Krishnan,Devansh Kukreja,Tsung-Yi Lin, 
 Bobby Otillar, Manohar Paluri, Maja Pantic, Lucas Pinto, 
 Vivek Roy,Jerome Pesenti,Joelle Pineau,Luca Sbordone, 
 Rajan Subramanian,Helen Sun,Mary Williamson,and Bill 
 Wu. Wealsoacknowledge Jacob Chalk for settingup the 
 Ego 4 DAWSbackend and Prasanna Sridhar for developing 
 the Ego 4 Dwebsite. Thankyouto the Common Visual Data 
 Foundation(CVDF)forhosting the Ego 4 Ddataset. 
 Theuniversitiesacknowledge the usageofcommercial 
 softw are forde-identificationofvideo. brighter.aiwasused 
 forredactingvideosbysomeof the universities. Personal 
 data from the Universityof Bristolwasprotectedby Prim- 
 loc’s Secure Redactsoftw are suite. 
 UNICT is supported by MIUR AIM - Attrazione e 
 Mobilita Internazionale Linea 1 - AIM 1893589 - CUP 
 E 64118002540007. Bristolissupportedby UKRIEngineer- 
 ingand Physical Sciences Research Council(EPSRC)Doc- 
 toral Training Program(DTP),EPSRCFellowship UMPIRE 
 (EP/T 004991/1). KAUSTissupportedby the KAUSTOf- 
 ficeof Sponsored Researchthrough the Visual Computing 
 Center(VCC)funding. National Universityof Singaporeis 
 supportedby Mike Shou’s Start-Up Grant. Georgia Techis 
 supportedinpartby NSFaward 2033413 and NIHaward 
 R 01 MH 114999. 
 
 
 
 
 
 
 
 
 
 
 10 
 
 
 

 
 
 
 
 
 
 totheparticipantsindifferentpartsof the country. Videos
 weresh are dbackei the rinexternalharddisksorover the
 Appendix 
 cloudstorage. Eachvideowasmanuallyinspected for any
 sensitivecontentbe for esharing. 
 Primarycontributors:Raghava Modhugu-datacollection
 Table of Contents pipeline,designof the setup and workflow. Siddhant Bansal
 - IRB application, consent forms and de-identification. C.
 Appendices 11 
 V.Jawahar -leadcontributor for data collection. Wealso
 A .Data Collection . . . . . . . . . . . . 11 acknowledge the contributionsof Aradhana Vinod(coordi-
 B .De-identification Process . . . . . . . 16 nation and communication),Ram Sharma(local data man-
 C .Demographics. . . . . . . . . . . . . 18 agement and verification),and Varun Bhargavan(systems
 D .Narrations . . . . . . . . . . . . . . . 20 andres our ces). 
 E .Benchmark Data Splits . . . . . . . . 24 
 Universityof Tokyo,Japan: Werecruited 81 Japanesepar-
 F .Episodic Memory Benchmark . . . . 25 
 ticipants(41 male,40 female)livingaround Tokyo,Japan
 G .Hands and Objects Benchmark . . . . 44 
 throughatemporaryemploymentagency. Theparticipant’s
 H .Audio-Visual Diarization Benchmark. 51 
 gender and age(from the 20 sto 60 s)werebalancedtocollect
 I .Social Interaction Benchmark. . . . . 63 diversebehaviorpatterns. Wefocusedontwosingle-actor
 J .Forecasting Benchmark . . . . . . . . 67 activities: cooking(40 participants,90 hours)andh and craft
 K .Societal Impact . . . . . . . . . . . . 82 (41 participants,51 hours). Inthecookingscenario,partici-
 pants were askedtorecordunscriptedvideosofcookingat
 theirhomes. Intheh and craftscenario,participantsvisited
 A.Data Collection 
 our laboratory and per for med various handcraft activities
 (e.g.,origami,woodworking,plastic model,cutoutpicture).
 Thissectionoverviews the collectionprocedures and sce- 
 Wecollected data using Go Pro HERO 7 Blackcamera for
 nariospersite. 
 cooking and Weeview SID 3 Dstereocameraforh and craft.
 International Institute of Information Technology Our data collectionprotocolwasreviewed and approvedby
 (IIIT), Hyderabad, India: At IIIT, Hyderabad, we fol- Universityof Tokyoethicalreviewboard.
 lowedaprotocolofdistributed data collection with acen- Primarycontributors: Yoichi Sato–leadcoordinator for
 tralizedteamdoingcoordination and verification. Wefirst datacollection,Takuma Yagi and Takumi Nishiyasu–con-
 identifiedlocalcoordinatorsindifferentpartsof the country tributedtoparticipantrecruiting,protocoldesign,datacollec-
 andexplained the datacollectionplans,goals and process. tion and inspection,and IRBsubmission,Yifei Huang and
 Theythenhelpedincollectingdatain the irownlocalregions Zhenqiang Li–contributedto data inspection and transfer,
 fromnaturalsettingswithin for medparticipants.Participants Yusuke Sugano–contributedtoselectingvideorecording
 wererecruitedlocallyconsidering the rangeofactivities,and scenarios,protocoldesign and IRBsubmission.
 also the guidelines and restrictionsof COVID-19. Thecen- 
 tralteamcouldnottraveltoall the selocations for training University of Bristol, UK: Participants were recruited
 thecoordinatorsorcollecting the data. Weshippedmultiple throughadvertsonsocialmedia and universityinternalcom-
 camerasto the localcoordinators and remotelyguidedthem munication channels. These participants then spread the
 ondatacollectionfollowing the COVIDprotocols. Thecol- wordto the iracquaintances and someparticipantsjoined the
 lected data and consent forms were then shipped back to projectthroughword-of-mouthrecommendationsofprevi-
 theuniversity,wheremanualverification,de-identification ousparticipants. Datawascollectedbetween Janand Dec
 (whereverapplicable),andsharing with the consortiumtook 2020,from 82 participants.With the pandemictakingoverin
 place. March,theprojectshiftedtoonlineoperationwherecameras
 At IIITHyderabad,werecorded 660.5 hoursof data with wereposted,andtrainingtookplaceover Zoommeetings.
 the help of 138 subjects. The videos were collected in 5 Participantsfirstexpressedinterestbysendinganemail and
 differentstatesin India,geographicallywellapart. Wecover they were provided with anin for mationsheet. Thiswasfol-
 36 differentscenarios,suchasmakingbricksusinghands, lowedbyapreliminary Zoommeeting with are searcherto
 knitting, making egg cartons, and hairstyling. The age of briefparticipantsabout the procedure,answeranyquestions
 subjects ranged from 18-84 years with 10 distinct profes- andagreeon the scenariostoberecorded.
 sionalbackgrounds(teachers,students,farmers,blacksmiths, Wesetalimitto the totalnumberofminutesperscenario,
 homemakers,etc.). Outofall the subjects,94 weremales, to increase diversity of recordings. For example, driving
 and 44 were females. We use Go Pro Hero 6 and Go Pro cannotbelongerthan 30 minuteswhilecooking can beup
 Hero 7 forrecording the videos. The Go Pro’swereshipped to 1.5 hours. Each participant was instructed to record a
 11 

 
 
 
 
 
 
 minimumof 2 hoursacross 4 scenarios. Importantly,partic- wascomprisedoffriendsorfamilymemberswhok new each
 ipants were enc our agedtocollectactivities the ynaturally otherpriortoparticipatingin the study. Participants were
 do. For example if one regularly cycles or practices mu- requiredtobeaged 18-64,tonotbeconsideredhighrisk for
 sic,they were askedtorecord the sescenarios. Additionally, COVID-19,andtobeabletoplaysocialdeductiongamesin
 pairedscenarios(peoplecookingtoge the rorplayinggames) English. Ourstudyprotocolwasreviewed and approvedby
 wereenc our aged and multiple(2-3)cameras were posted for the Georgia Tech Institutional Review Board(IRB).Intotal,
 participantssharingahousehold. Allparticipantssigneda approximately 43 hoursofegocentricvideo were collected
 consentformbe for eacamerawaspostedto the irresidence. from 19 participants(perparticipantdisclosure-10 male,
 Cameras were postedto 9 UKcitiesin Engl and,Wales and 7 female,1 non-binary,1 notreported). Participantshada
 Scotl and includingoneparticipantin the Isleof North Uist. meanageof 31.6 years with 7 participantsaged 20-29 years,
 Uponreceiptof the camera,asecond Zoommeetingwas 10 participants aged 30-39 years, and 2 participants aged
 scheduledtotraintheparticipanton the equipment and detail 40-49 years. 
 how footage is reviewed and uploaded. Participants were Participantsworeanegocentrichead-worncamera and
 given 2 weekstorecord,withanadditionalweekofexten- on-earbinauralmicrophones. Someparticipantswore the
 sionuponrequest. Oncerecordingiscompleted,footageis ORDROEP 6 camerawhileo the rswore the Pupil Invisible
 uploadedby the participant and reviewed for goodlighting, cameras. Theaudiowasrecordedusinga Tascam DR-22 WL
 correctsetting and viewpoint. Participants were reimbursed and Sound Professionals MS-EHB-2 Ear-hookbinauralmi-
 fortheirparticipationin the project. crophones. A third-person video was also captured via a
 Scenariosrecordedin the UKcovered: commuting(driv- Logitech C 930 e Webcam. Participantswore the provided
 ing,walking,cycling,taking the bus,hiking,jogging),en- recordingdeviceswhileeating,drinking,andplayingsocial
 tertainment(cardgames,boardgames,videogames,lego, deductiongamessuchas One Night Ultimate Werewolf and
 reading,practisingamusicalinstrument,listeningtomusic, The Resistance: Avalonin the irownhome. Thisat-home
 watching TV),jobs(labwork,carpentry),sports(football, game-nightsettingelicitedawiderangeofspontaneous and
 basketball,climbing,golf,yoga,workouts)andhome-based naturalistic social behaviors and interactions. In addition,
 daily activities (cooking, cleaning, laundry, painting, car- eating and drinkingbehaviors were captured from both the
 ing for pets,tidying,watering the plants),DIY(fixing,gar- egocentri can dthird-personcameras.
 dening,woodwork)andcrafts(col our ing,crafting,crochet, Inadditiontoparticipatingin the recordedsession,partic-
 drawing, knitting, sewing). Footage was captured using ipantscompletedasurvey that captured the irdemographic
 Go Pro Hero-7,Hero-8 and Vuzix. information. All data wasscreened and censoredbystudy
 Footagewas the nreviewedbyresearcherstoidentifyany personneltoremoveanyidentifyingin for mationincluding
 PII.36%ofallvideosrequiredde-identification. we used visiblepersonalin for mationon the irphonescreensor the
 Primloc’s Secure Redactsoftw are suite,withintegratedtools exteriorof the home. Participantsalsohad the opportunity
 anduserinterfaces for manualtracking and adjustingdetec- toreview the videos and requestadditionalcensoring.
 tions. Redactedrecordings were reviewedmanually, then Primarycontributors: Fiona Ryan-leadcoordinator for
 encoded and uploadedto the AWSbucket. Duringencoding, datacollection,includingsynchronization,de-identification,
 IMUmeta data wasseparatelyextracted. Integratedaudio and ingestion; Audrey Sou the rland - lead coordinator for
 andvideousingnative 50 fpsrecordings are available. IRBdevelopment and recruiting;Miao Liu-contributedto
 Intotal,262 hours were recordedby 82 participants. On datacollection and ingestion;James M.Rehg-contributed
 average,eachparticipantrecorded 3.0 hours(σ =0.7 hours) toprotocoldesign and datacollection.
 The data ispublishedunder General Data Protection Regula- 
 Indiana University,Bloomington,IN,USA: Participants
 tion(GDPR)compliance. 
 in the Bloomington, Indiana, USA area were recruited
 Primary contributors: Michael Wray - data collection, 
 throughadvertisementsonsocialmedia,onlineclassifieds
 consent forms and information sheets; Jonathan Munro - 
 boards,andemaillists. Wealsousedsnowballsamplingby
 datacollection and ethicsapplication;Adriano Fragomeni- 
 askingparticipantstosh are ourads with the irfriends. Were-
 datacollectionandde-identificationoversight;Will Price- 
 cruitedparticipantswho were willingtoper for minteractive
 dataingestion,encoding and meta data;Dima Damen-sce- 
 smallgroupactivitiessuchasplayingsports,playingboard
 narios,procedures,datacollectionoversight and participant 
 orcardgames,playingmusicalinstruments,assemblingpuz-
 communication. Weacknowledge the effortsof Christianne 
 zles, etc. The health of participants and study personnel
 Ferneeinmanuallyreviewingall data. 
 wassafeguardedbycollectingdataei the routdoors(where
 Georgia Tech, Atlanta, GA, USA: Participant groups people can moresafelyinteract with outwearingmasks),or
 from the Atlanta,Georgia,USAmetro are awererecruited indoorsinthehomesof the participants. Inei the rcase,we
 viaonlineposts and advertisementsonsitessuchas Face- initially required that all participants in a social group be
 book, Reddit, and Instagram. Each group of participants partofthesamehouseholdtominimize the riskofspreading
 12 

 
 
 
 
 
 
 diseasebetweenhouseholds,butlaterweallowedgroupsof Primarycontributors: David Crandall-leadcoordinator
 peoplewho were com for tableinteracting with oneanother for data collection;Yuchen Wang-contributedtoprotocol
 (e.g., because the yarevaccinated for COVID-19). Group design, participant recruiting, and data collection; Weslie
 sizesranged from 1 to 6 people,withgroupsof 2 or 3 being Khoo - developed multi-camera synchronization and de-
 themostcommon. identificationpipelines. 
 We collected data with four different devices: z Shade 
 Universityof Minnesota,Twin Cities,MN,USA: Partic-
 1080 p camera glasses, i Vue Rincon 1080 camera glasses, 
 ipants in the Minneapolis and St. Paul, Minnesota, USA
 ORDRO EP-6, and Pupil Labs Invisible camera and gaze 
 area were recruitedthroughadvertisementsonsocialmedia
 trackingglasses. we usedmultipledevicesbecauseeachhas 
 anduniversitybulletinssuchas Facebook AD,Craiglist,and
 variousadvantages and disadvantages; z Shadehasalarge 
 Redhat. A total of approximately 313 hours of data was
 horizontalfieldofview,forexample,whilei Vuehasanad- 
 collected from 45 participants (22 males and 23 females).
 justableverticalfieldofview,ORDROsitsby the earandis 
 Agegroupsinclude 5 teenagers,20 peoplein the irtwenties,
 mountedonaheadb and whichworkswell for peoplewear- 
 11 people in their thirties, 8 people in their forties, and 1
 ingprescriptionglasses,and Invisibleoffersgazetracking 
 personin the irfifties. Werecruitedparticipantsasmultiple
 but is very expensive. We asked as many participants as 
 groups and enc our aged the mtoengageinunstructurednat-
 possiblein the grouptowearcameras. Weprimarilyused 
 uralsocialinteractions. Suchinteractionsincludedplaying
 ourtwo Pupil Labs Invisibleswheneverpossible,because 
 cardgames, talkingin the kitchenwhilecooking, playing
 oftheireaseofuse and abilitytocollectgaze data,butwe 
 basketball,andbuildingatentatacampsite. Inallcases,
 alsoused the ORDROEP-6 when the rewerelargergroups 
 werequired that allparticipantsinasocialgroupbepartof
 orwhenparticipantsworeprescriptionglasses. 
 thesamehouseholdtominimize the COVID-19 risk. Group
 Our protocol was reviewed and approved by the Indi- 
 sizesranged from 1 to 6 people,withgroupsof 2 or 3 being
 ana University Institutional Review Board(IRB).Wefirst 
 themostcommon. 
 conductedanonlinemeeting with potentialparticipantsto 
 Wecollected data with thez Shade 1080 pcameraglasses
 describe the study,explaintheuseof the cameras,agreeon 
 that have alargefieldofview. Ourprotocolwasreviewed
 anactivity for the mtoperform,andanswer the irquestions. 
 andapprovedby the Universityof Minnesota Institutional
 We ask participants to try to limit capture of potentially 
 Review Board (IRB). We first conducted an online meet-
 privacy-sensitivecontentbychoosingaplace with intheir 
 ing with potentialparticipantstodescribe the study,explain
 home that didnot have personallyidentifiablein for mation, 
 the use of the cameras, agree on an activity for them to
 byavoidingrecordingpeopleo the rthanthoseparticipating 
 perform, and answer their questions. We then arranged a
 in the study, and by avoiding saying last names or other 
 time for them to receive the cameras and provided them
 sensitiveaudio. 
 withapostage-paidbox for camer are turn. Afewdayslater,
 Wethenarrangeatimetomeetthem,typicallyoutside 
 participants shipped the cameras to our designated return
 their home or in an outdoor public place. We set up the 
 address. Wedownloaded the dataaftersanitizingcameras
 cameras,helptheparticipantsput the mon,give the mour 
 and equipment. After the data capture was complete, we
 contact information in case they have any problems, and 
 visuallyinspectedeverysecondofvideoinordertoexclude
 thenweleavewhiletheyperform the activity. Wethenre- 
 anyprivacy-sensitivein for mation(e.g. licenseplates,smart
 turn after about one hour to pick up the cameras. Within 
 phonescreens,andcreditcardnumbers),andtoassess the
 a few days, we send each participant a copy of the video 
 durationofnon-socialactivities. Forincidentalparticipants
 takenby the ircamera,andaskthemtoreview the footage 
 (i.e. byst and ers)appearingin data collectedby the camera
 andidentifyanyprivacy-sensitivecontent(videooraudio) 
 wearerinpublicsettings(e.g.,shopping,concert,atapark,
 that the ywouldprefertobeblurredorremoved. Wemanu- 
 etc.),datacollectionconsistsonlyofrecordingpubliclyob-
 allyeditoutanysuchcontent(using Adobe Premiere Pro). 
 servablebehavior with nomanipulationordirectinteraction
 Wealsoreviewallvideo for facesofnon-participants and 
 with the participants, and this university’s IRB allows an
 personally-identifyingin for mationsuchashousenumbers 
 assumedwaiverofconsent for thoseparticipants.
 or license plates, and blurred these accordingly. We use 
 Primarycontributors: Hyun Soo Park-leadcoordinator
 Pupil Labssoftw are tosynchronizeeyegaze with the video 
 for data collection;Jayant Sharma-contributedtoparticipant
 foreachparticipant,and the nused Adobe Premiere Proto 
 recruiting, data collection, IRB submission, analysis, and
 temporallysynchronizevideoacrossdifferentparticipants 
 dataingestion. 
 usingaudiotrackcomparison. 
 Intotal,approximately 103 hoursofvideo were collected National University of Singapore, Singapore: Partici-
 from 66 participants(42 female,23 male,1 non-binary;for pants were recruited from Singaporethroughadvertisements
 age, 46 were 20-29 years old, 14 were 30-39 years old, 1 on social media, via flyers and surveys, as well as from
 was 40-49,2 were 50-59,1 was 60-69,and 2 were 70-79). sourcingby the projectcoordinator. Residentsof Singapore
 13 

 
 
 
 
 
 
 aged 21 to 70 whocouldwearacamerawhileparticipating 44 twenties, 3 thirties, 2 forties, 6 fifties, and 1 sixties).
 in social sessions were eligible for inclusion in our study. Our data collectionfocusesmainlyonsimultaneousvideo
 During the recordingsession,theparticipants were required recording in groups of camera wearers within a common
 toattendsocialeventssuchasfamilyga the rings,exercising setting. Thus,these data captureasinglescene and social
 with a trainer, hairdressing, getting manicure, attending a interactions from differentpointsofview. Weincludeboth
 session for teachingassistants,attendingagroupmeeting, outdoor and indoorscenariosin Colombia. Outdoorscenar-
 etc. Thedevicesused for datacollection were Go Pro Hero 8, iosinclude Bogota´ and Cartagena’shistorical and colonial
 Go Pro Hero 9,and ARglasses. Go Procameras have binau- centers,asurbansettings,anda Natural National Park and
 ralmicrophoneswhile the ARglasses can onlyrecordmono astream,asruralsettings. Indoorlocationsincludeprofes-
 audio. Intotal,51 hoursofvideos were collected from 40 sionalactivitiessuchaslaboratoryworkers and hairstylers.
 participants(25 males and 15 females). Agegroupsinclude Fur the rmore, we include sports events such as salsa and
 31 twenties,5 thirties,3 fifties,and 1 sixties. urbandancerehearsals and rockclimbing.
 Primarycontributors: Mike Zheng Shou-leadcoordina- Primarycontributors: Cristina Gonza´lezand Paola Ruiz
 tor for datacollection;Eric Zhongcong Xu-contributedto Puentes. 
 datacollection;Ruijie Tao-contributedto data collection. 
 Carnegie Mellon University, Pittsburgh, PA, USA and
 Facebook Reality Labs (FRL), Redmond, WA, USA: Kigali,Rwanda: Carnegie Mellon University(CMU)Pitts-
 Participants were recruited from the Seattle area through burghga the redalargeportionofits data from skilledwork-
 a FRL-hiredvendorcompany. Intotal,there were 400 hours ers such as carpenters, construction workers, landscapers,
 collected from 206 unique participantsin 6 scenes staged mechanics,arborists,painters,andartists. Thisportionof
 in FRL’sresearchlabsin 2019. Theethnicgroupsinclude the data setdoesnotincludeanygraduatestudents with the
 50.8% Caucasian, 28.2% Afri can, 11.9% Asian and 9% explicitgoalofcapturingadiverserangeofreal-worldoccu-
 Hispanic. The staged environments include four types of pationalactivities. Over 500 hoursofvideo were captured
 apartments, a clothing store, and a grocery store. During inthe Pittsburgharea. The data wasmostlyrecordedusing
 therecordingsessions,theparticipants were askedtowear a Go Pro camera and a small portion was collected using
 Vuzixglassestogothrough the followingeverydayscenarios Wee View,awearablestereocamera.
 asnaturallyaspossible: groceryshopping,buyingclothes, Carnegie Mellon University Africa gathered data from
 watching TV,playingvideogames,listeningtomusic,danc- hobbyistcraftspeople and dailyworkersworkingin Kigali,
 ing, weightlifting, stretching, readingemail, payingbills, Rwanda. Anef for twasmadetocollect data mostrepresen-
 onlinegaming,cooking,talkingwitho the rpeople,meetings, tativeofhowtasks are carriedoutin Rwanda(suchasdoing
 whiteboarding,andvideocalling. Theemails and bills were laundrymanuallyasopposedto with awashingmachine).
 always mock data, not personal emails or bills of the par- Over 150 hours of video were captured, and a portion of
 ticipants. Thevideocallstookplacebetweenparticipants thosehours are availablein the currentrelease. Allof the
 only. datawascollectedusinga Go Procamera. 
 Three out of four apartments have corresponding 3 D Primarycontributors: Kris Kitani-projectcoordinator
 scans. we use the state-of-the-artdensereconstructionsys- forboth CMUPittsburgh and CMUAfricavideocollection.
 tem[209]toobtain the 3 Dphoto-realisticreconstructionof Sean Crane-leadcoordinatorof CMUPittsburgh data col-
 thoseapartments. Volumetricrepresentations are obtained lection (over 500 hours), main lead of CMU IRB review.
 from a customized capture rig and dense 3 D meshes are Abrham Gebreselasie-leadcoordinatorof CMUAfrica data
 extracted by the Marching Cubes algorithm with textures. collection. Qichen Fuand Xindi Wu-developmentofvideo
 We further annotate the dense meshes by labeling object de-identification pipeline, manual video de-identification
 categoriesover the meshpolygons;35 objectcategoriesplus annotationof CMUPittsburgh data. Vivek Roy-mainarchi-
 abackgroundclasslabel are usedinannotation. tectureof the licensesigningwebserver,coordinating with
 Primarycontributors: Mingfei Yan,Richard Newcombe, America Web Developers. 
 Kiran Somasundaram,Chao Li. 
 Universityof Catania,Italy: Morethan 359 hoursofvideo
 Universidad de los Andes, Colombia: We gather 302.5 have been recorded from 57 different subjects recruited
 hoursacross 20 scenarios from 77 uniqueparticipants. We through word of mouth, starting from family members,
 recordvideosusing Go Pro Hero 9 camerasbetween July and friendsandacquaintancesofstudents and facultymembers
 August 2021. Werecruitvolunteerparticipants from within oftheresearchgroup. Videos are relatedto 25 scenarios. We
 the Uni and escommunity and the irfamilies and friends. The chose the participantstocoverawidevarietyofprofessional
 ethnicgroupsinclude 89.9%Hispanic,1.4%Afri can,and backgrounds(24 backgroundsincludingcarpenters,bakers,
 5.8%Caucasian.Thegenderdistributionfollows 41.6%male employees,housewives,artists,andstudents)andages(sub-
 and 58.4%female with agesranging from 18 to 65(6 teens, jects were aged from 20 to 77,withanaverageageof 36.42).
 14 

 
 
 
 
 
 
 
 
 
 Baker > 9.5 hrsof videos Carpenter > 7 hrsof videos Crafting> 12 hrsof videos Bike Mechanic> 5.5 hrsof videos Bike Mechanic> 17.5 hrsof videos Scooter Mechanic> 9.5 hrsof videos Car Mechanic> 3.5 hrsof videos
 
 
 
 
 
 
 Figure 11.Matterport 3 Dscans(top)relatedtosevendifferentlocationscoupled with somevideos(bottom).
 
 
 21 oftheparticipants were female,while the remaining 36 inour Facebookadvertisementsorpostersincampusrestau-
 weremale. Femaleparticipantscollectedabout 137 hours rants and supermarkets. Each candidate participant was
 ofvideo,whereasmalescollected 222 hoursofvideo. The requiredtoregisterthroughanonlineform,whichcontained
 averagenumberofh our sofvideosacquiredbyeachpartic- anintroductionto and requirementsof the recording task,
 ipantis 6 h:18 m:23 s,withaminimumnumberofh our sof andcollectedhis/herbasicdemographicin for mation. The
 06 m:34 s,andamaximumnumberofh our sof 15 h:40 m:42 s. participants’ ages range from 22 to 53. They come from
 Toprep are participantstorecordvideos,wedemonstrated 20 different countries, and about half are females. Many
 tothemtheoperationsof the camera and howtowearit. We participants were graduatestudents and researchers,while
 providedexamplesofvalidrecording and invalidrecordings othershadvariouskindsofoccupationssuchaschefs,facil-
 beforetheystarted the acquisitionsession. Therecording itymanagers,andteachers.
 procedurewasdescribedinadocumentleftto the partici- Inordertoprep are theparticipants for the recordingpro-
 pantstohelpthemremember the deviceusage and howto cess,theteamdescribedindocuments and demonstratedto
 per for magoodacquisition. Acquisitionofvideoshas been themtheoperationsof the camera. Theteamalsoprovided
 per for medusingdifferent model sof Go Procameras(Go Pro examples of what constitute valid and invalid recordings
 4, Go Pro 7, Go Pro 8, and Go Pro Hero Max), which were before the ystarted. Eachparticipantwasprovideda Go Pro
 handedoverto the participantswhotypicallyacquiredtheir mountablecamera with 2 batteriesanda 512/256 GBSD
 videosautonomouslyoveraperiodofafewdaysorweeks. card. Each participant needed to choose at least 2 differ-
 3 Dscans for 7 locations using the Matterport 3 Dscanner entactivities from ourscenariolist and record 1-10 hours
 have been alsocollected(Figure 11). ofvideowithin 2 days. Theuniversityteamwentthrough
 Primarycontributors: Giovanni Maria Farinella and An- therecordingsaftertheparticipantsreturned the camerato
 tonino Furnari-scenarios,procedures,datacollectionover- checktheirqualityaswellastomakesure the videosmeet
 sight, data formatting, encoding, meta data and ingestion. theuniversity’s IRBrequirements.
 Irene D’Ambra-datacollection,consentforms and informa- Primarycontributors: Chen Zhao,Merey Ramazanova,
 tionsheets,manualdat are view,de-identificationoversight. Mengmeng Xu,and Bernard Ghanem.
 King Abdullah University of Science and Technology 
 (KAUST), Saudi Arabia: A total of 453 hours of videos 
 have been collected from 66 uniqueparticipantsin 80 differ- 
 entscenarios with Go Pro Hero 7. All the participants were 
 KAUSTcommunitymembers,who are fromvariouscoun- 
 tries and havevariousoccupations.Allrecordingstookplace 
 inthe KAUSTuniversitycompound,whichis 3600 hectares 
 inarea with diversifiedfacilities(e.g.,sportscourts,super- 
 markets, a 9-hole golf course, and 2 beaches) and scenes 
 (e.g.,buildings,gardens,theredsea,and the desert). There- 
 fore,theteamwasabletocollectvideosofvariousscenarios 
 suchassnorkeling,golfing,cycling,anddriving. 
 The participants were recruited from multiple sources, 
 such as friends and families, individuals referred to us by 
 earlierparticipants,aswellaspeoplewho were interested 
 15 
 

 
 
 
 
 
 
 B.De-identification Process 
 The dataset has two types of video. The first includes 
 videos recorded indoors where informed consent for cap- 
 turingidentitiesisexplicitlycollected from allparticipants 
 inthescene,includingfaces and voice. Onlyvideoof this 
 type is used in our Audio-Visual Diarization and Social 
 Interaction benchmark studies. All 400 hours of data col- 
 lectedby Facebook Reality Labsfallsin that category. The 
 second category, which forms the majority of our videos, 
 requiresde-identificationasconsent for capturingidentities 
 isnotgiven—includingfootagecapturedoutdoorsinpublic 
 spaces.4 Onlyvideocollectedby the universitiesfallsinto 
 thissecondcategory. See Appendix Afordetailsabout the 
 per-sitecollectionapproaches. Figure 12. CMU’sde-identificationpipeline
 B.1 De-identificationoverview 
 bemanuallyidentified and blurredper-frame. For this part
 Allvideosin the secondcategory were manuallyscreened ofourde-identificationprocess,we usedbothcommercial
 to address any de-identification needs, and are further di- toolswithin the above-mentionedcommercialsoftw are and
 videdintotwogroups. Group 1: videos that donotcontain opens our cesoftw are,including Computer Vision Annota-
 anypersonallyidentifiablein for mation(PII).5 Thisiswhen tion Tool(CVAT)8,Anonymal 9 and Siam Mask 10.
 thevideoisrecordedindoors with onepersonwearing the 
 Timecosts. Therelativetimecosts with respectto the orig-
 camera per for ming tasks such as cleaning or knitting for 
 inalvideolengthvariedsignifi can tlyfor the differentscenar-
 example,andno PIIispresentin the video. Thesevideos 
 ios. Videoscapturedoutdoorscouldtake 10 xthelengthof
 didnotrequirede-identification. Group 2: videoswhere PII 
 thevideotoc are fullyredact. 
 is captured. These include indoor settings with multiple 
 participants present, PII captured accidentally such as an 
 addressonanenvelopeor are flectionof the wearer’sfaceon B.2 Samplepipeline 
 amirrororasurface,aswellasvideosrecordedoutdoorsina 
 publicspacewherebyst and ersorcarsappearin the footage. Whilepartnersfollowedvaryingpipelines,weofferasam-
 Videosin Group 2 weremarkedforde-identification,deploy- plepipelinetoshowcase the processfollowedby Carnegie
 ingadvancedvideoredactionsoftw are,opens our cetools, Mellon University that uses brighter.ai as the commercial
 andh our sofhumanreviewstoredactvisible PIIs.University softw are. Thissamplepipelineshowcases the combination
 partnersundertookthisde-identificationef for tfor the irown ofautomatedprocesses and humanlabor with relativespeeds
 data. Wesummarize the approachbelow. ofthesesteps. 
 Videos marked for redaction were processed through This semi-automatic de-identification process was per-
 de-identificationsoftw are thatremovesspecificidentifiers formedinf our sequentialstages(Figure 12): (1)automatic
 at scale. We used two commercial softwares: brighter.ai 6 face and licenseplatedetection,(2)falsepositiveremoval,
 and Primloc’s Secure Redact 7 thatenableddetectingfaces (3)negativedetectionh and ling,and(4)imageblurring.
 and number plates automatically. We care fully reviewed 
 Sensitiveobjectdetection Given the collectedvideos(raw
 all outputs from automated blurring, identifying both in- 
 data),areviewers can sthroughvideos and marksthosecon-
 stancesoffalsepositives(blurring that mistakenlyoccurred 
 tainingsensitiveobjectssuchashumanfaces,licenseplates,
 on non-privacy related items) or false negatives (inaccu- 
 creditcards,etc.Thende-identificationsoftw are(brighter.ai)
 rate or insufficient automated blurring of faces and num- 
 wasusedtoautomaticallydetectsensitivein for mation.
 ber plates). Additionally, other PII data such as written 
 names/addresses,phonescreens/passwordsortattooshadto False positive removal To improve the quality of the de-
 tection,falsepositives were removed. Reviewersmanually
 4 Theexceptionis data from Universityof Minnesota,whose IRBper- 
 scanned through the bounding boxes detected by the de-
 mittedrecordingofincidentalparticipantsinpublicspaceshavingnoma- 
 nipulationordirectinteraction with studypersonnel. identificationsoftw are,andrejectedthoseboundingboxes
 5 we use the abbreviation PIItocapture data protectedundervarious whichdidnotcontainsensitivein for mation.
 dataprotectionregimesincluding the General Data Protection Regulation 
 (GDPR)where the term“personal data”isused. 8 https://github.com/openvinotoolkit/cvat
 6 http://brighter.ai 9 https://github.com/ezelikman/anonymal 
 7 http://secureredact.co.uk 10 https://github.com/foolwood/Siam Mask 
 16 

 
 
 
 
 
 
 Falsenegativecorrection Additionally,reviewersstudied 
 everyvideotosearch for falsenegatives and manuallyan- 
 notated the musingaboundingbox. Tomake the process 
 more efficient, an online object tracking algorithm [222] 
 wasusedtogenerateboundingboxproposalsacrossframes. 
 Reviewers verified that all tracked bounding boxes were 
 correct. 
 Imageblurring Onceallof the detections were modified 
 and corrected, a robust blurring process was used to de- 
 identifyimageregionsdefinedby the boundingboxes. 
 Timecosts Therelativetimecosts with respectto the orig- 
 inal video length for each step are shown in Figure 12. 
 Though this number depends greatly on the scenario cap- 
 turedin the video,roughlyspeakingtode-identify 500 hours 
 ofvideo data,ittook 780 hoursofmanuallabor. Review 1 
 of 500 hoursofvideorequired 250 hoursofwork,removal 
 of false positive over 115 hours of video took 115 hours 
 ofwork, Review 2 of 115 videostook 115 hoursofwork, 
 correctingfalsenegativesin 35 hoursofvideosrequired 50 
 hoursofwork,and Review 3 of 500 hoursofvideotook 250 
 hoursofwork(250+115+115+50+250=780 hrs). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 17 
 
 
 

 
 
 
 
 
 
 C.Demographics US 39 
 China 10 
 Wefur the rprovideself-decl are din for mationonethnic 
 India 10 
 groups and/orcountryofbirthby the participants. Wereport 
 Bangladesh 2 
 theseseparatelyperstate/countrydueto the differencesin 
 Vietnam 2 
 granularityofethnicgroupings.Allparticipants are residents 
 Georgia,USA,Residents 100%ofparticipants that reside
 in the country specified per paragraph. This data is not 
 in Georgia,USA,self-reported the irethnicgroupmember-
 available for participants from Minnesota,US. 
 shipasfollows: 
 United Kingdom Residents Reportingdemographicswas White/Caucasian 16 
 optional and thus 63%ofparticipants(52/82)thatresidein Black/Afri can Ameri can 1 
 the United Kingdomself-reported the irethnicgroupmem- Asian/Indian&White/Caucasian 1
 bershipasfollows: Other/Taiwanese 1 
 White—English,Welsh,Scottish,Northern Irishor British 35 Japan Residents 100%ofparticipants that residein Japan
 White—Anyother Whitebackground 12 
 self-reported the irethnicgroupmembershipasfollows:
 Mixed—White and Asian 1 
 Asian(Japanese) 81 
 Mixed—Anyother Mixedor Multipleethnicbackground 2 
 Arab 1 Kingdomof Saudi Arabia Residents 100%ofparticipants
 Prefernottosay 1 that reside in KSA self-reported their country of birth as
 follows: 
 Italy Residents 100% of participants that reside in Italy 
 China 12 
 self-reported the ircountryofbirthasfollows: 
 Russia 9 
 Italy 53 Colombia 8 
 Germany 1 Mexico 5 
 Russia 1 Kazakhstan 4 
 Portugal 1 India 4 
 Poland 1 US 4 
 Saudi Arabia 3 
 India Residents 100%ofparticipants that residein India 
 Kyrgyzstan 2 
 self-reported the irethnicgroupmembershipasfollows: 
 New Zeal and 2 
 Eastern India 10 Greece 2 
 Northern India 15 Ukraine 2 
 Southern India 108 Italy 2 
 Western India 5 Lebanon 1 
 Jordan 1 
 Egypt 1 
 Pennsylvania,USA,Residents 100%ofparticipants that 
 Kashmir 1 
 residein Pennsylvania,USA,self-reported the irethnicgroup 
 Portugal 1 
 membershipasfollows: 
 South Afri can 1 
 White 42 
 Thail and 1 
 Asian 4 
 Mixed—White and Black Afri can 2 Singapore Residents 100% of participants that reside
 Black,Afri can,Caribbean 1 in Singapore self-reported their nationalities as follows:
 Chinese 26 
 Washington,US,Residents 100%ofparticipantsthatre- Singaporean 12 
 sidein Washington,USA,self-reported the irethnicgroup Indian 1 
 membershipasfollows: Malayan 1 
 Caucasian 101 Colombia Residents 90% of participants that reside in
 Blackor Afri can Ameri can 58 Colombia self-reported their ethnic group membership as
 Ameri can Indian(Native Ameri can) 24 follows: 
 Hispanic 19 Hispanic/Latin 62 
 Indian(South Asian) 4 White/Caucasian 4 
 Black,Africanor Caribbean 1 
 Indiana,US,Residents 95%ofparticipants that residein Mixed-Whitean Afri can 1 
 Indiana,US,self-reported the ircountryofbirthasfollows: Prefernottosay 1 
 18 

 
 
 
 
 
 
 Rwanda Residents 100% of participants that reside in 
 Rwandaself-reported the irethnicgroupmembershipasfol- 
 lows: 
 Black,Africanor Caribbean 14 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 19 
 
 
 

 
 
 
 
 
 
 D.Narrations D.2 Narrationanalysis 
 Thegoalof the narrationsistoobtainadensetemporally- Wepresentsomestatisticson the collectednarrations. Al-
 aligned textual description of what happens in the video, together, we collected 3.85 M sentences across the 3,670
 particularlyintermsof the activities and objectinteractions hours of video. Figure 15 (left) shows the distribution of
 bythecamerawearer. The Ego 4 Dnarration data isitselfa frequencyofnarrationsacrossallvideosin the dataset. De-
 newresource for learningaboutlanguagegroundedinvisual pendingon the activitiesdepicted,videos are annotatedat
 perception. Inaddition,asdescribedin the mainpaper,we varyingfrequencies. Forexample,avideoofapersonwatch-
 leverage the narrationsasa for mof“pre-annotation”toindex ing television is sparsely annotated as very few activities
 thevideosbysemanticterms. Specifically,thenarrations are occur (0.17 sentences/minute), while a video of a person
 usedtoconstructaction and objecttaxonomiestosupport harvesting crops, per for ming repetitive actions is densely
 variousbenchmarks,toidentifyvideos that are relevantto annotated(63.6 sentences/minute). Onaverage,therearean
 eachbenchmark,andtoselectregionswithin the videos that 13.2 sentencesperminuteofvideo.
 requireannotation. Figure 15 (middle and right) show the distribution of
 Thissectionoverviewshowweinstructedannotatorsto lengthof the collectednarrations. Theindividualtimepoint
 narrate the videos,andhowwetrans for mednarrationtext narrations are short, highlightasingleactionorobjectin-
 intotaxonomiesofobjects and actions. teraction,and have anaverageof 7.4 words. Thoughshort,
 thesenarrationscoveravarietyofactivitiesrangingfromob-
 D.1 Narrationinstructions and content jectinteractions,tooluse,camerawe are rmotions,activities
 ofo the rpeopleetc. Incontrast,thesummarynarrations are
 We divide the dataset into clips of (max) 5 minutes long 
 longer(onaverage,16.8 words)anddescribeactivitiesata
 whenacquiringnarrations.Each 5-minuteclipis the npassed 
 higherlevel. Table 2 showsafewtextexamplesofeachtype
 totwodifferentannotators,tocollecttwoindependentsets 
 ofnarrationinadditionto the visualexamplesin Figure 14.
 of narrations for every video clip in the dataset for better 
 Finally, we study the diversity of the video dataset by
 coverage and toaccount for narrationerrors.11 Narrators are 
 lookingatthefrequencyofoccurrenceofwordsin the narra-
 instructedtowatch the 5 minutevideoclipfirst, andthen 
 tionscollected for videosofeachscenariotype. Figure 16
 askedtoprovideashort 1-3 sentence“summary”narration 
 showswordcloudsdepictingobjects that prominentlyfea-
 fortheentireclip that correspondsto the overallactivity and 
 tureinacrossvariousscenarios. Thewordcloudshighlight
 settingof the videoclip(e.g.,“thepersondoeslaundryin 
 characteristicobjectsperscenario(e.g.,bowl,spoon,plate
 thewashingmachine”). Thesesummaries are marked with 
 in “Cooking” videos; card, dice, pawn in “Playing board
 thetag“#summary”inthereleasednarrations. 
 games”videos)whilealsohintingatcommonobjectsacross
 Following this first screening, which is critical for the 
 allscenarios(e.g.,hands,paper,phones). Thediversityin
 overallunderst and ingof the clip, thedensenarrations are 
 narrationscollectedhighlights the diversityofvideocontent
 collectedasfollows.Annotatorsre-watch the clip,pause and 
 capturedin the dataset. 
 markthetimepointwhensomethinghappensin the video, 
 thenenterashortnaturallanguagedescriptionof the ongoing 
 D.3 Action and objecttaxonomy 
 actionorinteraction,beforeresumingwatching the video. 
 Narrators are provided the followingprompt:“Pretendas 
 Intotal the rawnarrationsdescribe the Ego 4 Dvideousing
 youwatch this video that you are alsotalkingtoafriendon 
 1,772 uniqueverbs and 4,336 uniquenouns. Thedistribution
 thephone,andyouneedtodescribetoy our friendeverything 
 of the most frequently occurring verbs and nouns can be
 thatishappeningin the video. Yourfriend can notsee the 
 seenin Figure 17. 
 video.”Thispromptisintendedtoelicitdetaileddescriptions 
 Following ideas from [44], we leverage the narrations
 thatprovideaplay-by-playof the action. See Figure 13 for 
 datatoconstructataxonomyover the actions and objects
 anillustrationof the narrationtoolinterface. Eachnarration 
 thatappearin the video,asfollows. we useapart-of-speech
 thuscorrespondstoasingle,atomicactionorobjectinter- 
 (POS)tagger and dependencyparsertoidentifyverbs and
 action that the camerawe are rperforms(e.g.,“#Copens the 
 nouns from each narrated action. We use an ensemble of
 washing-machine”or“#Cpicksup the detergent”,where the 
 parsermodels from the Spacy[98]toolkittodo this. Given
 tag#Cdenotes the camerawearer). Importantly,ournarra- 
 a natural language narration, we first identify verbs using
 tionsalsocaptureinteractionsbetween the camera-wearer 
 their POStag. Thenusing the dependencytree,weidentify
 and others in the scene, denoted by other letter tags, e.g. 
 all direct objects of the verb. To ensure verbs and nouns
 #X(e.g. “#Cchecksmobilewhile#Xdrives the car”,“#C 
 areaccuratelyparsed, weadoptseveralheuristics: Parsed
 passesacardto#Y”).See Figure 14 fornarrationexamples. 
 verbs are splitintomultiplesenses(e.g.,“turn”issplitinto
 11 Wesimplykeepbothindependentnarrations; they are notmerged “turn-on”,“turn-off”and“turn-over”);compoundnouns are
 because the ydonotserveasgroundtruth for anybenchmark. decomposed into a root noun coupled with a modifier to
 20 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 13. Narrationtoolinterface. Narratorsmarkatimepointwheresomethinghappensin the video(bottombar),andenteratext
 descriptionof the activity(leftsidebar). 
 
 Objectinteraction Contextobjects Multi-personactions Manipulationactions 
 #ccflips the paper #cctapsahandon the floor #oamanxmoves the legs. #cccutsaleaf from the plant with hislefth and.
 #ccliftsthet-shirt #ccholds the wheel with hislefth and. #oamanysitsonachair #ccpullshish and off the chesspiece
 #ccdrops the plate #ccputsthebrushin the colours. #oawomanxsteps for ward. #ccholdstheknittingneedle with theo the rhand
 #ccholds the pieceofcloth #ccplacesplastic model skiton the table #oapersonxhits the cricketball #ccopens the screwdrivercontainer with hishands
 #ccfixeson the modelcraft #ccarrangesthedoughson the tray #oamanythrows the balltowardsmanx #cctouchesthepieceofwood with the hand
 Camerawe are rmotion Summarynarrations 
 #ccraiseshands cwasinaroom,fixedawood model kit.#summary 
 #ccstands ctightenedthemotorontheheadofthehoeof the lawnmower.ccutgrassesonthefield with the lawnmower.#summary
 #ccst and supfrom the stairs cwasinakitchen,hecutsausagesintopieces with aknife,mixedthesausages and cooked the mwithapan.#summary
 #ccwalksaroundakitchen cwasin the house and shestudied#summary 
 #ccsitsup cstudiedinaroom.cwentthroughamobilephone and amobiletabletwhilereadingin the room.#summary
 Table 2.Textexamplesofnarrations.Thecollectednarrationsdescribediverseaspectsofhumanactivity.Summarynarrationscapture
 highleveldescriptionsofactivitiesina 5 minuteclip.See Figure 14 forvisualexamples.
 ensure the nountaxonomyisunambiguous(e.g.,modifier D.4 Narrations for annotationprioritization
 “egg”androotnoun“shell”in“eggshell”);collectivenouns 
 aremappedto the irmainentity(e.g,. “pieceofcheese” 
 → 
 “cheese”). Finally,wemanuallycluster the verbs and nouns 
 toavoidredundancyin the taxonomy(e.g.,“cut”,“chop”, 
 Allvideosin Ego 4 Darenarrated,andsubsetsof the mare
 “slice”areallmappedto the verbcluster“cut”). 
 manuallylabeled for eachbenchmark. Ratherthanr and omly
 labelinstances for agivenbenchmark,weaimtotargetthose
 that are mostrelevantto the task. Forexample,videoslikely
 tocontainmulti-personconversation are mostinteresting for
 the AVDiarizationbenchmark,whereasvideos with ample
 Theresultingtaxonomyconsistsofasetof 115 verbs( ) hand-objectinteraction are mostinteresting for Hands and
 V 
 andasetof 478 nouns( ). Figure 39 shows the distribution Objects. Tothatend,we use the narrations and summaries
 N 
 ofverbs and nounsinasetofvideo data annotated with the asatooltoautomaticallyprioritizecertainvideostolabel
 taxonomy. See Section J.2 fordetailsonhow the taxonomy perbenchmark. Thebenchmarkappendicesbelowprovide
 isusedinthecontextof the benchmarktasks. details. 
 21 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 14.Examplenarrationsatkeyframesofvideo.#Crefersto the camera-wearer.Thelastrowshowsnarrations that includeother
 people that participateinactivities with the camera-wearer(denotedbyo the rlettertags,e.g.,#O,#X).
 
 
 D.5 Contributionsstatement 
 Tushar Nagaraj and eveloped the taxonomy,helpeddevelop 
 narrationinstructions,andper for med the narrationanalysis 
 presentedin the paper.Kristen Graum and evelopednarration 
 instructions,helpedcoordinatepilots and annotationwork, 
 andcontributedtotaxonomy for mation. Michael Wrayco- 
 developed the taxonomy. 
 
 22 
 
 
 

 
 
 
 
 
 
 
 1400 
 
 1200 
 1000 
 800 
 600 
 400 
 200 
 0 
 0 20 40 60 80 
 # narrations / minute 
 tnuoc 
 Narrations per minute 
 1.4 
 1.2 
 1.0 
 0.8 
 0.6 
 0.4 
 0.2 
 0.0 
 0 10 20 30 40 50 
 # words 
 tnuoc 
 1 e 6 Narration length distribution 
 2000 
 1750 
 1500 
 1250 
 1000 
 750 
 500 
 250 
 0 
 0 10 20 30 40 50 60 70 
 # words 
 tnuoc 
 Summary narration length distribution
 Figure 15.Collectednarrationstatistics.Left:Distributionoffrequencyofnarrationscollected.Middle and right:Thedistributionof
 lengthof the collectednarrations and summaries. Summaries are naturallylonger,anddescribeactivitiesatahigherlevelcomp are dto
 individualactionnarrations.Seetext for discussion. 
 Figure 16.Distributionofobjectsinnarrationsofvideos from eightcommonscenarios.Thevarietyofobjectscoveredacrossscenarios
 showcasesthediversityofactivitiesin the videocollected. 
 kcip tup pord ecalp dloh evom evomer ekat tsujda hcuot nepo tuc llup tfil ruop worht epiw hsup ssap xif yrrac nrut naelc esolc hsaw yalp sserp tih egnarra dlof rits bur poocs tniap pid ekahs esnir tresni esu no_nrut nruter esiar daerps tcelloc etarepo revoc klaw hcterts kcehc barg ylppa llor evig kcap gnah nethgit pilf erusaem ekam xim refsnart pots hcatta daer pat gard gnirb ngila etator rewol tfihs dda nethgiarts yarps eit hctarcs tink tniop tae ezeeuqs eunitnoc nehtooms hctaw wes burcs etarapes tif rehtag raew tcennoc tcepsni eparcs leep dnah nesool egnahcxe tser dlofnu llird pohc tes kram nioj kool peews nam erats raet ward hctiws egnahc evird kcits kaerb eraperp ffo_nrut elknirps esaeler mirt wercs yal yrt enimaxe parw wohs knird tel tsud peek elffuhs edir llorcs kooc tnuoc wolb evael hcated od wercsnu noitisop ecils yrd hsurb no_hctiws gniws wardhtiw dneb trats hctef bmilc
 105 
 104 
 103 
 secnatsni 
 # 
 dnah drac repap doow htolc elttob reniatnoc lwob hguod hsurb enohp noops etalp dil retaw eceip efink draob revoc puc koob gab daerht top doof kcits nap yart wercs enihcam xob nosrep cirbaf eriw tniap llab ehtolc tlob noino tekcub lewot epat tun elbac nolyn taem nac knalp emag tnalp epip ssalg elbategev nep latem rood rossics ruolf trid lian gel lios nit kcitspohc tfarc reppep ledom egnops elzzup epor regnif hcnerw aremac otatop nug evael ssarg loot eulg trihs nray eldeen relur rennaps lairetam kcirb eht reward llaw gge draobdrac rewom elbat reilp ebut nip remmah lio notrac elit llird yalc eohs daerb revirdwercs nottub tehcas kcap ecid licnep kcolb hsid evolg redloh revird teksab hcnarb rettuc meti rewolf tekcap enots paos erusaem alutaps guj eveis llor otamot pac rebmit dnas lewort torrac dor leehw raj pat rellor rab krof tam rac edalb roolf egap tnemec tiurf egabbac rebmucuc
 104 
 Figure 17.Narrationverb/noundistribution.Distributionofautomaticallyextractedverbs(top)andnouns(bottom)fromnarrations.Top
 150 mostfrequentlyoccurringofeachisshown for clarity. 
 23 

 
 
 
 
 
 
 Numhours Numclips Avgcliplength 
 EMVQ-2 D 432.9 5,831 6.1 min 
 EMVQ-3 D 13 159 4.9 min 
 EMMoments 328.7 2,522 7.9 min 
 EMNLQ 227.1 1,659 8.2 min 
 Hands+Obj. 196.2 88,585 8.0 sec 
 Forecasting 110.5 1,498 4.4 min 
 AVD 47.7 572 5 min 
 Social 47.7 572 5 min 
 Table 3.Amountofannotated data for eachbenchmark.EMrefers 
 to Episodic Memory and AVDrefersto Audio-Visual Diarization. 
 All 3,670 hoursofvideo have narrations and features. 
 E.Benchmark Data Splits 
 Foreachbenchmark task,certainportionsof the Ego 4 D 
 videorepository are labeled. Table 3 shows the breakdown 
 of theamount ofdata annotated foreach. Note that there 
 are 764 totalh our sofvideorelevantto the AVDand Social 
 tasks(i.e.,haveaudio,conversation,andunblurredfaces), 
 including the annotatedsetof 47.7 hoursabove. Forother 
 benchmarks,therelevancehasasofterdependencyon the 
 specificvideocontent(e.g.,amemoryquery can applytoany 
 ofthe 3,670 hours). Thefollowingappendices will explain 
 howwesampled data tobeannotated for eachbenchmark. 
 For the public Ego 4 Dbenchmarkchallenge,weensure 
 that the splits are consistent with inafamilyofrelatedtasks. 
 Forinstance,allthe Forecasting and Hands+Objectstasks 
 share the samesplits and ensuretrainingvideosinonedonot 
 occurasvalidationvideosinanother. Similarly,the Episodic 
 Memory task ssh are the samesplits. However,itisharder 
 toensure this acrossverydifferenttasks, since the videos 
 selected for annotations are different. Forexample,the So- 
 cialbenchmarkconsidersmulti-personinteractionswhich 
 maynot have manyh and-objectinteractions;hence the set 
 ofvideoslabeled for Social and Hands+Objects have little 
 overlap and the train/val/testsplits are naturallydifferent. 
 Sinceweplantousethetestset for the publicchallenge, 
 weare with holdingall the testannotations and makingthem 
 accessible only through a submission server. We are also 
 withholdingthenarrations that overlap with anyof the test 
 sets. 
 
 
 
 
 
 
 
 
 
 
 24 
 
 
 

 
 
 
 
 
 
 F.Episodic Memory Benchmark andnotfactualqueries,i.e.,queries that requireanexternal
 knowledge base toanswer. 
 Thissectiondetails the Episodic Memorybenchmark task 
 NLQisachallengingmultimodal task requiringvisual
 definitions,annotations,baselinemodels,andresults. 
 and linguistic underst and ing and reasoning. Consider the
 query “What did I pick up before leaving the party?” In
 F.1 Formal task definitions ordertofulfill this request,thesystemneedsto: (a)break
 down and underst and the languagequeryasasearchforan
 As presented in the main paper, there are three kinds of 
 object(what)withwhich the userinteracted(pickup)before
 Episodic Memory queries—visual, natural language, and 
 anevent(leaving the party),(b)gothrough the egocentric
 moments—eachofwhichrequireslocalizing the responsein 
 video and identify the desiredeventof“leaving the party”,
 thevideo. Their for maldefinitions are asfollows. 
 (c)visuallysearch for theobject with which the userinter-
 Visualqueries(VQ) This task aimstoqueryanegocentric acted prior to this event. This example demonstrates the
 video base donastaticimagecropofanobject. Specifically, complexity of NLQfrom bothvisual(recognizingevents,
 it asks the question ‘Where was object X last seen in the objects,places,etc.)andlinguistic(breakingdownreason-
 video?’,where Xisasingle‘canonical’imagecropinwhich ing,underst and ingrelations,etc.)perspective. Inaddition,
 theobjectisclearlyvisible and human-identifiable. Apo- thediversesetofquerieswithin NLQ,whilefacilitatinga
 tential use case for visual queries is where a user teaches flexiblesearch and retrievalthroughanintuitiveinterfaceof
 thesystema new objectbyshowingaphoto(“thesearemy language,alsoincreasesthecomplexityof the task.
 keys”) and then later queries for it among past video. By Concretely, NLQ is formulated as follows: Given an
 enablingvisualqueries,asopposedtocategoricalqueries, egocentricvideo andanaturallanguagequery ,thegoal
 V Q 
 thisisa for mofopen-worldobjectlocalization. isagaintoidentifya‘responsetrack’r,such that the answer
 Weformulate the problemasfollows.Givenanegocentric to can be deduced from r. The response track should
 Q 
 video ,aqueryobjectospecifiedviaastaticvisualcrop beasetoftemporallycontiguousframeswithin . Given
 V V 
 v, and a query frame q, the goal is to identify when the the episodic nature of our task, r should be sufficient to
 objectowaslastseeninthevideobefore the queryframeq. answer ,without the additionalneed for oranyexternal
 Q V 
 Theresponseisspecifiedasa‘responsetrack’rwhichisa knowledgebases. 
 temporallycontiguoussetofboundingboxessurrounding 
 Moments queries (MQ) This task aims to query an ego-
 theobjectoineachframe: 
 centricvideo base donacategoryofactions. Specifically,it
 poses the followingrequest‘Retrieveall the moments that I
 r = r ,r , ,r ,r , (1) 
 s s+1 e−1 e 
 { ··· } do Xin the video.’,where‘X’comes from apre-definedtax-
 onomyofactioncategories,suchas‘interact with someone’
 wheresistheframewhere the objecto(atleastpartially) 
 or‘usephone’. Comp are dto the naturallanguagequeries,
 enters the camera-wearer’sfieldofview,eis the framewhere 
 themomentqueriesfocusondaily-lifeactionsoractivities.
 theobjectexits the camera-wearer’sfieldofview,andr isa 
 i 
 Onemomentquery can correspondtomultipleresponsein-
 boundingbox(x,y,w,h)inframei. Iftheobjectappears 
 stances(temporalwindows)inthevideo. This task provides
 multipletimesin the video,theresponseonlyrefersto the 
 theuserafast and convenientwaytoretrievemultipleaction
 ‘mostrecentoccurrence’oftheobjectin the past, i.e., the 
 momentsatatime,where the userdoesnotneedtocomeup
 responsetrackwhichminimizesq r withq >r . 
 e e 
 − withasentencetodescribewha the/shewants,butinstead
 Whena 3 Dscanof the environmentassociated with the 
 candirectlychooseamong the pre-definedcategories.
 videoisavailable,theresponseadditionallyincludesa 3 D 
 Themomentqueries task isrelatedto the taskoftemporal
 displacementvector∆d = (∆x,∆y,∆z)between the 3 D 
 actiondetection[141,229,237],whichaimstoidentify and
 locationwhere the querywasmade(i.e.,atqueryframeq), 
 localizeallinstancesofallactioncategories that takeplace
 andthe 3 Dlocationintheenvironmentwhere the objectwas 
 inavideo. Bothtasks have alistofactioncategoriespre-
 lastseen(i.e.,attheendof the responsetrackr ). 
 e 
 defined, andbothaimtopredictmultipleactioninstances
 Naturallanguagequeries(NLQ) Themotivationbehind with their temporal boundaries. The difference is that 1)
 the NLQ task istoenablesearchingthroughanegocentric our moment queries task is a retrieval task where action
 videousinganaturallanguagequery. Thesystemresponds categories are providedasqueries,meaningitdoesnotneed
 toaquerybyprovidingatemporalwindowlocalizedin the to produce instances of categories that are not among the
 video,fromwhichtheanswerto the query can bededuced. queries; and 2)ourmomentstaxonomyisspecifictofirst-
 Thesequeries can berelatedtoobjects,places,people,and personactivity. Weaim for moments that are activitiesat
 activities that appe are dintheepisodicmemoryof the user. amediumlevelofgranularity—coarserthan the actionsin
 Note that we only consider episodic queries, i.e., queries Forecasting,and finerthan the“scenario”labelsshownin
 that can beanswered/deduced from the egocentricvideos, Figure 3 ofthemainpaper. 
 25 

 
 
 
 
 
 
 Navigationverbs for entropy-basedvideoselection Toselectvideos base don the seconsiderations, we use
 atwo-stepprocess. First,wefilteroutvideos base don the
 appear ascend bend bring carry catch 
 climb close come descend dig dispose associated‘scenario‘labels(see Figure 3)thatprovidehigh-
 drag dribble drop enter fall fetch levelin for mationabout the content and activitiesinvideos
 find fly gather get give grab (e.g.,cooking,cleaning,golfing,etc.). Wemanuallypreview
 hang jog jump kick lean leave 
 randomly sampled videos from each scenario to identify
 lift lower move navigate open propel 
 interesting scenarios such as cooking, indoor navigation,
 raise return ride rise run shut 
 farmer,cleaning,andgroceryshopping. Wethensortvideos
 steer step turn vaccum walk 
 withineachscenario base donascoringfunctionusing the
 Table 4.Weprioritizevideostoannotate for visualqueriesbased narrations for the video. Specifically,weextract the listof
 ontheentropyof the senavigation-relatedverbsin the narrations. verbsin the narrations(along with the irfrequencies). We
 then measure the entropy of the distribution of manually
 curated navigation verbs (See Tab. 4). The video is more
 The MQtaskisalsorelatedtotemporallanguageground- likelytoallowchallengingvisualqueriesifitsnavigation
 inginvideos[236],whichaimstoretrieveasegment from entropy is higher. For videos with near-zero entropy, we
 a video, as queried by a natural language sentence. Both observe that the camera-wearerisusuallystayingstaticin
 tasks have aquery and aimtopredictcorrespondingtempo- asinglelocation with outanymovement. Finally,alimited
 ral segments. The difference is that MQ uses pre-defined numberof 3 Dscans were available for the 3 Dlocalization
 querycategoriesra the rthannaturallanguagesentences,and task. Videos associated with these scans were prioritized,
 onequery can correspondtomultipleinstancesra the rthana regardlessof the irnavigationentropy,insupportof the 3 D
 uniqueone. responseversionof the VQtask. 
 Weformulate the problemasfollows. Givenanegocen- 
 Naturallanguagequeries For NLQweapplysimilarsam-
 tric video , and a query action category c, the goal is to 
 V pling criteria as above for VQ, but augment it to avoid
 retrievealltheinstancesof this actioncategoryin the video, 
 repetitiveactions(e.g.,sewingwhilesittingon the couch).
 assuming that the query is made at the end of the video. 
 First,wemanuallyselectamenablescenarios(see Figure 3).
 Theresponseisasetofactioninstancesof the categoryc 
 N Amongthose,weprioritizeclips with highentropycomputed
 Φ = φ =(t ,t ,s ) , where n is the number 
 c { n n,s n,e n }n=1 overnavigationaltermsasabove. Finally,weprioritizenon-
 of instances for this category, t and t are start time 
 n,s n,e 
 repetitiveactionsbycomputingtheratioof the numberof
 andendtimeof the nth instancerespectively,ands isits 
 n 
 unique verbs in a clip’s narration vs. the total number of
 predictionconfidence. 
 verbsin that samenarration—higherisbetter. 
 Momentsqueries Toselectclips for momentsqueries,we
 F.2 Selectingclips for annotation 
 compute the overlapofverbs/nouns with the momentstax-
 Forallbenchmarkswesamplevideoclipstoannotatebased onomy. Wecalculateasimil are ntropy-basedscore and sort
 on criteria for geographic diversity and scenario diversity. videosaccordingto this score. Inaddition,werestrictvideos
 For Episodic Memoryweimposeadditionalsamplingcrite- toafixedsetofcategoriespresentin our taxonomytoavoid
 riameanttohighlight data mostinteresting for the task,as labelingvideos that donotcontainrelevantactivities.
 follows. 
 F.3 Annotation 
 Visual queries Video clips to annotate for visual queries 
 (VQ)areselected base don the frequencyofobjectoccur- Nextwedescribe the annotationprocedures and outputs for
 rences and amount of navigation in the video. To have Episodic Memory. 
 interestingvisualqueriesinavideo,theremust beseveral 
 Visualqueries Forannotatingvisualqueries,wefirstsam-
 ‘interesting’ objects that can be queried about. An object 
 plecontiguousclipsofvaryinglengths(5 mins,8 mins,and
 is‘interesting’inthecontextofvisualqueriesif the reisa 
 16 mins)from the setofinterestingvideos. Theannotators
 sufficientlyhighseparationinspace and timebetweenany 
 are instructed to create and annotate 3 visual queries for
 twooccurrencesof the object. Thistypicallyhappenswhen 
 each clip. A visual query consists of the query frame q,
 thecamera-wearervisitsthelocationnear the objectbriefly, 
 thevisualcropv ofthequeryobjecto, theresponsetrack
 and then navigates elsewhere before revisiting the object 
 r = r ,r , ,r ,r , and a textual name for the
 s s+1 e−1 e 
 again. Forexample,considerapersonwhofinishescleaning { ··· } 
 object(eg. cup,hammer,broomstick,etc). Theannotators
 a living room, visits the kitchen for some period of time 
 per for med the followingstepstoannotateagivenclip:
 beforerevisiting the livingroomagain. Mostobjectsin the 
 livingroom are interestingtoqueryaboutwhen the person 1. Identifythreeinterestingqueryobjectsin the clip. An
 isin the kitchen. objectisinterestingifitoccursinatleasttwodifferent
 26 

 
 
 
 
 
 
 partsof the video. In order to validate an annotation we collect two 3 D
 bounding boxes per query from two different annotators.
 2. For a given object, enter a textual name. While our 
 Leveragingthetwoboxeswecompute the followingvalida-
 current task queries with the imagecrop,not the name, 
 tionmetrics: 
 thisannotation will allowfuturevariants that doquery 
 for the objectbyname. c c 
 1 2 2 
 d = (cid:107) − (cid:107) (3)
 norm 
 m 
 3. Selectoneoftheobjectoccurrencesin the video and diag 
 V 
 mark a visual crop v = (x v ,y v ,w v ,h v ). The visual V = global , (4) 
 norm 
 cropmust beagoodrepresentativeviewof the object, V union 
 anditmust have goodlighting,large-enoughsize,and 
 wherec andc arethecentroidsof the twoboxes,m is
 mustnotbeblurred. 1 2 diag 
 theaveragediagonallengthof the twoboxes,V isthe
 global 
 4. Mark a different occurrence of the object as the re- volumeof the 3 Dconvexhullof the twoboxes,and V union
 sponse track r = r , ,r . The response track is the volume of the union of the two boxes. These met-
 s e 
 { ··· } 
 starts from the frame when the object is first visible ricsmeasuretheagreementlevelbetwen the twoannotators.
 andendswhentheobjectleaves the field-of-view. The When the twoannotations are perfectlyaligned,themetrics
 responsetrackmustalsobecontiguousintime and the areequaltod norm =0 and V norm =1.0. Theassumption
 boundingboxesmustaccuratelymark the position and isthatifthetwoannotatorsagreeon the position,scale,and
 sizeof the object. orientationoftheboundingbox the nitislikelytobecorrect.
 Ifthetwoannotations are far from eacho the rwe will discard
 5. Thequeryframeq issampledsometimeafterthere- 
 the query. There are a couple of reasons that can explain
 sponsetrackr. Theobjectomustnotappearanywhere 
 suchcase: (1)oneannotatormislabeled the query,(2)the
 between the response track r and the query frame q, 
 queryishardtoannotate. Somequeriesrequireasignificant
 sothat the groundtru this well-defined and unique for 
 amount of hallucination to retrieve the object location in
 “whendid Ilastsee...?”. 
 thes can whichclearlyleadstosubjectiveannotations. We
 empiricallydefinedtwothresholdsof 1.5 over d and
 For each annotation, we apply automated and manual norm 
 15 over V to filter out poor annotations. Any query
 qualitycheckstoensurecorrectness. Incase the qualityfalls norm 
 thathaseitheroneofthetwometricsabove the thresholdof
 belowacertainthreshold,theclipisreannotated. 
 acceptanceisrejected. 
 For visual queries associated with 3 D scans, we also 
 collect 3 Dannotationsin the formof 3 Dboundingboxes 
 Naturallanguagequeries Tocollect NLQannotations,we
 capturingwhere the objectwaslastseen. Wethenusethose 
 sample contiguous clips of length 8 minutes and 20 min-
 boundingboxestoestablish the groundtruthdisplacement 
 utes. Theannotators are instructedtowatch the seclips and
 vector from thequeryframeto the object,whichis the target 
 generatenaturallanguagequeries,focusedonretrievingin-
 of the task. Each annotation a is collected in the scan 
 q 
 formationaboutobjects,places,andpeoplein the egocentric
 coordinatesystems: 
 videoclips. Toreducethecognitiveoverloadon the anno-
 T =[R t ], (2) tators,andfocus the iref for tsonmemory-relevantqueries,
 s s s 
 | wealsoprovidealistof 13 querytemplates(see Table 5),
 whereq 1,..., , thetotalnumberofqueries,and correspondingtoqueriesausermigh task toaugmenttheir
 where T s 
 ∈ {R 4 isth Q 
 e 
 } 
 tra 
 Q 
 nsformationmatrixof the bounding memory. Note that these templates are provided only to
 ∈ 
 box.R s andt s are the correspondingrotation and translation guide the irchoiceofquery,anddoesnotlimit the linguistic
 forannotationa q . variabilitysince the annotators are instructedtoparaphrase
 Theannotationprocedureisdefinedasfollows: Aquery thetemplate with outcopying the masis.
 consistsofavideoclip,avisualcrop,and are sponsetrack. To elaborate, the annotators per for med the following
 Foreachquery,thegoalistoretrieveinthes can thelocation steps: 
 of the object defined in the video. Once the location is 
 found,wedrawa 3 Dboundingboxat this position with the 1. Watch the entirevideoclip inordertounderst and the
 V 
 appropriate scale and orientation. Itisimportanttonote that high-levelcontext(optionallyin 2 fast-forward),
 × 
 3 Dscans and videos have beenrecordedatdifferenttimes. 
 Therefore,itislikely that anobjectatacertainlocationin 2. Pickaquerytemplate from the availablelist and para-
 thevideo will notbepresentat that samelocationin the 3 D phrase/reword the query to obtain , e.g., template
 Q 
 scan. Insuchcases, weask the annotatortohallucinatea ‘Wherewasobject Xbefore/afterevent Y?’can bepara-
 3 Dboundingboxin the 3 Dscanatthepositionof the target phrasedas‘Wherewas the bluebucketpriortomydog
 objectdefinedin the video. exiting the livingroom?’ 
 27 

 
 
 
 
 
 
 Category Template resultofmomentarygazeshift are stillconsideredtobe
 contiguous. 
 Whereisobject Xbefore/afterevent Y? 
 Whereisobject X? 
 • Foragivenquery,ifthere are multiplenon-contiguous
 Whatdid Iputin X? 
 temporalwindows(separatedbymorethan 3 seconds)
 Howmany X’s?(quantityquestion) 
 asindependentlyvalidanswers,weinstruct the anno-
 Objects What Xdid IY? 
 tatorstoeitherdiscard the query and createadifferent
 Inwhatlocationdid Iseeobject X? 
 one,oraddmoredetailsto the wordingtomakeitmore
 What Xis Y? 
 specific. Similarly,queries that requiremultipletem-
 Stateofanobject 
 poralwindows(separatedbymorethan 3 seconds)to
 Whereismyobject X? 
 deduce the answer are alsodisallowed. Forexample,
 Place Wheredid Iput X? 
 ‘Howmanyshirtsdid Ipackinmysuitcase?”isinvalid
 Whodid Iinteract with when Ididactivity X? ifpackinghappensacrossmultipletemporalwindows,
 People Whodid Italktoinlocation X? separatedbymorethan 3 seconds(e.g.,theuserpauses
 Whendid Iinteractwithperson with role X? tomakecoffee,and the nreturnstopacking).
 Table 5. The NLQtemplatescaptureadiversesetofqueries that • We enc our age diversity by instructing that the query
 humans can asktoaugment the irmemory and recollectobjects, responsesnotbeconcentratedatonepartof the video
 places,andpeoplein the ireverydayexperience. clip,oraroundfewobjects/places/people. Inaddition,
 wealsodisallow the queryresponsewindowtobemore
 than 50%ofthetotalcliplength. 
 3. Findthetemporalwindowwhere the responseto the 
 natural language query can be deduced visually, and • Finally,queries that requirereasoning and knowledge
 annotateitasr. on top of visual evidence are invalid. For instance,
 ‘Whatcountry‘sflagwashangingon the wall?”isin-
 During our data collection,wealsorequested the annota- validwhile‘Wherewas the flag that washangingon
 torstomark the slotvalues and correspondingverbs,forthe thewall?”isvalid.Similarly,queries that guessthemo-
 selectedlanguagequerytemplates. Whilewedonotuse this 
 tivationorintentionsoftheuserorpeoplein the video
 information for ourtask, itmaybeusefulforo the rfuture clip are also not allowed. As an example, ‘Why did
 research. thepersonatthedoorleaveapackageon the porch?’
 The desiderata for the collected queries are as follows. isdisallowedwhile‘Whatdid the personleaveon the
 Theyshould: (a)reflect the underlyingmotivationofaug- porch?’ isaccepted. 
 mentinghumanmemory,(b)berich and diverseintermsof 
 language and the objects,places,people,andevents,and,(c) After the annotation process, we apply both automatic
 bechallengingenough for anintelligentsystembutnottoo and manual quality checks, including the diversity of lan-
 complicatedorconvolutedtoreduce the naturalnessof the guagequeries and temporalwindowlocations,toscore the
 queries. Forinstance,thoughaquerylike‘Whatwasplaying annotations. Iftheoverallqualityscoreisbelowathreshold,
 onthetelevisionwhen Iwasfoldingmyseventh T-shirtafter theclipisre-annotated. 
 my dog exited the room?’ is challenging from a learning 
 Momentsqueries Toannotatemomentsqueries,wesam-
 perspective,itisnotnatural from anapplicationst and point. 
 plecontiguousclipsof 8 minutes from the setofinteresting
 Inordertoensure the abovequalities for NLQ,weenforce 
 moments videos. The annotators are instructed to mark
 thefollowingconstraints: 
 instancesofactivities with atemporalwindow and the activ-
 • Allparaphrasedlanguagequeriesmust beinpasttense, ity’sname from afixedtaxonomyofactivities.Wehaveeach
 and must be posed as questions asked at the end of instancelabeledbythreeindependentannotators. Byassum-
 theentirevideoclip. Thisresembles the real-lifesce- ingeachannotatorisreliable,wetake the unionofmoments
 narioofqueryingaboutepisodicmemory(past)ofthe acrossannotatorstoensurecompletenessofannotations.
 user,andresolvesambiguitywhen the rearemultiple Thetaxonomywascreatedsemi-automatically from the
 occurrencesofanobjectto the the lastrelevantone. narrations. Specifically,we use the summarynarrationscol-
 lected for five-minuteclipsegments,astheycapturehigher-
 • Toaccount for momentaryshiftsofview for the egocen- levelevents and activities that are suitable for the moments
 tricvideo,weallowsmallinterruptions(<3 seconds) retrieval task. Thisisincontrastto the verb-nountaxonomy
 between the trulyrelevantframes for agivenquery. In thatiss our ced from individualnarrations for eachatomic
 otherwords,frameswhere the object/person/placeof action, which are used in the Forecasting and Hands and
 interestgoesoutofview for lessthan 3 secondsasa Objectsbenchmarks(see Appendices Gand J).
 28 

 
 
 
 
 
 
 Thetaxonomywascreatedasfollows. First,eachsum- Split Train Val Test 
 mary narration was encoded into a feature vector using a 
 #videohours 262(19) 87(5) 84(9) 
 pre-trained BERT[51]language model,and the nconcate- 
 #clips 3.6 k(164) 1.2 k(44) 1.1 k(69) 
 nated with thewordembeddings for the mainverb and noun #queries 13.6 k(604) 4.5 k(164) 4.4 k(264)
 extracted from the summary. Thesesummaries were then 
 clustered into groups, and then labels were manually as- 
 Table 6. Visualqueries data setstatistics. Thenumbersin the
 signed to groups based on the coherent activities they de- 
 paranthesescorrespondto the subsetof data used for 3 Dlocaliza-
 scribed. 
 tion,wherewefocusonvideos for whichwe have Matterport 3 D
 Note that thisprocesswasdoneindependently for aset scans. 
 ofscenarios that weselected base donhowfrequentlythey 
 occurin the dataset,thediversityofactivities the yrepresent, 
 Split Train Val Test 
 andhowlikely the ycontainhigh-level,event-likeactivities. 
 For example videos that primary involve a single activity #videohours 136 45 46
 like“driving”arenotinterestingcategoriesin this context, #clips 1.0 k 0.3 k 0.3 k 
 whereas“householdcleaning”containsseveraldifferentac- #queries 11.3 k 3.9 k 4.0 k
 tivities that aresh are dacrosso the rindoortasks,makingitan 
 appropriatescenario. Intotal,weselectvideos from 5 sce- Table 7.NLQ data setstatisticsacross the train/val/testsplits.
 nariostocreate our momentstaxonomy: Cooking,Cleaning, 
 Shopping,Handyman,Farmer/Gardener. Eachannotationis 
 throughout the image,withveryfewboundingboxesanno-
 inthe for matof(starttime,endtime,label). 
 tatedat the top 10%oftheimage(see Figure 22,right). Our
 analyses indicate that there may be a potential bias in the
 F.4 Data Analysis 
 firsttwomeasures,while the boundingboxespositions are
 largelyunbiased. 
 Wenowoverviewthestatisticsof the annotationsperquery 
 Forthe 3 Dlocalization task,weannotateasubsetof 1,043
 type. 
 visualqueries with 3 Dannotations. Thesecompriseof 13
 Visualqueries The VQannotationsconsistofsamples from videoh our sassociated with 4 scans from the Universityof
 a diverse set of scenarios and universities (see Figure 20 Catania(UNICT). 
 and 21). In total, 433 hours of videos are annotated with 
 Naturallanguagequeries Asoutlinedin Table 7,the NLQ
 22,602 visualqueries. Thesevideos are sampled from 10 
 annotations are from 227 hours of video, with a total of
 universities and consistof 54 scenarios. Thestatisticsover 
 19.2 K queries spanning the selected 13 query templates.
 thetrain/val/testsplits are providedin Table 6. Weensured 
 Theassociatedvideoclipscome from 10 differentuniversi-
 that the splitscontainadisjointsetofvideos. Tolook for 
 ties with atotalof 34 scenarios(withatleast 1 hourofvideo
 possiblebiasesin the data,weplot the distributionoverthree 
 annotated). Similartoother task swithin the episodicmem-
 measures. 
 ory,weensure that the train/val/testsplits(60%,20%,20%)
 1)Querytoresponseseparationis the temporaldistance 
 contain a disjoint set of video clips. We further analyze
 (in frames) between the query frame and the end of the 
 the data through: (a) Distribution over template queries,
 responsetrack. Thismeasureshowfarbackintimeanalgo- 
 shownin Figure 24. Thechallenging‘Whereisobject Xbe-
 rithmneedstosearchinordertofind the queryobject. 
 fore/afterevent Y?’isthemostpopulartemplate with around
 2)Responsetracksizemeasures the temporallengthof the 
 3 K queries,with are asonabledistributionovero the rtem-
 responsetrack. 
 plates. Overall, thequeriesin NLQhave 8.3 2.1 words
 3)Responsebboxpositionis the spatialstart and end(x,y) ± 
 inthem. (b)Distributionof the responsewindowleng this
 coordinates for eachboundingboxin the responsetrack. We 
 shownin Figure 25. Typically,thewindows are 9.3 21.5
 normalize the coordinates by the image width and height ± 
 secondslong. 
 toaccount for varyingimagesizesin the data. Eachpixel 
 Mostresponsewindowsarequiteshortcomp are dto the
 within the boundingboxcontributestoanimageheatmap 
 fullvideoclip,making the taskachallenging“needlein the
 that shows the frequency of each pixel belonging to a re- 
 haystack”searchproblem. (c)Distributionofquerywords
 sponsetrackboundingbox. 
 is shown in Figure 19. The branching off evidences the
 The analyses are shown in Figure 22. The query to re- 
 richness and diversityof the queriesin NLQ. 
 sponseseparationdistances are fairlyspreadbetween 1 to 
 200 frames with a mode of 30 frames (see Figure 22, Moments queries For MQ, similar to other tasks in
 ∼ 
 left). Theresponsetracksizes are welldistributedbetween episodicmemory, wemaintainaratioof 6:2:2 among the
 1 to 40 frames with amodeof 8 frames(see Figure 22, train/val/test splits, which contains disjoint sets of video
 ∼ 
 center). Theboundingboxes are near-uni for mlydistributed clips. Tomakesure the reareenoughsamplesineachcate-
 29 

 
 
 
 
 
 
 
 
 
 
 enoemos 
 
 htiw tcaretni / esrevnoc 
 enohp 
 esu 
 ...o 
 smeti gnihtolc hguorht esworb 
 ...ippohs 
 / enizagam / koob a daer 
 ...eti 
 doof )tuo ekat ro( yawa tup 
 rorrim 
 eht ni sehtolc ta kool 
 gnikooc 
 elihw doof xim / rits 
 ...elbategev 
 a ecils / pohc / tuc" 
 sriats 
 pu klaw / sriats nwod klaw 
 ...ni 
 hsart tup / hsart yawa worht 
 ...o 
 ro ecafrus rehto epiw / naelc 
 hguod 
 tuo-llor / epahs / daenk 
 ...awekab 
 / slisnetu / sehsid hsaw 
 smeti 
 rehto ezinagro / egnarra 
 ...oof 
 ro seirecorg hguorht esworb 
 ...eoh 
 a htiw lios eht llit ro gid 
 egareveb 
 knird 
 ...nah 
 no / tesolc ni sehtolc gnah 
 meti 
 rehto xif 
 sdnah 
 hsaw 
 lian 
 xif ot nug-lian / remmah esu 
 trac 
 gnippohs ni smeti ecalp 
 dnuorg 
 morf sdeew evomer 
 pohs 
 / tekramrepus a retne 
 moorb 
 htiw roolf peews / naelc 
 retupmoc 
 / potpal a esu 
 emag 
 drac ro emag draob yalp 
 ...reniatnoc 
 / elttob / top a llif 
 noisivelet 
 hctaw 
 ...r 
 no smeti rehto hguorht esworb 
 .../ 
 sehsid )tuo ekat ro( yawa tup 
 ... 
 htiw oediv drocer / otohp ekat 
 steehs 
 / sehtolc dlof 
 etalp 
 a otno doof evres 
 sloot 
 rehto htiw ssarg mirt / tuc 
 ... 
 roolf / doow / llaw otni llird 
 ... 
 .g.e( tnempiuqe ytefas no tup" 
 rellor 
 / hsurb tniap gnisu tniap 
 ...wob 
 a ni stneidergni xim / rits 
 sehcnarb 
 ro segdeh mirt 
 ...tni 
 seirecorg / smeti doof kcap 
 reddal 
 a nwod / pu bmilc 
 ... 
 a ta enil / eueuq eht ni dnats 
 elbategev 
 ro tiurf a leep 
 ... 
 smeti gnihtolc raew / tuo-yrt" 
 sporc 
 / stnalp / lios retaw 
 loot 
 gnisu meti rehto tuc 
 renrub 
 evots eht thgil / no-nrut 
 ...o 
 erit a ecalper / evomer / xif 
 pohs 
 / tekramrepus a tixe 
 ...nehctik 
 ro elbat a epiw / naelc 
 retnuoc 
 gnillib ta yap 
 yrd 
 ot sehtolc gnah 
 ...a 
 gnisu tneidergni / doof hgiew 
 ...eidergni 
 )tuo ekat ro( yawa tup 
 sexob 
 / sgab otni smeti rehto kcap 
 ...enihcam 
 gnihsaw a daolnu / daol 
 meti 
 doof / tiurf / elbategev hsaw 
 lamina 
 / tep htiw yalp ro tcaretni 
 esicrexe 
 emos od 
 loot 
 gnisu seceip doow tuc / pohc 
 hguod 
 tuc 
 hcnarb 
 eert tuc 
 emag 
 oediv a yalp 
 rewomnwal 
 a htiw ssarg mirt / tuc 
 ecafrus 
 / llaw retsalp 
 kcans 
 a tae 
 ... 
 srewolf / stnalp / sdees tnalp 
 gniriw 
 xif 
 naelc 
 ot renaelc muucav a esu 
 ...rg 
 no sevael yrd ekar / tcelloc 
 ...r 
 no seirossecca hguorht esworb 
 ...gnisu 
 .ge( lios / dnuorg level" 
 hguod 
 yrf 
 ...s 
 htiw stnalp / sehcnarb pu eit 
 ...s 
 / repapdnas gnisu doow htooms 
 gniyap 
 erofeb yenom tnuoc 
 ecnailppa 
 nehctik epiw / naelc 
 dnah 
 yb lios eht llit ro gid 
 ...arepo 
 / mta morf yenom wardhtiw 
 ... 
 a ro dnuorg eht otni lios kcap
 rac 
 fo enigne / tennob xif 
 steehs 
 ro sehtolc nori 
 ....e( 
 seirossecca raew / tuo-yrt"
 ...c 
 ni sehtolc ezinagro / egnarra
 ...c 
 / stiurf / selbategev tsevrah
 ... 
 epat gnisu meti nedoow erusaem
 smeti 
 gnihtolc owt erapmoc 
 ...oitcurtsnoc 
 dnuora tfihs / evom 
 sloot 
 llams egnarra / tfihs / evom
 egdirf 
 ni smeti ezinagro / egnarra
 gnibmulp 
 / epip xif 
 ... 
 draobdrac / repap / daerht tuc
 ...rcnoc 
 / tnemec ylppa ro eraperp
 ...ffoc 
 a esu / aet ro eeffoc ekam
 ...swollip 
 egnarra / deb eht ekam"
 meti 
 cillatem lio / epiw / naelc
 koob 
 / repap a ni seton etirw
 ...tnempiuqe 
 llams riaper / naelc"
 ...s 
 htiw .g.e( egakcap a nepo tuc
 elcihev 
 a evird 
 ...m 
 / nep / licnep htiw meti kram
 riahc 
 / hcuoc no swollip egnarra
 meti 
 doof rehto yrf
 ...i 
 meti doof rehto niard / esnir
 ekab 
 ot nevo eht otni doof tup
 meti 
 rehto eltnamsid
 nevo 
 eht morf doof evomer
 epav 
 / etteragic / ragic ekoms
 gnikooc 
 elihw doof etsat
 103 
 102 
 secnatsni 
 # 
 Figure 18.Distributionofmomentslabels.Thefigureshows the numberofinstancespercategoryacross 5 scenarios and 300 hoursof
 data. All 110 categories are shown,sortedbyfrequency. Thedistributionislongtailed,with the smallestclassescontainingatleast 50
 instances.Note that the seareonly the Moments for Episodic Memory with temporalwindowannotationsin the currentrelease;Ego 4 Dhas
 manyo the rscenarios and activitiesnotreflectedin this distribution. 
 Split Train Val Test Total 
 Videohours 194.9 68.5 62.9 326.4 
 #Videoclips 1,486 521 481 2,488 
 #Instances 13.6 k 4.3 k 4.3 k 22.2 k 
 Table 8.MQdatasetstatisticsacross the train/val/testsplits.
 clip. Theaveragedurationeachinstanceis 45.2 seconds. (b)
 Thedistributionofdifferentcategoriesisshownin Fig 18.
 Wenotice that thisisalong-taileddistribution,somecate-
 gories(e.g.,‘usephone’,‘converse/interact with someone’)
 withover 1000 instances and somecategories with lessthan
 100 instances. Eachcategoryhas 205 instancesonaverage.
 (c)Thedistributionofinstancenumbersinavideoclipis
 shown in Fig 27. The majority of video clips have 1-20
 momentinstances,whereasveryfew can haveasmanyas
 over 80 instances. 
 Figure 19.Distributionofquerywordsin NLQ. 
 F.5 Evaluationmeasures 
 Next we detail the evaluation metrics for all three query
 gory,weonlykeepcategories that haveatleast 50 instances 
 types. 
 from the annotations and haveinstancesinalltrain/val/test 
 splits. Visualqueries Wedefine the followinglocalizationmetrics
 Consequently,the MQdatasethas 110 categories,spans forthe 2 Dlocalization task with top-1 retrieval.
 atotal 326.4 hoursofvideos,2,488 videoclips and 22.2 kac- Temporal AP (t AP) measures how closely the temporal
 tioninstances. Wesummarizethestatisticsacross the three extent of the prediction matches with the ground-truth re-
 splits in Table 8. We further explore the data through the sponsetrack. Itiscalculatedas the average-precisionof the
 followingaspects. (a)Thedistributionofactiondurationis predictedresponsetrack’stemporalextent,andis base don
 shownin Fig 26. we cansee that mostmoments have very the Activity Netm APmetric[61]. Weevaluatethet APat 4
 short duration. The majority of moments last less than 1 differentt Io Uthresholds 0.25,0.50,0.75,0.95 ,aswellas
 { } 
 minute,and 22.4%actions have durationlessthan 3 seconds. theiraveragevalue. 
 Note that the reisalsoapeak(2.6%instances)atthelargest Spatio-temporal AP (st AP) measures how closely the
 durationbin,wheretheactionsalmostcover the wholevideo spatio-temporalextentofthepredictionmatches the ground-
 30 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 20.Distributionoverscenarios for visualqueries.The data setcontainsalong-tailofscenarios.Theplottitleindicates the number
 ofscenarios and thetotalvideoh our sincludedin the dataset. 
 
 rithmsearching for the queryobject. Itiscalculatedas
 
 n 
 s Eff=1 (5) 
 − N 
 where n is the number of video frames previewed by an
 algorithmtopredict the responsetrack,and N isthetotal
 numberofframesinthevideobefore the querywasmade
 (i.e., the search window). An algorithm that accesses ev-
 eryframeinthesearchwindowbe for elocalizing the query
 objectgets 0.0 searchefficiency. This“timeliness”metric
 isdesignedtoenc our ageresearchonmethodsper for ming
 intelligentcontextual-search. 
 Weevaluateper for manceon the 3 DVQlocalization task
 using the rootmeansqu are error(RMSE)and the angular
 Figure 21.Distributionoveruniversities for visualqueries.The errormetrics: 
 datasetcontainsannotationscorrespondingtovideos from 10 uni- 
 versities.Theplottitleindicates the numberofuniversities and the RMSE= t s tˆ s 2 (6)
 (cid:107) − (cid:107) 
 totalvideoh our sincludedin the dataset. v T 
 vˆ 
 Q Q 
 angular error=acos( . ) (7) 
 v vˆ 
 Q 2 Q 2 
 (cid:107) (cid:107) (cid:107) (cid:107)
 where t and tˆ are the ground-truth and predicted object
 truthresponsetrack. Itiscalculatedas the average-precision s s 
 positioninthes can coordinatesystem. v andvˆ arethe
 ofthepredictedspatial-tube,andis base don the video-AP Q Q 
 ground-truth and predicted 3 Ddisplacementvectorin the
 metric from[88]. Weevaluatethest APat 4 differentst Io U 
 queryframe Qcoordinatesystem. Wealsodefineasuccess
 thresholds 0.25,0.50,0.75,0.95 ,aswellas the iraverage 
 { } metricleveraging the twoannotationsperquery:
 value. 
 Success (Succ) measures whether the prediction has any 
 succ= c tˆ <6 ( c c +δ) (8) 
 overlap with the groundtru that all. Itiscalculatedas the (cid:107) m − s (cid:107) 2 × (cid:107) 1 − 2 (cid:107) 2
 percentageofsampleswhere the predictedresponsetrack 
 With c 1 and c 2 the centroids of the two bounding box
 hasatleast 0.05 spatio-temporal Io Uwith the groundtruth. 
 annotations, c the mid-centroid between c 1 and c 2 and
 Recovery% (rec%) measures how much of the ground- m 
 δ =exp−mdiag,withm 
 diag 
 theaveragediagonallengthof
 truthresponsetrackisaccuratelyrecoveredby the prediction. 
 thetwoboxes. 
 Itiscalculatedas the%offramesin the responsetrackwhere 
 the predicted bounding box has at least 0.5 Io U with the Naturallanguagequeries Evaluation for NLQissimilar
 groundtruth. Thisismotivatedby the trackingrobustness toexistingvideo-languagegroundingproblems. Following
 metric from the VOTchallenge[121]. priorwork[236],we userecall@k,Io U=m,whereweselect
 Searchefficiency(s Eff)measurestheefficiencyof the algo- k = 1,5 and m = 0.3,0.5 . This metric computes
 { } { } 
 31 

 
 
 
 
 
 
 Distribution of query to response Distribution of response track lengths Distribution of response bboxpositions
 separation distances 
 
 
 
 
 
 
 
 
 Figure 22.Visualqueriesbiasanalysis.Weanalyze the full VQdataset for potentialbiases.Left:Theplotshows the distributionofquery
 toresponseseparationdistancesin the VQdataset.Whilethemodeof the distributionis∼30 frames,we cansee that separationdistances
 arefairlyspreadbetween 1 to 200 frames.Center:Theplotshows the distributionofresponsetracksizesin the VQdataset.While the
 modeof the distributionis∼8 frames,we cansee that the responsetracksizes are welldistributedbetween 1 to 40 frames.Right:The
 heatmapshows the normalizedfrequencyofeachpixelbelongingto are sponsetrackboundingbox.Theboundingboxesnear-uni for mly
 distributedacrossmostof the image. 
 
 
 
 
 
 
 gnikoo C yrdnual / gninael C snoc ot detaler sboj cinahcem ra C retnepra C reka B cinahcem retooc S w( noitagiva N roodn I dni gnippohs yrecor G ppohs rehto ,sehtol C teerts no gnikla W ylimaf htiw gnikla T eeffoc gnika M gnita E cinahcem eki B ni gnihtemos gnixi F c/pu-kcap/putes pma C step htiw gniyal P namydna H semag draob gniyal P emoh ta tuo gnikro W ep / god eht gnikla W htimskcal B s aetklim ni gnikro W es/gnittink/gnitfar C lcni( laicos roodtu O eki B aor ,gnitummoc -
 
 
 
 ra C 
 lacisum a gnicitcar P ksed ta gnikro W siybboh( scinortcel E eugaelloc ot gnikla T ediv / semag gniyal P l/enohp( neercs a
 n O 
 e - myg eht ot gnio G eneigyh ylia D gniggoj / gnilcy C su B ti gnikam( ba L reka M sdneirf htiw gnikla T krap eht ot gnio G cisum ot gninetsi L gninedra G
 80 
 60 
 40 
 20 
 0 
 Scenarios 
 sruoh 
 oediv 
 # 
 Total: scenarios=43, hours=358.93 
 Figure 23. Distributionoverscenarios for the NLQannotations,indicatingalongtailoverscenarios. Note that the scenariolabels are
 approximate and asinglevideo can containmultiplescenariolabels.For this plot,weequallydividethetimeacrossall the labelledscenarios.
 thepercentageoftimesatleastoneof the topk predicted temporalactiondetection data sets,suchas Activity Net[61],
 candidates have anintersection-over-union(Io U)ofatleast themean AP(m AP)overallcategoriesiscomputedgivena
 m. Note that weleantowardslowerthresholdvalues(m)as t Io Uthreshold. Multiplet Io Uthresholds are adopted,and
 theaveragelengthof the window( 10 s)ismuchsmaller theaveragem APoverall the set Io Uthresholdsiscomputed.
 ∼ 
 thanthatof the videoclip(500 s),about 2%ofthecliplength. Formomentqueries,weevaluatem APat 5 differentt Io U
 thresholds 0.1,0.2,0.3,0.4,0.5 ,aswellas the iraverage
 { } 
 Moments queries Considering that the moment queries value. 
 taskisrelatedto the tasksoftemporalactiondetection[61, Recall@kx, t Io U=m, is a metric adapted from the metric
 141,229,237] and video grounding [236], we adapt their recall@k, t Io U=m, used for NLQ. The metric recall@k,
 respectivemetricstomomentqueries. t Io U=mmeasuresthepercentageof the querysentences that
 Average Precision(AP)isacommonlyadoptedmetricin haveatleastonepredictionwi that Io Ulargerthan the thresh-
 temporalactiondetection. Itmeasureshowclosely the tem- oldmin the top-kresults. Inourmomentqueriescase,since
 poralextentofthepredictionsmatches the ground-truthac- wemight have morethanoneinstancecorrespondingtoa
 tioninstances for eachactioncategory[61,141,229,237]in querymomentcategory,weneedtomeasure the percentage
 termsofbothprecision and recall. Thetemporalintersection ofall the correctlypredictedinstances that haveatleastone
 overunion(t Io U)betweenaprediction and aground-truth predictionwi that Io Ulargerthanthethresholdmin the top-
 actioninstanceisusedtomeasure the irdistance. Ifthet Io U k resultsof this instance. Considering that predictions are
 is higher than a threshold, the prediction is considered as usuallymade base donacategorynotaspecificinstance,we
 true positive; otherwise, false positive. In representative modifythemetrictobe the followingrecall@kx,t Io U=m,
 32 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 X tcejbo si ereh W ?Y tneve retfa/erofeb ?X tup I did ereh W ?X tcejbo si ereh W ?X ni tup I did tah W ?s X ynam wo H )noitseuq ytitnauq( ?Y I did X tah W I did noitacol tahw
 
 
 n I 
 ? X tcejbo ees ?Y si X tah W tcejbo na fo etat S tcaretni I did oh W did I nehw htiw ?X ytivitca ?X tcejbo ym si ereh W ni ot klat I did oh W ?X noitacol ro ot klat I did neh W nosrep htiw tcaretni ?X elor htiw
 3000 
 2500 
 2000 
 1500 
 1000 
 500 
 0 
 Templates 
 sruoh 
 oediv 
 # 
 Figure 24.Distributionofqueriesover the correspondingtemplates 
 acrossobjects,place,andpeoplecategories(Tab.5). Seetext for 
 moredetails. 
 2000 
 1500 
 1000 
 500 
 0 
 0.0 10.0 20.0 30.0 40.0 50.0 60.0+ 
 window length (seconds) 
 seireuq 
 # 
 Figure 25.Distributionofresponsewindowlength for NLQ.For 
 thesakeofbrevity,we use the lastbintorepresentallwindows 
 longerthanaminute.Seetext for moredetails. 
 6000 
 5000 
 4000 
 3000 
 2000 
 1000 
 0 
 0 100 200 300 400 500 
 Action duration (seconds) 
 secnatsni 
 # 
 250 
 200 
 150 
 100 
 50 
 0 
 0 10 20 30 40 50 60 70 80 
 # instances in one video clip
 Figure 26.Distributionofmomentduration. 
 wherexst and sfor the numberofinstances for aquerycat- 
 egory in one video. This metric measures the percentage 
 of all the correctly predicted instances that have at least 
 one prediction with a t Io U larger than the threshold m in 
 the top-kx results of the action category. This metric has 
 a similar idea to the multi-label metric proposed in [240] 
 spilc 
 oediv 
 # 
 Figure 27.Distributionofinstancenumbersinonevideoclip. whendealing with multipleinstances for aquery. we use k = 1,2,3 andm = 0.3,0.5,0.7 inthemetric. Compared
 toaverageprecision,thismetriconlyevaluates the recall for
 the query categories, and does not penalize for false posi-
 tivepredictionsgivenacategory that hasnoinstancesin the
 video. 
 F.6 Baselines 
 Wedeveloped base linemodels for each task. Wedesigned
 these model stoaddress our tasks,usingstate-of-the-artcom-
 ponentswhererelevant. Theyrepresentastartingpointupon
 whichfuturework can build. 
 Visualqueries 2 Dlocalization base line 
 Wetreatvisualqueries with 2 Dlocalization(VQ 2 D) asa
 detection + tracking problem (see Figure 28). At a high
 level,ourapproachconsistsofthreesteps. First,weperform
 frame-leveldetectionover the inputvideowherewedetect
 the presence of the query object in each frame using an
 object detection model (Figure 28 top). For each frame,
 wegettheboundingbox that ismostsimilarto the visual
 crop and ascoreindicatingitsvisualsimilarity. Second,we
 consider the sequenceofper-framesimilarityscoresover the
 entirevideo and identifythemostrecentpeakin the sescores
 (Figure 28 bottom-left). Finally,weinitializeatrackerat the
 video-framecorrespondingto the peakdetection,andtrack
 thequeryobjectonboth for ward and backwarddirections
 torecover the completeresponsetrack(Figure 28 bottom-
 right). 
 Step 1: Frame-leveldetection Wepropose Siam-RCNN,
 a Faster-RCNN [189] based approach to detect the query
 object in a given image. See Figure 28 top. Given
 a video frame at time t, a pre-trained Region Proposal
 Network (RPN) [189] with a Feature Pyramid Network
 (FPN) [142] backbone is used to generate bounding box
 proposals b , ,b . The Ro I-Align operation [94] is
 1 N 
 { ··· } 
 thenusedtoextractvisualfeatures for eachboundingbox
 (b ), , (b ) . we use the same FPNbackboneto
 1 N 
 {F ··· F } 
 extractfeatures for the visualcropv. Todetect the presence
 ofthequeryobjectinframet,eachproposalfeature (b )is
 i 
 F 
 33 

 
 
 
 
 
 
 Step 1: Frame-level detection with Siam-RCNN 
 BBo Bx Bporxopporsoaplossals 
 { b 1, {· b ·1· ,b · N ··} b N } BBoxfeatures 
 RPN Ro IAlign (b 1), , (b N) 
 {F ··· F } 
 Backbone( ) 
 F 
 
 Backbone( ) (v) 
 F F 
 
 
 Inputvideo Visual crop (v) 
 
 Step 2: Temporal detection 
 
 
 
 )s( 
 erocs 
 
 ytiralimi S 
 Prediction @ frame t 
 Top -1 
 retrieval 
 (bt,st) 
 Per-frame predictions 
 [(b ,s ), ,(b ,s )] 
 1 1 Q-1 Q-1 
 ··· 
 Step 3: Tracking 
 Signal peaks Initialize tracker with 
 Tracker 
 Query Forward tracking 
 frame 
 Response track prediction 
 Nearest peak 
 detection 
 Initialize tracker with 
 Tracker 
 Q Backward tracking 
 Video time (t) 
 Figure 28.Visualqueries 2 Dlocalization base line.Ourapproachconsistsofthreesteps.Step 1:Weper for mframe-leveldetection for the
 entireinputvideotodetectthepresenceof the queryobject(specifiedvia the visualcropv).Foreachframet,weextract the regionproposals
 {b 
 1 
 ,··· ,b 
 N 
 }using are gionproposalnetwork(RPN),andextractfeatures for eachproposal{F(b
 1 
 ),··· ,F(b 
 N 
 )}.Eachproposalfeature
 iscomp are dwith the visualcropfeature F(v)usinga Siamesehead S,and the mostsimilarproposalb tisretrievedalong with itsscore
 s t. Thisprocessisrepeated for allframes. Step 2: Wetreat the similarityscoress={s 1 ,··· ,s q−1 }asatemporalsignal and perform
 temporaldetectiontoobtain the‘mostrecentoccurrence’ofthequeryobject.Wedetect the peaks(localmaxima)inthesignal and recover
 thepeakpne are stto the queryframe.Step 3:Given the detectedpeakp and itscorrespondingproposalb p,weinitializetwotrackers with
 b pandrunthemalongthe for ward and backwarddirectionstorecoveracontiguoustrackof the object,i.e.,theresponsetrackprediction.
 comp are dwith the visualcropfeature (v)usinga Siamese b with the highest similarity score s for frame t can be
 t t 
 F 
 head thatpredictsa 0-1 similarityscore obtainedasfollows: 
 S 
 s i = S ( F (b i ), F (v)) (9) b t = argmax { s 1 , ··· ,s N } (12)
 b∈{b 1,···,b N} 
 The Siamesenetworkprojectseachproposal/visual-crop 
 s =max s , ,s (13) 
 t 1 N 
 feature to a 1024-D feature vector using a convolutional { ··· } 
 projectionmodule , After repeating the above steps for all the video
 P 
 frames, we can obtain the final per-frame predictions as
 p = ( (b )); p = ( (v)) (10) 
 b i v 
 P F P F [(b ,s ), ,(b ,s )]. 
 1 1 q−1 q−1 
 ··· 
 andpredictsa 0-1 similarityscoreusingabilinearopera- 
 Step 2: Temporaldetection Sofar,we used Siam-RCNN
 tion: 
 togetthemostsimilarproposals and the irsimilarityscores
 s =σ(p TWp +b) (11) 
 i b v foreveryframein the video. Next,thegoalistotemporally
 where σ is a sigmoid non-linearity. After computing the detect the‘mostrecentoccurrence‘oftheobjectin the video
 similarities to each bounding box proposal, the proposal (see Figure 28 bottom-left). Thisisachallengingproblem
 34 

 
 
 
 
 
 
 since our goal is not to identify the best detection of the Wenextdetail the trainingprocedure for the Siam Head
 object,butinstead the mostrecentone,evenif the similarity ( ). we useasimilarityretrievalapproach were the model
 S 
 isnotashigh. Totackle this problem,wetreat the per-frame is trained to predict high visual similarity between the vi-
 similarityscoress = s , ,s asatemporalsignal, sual crop v and positives, and low visual similarity be-
 1 q−1 
 { ··· } 
 and use a signal peak detection approach to identify the tween v and negatives. The loss function for is a bi-
 S 
 salientpeaks(a.k.a. localmaxima)ins. Toavoidspurious nary cross entropy loss defined over each (v,D ,D ) tu-
 p n 
 peaks,wefirstsmoothsusingamedianfilter with awindow 
 ple (see Eqn. 16), where D = p 
 |Dp| 
 are positive 
 sizeof 5. 
 detections, D = n 
 |Dn| 
 a 
 p 
 re neg 
 { 
 ati 
 i 
 v 
 } 
 e 
 i= 
 d 
 1 
 etections, and 
 n { j }j=1 
 s = ( (x), (v)): 
 s¯=median filter(s) (14) x,v S F F 
 p , ,p =find peaks(s¯) (15) 
 1 k 
 ··· (cid:18) (cid:19) 
 1 (cid:88) (cid:88) 
 = log(s )+ log(1 s ) 
 Dependingon the video,thealgorithmmayreturnmultiple L S − D D p,v − n,v 
 p n 
 peaksspreadthroughout the video(seesignalpeaksin Fig- | ∪ | p∈Dp n∈Dn 
 (16) 
 ure 28 bottom-right). Since our goalistodetect the most 
 Bothpositives and negatives are defined base donpropos-
 recentoccurrenceof the object,weselect the peakpthatis 
 alsgeneratedby the RPN.Givenavisualcropv,aproposal
 temporallyne are stto the queryframe. 
 p fori (s,e)isapositiveif the Io U(p ,r ) 0.5,where
 i i i 
 Step 3: Tracking Aftertemporaldetection,wehaveidenti- r is the ∈ response track box in frame i. We r ≥ emove all r
 i i 
 fiedapeak-framepin the videowhichisestimatedto have which are toosmall, orhavesignifi can tly differentaspect
 the most recent occurrence of the object. For this frame ratios from thelargestboxinrsince the setypicallycorre-
 p,we canobtain the highest-scoringboundingboxb p from spondtoobstructedviewsof the object. Aproposalp j isa
 theper-framedetectionsinstep 1. Note that thisonlyrep- negativeifitsatisfiesanyof the followingtwoconditions:
 resentsoneframewhere the objectmostrecentlyoccurred. 
 However,the task objectiveistoobtain the responsetrack, 1. j (s,e)and Io U(p j ,r j )<0.5
 ∈ 
 i.e.,thecontiguoussetofallframes,starting from when the 
 2. p issampled from ano the rvideo. 
 j 
 objectfirstentered the field-of-viewuntil the objectexits the 
 field-of-view. See Figure 28 bottom-right. Tocompute the We also found it beneficial to use hard-negative mining,
 restof the responsetrack,we useb asastartingpoint,and whereweinitiallysamplealargenumberofnegatives and
 p 
 runasingle-objecttracker for ward and backwarduntil the thenselect the top-Knegatives with the highestlossvalue.
 trackingfails(i.e.,theobjectexits the field-of-view). 
 Forbothdirections,weinitialize the apperancemodelof We employ a few different augmentation strategies to
 thetrackerusing the proposalb . For the forwardtracking, artificiallyexp and the dataset. First,weaugmenteach data
 p 
 werun the trackerstarting from framep+1 toq 1 andobtain samplebyreplacing the visualcropvbyaboundingboxr
 i 
 thetrackedregions: b =[¯b , ,¯b ]. For − thebackward from the responsetrack. Thisworksbecause the response
 f p+1 e 
 ··· 
 tracking, we run the tracking starting from frame p 1 track and the visual crop correspond to the same object.
 to 0 and obtain the tracked regions: b = [¯b , ,¯b − ]. Next, we augment the visual crop v by applying random
 b s p−1 
 ··· 
 Wethenconcatenateb ,b ,andb toobtain the complete rotations between 120◦ to 120◦. This exploits the fact
 b p f 
 − 
 response track prediction. We use the KYS tracker [22], thatobjectscan have signifi can tviewpointvariationsinego-
 whichwasshowntoachievestate-of-the-artresults for single- centricvideos(unlikeinternetphotos). Finally,weapplya
 objecttracking. randombrightnessaugmentationto the videoframes and the
 visualcroptosimulatedifferinglighting. 
 VQ 2 Dbaselinetrainingsetup Wenowdiscuss the train- 
 ingprocedure for the VQ 2 Dbaseline. Each data point for the Implementation details We train the Siam Head using
 S 
 VQ 2 Dtask(definedon Ego 4 Dvideos)consistsof the fol- the Detectron 2 library[225]. we use the defaultconfigura-
 lowing: video V,visualcropimagev,queryframenumber tionfile and make the followingchanges for ourexperiments.
 q,andresponsetrackboxesr = r ,r , ,r ,where Foreachexperiment,we use 8 GPUs,64 visualcropsper
 s s+1 e 
 { ··· } 
 sande are the start and endframesofr,andr isabounding batch,andtrain for 300,000 iterations with aninitiallearn-
 i 
 boxdefinedonframeiofvideo V. ing rate of 0.02 followed by a 0.1 decay after 200,000
 × 
 As a high-level overview, we initialize and freeze the iterations. Weextractbackbonefeatures from the“p 3”layer
 backbone and RPNusingweightsfroman MS-COCOpre- of FPN. Based on validation per for mance, we use 6 pos-
 F 
 trained Mask-RCNN model. we usethe VQ 2 Dannotations itives and 64 negatives for each visual crop. Specifically,
 to train the Siam Head ( ). We initialize and freeze the we sample 58 negatives per video frame which results in
 S 
 KYS tracker using weights pre-trained on GOT-10 k [99], 58 64=3712 negativesperbatch. Foreachvisualcrop,
 × 
 La SOT[62],and Tracking Net[162]datasets. wesample the 64 hardestnegativesoutof 3712.
 35 

 
 
 
 
 
 
 Inthe Siam Head architecture,theprojectionmodule itremainsrelativelystable for the secondstrategywherewe
 S P 
 consistsoff our residualblocksfollowedbyaveragepooling, previewafractionofframesclosestto the query. Forexam-
 anda 2-layermulti-layerperceptron(MLP)withahidden ple,we canachieveasearchefficiencyof 48.0%withonlya
 sizeof 1024-Dand Re LUactivation. 6 16%relativedropinper for mancewithk =50%inthe
 − 
 For signal peak detection, we utilize the find peaks 2 ndstrategy. However,theper for mancedropssignifi can tly
 function from the scipylibrary 12 with the followinghyper- ifwereducekfurther. Forexample,weobserve are duction
 parametersselectedthroughvalidation: distance=25,width of 38 60%fork =10%with the 2 ndstrategy. Thissug-
 − 
 =3,andprominence=0.2. geststhatmoreintelligentmethods that per for mcontextual
 search are neededtoimprove the searchefficiency for VQ 2 D
 Experimentalresults Weevaluate the per for manceofmul- 
 whilemaintaininggoodper for mance. 
 tiple base lineson the VQ 2 Dtaskin Tab.9. Thefirstcolumn 
 inthetableshows the detection and trackingmethods,and 
 thesecondcolumnshows the Siam Headprojectionarchitec- Visualqueries 3 Dlocalization base line
 ture . Inadditionto the KYStracker,wealsoexperiment 
 P Nextwedescribethe base line for the visualquery with 3 D
 withasimpleparticlefiltertracker(denoted‘PF’)toassess 
 localization task. Recall the taskdefinition: givenavideo,a
 theimpactof the trackingquality. Asanablationof Siam R- 
 queryframe,andavisualcropofatargetobject,thegoalis
 CNN, we replace the 4 residual blocks in the Siam Head 
 tooutputa 3 Ddisplacementvector from the cameracenter
 projection module with a simple 3-layer CNN which has 
 ofthequeryframetothecenterof the targetobjectin 3 D.
 lowercapacity with noresidualconnections(indicatedby 
 The 3 D position of the target object is defined at its most
 ‘Simple’). 
 recentappearancein the video. Figure 31 showsasampleof
 We make several observations. When we use a simple 
 thetask. 
 projection model with a particle filter tracker, we already 
 Our base linestrategyhasthreesteps. Wefirstestimate
 observe a good validation per for mance of 32.4% success, 
 thecameraposesof the video. Then were trieve the most
 and 0.14 t AP . These can beattributedtousingastrong 
 25 
 recentinstanceofthetargetobjectin the video. Lastly,we
 proposal generator (RPN pre-trained on MS-COCO) and 
 estimatethedepthof the detectedobject and retrieveits 3 D
 alearnedsiamesecomparison model. Uponreplacing the 
 position from the queryframe. 
 particlefiltertrackerwitha So TAKYStracker[22],while 
 thevalidationsuccessrateremainssimilarat 33.0%,weob- Cameraposeestimation Thecameraposes are estimated
 serve significant gains (absolute) in all other metrics: 2% usingakeypointmatchingstrategyalongwitha Perspective-
 t AP, 2% st AP 25 , and 14.3% recovery. This suggests that n-Point(Pn P)resolutionapproach. Atahighlevelourap-
 a good tracker is necessary to accurately capture the full proachconsistsof the followingf our steps. Firstweestimate
 responsetrackafterlocalizingasingleframe with init. Fi- thecameraintrinsicparametersusing Structure-from-Motion
 nally,uponreplacing the‘Simple’siameseprojection with (Sf M).Secondly,weextract and matchkeypoints from each
 4 residualblocks,weobserveasignifi can tgainsof 6.8%in framein the videotokeypointsextracted from the Matter-
 success, 5% in t AP 25 , 4% in st AP 25 , and 5% in recovery port 3 Dpanoramas. Then,using the matchedkeypointswe
 %. Thissuggests that usingahighercapacity model for the setup and solvea Pn Pproblem for eachframein the video
 Siam Headishelpful for improving the per-framedetection toestimate the correspondingcamerapose. Lastly,werefine
 per for mance for the VQ 2 Dtask. Weobservesimilartrends theposesusingtemporalconstraints.
 onthetestset. Pleasesee Fig.29 forqualitativeexamplesof Step 1: Camera intrinsics estimation We start by ex-
 themodel’spredictions. tractingasetofcontiguousnon-blurryframes from the video.
 Inallcases from Tab.9,thesearchefficiencyis 0%since Inordertoselectnon-blurryframeswecompute the variance
 thedetectors are usedoneveryframein the searchwindow. ofthe Laplacianoneachimage and select the oneswitha
 In Fig.30 weexperiment with twosimpletechniquesforim- valuehigherthana 100 threshold. Wethenselect the largest
 proving the searchefficiency. Thefirstapproachuni for mly contiguous set of non-blurry images. We cap the number
 subsamplesk%oftheframesin the searchwindow(denoted ofselectedframesto 10 tolimit the computationaltimeof
 as‘SS’).Thesecondapproachsearchesoveronlyk%ofthe the Sf M module. Once we have selected the images we
 mostrecentframesin the searchwindow,i.e.,frames that are run the automaticreconstructionmoduleof COLMAP[196]
 nearestto the query(denotedas‘N’).Weconsider 3 values to estimate the camera instrinsic parameters with a radial
 ofkinbothcases:10%,25%,and 50%.Consider the results fisheyecamera model. 
 in Fig.30. Inbothstrategies,thesearchefficiencyimproves Step 2: Keypointextraction and matching we use Su-
 aswereducek. Theper for mancedropsdrastically for the per Glue [195] to extract and match keypoints. We first
 1 ststrategywherewesubsample the searchwindow,while extract keypoints from the scan panoramas k ,p
 {p,n} 
 { ∈ 
 12 Peak detection: https://docs.scipy.org/doc/scipy/ P ,n ∈ N} where P is the number of panoramas and N
 reference/generated/scipy.signal.find_peaks.html is the number of keypoints. The scan panoramas are gen-
 36 

 
 
 
 
 
 
 validation set test set 
 Detector+Tracker Succ t AP t AP 25 st AP st AP 25 rec% Succ t AP t AP 25 st AP st AP 25 rec%
 P 
 Siam-RCNN+PF Simple 32.4 0.06 0.14 0.02 0.06 13.2 32.7 0.06 0.14 0.02 0.06 12.9
 Siam-RCNN+KYS Simple 33.0 0.08 0.15 0.03 0.08 27.2 33.4 0.09 0.16 0.03 0.08 26.9
 Siam-RCNN+KYS Residual 39.8 0.12 0.20 0.04 0.12 32.2 41.6 0.12 0.21 0.05 0.13 34.0
 Table 9.Visualqueries 2 Dlocalizationresults.Wecomp are the per for manceofvarious base lineson the VQ 2 Dvalidation and test data sets.
 Column 1 indicates the detector and tracker.Column 2 indicates the projectionarchitectureusedincaseof the Siam-RCNN model.
 
 Predicted response track Query: When did I last see
 this object? 
 . . . 
 . . . 
 
 
 Predicted response track Query: When did I last see
 this object? 
 . . . 
 . . . 
 
 
 Figure 29. Qualitativeexamples for visualqueries 2 Dlocalization.Oneachrow,weshowthevisualcropofthequeryobjecton the right
 andthepredictedresponsetrackin the center(3 uni for mlysamplesimages). The model wasabletocorrectlylocalize the mostrecent
 occurrenceoftheobject and accuratelytrackitthroughout the occurrence. 
 
 
 (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) 
 (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:49)(cid:12)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12)
 (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) 
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) 
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) 
 
 
 (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)
 (cid:8)(cid:3)(cid:86)(cid:86)(cid:72)(cid:70)(cid:70)(cid:88)(cid:54) 
 (cid:24)(cid:19) 
 (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49)
 (cid:23)(cid:19) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) 
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:49)(cid:12)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:22)(cid:19)
 (cid:21)(cid:19) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) 
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12)
 (cid:20)(cid:19) 
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12)
 (cid:19) 
 (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19)
 (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)
 (cid:24)(cid:21)(cid:17)(cid:19)(cid:3)(cid:35)(cid:3)(cid:51)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:82)(cid:83)(cid:80)(cid:72)(cid:55)
 (cid:19)(cid:17)(cid:21)(cid:24) 
 (cid:19)(cid:17)(cid:21)(cid:19) (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12)
 (cid:49)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:19)(cid:17)(cid:20)(cid:24) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12)
 (cid:19)(cid:17)(cid:20)(cid:19) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12)
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12)
 (cid:19)(cid:17)(cid:19)(cid:24) 
 (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12)
 (cid:19)(cid:17)(cid:19)(cid:19) 
 (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19)
 (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)
 (cid:24)(cid:21)(cid:17)(cid:19)(cid:3)(cid:35)(cid:3)(cid:51)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:82)(cid:83)(cid:80)(cid:72)(cid:55)(cid:82)(cid:76)(cid:87)(cid:68)(cid:83)(cid:54)
 (cid:19)(cid:17)(cid:20)(cid:25) 
 View from the response track
 (cid:19)(cid:17)(cid:20)(cid:21) 
 (cid:19)(cid:17)(cid:19)(cid:27) 
 (cid:19)(cid:17)(cid:19)(cid:23) 
 (cid:19)(cid:17)(cid:19)(cid:19) 
 (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19)
 Figure 30. Searchefficiency for visualqueries 2 Dlocalization. 
 Weevaluatesimpletechniques for improving the searchefficiency, 
 andplot the corresponding VQ 2 Dper for mance.Theblue data point 
 isthe Siam RCNNper for mancewhenwepreview the entiresearch 
 window.Thered data points are the Siam RCNNper for mancewhen View from the query frame
 wesearchoverk%oftheframesuni for mlysubsampled(SS)from 
 thesearchwindow. Theyellow data points are the Siam RCNN Figure 31.Visualqueries 3 Dlocalization task demo.Thetopview
 per for mancewhenwesearchoverk%oftheframesnearest(N)to istheview from thelastframeoftheresponsetrack with the target
 thequery(withoutanysubsampling).Thevalueofkisindicated objectannotatedwitha 2 Dredboundingbox. Thebottomview
 aboveeach data point. istheview from the queryframe. Thetargetobjectisannotated
 with a 3 D red bounding box at the top right of the figure. The
 figureshows the ground-truth(green)and the predicted(red)3 D
 erated using the Matterport SDK.13 We render RGB and displacementvectors. 
 depth images at each scan position and sweep over pitch 
 13 Matterport-SDK: https://matterport.github.io/ values 
 ∈ 
 [ 
 − 
 30,30]withastepsizeof 5 deg. andyawval-
 showcase-sdk/sdk_intersection_inspector.html ues [ 180,180]withastepsizeof 15 deg. Wegenerate
 ∈ − 
 37 

 
 
 
 
 
 
 on average 7 K images per scan. Note that while we are Video frame View from the scan Superposition
 not releasing the panoramas because of data anonymiza- 
 tionconcerns,weareproviding the precomputedkeypoints. 
 Similarily, we extract keypoints from the video frames 
 k ,i ,m where is the number of im- 
 {i,m} 
 { ∈ I ∈ M} I 
 agesin the video and isthenumberofkeypoints. Once 
 M 
 the keypoints are extracted we loop through each frame 
 i in the video and match the extracted frame key- 
 ∈ I 
 points k ,m to all the panoramas keypoints 
 {i,m} 
 { ∈ M} 
 k ,p ,n . We use the pretained models 
 {p,n} 
 { ∈ P ∈ N} 
 available 14 of Super Point[50]forkeypoints and descriptors 
 extraction and Super Glue[195]formatching. 
 Step 3:Pn Presolution Wecompute the camerapose for 
 thevideoframeshavingatleast 20 matchedkeypoints. We 
 empiricallyfind that athresholdof 20 providesagoodtrade- Figure 32. Samplesofcameraposeestimation. Leftshows the
 off between the number of overall pose estimates and the frame from the egocentricvideo,middlehas the viewrendered from
 qualityof the estimations. Thepositionsof the 3 Dkeypoints theestimatedviewpointinthes can andrightis the superpositionof
 both.Weobserve that even with bigscenedifferencesbetween the
 arecomputed from apinholecameramodelof the Matterport 
 video and thes can(e.g.,thewheelin the secondrow),thealgorithm
 camerausing the renderedpanoramadepth,cameraintrin- 
 isabletoaccuratelyretrieve the camerapose. 
 sics,andcamerapose. Thepositionsof the 2 Dkeypoints are 
 directlyextracted from the videoframespixels. Wethenuse 
 the Open CVlibrarytosolve the Pn Psetup and estimate the 
 Theremainingunlocalizedframes are duetoabruptmo-
 camerapose from the matchedpairsof 3 Dand 2 Dpoints and 
 tion (lost track) and when the view is too close-up to the
 using the estimatedcameraintrinsicparameters. Using this 
 scene(notenoughkeypointsmatched). 
 methodwe canestimate the cameraposeofroughly 2%of 
 Targetobjectretrieval Webuild our solutionontopof the
 thetotalnumberofframesin the video. Nextweincorporate 
 visualqueries 2 Dlocalization base line. The 2 Dlocalization
 temporalconstraintstoincrease this number. 
 Step 4: Temporal constraints and final pose estima- baselineoutputs are sponsetrack with 2 Ddetectionsof the
 tion Toincreasethenumberofestimates were fine the pose target object. Our baseline combines these 2 D detections
 along with dep the stimation and cameraposeestimationto
 estimationpipelinebyincorporatingtemporalconstraintsin 
 retrieve the 3 Dpositionof the object. 
 aniterativeprocedure. Westartbyextracting and matching 
 2 Dkeypoints from localizedframestonon-localizedonesin Depth estimation We estimate the depth of the most re-
 thevideo. Thisstepissimilarto the above Step 2;we use centframeof the responsetrack for whichwe have apose
 thesame Super Glue[195]. Using the matchedkeypoints and estimate. We use the DPT network [185] with pretrained
 current estimated poses we triangulate new 3 D keypoints weightson NYU v 2[202]. Figure 33 showsdep the stima-
 for the non-localized images. We then solve a new Pn P tionresultswhereleftistheframe from the video,middle
 setup with the newkeypoints. Weapply this procedureit- istheestimateddepth,andrightis the depth from thes can
 erativelyuntilconvergence. Afterrefinementweachievea rendered at the estimated viewpoint (not available to the
 per for manceof 15%ofposeestimatesof the totalnumber baseline model). Note that duetoscenedifferencesbetween
 offramesaccrossallvideoclips. the video and the scan, the two depths frames will differ
 Camera pose estimation quality and sources of error in some region of the image. We then compute the depth
 valueofthetargetcentroidas the medianofasqu are region
 Wequalitativelyevaluate the cameraposeestimationpipeline 
 centeredat the 2 Ddetection. 
 byrendering the viewsin the 3 Dscans. Recall that the scans 
 andvideos have beenrecordedatdifferenttimes and thus 3 D displacement vector reconstruction Given the esti-
 thescenes can containlargedifferences. Figure 32 shows mated depth d of the object centroid c in frame f of the
 camera poses estimates where left is the frame from the response track and the estimated camera instrisics K, we
 video, middle is the view from the scan, and right is the construct the 3 Dvectordisplacementvˆ inthecurrentframe
 f 
 superposition. Wesee that even with largescenedifferences f coordinatesystemusingapinholecamera model:
 betweenthes can andvideo(e.g.,thewheelin the middle 
 example)thealgorithmiscapableofproducinggoodpose 
     
 estimates. x u 
 14 Super Glue weights: https://github.com/magicleap/ 
 vˆ 
 f 
 =y=d K−1 c=d K−1 v (17) 
 z 1 
 Super Glue Pretrained Network 
 38 

 
 
 
 
 
 
 Video frame Depth from DPT Depth from the scan RT depth L 2 angle Succ∗% Succ% Qw P%
 ground-truth random 7.93 1.99 0.00 0.00 1.83
 ground-truth scan 2.92 1.10 76.47 1.22 1.83
 ground-truth DPT 3.33 1.15 76.47 1.22 1.83
 Siam-RCNN+PF DPT 6.53 1.64 25.00 0.61 0.61
 Siam-RCNN+KYS(sim.) DPT 5.78 0.48 36.36 0.61 0.61
 Siam-RCNN+KYS(res.) DPT 5.98 1.60 30.77 1.22 1.83
 Table 10.Visualqueries 3 Dlocalizationresults.Wecomp are the
 per for manceofvarious base lineson the valsetof the VQ 3 Dtask.
 Column 1 indicates the VQ 2 Dnetworkusedtopredict the response
 track(RT).Thelastmetric Qw Pmeasures the queryratio for which
 wehaveposeestimation for theresponsetrack and the queryframe.
 The L 2 metricisexpressedinmeters and angles are inradians.The
 firstthreerows are ablationstudiesusing the ground-truthresponse
 tracks and withdep the stimatedr and omly,usingthes can andvia
 Figure 33. Samplesofdep the stimation. Leftshows the frame the DPT[185]network. 
 from the egocentricvideo,middlehas the estimateddepth from 
 DPT[185]andrighthas the depth from thes can renderedat the 
 estimatedviewpoint. using DPT(lines 2 and 3). Thissuggests that the reisalso
 room for improvementindesigningbetterdep the stimators.
 whereu,varethepixelindicesof the centroidcinframef. 
 Wethenestimatetheobjectcentroidpositiontˆ inthes can Naturallanguagequery base lines
 s 
 coordinatesystem: Since the naturallanguagequeries can beseenasalanguage-
 groundingprobleminavideo,weadopttwopriormethods
 tˆ =Psvˆ (18) 
 s f f inordertoimplement the baselines for this task.
 where Ps is the camera pose for the frame f. We further (a)2 DTemporal Adjacent Networks(2 D-TAN)[236]:
 f 
 retrieve the displacement vector vˆ in the query frame Q We apply 2 D-TAN with a sliding window method to im-
 Q 
 coordinatesystem: plement the naturallanguagequery base line. Thegoalof
 2 D-TANistoanswerwhere the semanticallycorresponding
 vˆ Q =P Q s−1 tˆ s (19) videomomentis,givenalanguagequeryinanuntrimmed
 video. Thelanguagequerystems from oneof the 13 tem-
 where Ps isthecameraposeof the queryframe. 
 Q plate questions. The core idea of 2 D-TAN is to consider
 Experiments and results Wecomp are the per for manceof adjacent moment candidates as the temporal context on a
 multiple base linesalong with ablationstudies. Wepresent two-dimensionaltemporalmap and retrieve the mostrele-
 theresultsin Table 10. Numbers are computedon the valida- vantmoment from the can didates.Moreconcretely,2 D-TAN
 tionset(164 queries)ofthe VQ 3 Dtask. Wereport the query takeseachmoment can didateasoneelementin the 2 Dtem-
 ratio Qw P,forwhichwe have cameraposeestimates for the poralmapsuch that the adjacentmoment can didateson the
 responsetrack and queryframe. Additionally,wereport the map can havemuch-overlappedcontentorsh are the same
 success rate Succ∗ which is the success metric computed startorendtimeslot. Itappliesaconvolutionalneuralnet-
 only for queries with associatedposeestimates. workon the 2 Dmaptopredict the Intersectionover Union
 Overall,wenoticealow Qw Pratioleadingtoalowsuc- of each moment candidate and the ground-truth moment.
 cessrate. Theselowmetrics are duetoasmallnumberof Pleasesee[236]formoredetails.
 cameraposeestimates(15%overall). None the less,weob- Since the 2 D-TAN enumerates all the possible combi-
 serve that the best VQ 2 Dbaselinemethodcombined with the nationsofstart-endpairs,the O(N 2)spacecomplexityof
 pretrained DPT[185]depthestimatoryields the bestper for- the 2 D map leads to a heavy model, especially when we
 mancesintermsof L 2 andsuccess. Thesenumberstell that requireaprecisemomentboundary. Tomake 2 D-TANmore
 there are opportunities for enhancementindesigningbetter appropriateto our problem,wefur the ruseaslidingwindow
 cameraposeestimators. Additionally,weper for mablation methodontopof 2 D-TAN.Webreakdown the clipintoa
 studiesusing the ground-truthresponsetracks and different numberofoverlappingwindows,whereawindowpresentsa
 dep the stimators(random,fromthes can,using DPT).For smallportionof the clip. Thewindows are takenas the input
 ther and omexperimentweuni for mlysampleadepthvalue ofthe 2 D-TAN model inbothtraining and testingphases.
 between 0.1 and 10 meters. From the ablationexperiments During the training of the 2 D-TAN model, we use
 wenote that rendering the depth from thes can attheesti- Ego 4 D’sprovidedpre-extractedfeatures for both the video
 mated viewpoint increases the per for mances compared to clip and language query. The clip feature is from a Slow-
 39 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 34.Baseline model architectures:momentqueries.Itstakesavideosequence and generatesdetectedactions with start/endtime,
 theircategories,andconfidencescores. Ithastwocomponents: graphpyramidnetwork(GPN),andscoring and localization(So L).
 GPNiscomposedofmulti-levelencoder and decoderpyramids.Theencoderaggregatesfeaturesindifferentlevelsviaastackofgraph
 networks(GN)(yellowtrapezoidarea;thedecoderrestores the temporalresolution and generatesmulti-levelfeatures for detection.So L
 (bluedashedbox)containsf our modules,thetoptwopredictingactionscores and boundaries,thebottomtwoproducingsupplementary
 scores and adjustingboundaries.Figureisadapted from[237]. 
 
 Fast[71]networkpretrainedon Kinetics 400 dataset, and Io U=0.3(%) Io U=0.5(%) 
 Baseline 
 the language feature is a based on the BERT model [52]. r@1 r@5 r@1 r@5 
 Thewindowdurationis 40 s,andstrideis 20 sin the sliding 
 windowmethod. Notably,weonlyusewindows that contain 
 or are next to a ground-truth moment in training, but we 
 useall the windowsintesting. Wekeepalltheo the rhyper- 
 parametersin 2 D-TAN the sameasitsdefaultexceptfort Io U 
 threshold and learningrate. Wedecreasedthet Io Uthreshold 
 from 0.5 to 0.3 toenablemorepositivesamplesduringtrain- 
 ing and empiricallyset the learningrateto 0.001. Wetrain 
 themodel for 100 epochs and report the testsetper for mance 
 onthebestcheckpointon the validationset. 2 D-TANgives 
 top-1 and top-5 recalls of 5.80% and 13.90% at Io U=0.3, 
 respectively. Inaddition,wealsoablate the modeltoobtain 
 per for mance by randomizing the video features ( visual) 
 − andtextualfeatures( text)for NLQin Tab.11. 
 − 
 (b)Span-based Localization Network(VSLNet)[235]: 
 Unliketraditionalapproachesinvideonaturallanguagelo- 
 calizationworks,VSLNettreats the inputuntrimmedvideo 
 asatextpassage,andusesaspan-basedapproachtoidentify 
 therelevantsectionssemanticallyrelatedto the givennatural 
 languagequery.Atitscore,VSLNetfirstencodes the natural 
 languagequery and videofeaturesusingacommon,shared 
 Trans for mer[215]network. Next,ituses the encodedquery 
 tothenattendtotherelevantpartsof the videoclip(akinto 
 atextparagraph). Theattendedsections are fur the rrefined 
 usingaquery-guidedhighlighting(QGH)strategybyextend- 
 ingtheselection for egroundof the videobyahyperparamter 
 tocapturemorevisualcontext.Pleasereferto[235]formore 
 la V (cid:8) 2 D-TAN[236] 5.04 12.89 2.02 5.88
 VSLNet[235] 5.45 10.74 3.12 6.63 
 tse T 
  
 2 D-TAN[236] 5.80 13.90 2.34 5.96 
  
 − 
 v 
 t 
 i 
 e 
 s 
 x 
 u 
 t 
 al 
 3 
 2 
 . 
 . 
 4 
 2 
 6 
 9 
 1 
 6 
 0 
 . 
 . 
 7 
 1 
 7 
 3 1 
 1 
 . 
 . 
 7 
 3 
 8 
 2 
 4 
 3 
 . 
 . 
 3 
 4 
 8 
 6 
 − 
 VSLNet[235] 5.47 11.21 2.80 6.57 
  
 − 
 v 
 t 
 i 
 e 
 s 
 x 
 u 
 t 
 al 
 3 
 1 
 . 
 . 
 0 
 8 
 5 
 0 
 7 
 5 
 . 
 . 
 3 
 4 
 9 
 4 
 1 
 0 
 . 
 . 
 4 
 9 
 5 
 0 
 4 
 2 
 . 
 . 
 1 
 4 
 2 
 5 
 − 
 Table 11.Per for manceof the NLQ base linesonval and testsplits.
 detailson the motivation and architecture. 
 For our experiments,wemaintainconsistency with the
 other NLQ base lines and usepre-extractedfeatures for both
 thevideoclip(Slow Fastnetwork[70])andnaturallanguage
 query(BERT[52]). we use the implementationprovided
 by the authors 15 with the following changes: (a) Set the
 videofeaturessizeto 2304 dimensionstoaccommodate the
 featuresextracted from the Slow Fastnetwork,(b)Replace
 thetextencodertoafrozen,pretrained BERT[52]model,
 (c)Settheinternaldimensionof the multimodalnetworkto
 128,andproject the pre-trained BERTfeatures from 768 to
 128. Wetrain the model for 200 epochs and pick the model
 with the bestper for manceonvalsplit. Thecorresponding
 15 https://github.com/Isaac Changhau/VSLNet
 40 

 
 
 
 
 
 
 testper for manceof this VSLNet model isreportedin Tab. Table 12. Momentqueriesresultson the validationset and the
 11,along with visual and textualablations. test set,measuredbym AP(%)atdifferentt Io Uthresholds.
 t Io Uthreshold 0.1 0.2 0.3 0.4 0.5 Average
 Momentsqueries base line 
 validation set 9.10 7.16 5.76 4.62 3.41 6.03
 test set 8.61 6.52 5.43 4.30 3.57 5.68 
 Weformulateamomentqueries base lineasatemporalac- 
 tion detection method [141,229,237], plus simple post- 
 processing. 
 5 levelsin the graphpyramidnetwork,each with temporal
 The MQtaskonlyexpectspredictions for the querycat- 
 length 232,116,58,29,and 14 respectively. Wepre-define
 egories,whereas the temporalactiondetection task returns 
 two base anchorsofsizes 4 and 12 for Level 1 andincrease
 the predictions for all categories. Therefore, we can first 
 thesizesby 2 foreachdeeperlayer. Wetrain for 30 epochs
 use a temporal action detection method to predict for all 
 withabatchsize 32 andlearningrate 0.0001. Ininference,
 categories,andonlyoutput the resultscorrespondingto the 
 weonlyapplyper-category NMS with aconfidencethreshold
 querycategories. 
 0.0005. 
 To predict all categories, we adopt a recent method 
 VSGN[237],whichwasdesigned for temporalactiondetec- 
 Experiments and results We show our baseline perfor-
 tioninthird-personvideos. we use VSGN with out the VSS 
 manceintermsofm APin Table 12 andrecall@kx,t Io U=m
 component. Figure 34 illustrates the architecture. Ittakes 
 in Table 13. 
 avideo asinput,extractsfeatures for eachsnippetin the 
 V Weprovidefurtheranalysison the averageprecisionre-
 videousinganetworksuchas Slow Fast[70],andfeedsthese 
 sultsusing DETAD[9]. In Fig 35,weillustrate the propor-
 featuresintoagraphpyramidnetwork. Thegraphpyramid 
 tionofeacherrortype for the falsepositivepredictions. It
 network contains a encoder and a decoder, where the en- 
 shows that both localization and classification are respon-
 coderiscomprisedofmultiplelevelsofgraphconvolutional 
 sible for the false positive, improving either can increase
 networks,and the decoderiscomprisedofmultiplelevels 
 theoverallper for mancebyanontrivialamount. In Fig 36,
 ofde-convolutionalnetworks. Itisananchor-basedmethod 
 wedemonstrate the per for manceofdifferentgroupsofmo-
 thatpre-definestemporalsegments for eachfeaturelevelas 
 mentinstances basedonmoment duration and number of
 predictionreference. Itpredicts the scores and refines the 
 instancesbelongingto the samecategorypervideoclip. We
 locationsof the anchorsintwostages. Inthefirststage,it 
 notice that short moments tend to have low per for mance
 uses are gionproposalnetwork(RPN)from the decoderto 
 eventhough the yarelargeinnumber. When the reare 2-3
 predictclasslabels and regressboundaries for eachanchor; 
 instancesinonevideo,they are easiesttodetect.
 inthesecondstage,itappliesaboundaryadjustmentmodule 
 torefinetheboundaryoffsets base don the updatedanchors 
 from the firststage. Italsohasstartness/endnesspredictions 
 toprovideauxiliarysupervision and supplementscores for 
 each predicted segment. Its output predictions are formu- 
 M 
 lated as Φ = φ =(t ,t ,c ,s ) , where m { m m,s m,e m m }m=1 is the number of predictions, t and t are start time m,s m,e
 andendtimeof the mth predictionrespectively,c isthe m 
 predictedcategory,ands istheconfidencescore. Formore m 
 details,pleasereferto[237]. 
 Given a query category c, the retrieval results for the 
 momentqueries task are obtainedasfollows 
 Φ = φ =(t ,t ,c ,s ) c =c,1 m M) . 
 c m m,s m,e m m m 
 { | ≤ ≤ } 
 (20) 
 Implementation details For feature extraction, we use 
 Ego 4 D’s provided pre-extracted features using a Slow- 
 Fast[70]networkpre-trainedon Kinects 400[108]at 1.87 
 featurespersecond. Thefeaturedimensionis 2304. 
 Considering that the maximumclipleng this 8 minutes, 
 which has 897 features, we make the input length of our 
 network 928 framestocover the longestvideoclip. Wehave 
 G 1 G 2 G 3 G 4 G 5 G 6 G 7 G 8 G 9 G 01 
 100 
 90 80 
 70 60 
 50 
 40 
 30 
 20 
 10 
 0 
 Top Predictions 
 )%(nwodkaer Brorr E 
 Background Err Localization Err Double Detection Err
 Confusion Err Wrong Label Err True Positive
 False Positive Profile 2.00 
 1.75 1.50 
 1.25 
 1.00 
 0.75 
 0.50 
 0.25 
 0.00 
 Error Type 
 NPAm-egarev A )%(tnemvorpm I
 Removing Error Impact
 1.2 
 0.8 0.8 0.8 
 0.1 
 Figure 35.Momentqueriesresults:falsepositiveanalysis.The
 errortypes are determinedbythet Io Ubetweenground-truth and
 predicted moments, as well as the correctness of the predicted
 labels,accordingto[9]. Backgrounderror: t Io U<1 e−5;confu-
 sionerror: 1 e−5 < t Io U < α,labeliswrong;wronglabelerror:
 t Io U>=α,labeliswrong;localizationerror:1 e−5 <t Io U<α,
 labeliscorrect,whereαreferstothet Io Uthresholds{0.1,0.2,0.3,
 0.4,0.5}.‘G’refersto the numberofground-truthinstances.
 41 

 
 
 
 
 
 
 Table 13.Momentqueriesresultsonthevalidationset and the testset,measuredbyrecall(R)@kx,t Io U=m(%).
 m 0.3 0.5 0.7 
 k 1 3 5 1 3 5 1 3 5 
 Validation Set 33.45 51.26 58.43 25.16 39.46 46.18 15.36 22.67 25.81
 Test Set 33.56 52.23 59.79 24.25 39.22 46.22 14.83 23.15 26.28 
 
 
 80 
 70 
 60 50 
 40 
 30 
 20 10 
 0 XS S M L XL XS S M L XL 
 htur T 
 dnuor G 
 fo 
 % 
 Length #Instances 
 63.4 
 52.2 
 31.0 
 20.5 
 12.5 7.5 3.5 5.7 3.1 0.5 
 20.0 
 17.5 
 15.0 
 12.5 
 10.0 
 7.5 
 5.0 
 2.5 
 0.0 XS S M L XL XS S M L XL 
 )%( 
 NPAm-egarev A 
 video clips, moving a step closer to augmenting a user’s
 episodicmemory. 
 Momentqueriesinegocentricvideosisachallenging task
 dueto the long-taileddistributionofcategories and the large
 variationinmomentduration. Our base lineachievesarea-
 sonableresultaccordingto the metricrecall@kx,t Io U=m,
 whichevaluates the per for manceofeachquerycategory and
 doesnotrequirecorrectclassificationofallcategories. In
 contrast, itsaveragem APscoreof 5.96%islowwhenall
 13.9 
 11.4 10.5 11.2 categories are evaluated.Accordingto the falsepositiveanal-
 8.0 ysisin Fig 36,errorscausedbywronglabels are significant.
 6.9 6.3 6.36 
 5.1 Amoresophisticatedclassifier for all can didatemoments
 3.1 
 can be explored in future work. In addition, as shown in
 0.1 
 Fig 36, theper for manceofshortmoments, whichoccupy
 a large proportion in the dataset, is not as good as that of
 Figure 36. Momentqueriesresults: sensitivityanalysis. Top: 
 long moments. Therefore, improving short moments will
 Distribution of instance per action characteristic: length; # in- 
 signifi can tlyimprove the overallper for mance.
 stances. Bottom: averagem APN (%)[9]ineachcharacteristic 
 bucket.The‘length’characteristicdividesallmomentinstances 5 
 buckets base don the momentsdurationinseconds:XS(0,10],S Contributionsstatement 
 (10,60],M(60,180],L(180,300],and XL(300,inf].The‘#in- 
 stances’characteristicdividesallmomentinstancesinto 5 buckets Kristen Graumanled the Episodic Memorybenchmark and
 basedonthenumberofinstancesbelongingto the samecategory paper writing, wrote annotation instructions, contributed
 inonevideoclip:XS(0,1],S(1,3],M(3,10],L(10,20],and XL todataselection and taxonomy for mation,andco-advised
 (20,inf]. the VQbaselinedevelopment. Bernard Ghanemco-ledthe
 Episodic Memorybenchmark,managed base linedevelop-
 Discussion ment and evaluation for the MQand NLQtasks, andcon-
 tributed to the annotation instructions, data selection, and
 Visual queries presents a novel and challenging task for taxonomy for mation for the MQand NLQ data sets. Jackson
 objectlocalizationinegocentricvideos. While our proposed Hamburger contributed to the development of the annota-
 baselineachieves are asonablesuccessrateof 42.9%,itonly tioninstructions and taxonomiesof the NLQ,VQ,and MQ
 achieves a localization per for mance of 0.13 t AP and 0.06 datasetsalong with thedesignof the early VQbaselines.
 st AP.Fur the rmore, thebestper for manceisachieved with Santhosh Kumar Ramakrishnan led VQ data selection,
 0%searchefficiency,andna¨ıvetechniquestoimprove the annotation,analysis and auditing,contributedto the formu-
 searchefficiencyleadtodrasticper for mancereductions. We lation and annotationinstructionsof VQ,dataselection,and
 hope that this task will spurfutureresearchintoaccurate and implemented the VQbaseline. Vince Cartilliercontributed
 efficienttechniques for objectsearch. tothe VQ-3 Dformulation and annotationinstructions,led
 Natural language queries is a challenging multimodal VQ-3 D data selection, annotation, analysis and auditing,
 task that haswideapplicationsinhelpinguserssearch and and implemented the VQ-3 D baseline. Dhruv Batra co-
 retrieve relevant pieces of their episodic memory, thanks mentored Vince Cartillierondeveloping base lines and pro-
 to the flexibility of the queries. The per for mance of the vided guidance on 3 D scans using Matterport. Hyun Soo
 existingstate-of-the-artvideolocalization model shighlights Parkcontributedto 3 Dreconstructionofegocentricvideos
 theneedle-in-a-haystacknatureof the task, duetoshorter with respect to 3 D Matterport scans. Tien Do developed
 response windows of about 10 s in a large video clip of 8 algorithmstoreconstruct 3 Degocentriccameraposes with
 minutes. Wehope that the NLQ data setopens the doorto respectto 3 DMatterportscans. 
 futureresearch that specializesinidentifying and retrieving James Hillisprovidedbackgroundknowledgeonhuman
 a large diversity of language queries in longer egocentric episodicmemoryfunction and contributedtoearlydiscus-
 42 

 
 
 
 
 
 
 sionsonbenchmarkdefinition and annotation.Satwik Kottur 
 led the designof NLQ data selection and annotationinstruc- 
 tions,contributedto the NLQtask for mulation,coordinated 
 NLQ data annotation and data analysis, implemented the 
 VSLNet NLQ base line,wrotepartof the NLQsections.Men- 
 meng Xudesigned and implemented the experimentpipeline 
 forthe NLQtask,implementedseveral NLQmethods,did 
 NLQresultanalysis and visualization,andwrotepartof the 
 NLQsections. Michael Wraycontributedtoearly for mula- 
 tionof the benchmarktasks,definitionsof NLQqueries and 
 annotationinstructions,providedinput for datasetconstruc- 
 tion and evaluationmetrics,andhelpedin the creationof the 
 MQtaxonomy. 
 Chen Zhaodesigned and implemented the MQbaseline, 
 proposed and implemented the newmetric for MQ,wrote 
 the MQsections,did MQresultanalysis and visualization, 
 contributed to the formulation, data selection and annota- 
 tioninstructionsof MQ.Tushar Nagarajancontributedto the 
 MQformulation and annotationinstructions,developed the 
 MQlabeltaxonomy,andled the dataselection and annota- 
 tionof the MQdataset. Merey Ramazanovamanaged the 
 datasets for the experimentsof MQand NLQ base lines,and 
 assisted with the taxonomy for mation for the MQbaseline. 
 Antonino Furnariprovidedkeypointfeatureextraction from 
 the Matterport 3 Dpanoramas for the VQ 3 Dbaseline. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 43 
 
 
 

 
 
 
 
 
 
 G.Hands and Objects Benchmark 
 Thissectiondetails the Hands and Objectsbenchmarkin- (a) 
 cludingdefinitions,annotations,baselinemodels and results. 
 
 G.1 Motivation 
 
 Inavideoofahumanoperating and manipulatinganobject (b) 
 with their hands, there may exist an object state change, 
 i.e.,thepointwherethestateof the objectsbeingoperated 
 changes, either temporarily or permanently in a way that 
 cannot be easily reversed. Examples of temporary state 
 (c) 
 change include turning on a machine, while examples of 
 permanentstatechangesincludephysicalchangessuchas 
 choppingatomatointopieces and chemicalchangessuchas 
 mixingwater and cementpowdertoge the rtocreatea new Figure 37. Examples of object state change. (a) State change
 composition of cement. Some examples are illustrated in through construction: attaching to two metal plates results in a
 Figure 37. newobject. (b)Statechangethroughphysicalchange: cuttinga
 piece of wood results in two smaller pieces of wood. (c) State
 Theconceptofanobjectstatechangehas been explored 
 changethroughchemicalreaction:combiningtwoobjects,water
 onlyinalimitedmannerin the videoliterature[8,45,69] 
 andcementpowder,resultsina new object,cement.
 and the characterizationofstatechangeshasdependedon 
 manybrittlevision-basedcomponenttechnologies,makingit 
 difficulttoanalyzestatechangesat scale. Fortunately,inthe 
 tions that requirerichhum and emonstrations,suchasrobotic
 lastdecadewe have seentremendousadvancesincomputer 
 manipulation. 
 visionalgorithms for understandingbothobjects and hands. 
 Defining Object State Changes: This benchmark fo-
 Asaresult,webelieve that nowitistimetoinvestigate the 
 cusesonidentifying and localizing the statechangeofan
 ideaofcharacterizingstatechangesat scale and indepth. 
 object in an egocentric video. Specifically, a object state
 Whyisrecognizing the impactofagentsonobjects and 
 change can berepresentedbythethreeaspectsin the video:
 environments so critical? We believe that underst and ing, 
 temporal,spatial,andsemantic. 
 recognizing,andreplicatingobjectstatechanges are anes- 
 sentialaspectofcreatingartificialintelligence(AI)systems. 
 Whilecurrent AIsystems have the abilitytoreplicatecertain Temporal: An object state change can be represented by
 typesofhumanactionssuchasassemblingfurniture[116]or three distinct temporal points in the video. (1) Point-of-
 cuttingtomatoes[200],mostsystemsdonotpossessagen- no-return: Thepoint-of-no-return(PNR)istheframe I pnr
 eralunderst and ingofhowtheenvironment and the objects in a video that identifies the beginning of an object state
 can betrans for medas are sultofinteraction. Underst and ing change that cannot be easily reversed. (2) Pre-condition:
 theimpactofinteractionsonobjects and the environmentis Thepre-conditionisdefinedassomeframe I pre thatmarksa
 animportantaspectofreasoning and can help AIsystems momentpriorto the state-changeinwhich the relatedobjects
 per for mmoreadvancedtasks. Forexample,underst and ing werevisible with inthefieldofviewof the camera. (3)Post-
 theimpactofinteractionson the environment can help AI condition: Thepost-conditionissomeframe I post atwhich
 systems relate multiple ways to achieve the same change, thecompletionofthestatechangeisvisibleafter the point-
 discoverefficientmethods for achievinggoalstates,recog- of-no-return. Thesethreeframesmark the distincttemporal
 nize the completion/incompletionofgoals[58,97],recover stagesof the objectstatechange:before and after the change,
 fromfailure,andlearn from mistakes. respectively. Thisproposalmatches the Rubicon Boundaries
 Inegocentricvideosspecifically,theobjectstatechanges proposedin[160]. 
 offerrich and importantin for mation that are relatedtomany 
 otherproblems. Forexample, theobjectundergoingstate Spatial: Anobjectstatechange can berepresentedby the
 changeinanegocentricvideo can implyhuman-centricin- boundingboxof the objectat the PNR,pre-condition and
 formationsuchashumanactivity and intention. Moreover, post-condition,along with anytoolsinvolvedinper for ming
 the state change of an object shown provides cues about the state change. Tools offer extended capabilities of the
 human-specificaf for dance and actionablein for mationofan actor’shand,suchasusinganelectricsawtocutapieceof
 objectortool,which can notbeeasilyinferred from static woodinhalf. Theseboundingboxesrepresent the spatial
 images. Additionally,ajointunderst and ingofhumanhands dimensionsofhands,tools and the objectsundergoing the
 and the objectsundergoingstatechange can benefitapplica- statechange. 
 44 

 
 
 
 
 
 
 Semantic: Werepresentanobjectstatechangethrough the get task isactivityrecognition[180]. Inthe UT-Egocentric
 humanaction(verb),theobjectidentity(noun)and the type dataset(UT-Ego),subjectswearahead-mountedcamera and
 ofstatechangeapplied. Thesamestatechange can beper- per for mlongunscriptedactivitiesinside and outsideof the
 formedondifferentobjectsusingdifferenttools. Forexam- home,withatotalof 17 hours from 4 subjects(4-5 hoursof
 ple,cuttingapieceofwoodwi the lectricsaw and cuttinga continuouscapture for eachperson);thetarget task isvideo
 pieceofpaper with scissors are differentinteractions with summarization[130]. The UTEgocentric Engagement(UT
 differentobjects and differenttoolsbut the ybothresultin EE)datasetconsistsof 14 hoursofhead-mountedcamera
 thesameobjectstatechangeofbeingcut. videocapturedinpublicspaceslikemuseums, malls, and
 grocerystores,andisannotated for momentsofengagement
 bythecamerawe are rwith the environment. Inthe EGTEA+
 G.2 Related Work 
 dataset,32 subjectswearinghead-mountedcamerasinasin-
 Object State Changes: Existingapproaches for modeling glekitchenenvironmentcapture 28 hoursofvideo;thetask
 object states and/or their changes can be categorized into is to recognize 44 meal preparation activities [136]. The
 two research lines. The first deals with collections of im- EPIC-KITCHENS data setconsistsof 100 hoursofkitchen
 ages. Arepresentative data set for thispurposeis the MIT activitiesrecordedin 45 uniqueenvironments,withatotalof
 States dataset [103]. By considering object states as ob- 89,977 differentobjectinteractionsacross 97 verb and 330
 jectattributes(e.g. burnt,sliced),thislineofworkstudies nounclasses;the task istorecognizeobjects and activities
 attribute-object composition, e.g. composition with con- andanticipateinteractionsin the nextmomentofvideo[43].
 text [158], modeling attributes as operators [164], and an The Charades-Ego dataset consists of 34 hours of video
 architecture for compositionalreasoning[182]. from 71 participants,withbothfirst-andthird-personpaired
 Thesecondresearchlinedeals with video and viewsan instanceslabeled for 156 actions[201].
 actionasastatetrans for mationovertime. Onedirectionis 
 thediscoveryofobjectstates and/ormanipulatingactions, G.3 Benchmark Definitions
 e.g.inegocentric[45,69]andinstructionalvideos[8]. Fathi 
 et al. [69] explore object state detection in video using a Wenowdefine the threetasks that comprise the Hands and
 weaklysupervisedapproach. Anotherdirectionis the mod- Objectsbenchmark. Thethree task scorrespondto the three
 elingofstatetransitions. Zhouetal.[244]studytemporal aspectsofobjectstatechangesdescribedabove,namely,the
 trans for mationsofasingleobjectstateintime-lapsevideos. temporal,spatial and semanticaspectsofastatechange.
 Wangetal.[223]proposeto model statetrans for mationsin (1) PNR Temporal Localization. The goal of Point-of-
 ahigh-levelfeaturespace with Siamesenetworks. Doughty no-return (PNR) Temporal Localization is to predict I .
 pnr 
 et al. [55] leverage natural language and treat adverbs as Onepossible for mulationistoview this problemasaper-
 modifiersforstatetrans for mations. Intermsofapplications, frame classification problem, predicting the Point-of-no-
 Changetal.[30]showstatetrans for mations can beutilized return frame within a short video clip. The per for mance
 forprocedureplanning. is evaluated only on the videos that contain object state
 change,andismeasuredby the absolutetemporalerrorof
 Human Hand Action Datasets: Several video datasets 
 I predictioninseconds. 
 have been proposed for humanh and actionrecognition. The pnr 
 The PNRwasfirstdiscussedby P.Gollwitzerinhiswell-
 Yalehumangrasping data set[25]focusesonhumangrasping 
 cited handbook of behavior [89]. Specifically, the book
 behavior and consistsof 27.7 hoursofannotatedvideos. The 
 proposes the Rubicon Model of Action Phases, focusing
 Something-Something data set[90]consistsof 220,847 short 
 onhand-objectinteraction. Actionphases are delimitedby
 videosannotated with 174 categoriesofgeneralh and-object 
 threetransitionpoints: initiationofpriormotion,PNR,and
 interactions. The Jester data set[214]provides 148,092 short 
 goalachievement. Thiswaslaterexperimentallyassessedby
 videos in 27 hand gesture types. Wang et al. [220] con- 
 ourpreviouswork[160],where PNRannotationswereac-
 structasyn the ticvideo data setofhuman-objectinteraction 
 quired for threeegocentric data sets,demonstratingincreased
 throughrenderingh and and object CADmodels. Therecent 
 accuracyofannotations(see Fig.10 in[160])andimproved
 Human Hands data set[198]annotates 100 Ksingleframes 
 robustnessintrainingmodels(see Sec.5 in[160]). Below,
 fromweb-basedvideos,focusingonh and interactions and 
 wefind PNRcloselyaligns with the narrationtimestamps
 theoffsetbetweentheh and and the interactingobjectduring 
 thatweindependentlycollected,suggesting PNRisanatural
 interaction. 
 timepoint for humanunderst and ing(andthusnarration)of
 Several egocentric video datasets capture daily living 
 theinteraction. 
 activitiesbypeople[43,130,136,180,201,210]. Inthe Ac- 
 tivitiesof Daily Living dataset(ADL),subjectswearchest- (2) State Change Object Detection. We define a State
 mountedcameras and per for munscriptedactivitiesathome, Change Objectas the object that ismanipulatedbyaperson
 withatotalof 10 hoursofvideo from 20 participants;thetar- andundergoesachangeinitsstate. Thegoalof this taskis
 45 

 
 
 
 
 
 
 topredict the 2 Dboundingboxesof the State Change Object toselectthreecriticalframesintime: PNR,PRE,and POST.
 in Point-of-no-return frame I given three frames: Pre- Weask the annotatorstostart with the PNRframe that iden-
 pnr 
 condition I , Point-of-no-return I , and Post-condition tifiesthebeginningof the statechange. Thisframeisless
 pre pnr 
 I . We expect that a good solution to this task would ambiguous and helpsprovidethecontext for the interaction.
 post 
 incorporate the visual information before and after state Wethenasktheannotatorstolabelaframepriorto the state
 change to detect the State Change Object. The detection change(PRE)andaframeafterthecompletionof the state
 per for manceisevaluatedon the boundingboxesestimatedin change(POST).Note that the PREand POSTframes are not
 the Point-of-no-returnframe I andmeasuredby Average uniquelydefined. Welet the annotatorspickany,aslongas
 pnr 
 Precision(AP). therelevantobjects are fullyvisiblewithin the fieldofview
 ofthecamera. 
 (3)Object State Change Classification. Thetaskof Ob- 
 ject State Change Classificationclassifiesashortvideoclip Preperiod. Next,welabelboundingboxes for the hands,
 to a state change type. With N object state change types tools, and objects, as well as the category names for the
 defined,objectstatechangeclassificationisessentiallyan tools and objects. Wedo this intwosteps. Firstwelabel
 (N+1)-wayclassificationproblem,where the oneadditional the frames in the pre period, starting at PNR and going
 category is “without state change.” Object State Change backwardto the preframe. Thevideoframes are reversed
 Classificationisevaluatedbyclassificationaccuracy. and the annotators can play the video.Wefind that itiseasier
 tostart from the PNRframesince the hands and objects are
 clearlyvisible. Tospeeduph and boxlabeling,weinitialize
 G.4 Data Selection 
 theh and boxes with apre-trainedobjectdetector[198]and
 Nextwedescribe our data selectionprocedure and annotation ask the annotatorstocorrectthese.
 pipeline, and we present the analysis of the data for the 
 Postperiod. Finally,weask the annotatorstolabelspatial
 objectstatechangebenchmark. Webeginbydescribing our 
 annotations and categories for the postframe. Asbefore,we
 procedure for selecting the subsetof data toannotate for this 
 firstpresent the annotators with the PNRframe. Note that
 benchmark. 
 inthiscase the PNRframeisalreadylabeledwhichhelps
 Westartwithalargepoolofvideosannotated with high- 
 identifythehands and objectstolabelin the postframe.
 levelscenariolabels(e.g.,gardening,cooking,landscaping, 
 etc.)andnarrations. Weassesseachscenarioon the scale 
 G.6 Data Analysis 
 of 0 to 3 based on how likely it is to contain hand-object 
 interactions (e.g., 0 for “watching tv”, 3 for “carpentery”, Finally,wepresent the analysisof our annotations.
 etc.).Wethensample data toannotatefollowing the resulting 
 Critical frames. In Figure 40 we show the temporal dis-
 scenariodistribution. Givenascenario and atargetnumber 
 tributionofcriticalframeswithin the 8 secondh and-object
 ofhours,wesampleclipsr and omlyinahierarchicalfashion: 
 interactionsnippets. First,weobserve that the PNRframe
 wefirstsampleaparticipant, thenavideo, andfinallya 5 
 distributioniscenteredaround the middleof the 8 second
 minuteclip from the video. Ifthevideoisshorterthan 5 min 
 snippet. Interestingly,thiscloselyaligns with the narration
 wetake the wholevideo. Foreachscenario,webalance the 
 point(4 smark). Next,wesee that mostof the pre and post
 data across universities to maximize geographic diversity. 
 framescomeshortlybefore and after the PNRframe,respec-
 Theresultingscenario and universitydistributions are shown 
 tively,highlightingthequicknatureof the sestatechanges,
 in Figure 38. Intotal,our data sethas 120 hoursrepresenting 
 andthus the challengein this benchmark.Wealsonoticetwo
 53 scenarios,7 universities,and 406 participants. 
 additionalmodes for pre and postframes that comeat the
 start and the endof the 8 sinterval,respectively. Thesecorre-
 G.5 Data Annotation 
 spondtolongrepetitiveactions that startbe for eorcontinue
 past the videosnippet(e.g.,knitting). 
 Weannotateh and-objectinteractionscorrespondingtoeach 
 narration within the selected 5 minute clips. We use the Hands and objects. Ourbenchmarkcontainsalargenum-
 taxonomy from Section D.3 for semantic verb and noun berofhands and objectsannotated with boundingboxes. In
 labeling.Theannotationpipelineconsistsofthreesequential total, we have 825 K bounding boxes, including 245 K
 ∼ ∼ 
 stages: criticalframelabeling,pre-periodlabeling,andpost- forlefth and, 260 Kforrighth and, 280 Kforobjects,and
 ∼ ∼ 
 periodlabeling. 40 Kfortools. In Figure 41 and Figure 42,weshow the
 ∼ 
 distributions of box sizes and locations, respectively. We
 Criticalframes. Givenanarration,wecreatean 8 second 
 observe that our data containshands and objectsatavariety
 videosnippetcenteredat the narrationtimepoint and present 
 ofsizes and locations. 
 itto the annotators. Weask the annotatorstofirstread the 
 narration and select a corresponding verb from the taxon- Actions. Oneofthefeaturesof our benchmarkis the diver-
 omy. Theannotators can thenplay the videoback and forth sityofinteractions. Wefocusonlow-levelatomicactions
 46 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 (a) Scenarios (b) Universities 
 
 Figure 38.Numberofhours.Weshowthedistributionof the numberofh our sacrossscenarios(left)anduniversities(right)
 
 
 
 
 
 
 
 
 
 
 Figure 39.Labeledactions.Distributionofverbs(left)andnouns(right)inannotatedactioninstances.Top 45 verbs and nouns are shown
 forclarity.See Section D.3 formoredetails. 
 
 
 
 
 
 
 Figure 42.Hand and objectlocations.Distributionofbounding
 boxcenters.Showninnormalizedimagecoordinates.
 Figure 40. Criticalframes. Distributionofcriticalframetimes. 
 Shownrelativeto the 8 shand-objectinteractionsnippet. 
 Wenote that ourobjects are commondailyobjects that are
 nottypicallypresentinobjectdetection data sets(e.g.,442
 outof our 478 objectcategoriescovercategoriesbeyond the
 80 COCO[143]categories). 
 G.7 Baselines: Object State Change Classification and
 PNRTemporal Localization 
 Figure 41. Hand and objectsizes. Distributionofboundingbox 
 sizes.Shownintermsofthesqu are rootof the boxareas. Wepresent the implementationofseveral base linemethods
 for the Object State Change Classification and PNR Tem-
 poral Localizationtasks. Among the implemented base line
 ratherthanhigh-levelactions. Weshow the distributionof models,ingeneral the reareoneortwotypesofoutputnet-
 verbs(Figure 39,left)andnouns(Figure 39,right). Wesee workheads: aclassificationhead for the videoclipused for
 thatwe have alargenumberofverbscorrespondingtocom- statechangeclassification,and/oraper-frameclassification
 monmanipulationactions(e.g.,put,take)andanaturallong head for temporallocalization. One can choosetotraintwo
 tail. Theobjectdistributionfollows the samegeneraltrend. modelsseparately,oruse the samebackbone model buttwo
 47 
 
 
 

 
 
 
 
 
 
 networkoutputheads and train the joint model with amulti- Table 14. Numberofpositive and negativevideoclipsofobject
 tasklossfunction. Thefollowing base linemethodsincludes statechangeintrain,validation and testsplits.
 bothtypesof model designs: 
 I 3 DRes Net-50. we use I 3 D[29]with Res Net-50[95]as Split Positive Negative Total
 Train 20,041 21,044 41,085 
 backbonearchitectureof the model for both the Object State 
 Val 13,628 14,720 28,348 
 Change Classification and the PNRTemporal Localization 
 Test 13,561 14,870 28,431 
 tasks. The Res Net backbone is followed by two network 
 outpu the ads: astatechangeclassificationheadanda PNR 
 temporallocalizationhead. Thestatechangeclassification 
 Table 15.Resultsof State Change Classificationaccuracy(%).
 head is produced by global average pooling on the entire 
 spatiotemporal feature tensor followed by a classification 
 Baseline Val Test 
 layer. The PNRtemporallocalizationheadisproducedby 
 Always Positive 48.1 47.7 
 per-frameaveragepoolingfollowedbyaclassificationlayer. 
 Bi-directional LSTM[91] 65.3 63.8 
 Theoveralltraininglossofthemodelis the combinationof 
 I 3 DRes Net-50[29] 68.7 67.6 
 thelossoftwoheadswhich are bothcross-entropyloss for 
 classification. 
 Boundary Matching Network (BMN). We use BMN 
 [140]asa base line for the PNRTemporal Localization task. negativeclips are balancedinnumber.
 BMNisatemporalsegmentdetectionmethod base doncon- 
 Besides the above learnable baselines, for object state
 fidencepredictionofdensetemporalsegmentproposals. We 
 changeclassification,wealsopresenttheresultof the naive
 viewthestartofthevideoasthestartof the temporalseg- 
 baseline of always predicting the positive category as the
 ment and Point-of-no-return I astheendof the temporal 
 pnr prediction. Forthe PNRtemporallocalization task,wead-
 segment,sowe canconvert the problemoflocalizing Point- 
 ditionallypresenttheresultof the naive base lineofalways
 of-no-return I totheproblemofdetecting the temporal 
 pnr selectingthecenterframeof the trimmedvideoas the PNR
 segment. Inourimplementation,BMNuses Res Netas the 
 frame,giventhepossiblecentrebiasof the data.
 backbone model. Fur the rmore, BMNisonlyused for the 
 PNRtemporallocalization task. Theresults for objectstatechangeclassification task are
 Slow Fast+Perceiver. Weimplementa base line model illustratedin Table 15. Thenaive base lineofalwayspositive
 whosearchitectureconsistsof Slow Fast[70]and Perceiver predictionyieldsstatechangeclassificationaccuracyofclose
 [105] for both object state change classification and PNR to 50%. All the learnable baselines outperform the naive
 temporallocalization. Slow Fastactsas the videodeepfea- baseline and achieveaccuracyofmorethan 60%while Bi-
 tureextractor. Thefeatures are the npassedtoa Perceiver directional LSTM base lineachieves the bestper for mance.
 model. Similarto the previous BMNmodel,the Slow Fast Thisshows that the learnable base lines can learnmeaningful
 +Perciever model isonlytrained for temporallocalization informationaboutobjectstatechange,though the reisclearly
 task. Thetraininglossofthemodelis the cross-entropyloss stillspace for improvement. Onechallengein this taskis
 forper-frameclassification. thatthereisverylargevarianceintermof the typesofobject
 statechanges and objectscontainedin the videos.
 Bi-directional LSTM. Weimplementa Bi-directional 
 LSTM model[91]forboth the objectstatechangeclassifica- The results for the PNR temporal localization task are
 tion and PNRtemporallocalization. Wefirstpassindivid- illustratedin Table 16. Thenaive base lineofalwayspredict-
 ualframestoa Res Netmodel[95]toextractdeepfeatures. ing the centerframeyieldsatemporallocalizationerrorof
 The sequence of per-frame features is then passed to the around 1.1 seconds. Otherlearnable base lines can achieve
 Bi-directional LSTMasinput,with the outputsenttoboth bettertemporallocalizationerrorofaround 0.85 secondsor
 the per-frame classification head and the whole-sequence lesswhichshows the baselinemodels can learnmeaningful
 classificationhead. Theoveralltraininglossof the model information for temporallocalizationofobjectstatechange.
 isthecombinationof the lossoftwoheadswhich are both Note that the Slow Fast+Perceiver model achieves the best
 cross-entropyloss for classification. temporallocalizationper for manceof 0.425 secondsonvali-
 For the objectstatechangeclassificationtasks,inthecur- dationset and 0.489 secondsontestset,whichhighlights the
 rentversionwefocuson the two-wayclassificationproblem necessityofusingattention-basedmechanismto model the
 ofwhetherthereisaobjectstatechangein the egocentric changeofobjectstate. Onechallenge for this task isthatin
 video.In Table 14,weillustrate the numberofpositivevideo someactions,e.g.,cuttingapieceofpaper with scissors,the
 clips that containsanobjectstatechange and the numberof statechangeofanobjectdoesnotnecessarilycausesignifi-
 negativevideoclips that donotcontainobjectstatechange cantchangeofvisualappearance and the reforeitisdifficult
 inthetrain/val/testsplits. Inallthreesplits,thepositive and tolocalize the PNR. 
 48 

 
 
 
 
 
 
 Table 16.Resultsof Point-of-no-returntemporallocalizationerror Table 17. Number of State Change Object and hand bounding
 (seconds). boxesintrain,validation and testsplits. 
 Baseline Val Test Split State Change Object Hand 
 Always Center Frame 1.032 1.056 Train 19,347 33,254 
 BMN[140] 0.780 0.805 Val 12,912 22,098 
 I 3 DRes Net-50[29] 0.739 0.755 Test 13,118 22,576 
 Bi-directional LSTM[91] 0.790 0.759 
 Slow Fast[70]+Perceiver[105] 0.804 0.828 Table 18.Resultsofsingle-frame State Change Object Detection.
 Theper for manceismeasuredin Average Precision(AP).
 G.8 Baselines: State Change Object Detection 
 Baseline Backbone AP AP 50 AP 75 
 Faster-RCNN[190] Res Net-101[95] 13.4 25.6 12.5
 Whileweexpect that newmethodsdeveloped for the tasks 
 DETR[27] Res Net-50[95] 15.5 32.8 13.0
 ofstatechangeobjectdetection will utilizeallthreeinput 
 Center Net[241] DLA-34[233] 6.4 11.7 6.1 
 frames (pre, PNR, post), in this initial stage of the bench- 
 100 DOHModel[199] Res Net-101[95] 10.7 20.6 10.1
 mark, we only evaluate single-frame detection baselines, 
 whereonly the PNRframe I isusedasinput. Welimited 
 pnr 
 ourinputasmanymethods for objectdetection are primarily the 100 DOH model pre-trainedon 100 DOH data set[199]
 designedtowork with asingleimage. to first detect hand bounding boxes and then predict state
 Wepresent the implementationofseveral base linemeth- changeobjectboundingboxesgiven the hands.
 ods for the state change object detection task. In general, We show the number of state change objects and hand
 the baseline models for the task can be categorized into boundingboxescontainedin our data setin Table 17. The
 two types: (1) directly detecting the bounding box of the resultsofsingle-frame State Change Object Detection are
 statechangeobjectincluding Faster-RCNN[190],Center- illustratedin Table 18. All base linesstruggleindetecting
 Net[241],and DETR[27],and(2)detectingh and bounding the State Change Objects with onlyoneframeasinputas
 boxesfirst the npredictstatechangeobjectboundingboxes an APof 8-14%. There are severalchallengesin this task.
 given the handssuchas the 100 DOHmodel[199]. Specifi- First,theboundingboxsizesofstatechangeobjects have
 cally,the base linemethods are the following: largevariance. Forexample,thesizeofstatechangeobjects
 Faster-RCNN[190] isatwo-stageanchor-based 2 Dob- can beaslargeashalfofimagein the actionof“painting the
 ject detector on a single RGB image. In its classification wall”andassmallasafewpixelsin the actionof“igniting
 head,thestatechangeobjectis the onlypositivecategory. the match.” Second, when only using one frame as input,
 Wetrain Faster-RCNNon our benchmark and useittodi- thedetection model sdidnotconsider the changeofobject
 rectlydetect the boundingboxesofstatechangeobjectsin appearance across different frames. As future work, we
 PNRframes. hope the researchers will investigateusingmodels that take
 Center Net[241] isano the robjectdetectionmethodon multipleframesasinput and perhapsdevelopframeworks
 asingle RGBimage. Itestimatesobjectkeypointstofind thatincorporatetrackingorassociation.
 objectcenterpoints and regressesallo the robjectproperties, 
 suchassize,3 Dlocation,andorientation. Wetrain Center- 
 G.9 Discussion 
 Net to directly detect the bounding boxes of state change 
 objects. Thisnovelbenchmarkexploresthreeaspectsofobjectsun-
 DETR[27]isanobjectdetection model onasingle RGB dergoingstatechangesas are sultofh and manipulation: the
 image base don Trans for mer[216]. Itviewsobjectdetection when(i.e. temporallocalizationofstatechange),where(i.e.,
 as a direct set prediction problem and uses a trans for mer spatial localization of objects that undergo change) and
 encoder-decoderarchitecturetoproduceasetofobjectpre- what (i.e., semantic notion of action and object trans for-
 dictionsincludingboundingboxin for mationaswellasother mation). As a first step, we have explored these indepen-
 information such as category. We train DETR to directly dentlyusingreadilyavailablelocalization and classification
 detect the boundingboxesofstatechangeobjects. methods. However,approaches that aimtotackle this chal-
 100 DOHModel[199] firstdetects the boundingboxes lenge should focus on jointly underst and ing the manipu-
 of the human hand and objects as well as the relational lation with its spatio-temporal impact on objects as these
 vectors that links from each hand bounding box center to aretrans for med. Forexample,knowinganobjectisbeing
 anobjectboundingboxcenter. Thefinalpredictionof the splitshouldofferastrongpriorto the PNRlocalisation and
 objects are decidedas the objectpredictions that satisfies the detect two or more bounding boxes after the point-of-no-
 both the predictionsofh and and relationalvectors. we used return. Suchmethods that tackle the dependenciesbetween
 49 

 
 
 
 
 
 
 thetasks are yettobedeveloped. Wehope this benchmark 
 willspurinnovativeapproaches that bridge the gapbetween 
 actionperception and the impactofactionsonobjects and 
 environments. 
 G.10 Contributionsstatement 
 
 Kris Kitani helped formulate and write the object state 
 changebenchmark,designed the annotations and tasks for 
 the HObenchmark. Dima Damenhelped with the formu- 
 lation and writing of the object state change benchmark, 
 designed the annotations for the Hands and Objects(HO), 
 and Forecastingbenchmarks. Ilija Radosavoviccoordinated 
 HO data annotation, annotation analysis, and contributed 
 to the definition and writing of the HO benchmarks. Ro- 
 hit Girdharhelpedcoordinate the HOdataannotation and 
 annotationanalysis. Abrham Gebreselasieadapted the Slow- 
 Fast+Perceiver model for PNRtemporallocalization.Qichen 
 Fuimplementedallof the statechangeobjectdetection base- 
 lines. Raghava Modhuguimplemented the BMN base line 
 for PNRtemporallocalization.Kristen Graumancontributed 
 tothe for mulation and writingofobjectstatechangebench- 
 mark. Siddhant Bansalhelped with the processingof HO 
 data,developmentof HOdataloader for PNRtemporallo- 
 calization and implemented the I 3 D Res Net-50 baselines. 
 Xingyu Liuwas the leadcoordinator and mentorof the HO 
 benchmark base lineimplementations,andalsocontributed 
 to the definition and writing of HO benchmarks. Xuhua 
 Huangdevelopedof the initial Slow Fast+Perceiver model. 
 Yifei Huangimplemented the Bi-directional LSTM base line 
 forthe PNRtemporallocalization and statechangeclassifi- 
 cation. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 50 
 
 
 

 
 
 
 
 
 
 H.Audio-Visual Diarization Benchmark thevisualstream—somesuchnoiseisstructured and rele-
 vant for underst and ing the context and semanticcontentin
 Thissectiondetails the Audio-Visual Diarization(AVD) 
 thescene. 
 benchmark task definitions, annotations, baseline models, 
 andresults. Asnotedin Appendix B,the AVDbenchmark 
 usesonlyvideowhereinformedconsent for capturingiden- H.2 Related Audio Visual Learning Work
 titiesisexplicitlycollected from allparticipantsin the scene, 
 includingfaces and voice. Thereis are centresurgenceofworkonaudio-visualanalysis
 within and beyond the computervisioncommunity. These
 workstacklevariousaspectsofaudio-visualunderst and ing,
 H.1 Motivation 
 includings our celocalization,cross-modalfeaturelearning,
 Egocentrichumanperceptionisdrivenbyinferringuseful audio spatialization, and audio source separation, as we
 information from all the primarysenses. Whilevisualscap- brieflyreviewnext. 
 tured by the eyes are one of the main information chan- On audio-visual detection and tracking, recent works
 nels, sounds as captured by the ears are equally relevant. on multimodal learning explore ways to localize sounds
 In particular, for underst and ing humans’ interaction with inagivenvideoframe[14,197,212]andinferspatialized
 the environment from the first-person perspective, detect- sound from video[80,161]. Capturing and processingmulti-
 ing, localizing, tracking (both in 3 D space and time) and channelaudioisbeingstudiedinaudio and microphonearray
 underst and ingsoundsbycombining the necessaryacoustic signal processing communities, specifically from a user’s
 information with visualsignalsbecomesevenmorecritical. perspectivetounderst and agivenscene[101,169]. Building
 Severalpsychophysicalstudies have proven that humans are upon these, it is reasonable to expect that human-centric
 remarkablygoodatlocatingwhereasoundcamefromin 3 D audiohasin for mationcontent that can directlyimprovevi-
 space with respectto the irheadposition[156]. Sensitivityof sualobjectcategorization and recognition. Indeed, thisis
 humanstomovingsoundsinhorizontal and verticalplanes observedinsomerecentworkwhereaudiodisambiguates
 isalsowelldocumented[117,178]. certainvisuallyambiguousactions[110,226]. Foractions
 For a long time, the computer vision community has andactivity, audioevents can alsobedirectlyusedtoper-
 studyied the problemofpreciselocalizationofobjects and formsummarization[13]. Inparticular,capturingego-driven
 people, robustly tracking and segmenting them using im- actionsandactivity and separating the mfromgeneralback-
 ages. Inthiseffort,weaimtobringaudio(humanspeech groundactions and activityin the sceneiscritical.
 inparticular)into the mix. Trulyaudio-visualsystemsnot Alternatively,visualin for mationhas been usedtodisam-
 onlyenablerichercapture and analysisof the environment biguatecertainaudio task slikespeechtranscription. Specifi-
 (and a user’s interaction with it), but they also help build cally,audio-visualspeechrecognitionhasreceivedalotof
 technologies for visuallyoracousticallyimpairedusers(e.g., attention in the last decade with multiple studies suggest-
 hearingaids,augmentedreality). ing that automaticspeechrecognition(ASR)couldbenefit
 Thegoalof this benchmarkistohelpadvance the state from visuals of the scene, or other non-acoustic informa-
 oftheartinaudio-visualunderst and ingfrom the egocentric tion[5,104]. Asshowninhere, itisreasonabletoexpect
 viewpoint. Specifically,fromaconversationalperspective, thatlipreading from afirstpersonpointofviewwouldalso
 thebenchmarkaimstounderst and whoistalkingwhen,and benefit ASRsystems. 
 aboutwhat.Fromavisualperspective,wearealsointerested Inaddition,audio-visualcross-modallearningmaypro-
 inwhere the speakerislocated. Givenanegocentricvideo, videinsight and solutionstooneof the oldestproblemsin
 theproposed task srequireextracting the spatiallocationof egocentric human communication ecology, referred to as
 the speakers, their voice activity across the length of the cocktailpartyproblem(CPP).Theessenceof CPPis“How
 video,andthecontentof the irspeech. dowerecognizewhatonepersonissayingwheno the rsare
 Egocentric data presentsseveraluniqueattributesto this speakingat the sametime?” Humanlistenersmustperceptu-
 problem. Firstly, sounds our cesmaybevisible with inall, allyintegrate the simultaneoussoundsoriginating from one
 some,ornoneof the visualframes,dependingon the irmove- person’svoice(e.g., harmonics and speech formants)and
 ment within the scene and the movement of the camera segregatethese from theconcurrentsoundsofo the rtalkers.
 wearer. Secondly,although the camerawe are risnevervisi- Insuchsituations,humansleveragevisualin for mationsuch
 ble(due the headmountedcameradevice)they are clearly asfromlipmovementstobetterunderst and,while the irau-
 audible and in fact often amplified compared to the other ditory system helps with focusing on a particular speaker
 conversationparticipantsduetotheclosenessto the micro- characteristic while ignoring other speech/noise. Recent
 phone that captures the video. Third,naturaldynamicsin workonaudio-visualdiarization[83]andmultimodalsource
 thescene(camerawe are rwalking,running,rapidchanges separation from videoshow that CPP and itsvariations can
 inheadmovementetc.) addsignifi can tblur and distortionto benefit from visualsignals[6,57,79,81,82,171,238].
 51 

 
 
 
 
 
 
 Fur the rmore,humans are prettygoodinunderst and ing annotated audio-based speech activity collection of AVA
 thecontextofaconversationevenwhenwords are incom- 1.0 third-personvideos,andexplicitlylabels 3 background
 prehensible. They are abletofillin the missingdetailsusing noiseconditions,resultinginapproximately 46,000 labeled
 their context knowledge. This can be extended to sound segments spanning 45 hours of data. AVA active speaker
 sources that are non-humansaswell. Foramoredetailed associatesspeakingactivity with avisibleface,resultingin
 accountof CPPpleasereferto[17]. Fullyaddressing CPP 3.65 million frames labeled across approximately 39,000
 requires not only identifying and separating the different facetracks. 
 sounds our cesin the scene,butalsounderst and ing the audi- AVDIAR:[84]Theclosestegocentric data set for audio-
 toryattentionof the camerawearer—ino the rwords,which visual diarization is AVDIAR. It consists of 23 staged se-
 sounds our ceistheuserattendingtoat the moment,orwhich quences,witheachsequencedurationranging from tensec-
 onemaytheuserwanttoattendtoin the nearfuture. ondstothreeminutes(atotalof 27 minutesofvideo). Each
 sequencecomprisesof 1-4 speakerssomestanding and some
 walkingaroundin the visual FOV and havingaconversation.
 H.3 Related Datasets and Benchmarks 
 Thecaptureisdoneviaaheadmountedcaptureonadummy
 EPIC-Kitchens:[42,44]EPIC-Kitchensisamong the most head. 
 widely known ego-centric dataset with first-person view 
 EASYCOM:[53]EASYCOMis are cent data setopen 
 events and annotations. The dataset comprises of multi- sourced for the purposeofboostingegocentricaudio-visual
 faceted,audio-visual,non-scriptedrecordingsinnativeen- learning research with a focus on multi-channel data and
 vironments, i.e. the wearers’ homes, capturing all daily CPP.The data setcorrespondsto 5 hoursofconversational
 activitiesin the kitchenovermultipledays. The data setis content with 3 5 participants in a closed room setting.
 − 
 100 hours,20 Mframes,90 Kactionsin 700 variable-length The content involves playing games, ordering food from
 videos, capturinglong-termunscriptedactivitiesin 45 en- a menu, and a general discussion on a prespecified list of
 vironmentsusinghead-mountedcameras. Annotations are topics. Duringtherecordingof the conversations,restaurant-
 collected using a Pause-and-Talk narration interface. The likenoisewasplayedonloudspeakersin the roomtomimic
 dataset is widely used in action recognition, action detec- arealrestaurantscene. The EASYCOMcapturedeviceuse
 tion, action anticipation, cross-modal retrieval, as well as glasses with 6 micsattachedto the frame. Althoughrichin
 unsuperviseddomainadaptation for actionrecognition. termsofmulti-channelegocentricacousticcontent,thesetup
 Vox Celeb:[40,165]Vox Celeb 1 and 2 compriserecord- isconstrainedintermsofrealism,the data isnotin the wild,
 andmostimportantly the datasetissmall. 
 ings of more than 6 K speakers spanning a wide range of 
 differentethnicities,accents,professions,andages. Thedata 
 Existingaudio-visual data setsvs. Ego 4 D:Oftheseex-
 is non-egocentric and is annotated for active speaker face isting data sets,EPIC-Kitchens,AVDIAR and EASYCOM
 bounding boxes, face tracks, and anonymous person IDs. areegocentric. However,EPIC-Kitchensfocusesonsolitary
 Vox Celeb 2 inparticularisdefined for boostingresearchin activity by the camera wearer, and neither the video nor
 speakerrecognition,anditcontainsoveramillionutterances. annotationsaccommodateaudio-visualconversationtasks
 Videos included in the dataset are shot in a large number requiringmultiplepeople. Although EASYCOMcontains
 of challenging visual and auditory environments. These audio-visual conversation, it is a small dataset containing
 includeinterviews from redcarpets,outdoorstadiums and partlyscriptedconversations that are notin-the-wild. The
 quietindoorstudios,speechesgiventolargeaudiences,ex- participantsin the sessionsalsodonotmovearound. AV-
 cerpts from professionallyshotmultimedia,andevencrude DIARdoesincludesomeparticipantswhomovearound,but
 videosshotonh and-helddevices.Audiosegmentspresentin thecamerawe are risadummyhead and,similarto EASY-
 the data set are degraded with backgroundchatter,laughter, COM,the data isnotin-the-wild(sessionsall are donein
 overlappingspeech and varyingroomacoustics. thesameenvironment/scene). Ego 4 Daccounts for allthese
 Vox Converse:[39]Vox Converseis are latedaudio-visual aspects. Lastly,incontrastto Vox Celeb,Vox Converse and
 AVA,Ego 4 Doffersfirst-personvideo and itsconversation
 diarization dataset consisting of over 50 hours of multi- 
 videostakeplaceincasualdaily-lifeenvironments with mul-
 speaker clips of human speech, extracted from You Tube 
 tiplespeakers. 
 videos. Similarto Vox Celeb,this data isalsonon-egocentric. 
 This data setwasproposedtoboostresearchinspeakerdi- 
 arization for audio-visualinputs.Abulkof the datainstances 
 H.4 Tasks: Definition and Annotations 
 are from politicaldebates and newsanchorssoastocapture 
 conversationalscenarios with overlapping and interrupting Herewedetail the taskdefinitions,thecorrespondingannota-
 speech. tions,and the evaluationmetrics. Weproposeasuiteoftasks
 AVA:[31,192]The AVAspokenactivity data sets are AVA forthe Audio-Visual Diarization(AVD)benchmark. These
 speech and AVA active speaker. AVA speech is a densely tasks are abbreviated as: Localization & Tracking, Active
 52 

 
 
 
 
 
 
 Speaker Detection, Diarization and Transcription. These people in the scene are speaking at a given time [192]. It
 tasks jointly capture who is talking when, to whom, and buildsontopof the previouslocalization and trackingtaskto
 aboutwhatinagivenegocentricconversationalscene. Ob- recognizeeachof the speakerswhosefaceboundingboxes
 serve that the setasks are implicitlytiedtoeachother;each are detected. Hence, this task does not take into account
 subsequent task isdriveninsome for mbyaprevious task speakers who are not visible in the camera’s FOV. Note
 (asfurtherclarifiedin the taskdescriptionsbelow).16 thatactivespeakerdetectionisalsoanimportantaspectof
 speakerdiarization(whichisthenexttaskin the benchmark).
 Task 1: Localization & Tracking: Where is the person 
 Annotations: We provide an anonymous speaker label
 inthevisualfieldofview? Thisfirsttaskin AVDcaptures 
 (e.g.,speaker 1,2 etc.) foreachspeakervisiblein the clip.
 thespatialpositionofalltheprobablespeakersin the scene, 
 The camera wearer is assigned the label C. This is done
 fromthepointofviewof the camerawearer. Thegoalof the 
 byutilizing the faceboundingboxtracksannotations and
 taskistocomputeboundingboxes for them.Unlikeclassical 
 labeling each track one at a time. Hence, each face track
 face detection benchmarks, this task is challenging in the 
 getsassignedoneuniquelabel,andmultipletracks with ina
 sense that thedynamicsof the camerawearer’shead(coming 
 singleclipmaysh are the samelabel(correspondingto the
 fromnaturalconversations)leadstosignifi can tmovementin 
 samespeaker). However,thelabels are clip-specific,i.e.,a
 aspeaker’sapp are ntspatiallocation. 
 speakerwhomaybepresentacrossmultipleclipsdoesnot
 Annotations: Foreachspeakerpresentin the 5 minclip 
 getassignedash are duniquelabelacross the clips. Again,
 a bounding box is provided. Each frame of the video is 
 speakerswho are neverin the visual Fo Varenotassigneda
 annotated for the task. Wefirstutilizedafacedetection and 
 label. 
 tracking model toestimate the seboundingboxes,andthen 
 Evaluation: we use the objectdetectionm APtoquantify
 ateamofhumanannotatorsvalidated and correctedthese 
 the active speaker detection result. This is a frame-wise
 machine-generatedboxestoimproveannotationquality. A 
 metric. Inavideoframe,iftheintersectionoverunion(Io U)
 boundingboxisconsideredavalidhumanannotationifit 
 betweenadetectedfaceboundingbox and the groundtruth
 captures 80% of the speaker’s face; we peform a quality 
 faceboundingboxexceedsapredefinedthreshold,i.e. 0.5,
 checksteuptoensure this. Sidewayslookingfaces are also 
 we have a positive face detection. Each detection has an
 annotated. Note that speakers who are very far from the 
 associated class to indicate whether it corresponds to an
 camerawearer(oftentimesseveralmetersawayin the scene) 
 active speaker. Active speaker detection methods give a
 andwhodonotcomeintoconversationalcontact with the 
 confidencescoreof the activespeakerclass for eachdetected
 wearer are notannotated. 
 faceboundingbox[211]. 
 Evaluation: Recall that thegoalof the taskistolocalize 
 Camera Wearer’s Voice Activity Detection: Note that the
 aswellastrackthespeakersin the scene. Hence the evalua- 
 camerawearer’sfaceisnevervisiblein the camera’sfield
 tionmetricsproposedaccount for the accuracyoftrajectory 
 ofview,andso the ydonot have anyfacetracksassociated
 ofdetectedboundingboxes.Wefollow the standardmultiple 
 withthem. However,inmanycases,they are the dominant
 objecttracking(MOT)metricstoquantify the speakertrack- 
 speakers. Thisismainlybecausethey are driving the inter-
 ingresults. There are manydifferent MOTmetrics,inwhich 
 actionsinmanycases,andsincetheirmouths are the closest
 we are most interested in the MOTA in the CLEARMOT 
 tothemicrophones,theirvoiceisingeneralamplifiedin the
 metrics[19],and IDF 1,IDP,IDRin the Identitymetrics[18]. 
 audio stream compared to other speakers. We propose to
 MOTA,themultipleobtecttrackingaccuracy,isacombined 
 alsoconsiderthemasactivespeakers and detect the irvoice.
 metricoffalsealarms,falsepositives and identityswitches. 
 we use the objectclassificationm APtoquantify the result
 MOTAisbased onmatching thetracking results with the 
 ofthecamerawearer’svoiceactivitydetection.
 ground truth at frame level, while the IDP (ID precision), 
 IDR(IDRecall)and IDF 1(IDF 1 score)are base don the Task 3: Diarization: Who spoke when? This next task
 tracking result to ground truth matching at the trajectory further expands on the temporal aspect of active speaker
 level. IDmetricsgiveatracker’sper for manceonmaintain- detection(from the previous task). Given the setofspeakers
 ingcorrectidentification for eachtarget. andtheirspatiallocalizationin the visualfieldofview,this
 Task 2: Active Speaker Detection: Who is speaking? task aims to capture the voice activity of the speakers. It
 is identical to speaker diarization, a well studied research
 Thenexttaskin AVDistodetect the activespeakerin the 
 problem in the speech and audio domains [10,177] and
 scene. This task is in principle similar to active speaker 
 answers the question, “who spoke when”. While speech
 detection—wherethegoalistodetectwhichof the visible 
 from speakers that overlap with each other is one of the
 16 Note that althoughspeechtranscription and sourcelocalization are biggestissuestosolvein this task,theegocentricperspective
 distinct from audio-onlyspeakerdiarization—allofwhich are welldefined 
 addsmorecomplexityintermsofheadmotions and other
 researchparadigmsinmainstreamaudio,speech and visioncommunity— 
 dynamicsassociated with naturalconversations. Note that
 wecumulativelyrefertoallthesetogetherunder the umbrellaofaudio- 
 visualdiarization for Ego 4 D. theoutputsofactivespeakerdetection(theearliertaskin the
 53 

 
 
 
 
 
 
 benchmark)alsodrive this task. arenotthesameas the onesusedindiarizationbecausewe
 Annotations: Foreveryactivespeakerlabel(where the separatelyannotated the overlappingregionsheretoreduce
 annotations are from the previous Active Speaker Detection transcriptionerrors and account for speakerstalkinginlow
 task), a human annotator marks the start and end time of volume. This allows us to also distinguish voice activity
 thatpersonspeaking. Weaccount for overlappingspeech fromspeechactivity. Inaddition,theuseoftime-segmented
 segmentswheremultiplespeakerstalkovereachother,but transcriptionsisalsoslightlydifferentfromst and ard ASR
 we ignore speech not relevant to the conversation such as datasetsinspeechcommunitywhichmainly have text and
 backgroundspeechfroma TVorspeechfur the raway from notimestamps. 
 the camera wearer. Note that speech segments from the Evaluation: We utilize the Word Error Rate (WER), a
 camerawearer are alsoannotated. Theannotatorsrelyboth standard ASRmetric,forevaluating this task[114].First,the
 ontheaudio and thevisualstream for creating the selabels. minimumeditor Levenshteindistanceiscomputedbetween
 Evaluation: Diarizationerrorrate(DER)isthedefacto the reference and hypo the sized transcription. WER then
 evaluationmetric for speakerdiarization[11],anditiswell measurestheratioof the numberofwordsubstitutions(S),
 studiedin the audio and speechprocessingcommunity. DER deletions(D)andinsertions(I),i.e.thetotalnumberofedits
 measures the fractionoftotaltime(inagivenclip)thatis necessarytoconvertthehypo the sizedtranscriptioninto the
 notattributedcorrectlytoaspeakerortonon-speech. Itis referencerelativeto the totalnumberofwords(N )inthe
 w 
 definedasfollows: reference: 
 DER(%)=(E +E +E ) 100, (21) S+D+I 
 miss fa spk 
 × WER(%)= 100. (22) 
 N × 
 where E denotes the fractionoftime that has been pre- w 
 miss 
 dictedtobenon-speechwhile that segmentisattributedto 
 H.5 Data Statistics 
 aspeakerin the reference. E denotes the fractionoftime 
 fa 
 thathas been predictedtobeassociated with aspeaker,but 
 From across the 3,670 hours of video in Ego 4 D, approxi-
 isactuallylabelledasnon-speechin the reference,and E 
 spk mately 764 hours of data contains conversational content,
 denotes the fractionoftimewherespeechisassociated with 
 and are directlyrelevant for the AVDand Socialbenchmarks.
 thewrongspeaker. Allerrors are computedasafractionof 
 Pleasereferto Section I.5 foracompletedescriptionof the
 thetotalamountofspeech. 
 experimental design and scenarios used in these sessions.
 Task 4: Transcription: Whatdid the speakersay? The From this set,arandomlychosensubsetof 572 clips(each 5
 finaltaskof AVDistotranscribe the speechofeachspeaker, minuteslong)areannotated for thisfirstversionrelease. Of
 i.e.,per for ming ASR.Similarto the diarization task,someof these 572 clips,389 clips are marked for training,50 clips
 thechallengesassociated with the transcription task include forvalidation,andtheremainderis the testingset.
 overlapping speech and environmental noise. In addition, Table 19 and Figure 43 summarize statistics about the
 thecamerawearer’sheadmovementresultsinasignificant speakercontent from across the seclips. Observe the long
 change of the audio volume of the speech recorded from tails of mean and maximum number of speakers in the
 others. dataset. Wenote that inthefirstversionof the dat are lease,
 Annotations: Since the clipscontainmultiplespeakers due to the fact that the total number of clips is relatively
 with overlapping speech segments and with different vol- small, the test and/or validation batches may be biased in
 umes,thefinaltranscriptions are obtainedinmultiplepasses. termsofchangesinspeakers’accents,changesinvocabulary
 Inthefirstpass,initialhumanannotations base donvoice usage(since the participants are fromdifferentculturalback-
 segments are merged with automati can notations for regions grounds from across the world),andingeneralchangesin
 withlowvolume. Inasubsequentpass,humanannotators natureofinteractivenessbetweenspeakersinascene. There
 hadtocorrect and assignsegmentsoftranscriptionsto the ismarginaldistributionalshiftamong the training,testing
 corresponding voice activity segments per speaker while andvalidationsplits. Thisismainlybecauseof the smaller
 also annotating overlapping speech. Note that annotators numberofannotationsin this versionof AVDfor Ego 4 D.
 hadboth the audio and videoavailable for annotation and, Weexpect the sedistributionalshiftstobelesssignifi can tin
 besidesspokenwords,theoccurrenceofo the rartifactssuch futurereleases and asmore data will beannotated.
 asunintelligiblespeechorincompletewords have also been 
 annotated. Thefinaltranscriptionannotations for aclipcon- 
 H.6 Baseline Modeling Framework 
 sistofasequenceofsegmentslabeled with begintime,end 
 time, transcript and speaker IDwithin the clip. Inevalua- Recall that the 4-part task sin this benchmark are tiedtoeach
 tions,weapplied ASRto the sesegmentsindividually and other,inthesense that representationslearned from onetask
 computedtheper for manceoverallof the sesegments.Please mayberelevant for the others. Tothatend, weproposea
 note that thetimesegmentsassociated with the transcripts baselinelearningframework that addresseseach task ina
 54 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 43. AVDiarization data statistics.Mean and maximumnumberspeakersin FOV,andnumberspeakersperclip.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 44. AVDiarizationbenchmarkannotationssummary.Thef our tasks are annotatedinasequentialfashion,starting with localization
 andtrackingofspeakers,activespeakerdetectionlabels,diarizationtimestamps,andfinallytranscriptions. Thefigureshows the face
 detections(highlightedbyboundingboxes),speakerdetection(shownby the anonymousperson IDs 1,2,etc.),activespeaker(highlightedin
 green)andvoiceactivity(shownbelowingreenhighlightedtimesegments).Speakersin the visual FOVwhoarenottalking are highlighted
 indottedredboxes.Theclipsused for AVD(and Social Interaction)haveconsent from participantstoleave the irfacesunblurred.
 
 Statistic(Avg.) Value sequentialfashion. Theframeworkincludes the following
 Speakersperclip 4.71 steps: 
 Speakersperframe 0.74 
 • Wefirstdetectpeople’sheads and doshorttermtrack-
 Speakingtimeinclip 219.81 sec 
 ingin the video. Theshorttermtrackerfollowseach
 Speakingtimeperpersoninclip 43.29 sec 
 detectedheadbyexp and ingasetoftrajectoriesbased
 Camerawe are rspeakingtime 77.64 sec 
 ontheirpositions,sizes and theappearanceof the per-
 son. Thetrajectoriesmayendwhenocclusionhappens
 Table 19. AVDData Statistics. 
 orwhenthetrackedpersongoesoutof the fieldofview.
 Newtrajectories can alsobeaddedto the trajectoryset.
 • Theshorttermtracker’strajectory for eachpersonis
 55 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 45. Exampleannotationsshowing the facedetections(highlightedbyboundingboxes),speakerdetection(shownby the anonymous
 person IDs 1,2,etc.),activespeaker(highlightedinred)andvoiceactivity(shownbelowinbluehighlightedtimesegments).Asillustrated
 here,thedata for AVDincludespeoplewalkingaround and talking,sitting and playinggamesetc.Theclipsused for AVD have consent
 fromparticipantstoleave the irfacesunblurred. 
 
 oftenfragmentedintomultipleparts. Hence,wethen across all segments. Evaluating the system by using
 optimizethegroupingof the trackletsinsteponeso that ano the rsegmentationmethodischallengingespecially
 thetrajectoriesofeachperson can belinkedtogether. in the case of overlapping speech segments. Jointly
 We formulate the problem as a constrained combina- modeling time segments and transcriptions will be a
 torialoptimizationproblem. Integerprogramming can challengingproblem(aswediscussin Section H.7).
 beusedtosolve the problemdirectlybutithasexpo- 
 Wedescribefurtherdetailsabouteachof the sestepsbe-
 nentialcomplexity. Forefficiency,wedevelopagreedy 
 low,and Tables 20–29 summarize the resultingper for mance
 approach which is much faster and still gives strong 
 metrics for the tasks. 
 results. 
 • Wethenclassifyeachperson/headineachvideoframe Audio Only Models for Speaker Diarization The prob-
 as an active speaker or not. Based on the classifica- lemofspeakerdiarization from audiohas been studiedtoa
 tion result and the corresponding detected long-term considerableextentin the fieldofspeechprocessing[10,177].
 trajectories, we further associate the audio/speech to For the audio-only baseline system, the VBx diarization
 eachpersonin the video. we use this preliminarylistof approach has been utilized [128] for having shown supe-
 audiofeatureembeddingstofur the rextract and match rior results on different types of datasets such as CALL-
 un-associatedaudiosegmentstospeakerlabels. HOME[3](telephoneconversations),AMI[28](meetings)
 • Wethenusetwomethodstodetect the camerawearer’s and DIHARDII[59](myriadofdomainsrangingfromau-
 voiceactivity. Thefirstmethoduseshighenergyaudio diobooksto You Tubevideos). Thismethodrequiresspeech
 segment in the clip (under the assumption that their activityregions and the sewereobtainedusing the ASp IRE
 voicehasnaturalamplificationcomp are dto the remain- model base donatimedelayneuralnetwork(TDNN)with
 ingspeakers). Thesecondmethodisadeepclassifier statisticspooling,available with the Kaldispeechrecogni-
 thatpredictswhether the wearerisspeaking. tiontoolkit[181]. Wereferto this ask VAD(the Kaldi VAD
 • Lastly,weapplied ASRto the speechregions base don model). Althoughthisk VADhas been trainedonslightly
 thegroundtruthsegmentation and evaluated the WER different data(telephoneconversations),andthusdoesnot
 56 

 
 
 
 
 
 
 provide the bestpossibleresults,ithas been chosen for the 0,itisremoved from the trajectoryset.
 baselinesystembecauseofitsgeneralavailability. Thekeycomponentof the short-termtrackerismatching
 Thespeechactivityregions are uni for mlysegmentedto trajectories to the candidate head bounding boxes in each
 obtainshortersegments and speakerembeddings(so-called frame. This can beformulatedas the followingoptimization
 x-vectors[206])areextractedonepersubsegment. Thex- problem: 
 vectors are obtainedwitha Res Net 101 extractor[96]trained 
 toproducespeaker-discriminativeembeddings. Theinput (cid:88) 
 min c x (23) 
 tothenetwork are log Mel-filterbankfeaturesevery 10 ms, i,j i,j 
 and given a segment of speech, it computes a single 256 (i,j) 
 dimensionalvector that represents the wholesegment. The s.t. x i,j formsamax-matching,
 informationof the wholesegmentisaggregated with asta- x =0, if(i,j) E, 
 i,j 
 tisticalpoolinglayerwhichcomputes the meanandst and ard ∈ 
 x =0,1, 
 i,j 
 deviation of activations over the time domain. A linear 
 trans for mationisthenusedtoreduce the dimensionalityto wherex is 1 iftrajectoryimatches can didateheadboxj
 i,j 
 256. Thetraining data consistedof Vox Celeb 1[165],Vox- and 0 otherwise. E isasetinwhich the pairsoftrajectory
 Celeb 2[40]and CN-CELEB[64]together,totalling 2877 andc and idate can notmatcheachother, examplesinclude
 hoursofspeech from 8178 speakers. casessuchas the can didateistoofaraway,thesizeistoo
 Thex-vectors are initiallyclusteredtoafewdozensof differentor the appearancedoesnotmatch. c isthecostof
 i,j 
 classes using agglomerative hierarchical clustering. This matchingtrajectoryi and can didateheaddetectionj. This
 initial clustering is fed as initialization to a Bayesian hid- costofmatching,c ,iscomputedasalinearcombination
 i,j 
 den Markov model whichestimatesaltogether the number ofthenormalizedboundingboxdistances and the difference
 of speakers in the recording as well as the assignment of oftheappearancefeatures. Thenormalizedboundingbox
 x-vectorsto the states. Eachstatein the modelcorresponds distance is defined as the ratio of the Euclidean distance
 toonespeaker and the probabilityofobservingaparticular between the two corners of the last bounding box in the
 x-vectorinaparticularstate can beinterpretedas the cor- trajectory and thedetectedheadboundingboxin the image
 respondingspeakerproducing the correspondingsegment tothesizeof the detectedboundingbox.Eachtrajectoryalso
 ofspeech. Themostrelevanthyperparametersof the model maintains a feature vector to characterize the most recent
 werefine-tunedtoobtain the best DERper for manceon the appearance of the tracked person. This feature vector is
 Ego 4 Dvalidationset. The VBximplementationpublished obtained from a feature embedding network trained on a
 by Brno Universityof Technologyispubliclyavailableas largepersonhead data set. 
 wellas the trainingrecipepublishedby Phonexia Research. Thisoptimizationproblem can besolvedefficientlyus-
 ingthe Hungarianalgorithmor the primaldualalgorithm.
 Short-term People Tracking Thegoalhereistotrackpeo- 
 Due to the imperfect features, the optimization may have
 ple’sfaces. However,ourmethod can alsobeusedtotrack 
 an identity switching problem if two targets cross paths.
 thewholebodyofeachperson. Theshort-termtrackermain- 
 Tosolve the problem,weenforce the longertrajectoriesto
 tains a set of trajectories. The trajectories include the at- 
 havehigherprioritytomatch. we useatwo-stepmatching
 tributes such as the person-ID, the frames tracked, a life 
 scheme. Wefirstmatchall the trajectories that are longer
 counter, the appearance features and the positions of the 
 thanaspecificthresholdchosenempirically. Oncedone,we
 tracked bounding boxes. Throughout, we use the term 
 thenmatch the shortertrajectories. Thisschemenaturally
 “person-ID” to refer to an anonmyous tag for a person in 
 giveshigherprioritytolongertrajectories,therebyreducing
 thevideo(person 1,person 2,etc.);noactualidentities are 
 mismatchesamongthem. Thisismorerobustthanasingle
 available in the data, and the benchmark does not aim to 
 stagematchingwherealltrajectoriesareh and ledtogether.
 per for manypersonidentification. There are twokindsof 
 Inourimplementation,thepersondetectorisa Yolo-V 3
 trajectories. Ifatrajectory’strackedframes are lessthana 
 detector[187]whichdetects the head and personbounding
 threshold,e.g.5,itisinprobation and isnotcountedasareal 
 boxsimultaneously. Thedetectoristrainedonimages from
 trajectoryeventhoughwemaintainall the information for 
 the Google Open Image data set[123]andafisheyeimage
 them.Whenatrajectory’strackedframes are greaterthan the 
 dataset[73]. we use the detectedheadboundingboxes for
 threshold,itbecomes are altrajectory. Eachtrajectoryalso 
 people tracking. The person head appearance’s feature is
 hasalifespan.Thelifeofa new trajectorystarts from afixed 
 extracted using the person embedding network, which is
 value. Thelifeofatrajectoryisrestoredtoafixedmaximum 
 trainedon the Vox Celeb 2 datasetusing the tripletloss. The
 value,suchas 10,ifthetrajectoryismatchedtoa can didate 
 networkhas the structureofa Res Net-18. 
 personheadboundingboxes. Otherwise,thetrajectorygoes 
 into a maintenance mode and its life decreases by 1 each Long-term Trackingby Trajectory Matching Theshort
 timeitfailstofindamatch. Ifthelifeofatrajectorygoesto termtrackergeneratesfragmentedpersontrajectories. Ifa
 57 

 
 
 
 
 
 
 personisoccludedorgoesoutof the fieldofview and reap- Metric Valid Test 
 pears,itwillreceivea new ID.Thefragmentedtrajectories MOTA 74.52 71.94 
 arereferredtoastracklets. Weneedtogroup the tracklets MOTP 79.07 79.17 
 throughoutthewholevideotogenerate the finaltrajectories IDF 1 84.92 80.07 
 foreachperson. Thegroupingproblem can beformulated IDR 80.40 73.52 
 asfollows: IDP 89.97 87.90 
 (cid:88) Table 20. Localization and tracking base linemetricson the valida-
 min D y (24) 
 m,n m,n tion and the testsetsrespectively. 
 m,n 
 s.t. y =y , m,n, 
 m,n n,m 
 ∀ 
 would probably never occur and the greedy result would
 y +y 1+y , m,n, 
 m,k k,n m,n 
 ≤ ∀ approach the globallyoptimalsolution. 
 y =0,ifm and noverlapintimeor D >g, 
 m,n m,n Table 20 summarizes the trackingmetrics MOTA,MOTP,
 y m,n isbinary, IDF 1,IDR,and IDPon the validation and testsets.
 wherey =1 iftrackletmandn can begroupedtogether Active Speaker Detection: we usetwoapproachesforac-
 m,n 
 ando the rwisey = 0. D istheappearancedistance tive speaker detection. One approach is based on mouth
 m,n m,n 
 between the trackeletm and nandg isathreshold. Here region classification, and the second method is a trans-
 D =min f f 2,where T isthesetof former based audio-visual method for active speaker de-
 m,n {i∈Tm,j∈Tn} 
 || 
 i 
 − 
 j 
 || 
 i 
 personheadboxesintrackletiandf isthecorresponding tection[211]. 
 i 
 featureembedding. Theconstraintsrequire the groupingto Region Cls: Ourfirstapproachis base don the classification
 bereflective: iftrackletmandn can begroupedtogether ofmouthregions. Itfirstcomputes the 3 Dheadorientation
 socannandm, transitive: ifmandk can begroupedto- using a regression network. In our implementation, the z
 gether and so can k and n, then m and n can be grouped direction is into the image; if the head 3 D orientation z
 together. Twotracklets can notbegroupedtoge the rifthey coordinateon the unitsphereisgreaterthan 0.3,weassume
 havetimeoverlapor the irdistanceisgreaterthanathreshold the face is away from the camera. If the face is facing
 g. Theoptimization can besolvedusingintegerprogram- away from the camera,weignoretheimage and the active
 ming. However, this method has exponential complexity. speakerdetectionresultissettonull. Forfaceslookingat
 Weproposeafastgreedyalgorithmtosolve the problem. thecamera,ourmethodfirstregresses the facialkeypoints
 Thegreedyalgorithmstartsbytreatingeachinitialtrack- usingtheimagewithin the person’sheadboundingbox. We
 let as a trajectory and progressively groups two trajecto- usethemouthkeypointstocropout the mouthimage. The
 ries with the closest Duntilnotrajectories can begrouped croppedmouthimageis the nsenttoaclassificationnetwork
 together. Since the distance between two trajectories can toclassifywhether the speakeristalkingornot.
 becomputedbyfinding the minimumofall the“element” Note that we also explored using multiple images,
 trackletpairdistances,themergingprocedureisefficientif wherein we stack a short sequence of cropped mouth im-
 wepre-compute and cache the elementpairdistance. This agesinatimeinterval for activespeakerclassification. Our
 greedyapproachgivesstrongresultswhilemaintaininglow experimentsshow the multiplemouthimagesinputdonot
 complexity. signifi can tlyimprove the result. Thisisprobablydueto the
 The algorithm reduces to the minimum spanning tree fastmovementof the camera and sometimesdifficultangles
 methodif the reisconflictbetweeneachpairoftrajectories. oftheface. Thiscausesinaccuratecroppedmouthregions.
 However,ifthere are time-conflictingtracklets,thereisno Talk Net:[211]Talk Netisanend-to-endpipeline that takes
 guaranteethegreedyalgorithmgives the globallyoptimal the cropped face video and corresponding audio as input,
 solution. Weillustrate the methodthroughasimpleexample: anddecidesif the personisspeakingineachvideoframe. It
 Assume the rearetrackelets T ,T ,T ,T ,T and T have consistsofafeaturerepresentationfrontend and aspeaker
 1 2 3 4 1 2 
 { } 
 timeconflict, and T and T havetimeconflict. D(T ,T ) detectionbackendclassifier,asillustratedin Figure 46. The
 3 4 1 3 
 = 10, D(T ,T ) = 1, D(T ,T ) = 3 and D(T ,T ) = 4. We frontend contains an audio temporal encoder and a video
 2 4 1 4 2 3 
 assume g = 20. Using the proposed greedy method, the temporalencoder. Theyencode the frame-basedinputaudio
 solution P is T ,T , T ,T whose overall cost is 11. andvideosignalsinto the timesequenceofaudio and video
 2 4 1 3 
 {{ }{ }} 
 However,theoptimalsolutionis T ,T , T ,T whose embeddings, representingtemporalcontext. Thebackend
 1 4 2 3 
 {{ }{ }} 
 overall cost is 7. Even though the greedy method does classifierconsistsofaninter-modalitycross-attentionmech-
 not guarantee the global optimal solution, empirically we anismtodynamicallyalignaudio and visualcontent,anda
 observe that the proposed method give strong results. In self-attentionmechanismtoobservespeakingactivities from
 fact,ifthepersonembeddingisaccurate,thesecornercases thetemporalcontextat the utterancelevel.
 58 

 
 
 
 
 
 
 Algorithm 1 Greedy Tracklet Grouping 
 Initializesets P= S ,S ,...,S ,where S = T ,T isthetrackleti and N isthenumberoftracklets.
 1 2 N i i i 
 { } { } 
 for(m,n),m=1..Nandn=1..Ndo 
 compute D(m,n) 
 endfor 
 while Truedo 
 for (S ,S ),S P and S P,and(S ,S )donot have timeconflictdo 
 m n m n m n 
 ∈ ∈ 
 compute F(S ,S )=min D(a,b) 
 m n Ta∈Sn,Tb∈Sm 
 endfor 
 (m∗,n∗)=argmin(F(S ,S )) 
 m n 
 if(m∗,n∗)isemptyor F(S m∗,S n∗)>gthenbreak 
 endif 
 S m∗ =S m∗ S n∗ and P.pop(S n∗) 
 ∪ 
 endwhile 
 Pincludes the groundedtrajectories 
 
 Figure 46. Talk Net:Anaudio-visualtemporalnetwork for detecting and tracking the activespeakerinavideo[211].Figureis from[211].
 
 Tables 21, 22, 23 and 24 summarize the resulting per- Model m AP@0.5 
 formance. Foreachof the twoproposed base linemodels, Reg Clsw/osmoothing 29.65 
 wereportper for mancesummaries with pretraining base don Reg Cls+max-filtering 32.77
 AVA andalso model strainedusingonlyvideos from the Reg Cls+max-filtering+s VAD 34.35
 Ego 4 Dtraining data set. Note that the video-onlyapproach Talk Net 50.90 
 can becombined with anyvoiceactivitydetectiontoremove Talk Net+s VAD 49.66 
 falsealarms. Herewe usesuchanalgorithm from[203],and 
 wereferto this ass VADThis can greatlyimprove the active Table 22. Activespeakerdetection base linemetricson the testset
 speakerdetectionresults. Themax-filteringhasawindow usingtrainingvideosin the Ego 4 Ddataset.
 sizeof 11. Talk Netalsohasabuilt-insmoothnessfiltering 
 topost-process the rawclassificationresult. 
 Model m AP@0.5 
 Reg Clsw/osmoothing 22.09 
 Model m AP@0.5 Reg Cls+max-filtering 22.88 
 Reg Clsw/osmoothing 29.68 Reg Cls+max-filtering+s VAD 25.53 
 Reg Cls+max-filtering 31.95 Talk Net 34.36 
 Reg Cls+max-filtering+s VAD 33.72 Talk Net+s VAD 34.65 
 Talk Net 34.75 Always Speak 20.94 
 Talk Net+s VAD 34.56 
 Always Speak 24.46 Table 23. Activespeakerdetection base linemetricson the valida-
 tionset with modelstrainedon AVA data set.In Always Speak,all
 thedetectedfaces are classifiedasactivespeakers.
 Table 21. Activespeakerdetection base linemetricson the test 
 set with pre-trainingusing AVA.In Always Speak,all the detected 
 faces are classifiedasactivespeakers. 
 Matching Speakers Outside Fo V: Based on the tracked
 heads and the activespeakerdetectionresults,we canasso-
 59 
 

 
 
 
 
 
 
 Model m AP@0.5 wearer’s voice often has higher energy than other partici-
 Reg Clsw/osmoothing 20.33 pant’svoices. we use this heuristictoextract can didatesof
 Reg Cls+max-filtering 21.93 thewearer’svoicebychoosingportionsofaudiowi the nergy
 Reg Cls+max-filtering+s VAD 24.60 higher than certain threshold. Since different recordings
 Talk Net 51.04 have different levels of loudness, we normalize the audio
 Talk Net+s VAD 50.58 using the maximum energy and then choose the possible
 wearer’s voice using a fixed percentage of the maximum
 Table 24. Activespeakerdetection base linemetricson the valida- energy. This threshold percentage is set to be as high as
 tionsetusingtrainingvideosin the Ego 4 Ddataset. possibletoavoidfalsealarms. Once the can didateaudiois
 selected,we use the sameaudiomatchingmethoddescribed
 intheprevioussectiontofindall the audio that belongsto
 ciatetheaudiotothevisiblepeoplein the scene. However, thecamerawearer. Thissimplemethodworksreasonably
 thisisstillnotcompletebecause the rearecasesinwhich the wellassummarizedin Table 25. Theapproachfailswhen
 speakerisoutsideof the visualfieldofview. Tosolve this thewe are rnevertalksortalksinaverylowvoice,andin
 problem,wefirstcreateanaudio-signature for eachvisible general the baseli new orksbetter for nearrangemicrophones
 personin the video. thanlongrangemicrophones. 
 Weextractonesecondofaudiocenteredateachvideo Method II: In the second method, we directly classify
 frametimeinstant. Iftheaudiocorrespondstoaspeaking the audio at each time instant to two categories: wearer’s
 head in the image, we compute the audio embedding of voiceornotwearer’svoice. Thelogarithmmagnitudeof the
 theonesecondaudio and insertthefeatureinto the audio spectrogramat 40 mswindowis the input. Thenetworkisa
 signaturelibraryof the person.Theaudioembeddings can be modified Res Net. Thenetworkistrainedon the Ego 4 d AV
 obtained from anyspeechrepresentationlearningmethods. training data setusingast and ardcross-entropyloss.
 Weexploredseveral model sincludingamodified Res Net 18 we useclassificationm APtoquantify the weareraudio
 whichtakesaudiospectrogramlogarithmmagnitudeinone- activity detection result. We report the average m AP on
 secondwindowsas the input and trainedon the Vox Celeb 2 both the testvideos and validationvideosin Table 25.
 datasetusingtripletloss,andaversionofwav 2 vec 2.0[15]— 
 aself-supervisedapproachtospeechrepresentationlearning. Model Valid Test 
 We parse the video and find instants when a particular Method I 43.95 50.61 
 person is not in the video frame and match the audio em- Method II 72.00 74.29
 bedding to the person’s audio signature library. We find Always Speak 21.30 26.09
 the minimum distance of this audio embedding to all the 
 signatureaudioembeddingsin the library. Ifthedistanceis 
 Table 25. Camerawe are ractivitydetection base linemetrics(m AP)
 lessthanapredefinedthreshold,weclassify the personas onthevalidation and testsetsrespectively.Always Speakassigns
 speakingando the rwisenot. Note that the audioembedding that the wearerspeakingineachvideoframe.
 isusedonlywithin the same 5 minutevideoclip and never 
 acrossvideoclips. Person IDs are alwaysanonymoustags 
 (person 1,2,etc.). Speaker Diarization Tables 26,27 and 28 summarize the
 we use this methodtodetectall the backgroundaudioof speaker diarization DER metrics for the baseline models
 thepeopleofinterestwhen the yarenotvisible. Thismethod proposedin the earliersections. Wereport the results with
 assumes that the activespeakerisperfect. Inreality,active trainingonlyon Ego 4 ddataaswellason with trainingon
 speakergivesnoisyresults. Thiswouldcauseo the rpeople’s existingdiarization data sets. Note that the audio-only DER
 voicefeaturetobeincludedinaperson’ssignaturelibrary isaggregatedwhile the audio-visual DERisaveraged. Also
 andaffect the finalaudioclassificationresult. note the impactof the VADon the diarizationper for mance
 with the audio-only base line. Itshould benoted that amodel
 Tracking Camera Wearer’s Audio: Thecamerawe are ris 
 more tailored to Ego 4 D-like data could be used to obtain
 a special participant because their face is invisible in the 
 better per for mance. Never the less, this aspect still poses
 egocentricvideos. Theactivespeakerdetectionmethodthus 
 challengeson the AVDbenchmark. 
 cannotbeusedtoassociatethewe are rwith the irvoice. We 
 usetwomethodstodetect the camerawearer’svoice. Transcription To obtain baseline transcriptions, we used
 Method I:Thefirstmethodusesenergyfilteringfollowed thepre-trained Gigaspeech model providedin the ESPNet
 byaudiomatching. Thismethoddoesnotneedgroundtruth model zoo [1]. This model is trained on the Gigaspeech
 labelingof the camerawearer’svoiceactivities. Since the dataset[34]whichcontains 10000 hoursofspeech. Input
 microphoneofthecameraisusuallycloserto the wearer’s featuresto the model are logmelfeaturesaugmentedusing
 mouththanothersubjectsin the scene,theamplitudeof the the Spec Augmentmethod[173]andnormalizedbyglobal
 60 

 
 
 
 
 
 
 Model trained s VAD DER[%] language model isused. Fordecoding,we used CTCweight
 on Ego 4 D of 0.3 andbeamsize 20 whichwedidnotfine-tuneon the
 Region Cls no no 84.79 Ego 4 D dataset. The pre-trained model obtained from [1]
 Region Cls no yes 83.88 cannotsupport 5-minvideos,hence,we usedoraclesegment
 Talk Net no no 86.68 information from the transcriptionannotationstosegment
 Talk Net no yes 85.85 thedata and wedecodedeachsegmentseparately. Thefinal
 Region Cls yes,only no 80.52 WERisobtainedbycounting the totalnumberoferrorsover
 Region Cls yes,only yes 80.17 thewholevalidationortestset. 
 Talk Net yes,only no 73.14 In Table 29,wesummarize the WERresultsdepending
 Talk Net yes,only yes 73.32 onthe VADsegmentationmethodonbothvalidation and test
 Always Speak - - >100 sets. Tocompute the final WER,we 1)removedpunctuation
 Never Talk - - 100 fromboth the reference and the ASRhypo the sis,2)allowed
 soft-matchoncontractionssuchas(Iwillvs. I’ll)using the
 Table 26. Diarization Baseline Metricsshowing DERon the testset. Englishglobalmappingfile from Kaldirepository[2],and 3)
 In Always Speak,all the detectedpeople are labeledas”speaking” used the NISTsclitetool[72]. Aswe cansee from Table 29,
 ineachvideoframe. In Never Talk, all the detectedpeople are onboth the test and validationsets,the WERs are quitehigh.
 labeledas”notspeaking”ineachvideoframe. Thisshows that the datasetischallenging for anoff-the-shelf
 ASR model becauseofoverlappingspeech,noise,different
 volumelevels for differentspeakers,occasional for eignword
 Model trained s VAD DER[%] 
 usage,etc. 
 on Ego 4 D 
 Region Cls no no 98.82 
 Region Cls no yes 90.98 Speech Segments Valid Test 
 Talk Net no no 99.73 
 Ground Truth 64.8 59.2 
 Talk Net no yes 92.14 
 Region Cls yes,only no 81.66 
 Table 29. ASRtranscription WERs(%)onthevalidation and test
 Region Cls yes,only yes 79.97 
 datausing the referencespeechsegmentation. 
 Talk Net yes,only no 80.58 
 Talk Net yes,only yes 79.30 
 Always Speak - - >100 
 H.7 Discussion 
 Never Talk - - 100 
 Although AVdiarizationpresentsa task suitecomposedof
 Table 27. Diarization base linemetricsshowing DERon the val- reasonably well understood tasks from the vision, speech
 idationset. In Always Speak,all the detectedpeople are labeled 
 andaudiocommunities,our base lineresultsclearlysuggest
 as”speaking”ineachvideoframe.In Never Talk,all the detected 
 thatefficientspeakerlocalization,tracking,diarization and
 people are labeledas”notspeaking”ineachvideoframe. 
 transcriptionisarathercomplexproblemin the egocentric
 perspective and within-the-wild data. Thisisspecifically
 Typeof VAD Valid Test evident from theper for manceof the jointaudio and video
 drivendiarization and transcription base lines(with DERof
 k VAD 67.24 65.28 
 > 80%and WERof> 60%). Overlappingspeechmakes
 Ref. Activity 36.56 39.99 
 both these tasks particularly difficult to annotate as well
 asevaluateanyproposedmodels. Per for mingsomeaudio-
 Table 28. Diarizationper for mance with audio-onlymodels for 
 visuals our ceseparationpriorto the setasksmayimprove the
 validation and testsetsusingk VAD and reference(groundtruth) 
 voiceactivityannotations. efficacy,never the lesssensitivitytochanges and difference
 inspeechamplitudesofoverlappingspeakerswouldstillbe
 challengingtoaddress. 
 mean-variancenormalization. Theencoderof the acoustic Novelcross-modallearningapproaches that jointly model
 model is based on macaron-style con for mer [93] with 12 audio and visual modalities while accounting for such at-
 blocks and 8 attentionheads and the decoderis base dona tributes (overlapping speakers, interruptions, noise in the
 6-layertrans for mer[217]with 8 attentionheads. Inboth the wildetc.) areneededtofurtherimprove the seper for mances.
 encoder and decoder,linearlayers have 2048 units and the The base lineframeworkweutilizedherealsodoesnotac-
 encoderoutputis 512 dimensional. Thedecoderoutputhas countforefficientin for mationsharingacross the fourtasks
 5000 sentencepiece[122]units. The model istrainedusing inthebenchmark. Specifically,therelationshipbetweenro-
 ajoint CTC and attentionobjective[112]. Fordecoding,no bustlocalization and tracking with multi-speakerdiarization
 61 

 
 
 
 
 
 
 is not studied, and this is also not well understood in the thebenchmark for mulation and writing.
 literature. Weexpect this tobeachallengingproblem. 
 Wealsoobserved that subjectiveattributesinconversa- 
 tions, like speaker accents, changes in vocabulary usage 
 basedonculturaldifferencesetc.,influenceboth the content 
 ofthespeech and the clarity with whichit can becaptured 
 in human annotations. The camera wearer’s head motion 
 addssignifi can tblurtospeakers’faces. Toaccount for such 
 aspectsweper for medqualitychecksonhumanannotations, 
 andweexpectnovelunsupervised and self-supervisedlearn- 
 ing will helpfur the raddresssuchsubjectiveattributes. 
 In future versions, we expect to increase the scope of 
 the task suite (i.e., proposing new tasks and annotations), 
 thereby open ing new avenues for bothcoremachinelearning 
 infirstpersonperspective,andalso for robustmulti-modal 
 representationlearning. Wecouldalsoinvestigateresearch 
 directionsfocusedonspatialaudiobycreating 3 Denviron- 
 ments coupled with Sound Spaces [32]. This enables new 
 research and tasks in audio-visual sound source localiza- 
 tion,audio-visualdirection-of-arrivalestimation and related 
 immersive reality applications. We note that a small frac- 
 tionof our data setdoescompriseofbinauralaudiocaptured 
 using in-ear microphones and an audio recorder (Tascam, 
 Appendix A). 
 H.8 Contributionsstatement 
 Vamsi Krishna Ithapu co-led the audio-visual diarization 
 benchmarkworkstream,thecorresponding task sdefinition, 
 data selection methodology, data annotation tooling and 
 guidelines and writing. Christian Fuegenco-lead the audio- 
 visualbenchmarkworkstream,thediarization and transcrip- 
 tion task sdefinition,thecorrespondingannotationguidelines 
 and paper writing. Hao Jiang worked on data annotation 
 tooling,tasksdefinition for localization and tracking,active 
 speakerdetection and diarization;alsoworkedonbuilding 
 the baseline models for these tasks and writing. Federico 
 Landini and Jachym Kolarworkedon base linemodels for 
 audio-onlyvoiceactivitydetection and diarization,andwrit- 
 ing. Leda Sariworkedontranscription task definition,corre- 
 spondingannotationguidelines and baseline model ing. Eric 
 Zhongcong Xuworkedon data selectionmethodology and 
 the baseline modeling of active speaker detection. Ruijie 
 Taoand Mike Zheng Shouworkedon the modelingofactive 
 speakerdetection. Hanbyul Jooworkedon data annotation 
 tooling and dataselectionmethodology. Christoph Feicht- 
 enhoferworkedon the taskdefinition and metrics. Anurag 
 Kumarworkedonactivespeakerdetection and diarization 
 tasksdefinition,andonaudioembeddings model ing for these 
 tasks. Morrie Doulatyworkedon base linemodels for voice 
 activitydetectionanddiarization and dataanalysisofanno- 
 tations. Lorenzo Torresaniworkedon the tasksdefinition 
 andannotationguidelines. Kristen Graumancontributedto 
 62 
 
 

 
 
 
 
 
 
 I.Social Interaction Benchmark 
 Thissectiondetails the Social Interactionbenchmark task 
 definitions, annotations,baselinemodels, andresults. We 
 also provide details on the video data collection process 
 formulti-personcapture with participantswhoconsentedto 
 have the irfacesunblurred and conversationrecorded(Ap- 
 pendix I.5). Asnotedin Appendix B,thesocialbenchmark 
 videos were screenedtoremoveanyin for mation(e.g. last 
 namesorsocialmediaaccounts)thatcoulddirectlyidentify 
 participants. However, participants’ faces and voices are 
 presentasperourin for medconsent. 
 
 I.1 Formal Task Definition 
 LAM and TTM are defined as follows: (1) LAM: y = 
 f(I,B); (2) TTM: y = f(I,A,B) where I = I T 2 , 
 { t }−T 1 (a) Annotationtool 
 A = A T 2 , and B = B T 2 aretime-synchronized 
 { t }−T 1 { t }−T 1 
 pastsequencesofvideo,audio,andboundingboxes,respec- 
 tively,where T and T arethelengthof the past and future 
 1 2 
 time horizon, respectively, and t = 0 is the center frame. 
 Theboundingboxindicates the targetpersontoclassify. y 
 isabinaryclassificationlabeldefinedby: 
 (cid:26) 
 1 if targetlooks/talksatcamerawearer 
 y = 
 0 otherwise. 
 (25) 
 (b) Visualizationofannotations. 
 The LAM and TTM tasks are defined as a frame-level 
 predictiony,whichst and sincontrasttoaudioanalysistasks 
 Figure 47. (Top)The GUIof the annotationtool; (Bottom)Vi-
 wherelabels are oftenassignedat the levelofaudioframes 
 sualizationofexampleannotations. Note that LAM(denotedby
 orsegments. Adesired model must beabletomakeacon- magentatext)and TTM(denotedbycyantext)maynotnecessarily
 solidateddecision base don the video and audiocuesover occurtogetherasshownin the seexamples.
 thetimec our seofanutterance. Forexample,ifthespeaker 
 turnstheirheadto the sidemomentarilywhilespeakingto 
 thecamera-wearer,thenaframewhere the speakerislooking short-duration LAMor TTMbehaviors,lasting 1 or 2 sec-
 awaywouldhavey = 0 whiley = 1. Figure 47 onds. The data wasorganizedasfollows for baseline model
 LAM TTM 
 givessomeframelevelvisualizationofannotationsthatil- trainingin Section I.3: 389 clips were heldout for training,
 lustrate the taskdefinitions. comprising 32.4 hoursintotal. Anadditional 50 clips(4.2
 hours)and 133 clips(11.1 hours)wereheldouttoform the
 validation and testingsets,respectively. 
 I.2 Annotation Statistics 
 Thesocial task annotations, LAMand TTM,buildon the 
 I.3 Social Baseline Models and Results 
 same video clips used in the AV diarization tasks and de- 
 scribedin Appendix H.5. Fig 48 summarizes the statisticsof LAM Our base line model for LAMisavideo-based model
 LAMand TTMannotationsacross the seclips. Wecompute using Res Net-18 and Bidirectional LSTM.Our model uses
 thepercentageof the frames with LAMor TTMannotations thecroppedfaceregionsinvideoasinputinordertofocus
 ineachclip and show the histogramsin Fig 48(a)and(b), oncuesabout the headpose and socialattentionvisiblein
 respectively. Inmanyclips,theseeventshappenrarely(10 theface. Thearchitectureof our base lineissimilarto the
 %orlower),and the frames with LAMannotations are less Gaze 360[111]. Asillustratedin Fig 49(a),weinput 7 con-
 frequentthan TTMcases. Wealsolist the durationofeach secutiveframes(T =3 and T =3)fromonefacetracklet,
 1 2 
 LAM or TTM annotation (the duration between start and andeachimageisresizedto 224 224. Eachframeisthen
 × 
 end time) in Fig 48 (c) and (d), in order to illustrate the processedby the Res Net-18 backboneindependentlytogen-
 signifi can tvariationsinlength. Themostfrequentcaseis erate 256 dimensionalfacefeatures. Thefeaturesequenceis
 63 

 
 
 
 
 
 
 
 120 
 100 
 80 
 60 
 40 
 20 
 0 0 20 40 60 80 Percentage of frames w/ LAM annotations (%) 
 spil C 
 fo 
 rebmu N 
 100 
 80 
 60 
 40 
 20 
 0 0 20 40 60 80 Percentage of frames w/ TTM annotations (%)
 (a) %of LAMperclips 
 spil C 
 fo 
 rebmu N 
 (b) %of TTMperclips 
 2000 
 1500 
 1000 
 500 
 0 0 2 4 6 8 
 Duration of LAMM annotations (sec) 
 ycneuqer F 
 2000 
 1500 
 1000 
 500 
 0 0 2 4 6 8 
 Duration of TTM annotations (sec) 
 (c) Durationof LAM 
 ycneuqer F 
 val test 
 Acc m AP Acc m AP 
 Random Guess 8.57 51.19 7.98 50.96 
 Baseline(Gaze 360) 91.78 79.90 87.97 78.07
 Baseline(Random) 86.45 72.11 75.38 66.07
 Table 30. Resultsof LAM.The base line model wasinitialized
 from Gaze 360[111](2 ndrow)andatrandom(3 rdrow).
 thesameas LAM.However,sometimes the speakersleave
 thefieldofvieworbecomeinvisibledueto the rapidmotion.
 Inthiscase,wepad the facesequences with blankimages.
 The MFCCfeatureisextractedevery 10 mswitha 25 mswin-
 dowlength. Thefeatureisthenfedinto the audiobackbone,
 a Res Net-18 designed for audiotasks[38]. Following the
 encoders,weconcatenate the audio and visualembeddings
 (d) Durationof TTM andpassthemto the finalclassificationheadtoget the TTM
 result for thevisiblefacesassociated with the segment. To
 Figure 48. Social task annotationstatistics.(a)Histogramshowing train the modelinparallel,wefirstsort the shortsegments
 thenumberofclipsvs.thepercentageofframes with look-at-me basedonthelength and group the segmentsintoabatchif
 annotations;(b)Histogramshowing the numberofclipsvs. the they have the sameduration. Thebatchsizeisrestrictedby
 percentageofframes with talk-to-meannotationsineachclip;(c) the GPUmemory;we useabatchsizeof 400. Themodelis
 Histogram showing the duration of look-at-me annotations; (d) alsooptimizedusing Adam with alearningrateof 5 10−4.
 Histogramshowing the durationoftalk-to-meannotations. × 
 Table 31 summarizes the TTMresults. TTMismorechal-
 lengingthan LAM.we cansee that our base line model only
 increasesthem APby 9.77%onthetestsplitincomparison
 encodedbya Bidirectional LSTM,whichhastworecurrent 
 tother and omguess model. 
 layers with dimensionality 256. The output is fed into a 
 classificationheadtopredict the binary LAMresult for the 
 center frame at the t-th timestamp. The LAM task has a I.4 Discussion 
 classimbalanceissue,andwe useweightedcross-entropy 
 While the benchmark task sofdetectingwhenattention and
 loss. Since the architectureissimilarto Gaze 360,wehave 
 speakingbehaviors are directedtowards the first-person are
 twooptions for the initialization: first,initializing the back- 
 closely related to existing analysis tasks, it is clear from
 bone from apretrained Gaze 360 model;second,initializing 
 the base lineper for mance that the reissubstantialroom for
 the model randomly and training from scratch on Ego 4 D. 
 improvement,withm APof 78.07 for LAMand 55.06 for
 Duringtraining,wesamplecenterframes with astrideof 3. 
 TTM. 
 Thenetworkisoptimizedby Adam with alearningrateof 
 5 10−4. The TTM task isparticularlychallengingbecauseitre-
 × quiresanalysisoftheaudiocontenttounderst and the target
 Theresults are shownin Table 30. Our base line model 
 audienceofanutterance,aswellas the fusionofaudio and
 achievesanm APof 66.07%onthetestsplitwheninitialized 
 videocues. Themostcompletesolutionto this problem will
 randomly, and the per for manceishigherat 78.07%when 
 requireanunderst and ingofthesemanticsof the utterancein
 initialized from Gaze 360. Thesefindingshighlight the close 
 thecontextofanevolvingconversationalinteraction. Future
 relationshipbetween the LAMtask and gazeestimation.The 
 workon this taskmightinvolvemoresophisticatedlanguage
 randomguess model achievesabout 8%accuracybecause 
 modeling and possiblyhierarchicalanalysisapproaches that
 thenegativesamplesaccount for 92%ofthetestsplit and 
 allow the integrationofcuesatmultiplelevels,e.g. atthe
 the model alwayspredictslookingatme. 
 dialogleveltounderst and whoisparticipatinginaconversa-
 TTM The base line model for TTMdigestsmulti-modalin- tionalexchange,attheutteranceleveltoaccesssemantics,
 puts: eachaudiosegmentispaired with anassociatedface andat the audioleveltoexploitprosodi can dothercues.
 crop.Since the audiosegmentsvarysubstantiallyinduration, The LAM task presentsadditionalchallengessuchas the
 webreak the longutterancesintoshortsegmentswhosemax- need to deal with motion blur and fast head movements,
 imumdurationislimitedto 1.5 s. Ifthesegmentisshorter andmayalsobenefit from amoreexplicit model ingofhead
 than 0.15 s,weskipitin the trainingstage. Theassociated movement and the patterns of gaze behavior that arise in
 faces are alsoresizedto 224 224,and the videoencoderis conversationalinteraction. 
 × 
 64 

 
 
 
 
 
 
 datacollectiontookplaceduring the COVID-19 pandemic,
 and the resultingstudyprotocols were designedtosafeguard
 participantsagainstadditionalrisk. 
 Thesocialdataconsistsof data collectedatfivesites: At-
 lanta,Bloomington,Redmond,Twin Cities,and Singapore.
 In total, 764 hours of video and audio were collected for
 (a) LAM 
 thesocialbenchmark task. Adetailedsummaryof the data
 collectionpracticesateachsite can befoundin Appendix A.
 I.6 Derived Tasks for Future Social Benchmarks
 Thecore task sof LAMand TTMdefineastartingpoint for
 analyzingmulti-modalegocentric data and inferringsocial
 interactions. Wenowdescribetwogroupsofpotentialfuture
 tasks,attentiontasks and speakingtasks,thatcouldbesup-
 portedvia the existingannotationsin Ego 4 DSocial and the
 (b) TTM gaze data collected from eyetrackers. 
 Egocentric Attention Prediction(EAP) Priorwork[135,
 Figure 49. Baseline model architectures. (a)LAM model uses 
 137] has demonstrated the feasibility of predicting where
 a Res Net-18 asabackbonetoextract the featureofeachframe. 
 A Bidirectional-LSTM then takes the sequence and encode the the camera-wearer is looking (i.e. their egocentric atten-
 featuresintooneembedding.Wepass the embeddingto FClayer tion)usingonlyegocentricvideocaptured from ahead-worn
 thatpredicts the LAMresult. (b)TTM model hastwoencoders. camera. Thisworkleveraged the contextofh and-eyecoor-
 Thevideoencoderis the sameas LAM.Theaudioencoderextracts dination tasks, which require gaze to be coordinated with
 the MFCCfrequencymapoftheaudiosegment and the featureis handmovements and objects. Asubsetof the Ego 4 DSocial
 fedintoa Res Net-18 network.Thevisual and audioembeddings dataincludesgazemeasurementsproducedbywearableeye
 areconcatenated and passedthrough the FClayertopredict the 
 trackersby Indiana University and Georgia Techparticipants
 targetof this utterance. 
 (e.g.,Pupil Invisible),which will greatlyexp and the sizeof
 dataforh and-eyecoordinationin the wild. 
 val test 
 Acc m AP Acc m AP Social Gaze Prediction (SGP) The LAM task addresses
 Random Guess 32.44 53.82 47.41 50.16 thespecialcaseofsocialgaze: apersonlooksat the camera-
 Baseline 64.31 56.50 49.75 55.06 wearer. Itispossibletogeneralize the taskbypredicting the
 socialgazetarget for eachof the visiblefacesinanegocen-
 Table 31. Resultsof TTM.The base line model isinitializedran- tric video, i.e., yp 0,1,...,M , where M is the total
 domly. ∈ { } 
 number of participants in a group social interaction, and
 p 0,1,...,M . pis the index for socialmembers. The
 ∈{ } 
 caseyp = qmeans that targetpwaslookingatparticipant
 I.5 Social dataset Collection 
 q. The case yp = 0 captures alternative gaze targets, in-
 The Ego 4 DSocial data collectionprocesswasdesignedto cludingnon-socialgazetargets(e.g. lookingatanobject),
 achieve: 1)naturalisticinteractions,2)multi-modalcapture, lookingatpeoplewho are notwearinganegocentriccamera
 and 3)diverseparticipants and environments. Participants (with the result that groundtruthannotations are notavail-
 consistedoffriendsandfamilygroups and datawascaptured able),andlookingatunknowntargetsnotcapturedinany
 inresidences and localneighborhoods,ensuringnaturalistic of the egocentric videos. Let yˆq,p denote the LAM label
 interactions. Capture hardw are varied across sites but in- fortargetpersonpvisibleinframeofegocentricvideo I
 q 
 cludedwearablecameras,wearableeyetrackers(at Georgia capturedbyparticipantq. Then the SGPlabelisgivenby
 Tech and Indiana University), binauralrecordingsystems, yp =argmax yˆq,p . The Ego 4 DSocial data includessyn-
 q{ } 
 and smart watches (at Georgia Tech). Protocols included chronizedvideos from multiplesocialmembers,which will
 highly-structuredsettings,whereparticipants were askedto allowustoexp and theannotationbymatching the person ID
 playgamesoveraperiodofafewh our sin are sidence,and with the camera-wearers. Note that since the videorecorders
 unstructuredsettingswhereparticipantscapturedsocialin- arenotgenlocked,theidentificationofcorrespondingframes
 teractionsindailylifeoveraperiodaweekormore. Sample willonlybeapproximate. However, sincegazebehaviors
 socialinteractioncontextsincludedplayingboard and card persistovermultipleframeswedonotbelieve this will be
 games,preparingmeals,andgoingonwalks.Thebulkof the anissue. 
 65 

 
 
 
 
 
 
 Akeyissueindefiningthetaskis the determinationof the tonsite and contributedto the socialbenchmark for mulation
 participantset. Fora 2 Dversionof SGP,termed SCG-2 D, andpaperwriting. Vamsi Ithapucontributedto the social
 theparticipantsetisdefinedbyparticipantswho are visible benchmark for mulation and dataannotation. Hyun Soo Park
 inframet. Thisisasocialversionof the video-basedgaze led data collectionat the Twin Citiessite and contributedto
 follow task[37],wherethegoalistopredictwhe the reach thesocialbenchmark for mulation and paperwriting.
 targetparticipantislookingatanyoftheo the rparticipants Hao Jiang contributed to model development and data
 who are visiblein the frame. Amorechallenging 3 Dversion annotation. Yunyi Zhucontributedto model implementation
 of the task, SCG-3 D, uses all of the participants who are and experiments. Eric Zhongcong Xu contributed to the
 presentinthesocialsceneat the timeofframet. This task socialbenchmark data preparation and the modelimplemen-
 requires the ability to predict which participant the target tation and experiments,andcontributedtoall data collection
 personpislookingatin the casewhere that participantis relatedtasks for the Singaporesite. Ruijie Taocontributed
 notvisibleinframet. This can inprinciplebeaccomplished todatacollection for the Singaporesite. Fiona Ryanled the
 by maintaining a birds-eye view layout map of the social datacollectionef for tfor the Atlantasite,includingprotocol
 scene, that captures the approximate spatial relationships design,multimodalsensordeployment and synchronization,
 between the participants. Suchalayoutmapcouldbeused andde-identification. Miao Liucontributedto data collec-
 inconjunction with anapproachlike Gaze 360[111]tosolve tion and analysis for the Atlantasite. Audrey Sou the rland
 the SCG-3 Dtask.Note that this task couldpotentiallybenefit contributedto the protocoldesign,IRBauthoring and sub-
 fromtakingrecordedbinauralaudioasanadditionalinput,as mission, participant recruiting, and data ingestion for the
 theabilitytolocalizesounds our cescouldprovideadditional Atlanta site. Jayant Sharma contributed to participant re-
 cues for determining the locationsofgazetargetswhich are cruiting,datacollection,IRBsubmission,analysis,anddata
 notvisiblein the video. ingestion for the Twin Citiessite. Yuchen Wangcontributed
 totheprotocoldesign,participantrecruiting,and data collec-
 Utterance Target Prediction(UTP) The TTMtask can be 
 tion for the Bloomingtonsite. Weslie Khoodeveloped the
 generalized to the full set of participants in the same way 
 multi-camerasynchronizationandde-identificationpipelines
 that LAMcan beextendedto SGP.Theinputspaceis the 
 atthe Bloomingtonsite. 
 sameas TTMand the outputspaceissimilarto SGP,where 
 yp = q means that participant p is talking to participant Acknowledgements The social benchmark team would
 q, and yp = 0 denotes the cases where the participant is liketoacknowledge the followingadditionalcontributions
 nottalkingtoanyone,oristalkingtosomeo new hoisnot fromindividualsateachsite: Atlanta: Jeffrey Valdez(re-
 wearinganegocentriccamera(and the reforegroundtruth cruitment and data collection), Gabriella Stripling, Ruth
 cannotbedetermined). Incontrastto SGP,UTPrequires the Stolovitz,and Andrea Sucre-Pardo(recruitment and dataset
 identificationofallof the targetrecipientsofanutterance. de-identification). Twin Cities: Reese Kneel and, Angad
 Infact,our TTMannotationalreadysupports this task,as Cheema, Silong Tan, Anjali Oleksy, Zhiteng Cao, Di-
 itdifferentiatesthecasewhere the utteranceisdirectedto ana Begelman(datacollection and annotation)Facebook:
 multiple participants including the camera wearer. This Samuel Clapp and Peter Dodds (binaural audio recording
 additionallabelisignoredinthedesignof the simpler TTM andmultimodalsynchronization). Bloomington: Zunaeed
 task. Salahuddin,Zehua Zhang,Ziwei Zhao. 
 Transcript-based Variants For all of the previously- 
 definedsocial task sitispossibletodefineavariantwhich 
 utilizesatranscriptof the audiofileasanadditionalinput 
 modality. For example, the TTM-T task is the variant of 
 TTM with the prediction defined as yp = f(I,A,T,B), 
 where T the transcript (time-stamped sequence of words) 
 obtained from A. This can potentiallysimplify the useof 
 dialogcuestoidentify the intendedtargets for utterances and 
 socialgaze. 
 I.7 Contributionsstatement 
 James M.Rehgco-led the socialbenchmarkeffort and paper 
 writing. Hanbyul Jooco-led the socialbenchmarkeffort and 
 dataannotation. Mike Zheng Shouco-led the socialbench- 
 markeffortandproblem for mulation and modelingexperi- 
 ments. David Crandallled data collectionat the Blooming- 
 66 

 
 
 
 
 
 
 J.Forecasting Benchmark Short-Term Object Interaction Anticipation 
 Thissectiondetails the Forecastingbenchmark task defi- This task aimstopredict the nexthuman-objectinteraction
 nitions,annotations,baselinemodels,andresults. happeningafteragiventimestamp. Givenaninputvideo,
 thegoalistoanticipate: 
 
 J.1 Formal task sdefinitions • Thespatialpositionsof the activeobjects,amongthose
 whicharein the scene(e.g.,boundingboxesaround the
 Asnotedin the mainpaper,there are four for ecastingtasks: objects). Weconsider the nextactiveobjecttobe the
 future locomotion movement prediction, future hand pre- nextobjectwhich will betouchedby the user(either
 diction,short-termobjectinteractionanticipation,andlong- with the irh and sor with atool)toinitiateaninteraction;
 termactionanticipation. 
 • Thecategoryofeachof the detectednextactiveobjects
 (e.g.,“knife”,“tomato”); 
 Future Locomotion Movements Prediction 
 • Howeachactiveobject will beused,i.e.,whataction
 will be per for med on the active objects (e.g., “take”,
 This task aims to predict the future locomotion of a user 
 “cut”); 
 givenasequenceofpastimages. Weformulate the problem 
 as: • Whentheinteractionwi the achobject will begin(e.g.,
 “in 1 second”, “in 0.25 seconds”). This is the time
 (cid:2) (cid:3)T 
 = x t+1 x t+F =f(x t−T , ,x t−1 ; ), tothefirstframeinwhichtheusertouches the active
 X ··· ··· I 
 (26) object(timetocontact).Thisprediction can beusefulin
 scenarioswhichinvolvehuman-machinecollaboration.
 where isthefuturetrajectory,T and F are the past and For instance, an assistive system could give an alert
 X 
 future time horizons, respectively, x t is the point on the if a short time to action is predicted for a potentially
 trajectoryattimet,and istheegocentricimageattimet. dangerousobjecttotouch. 
 I 
 Withanassumption that the personwalksoveramajorplane 
 Inthis task,models are requiredtomakepredictionsat
 (e.g., ground plane), we represent the trajectory in a 2 D 
 plane,i.e.,x R 2. a specific timestamp, rather than densely throughout the
 t 
 ∈ video. Figure 51 illustrates the set-up. The model isallowed
 Theessenceof the locomotion task istodesignafunc- 
 to process the video up to frame t, at which point it must
 tion f to predict a set of plausible K future trajectories 
 anticipate the next active objects, and how they will take
 k given the currentimage. Since the reexistsanumber 
 k 
 {X } partinaninteractioninδseconds,whereδisunknown. The
 ofplausiblefuturetrajectories with differenttopology,e.g., 
 model can makezeroormorepredictions. Eachprediction
 trajectories that bifurcate at an Y-junction, we predict K 
 indicates the next active object in terms of noun class (nˆ)
 futuretrajectories. 
 andboundingbox(ˆb),averbindicating the futureaction(vˆ),
 aswellas the timetocontact(δˆ),whichestimateshowmany
 Future Hand Prediction seconds in the future the interaction with the object will
 begin. Eachpredictionalsocomprisesaconfidencescore(sˆ)
 In addition to future locomotion movements prediction, used for evaluation. 
 we consider another challenging task of predicting future Specifically,let V beanuntrimmedvideo.Wewilldenote
 hand positions of key-frames (see visual illustration in with V theframeof V occurringattime-stept and with V
 t :t 
 Fig. 50). Specifically, we denote the contact frame 17 as thevideosegmentstartingat the beginningof V (timestamp
 x 
 c 
 , pre-conditionframe 18 asx 
 p 
 , and the threeframespre- 0)andendingattimestampt. Givenatimestampt,denoted
 ceding the pre-condition frame by 0.5 s, 1 s and 1.5 s as as“stoppingtime”,theshort-termobjectinteractionanticipa-
 x p 1 , x p 2 , x p 3 , respectively. Formally, given an input ego- tion task requires that amodelisabletoexploit the observed
 centric video 1.5 s before the pre-condition time step (de- video V topredict N tuples(where N isarbitrary):
 :t 
 noted as x = x ,...,x , with t referred as 
 { p 3−to−1 p 3−1 } o (ˆb ,nˆ ,vˆ,δˆ,sˆ) N (27) 
 observation time), this task seeks to predict the positions { i i i i i }i=1 
 of both hands (hl,hr) in the future key frames, where 
 i i where: 
 i c,p,p ,p ,p . 
 ∈{ 1 2 3 } • ˆb R 4 isaboundingboxindicating the positionof
 i 
 ∈ 
 17 Thecontactframeisdefinedasthefirstframeinwhich the usertouches thepredictednextactiveobject;
 theobject,hencethemomentinwhich the objectbecomesactive. 
 18 Asdefinedin Section G,thepre-conditionframemarksamomentprior • nˆ i 
 ∈N 
 isanounindicatingtheclassof the nextactive
 tothestate-changeofanobject. object,where isthesetofpossiblenouns. 
 N 
 67 

 
 
 
 
 
 
 { ℎ𝑙,ℎ𝑟} { ℎ𝑙,ℎ𝑟} 
 𝑖 𝑖 𝑗 𝑗 
 
 … … 
 
 
 
 Input Video Future Key Frames 
 Figure 50.Exampleoffutureh and prediction. 
 
 
 
 Last observedframe (𝑽 ) Unobserved future frame (𝑽 ) 
 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝟏 𝒕 𝒕+𝜹 
 𝑏෠ = 450,90,510,140 
 1 
 𝑛ො =𝑑𝑜𝑢𝑔ℎ 
 1 
 𝑣ො =𝑡𝑎𝑘𝑒 
 1 
 𝛿መ =0.75𝑠 
 1 
 𝑠Ƹ =0.8 
 1 
 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝟐 
 frame of 
 𝑏෠ = 500,100,550,150 
 2 contact 
 𝑛ො =𝑑𝑜𝑢𝑔ℎ 
 2 
 𝑣ො =𝑡𝑎𝑘𝑒 
 2 
 𝛿መ =0.75𝑠 
 2 
 𝑠Ƹ =0.75 
 2 
 𝛿 
 𝑡 𝑡+𝛿 
 Input video: 𝑉 
 :𝑡 
 Figure 51.Exampleofshort-termobjectinteractionanticipation. 
 • vˆ is a verb indicating the action which will be Figure 51 illustrates the proposed task. Givenavideo V ,
 i :t 
 ∈ V 
 per for med,where isthesetofpossibleverbs; a method should be able to detect the next active objects
 V 
 (e.g., two instances of “dough”), predict the action which
 • δˆ i R+isthetimetocontact,apositivenumberwhich will beper for med with thatobject(e.g.,“take”),and the time
 ∈ 
 estimateshowmanysecondintothefuture the interac- tocontact(e.g.,0.75 s). 
 tion with the object will begin; 
 • sˆ [0,1]isaconfidencescoreassociatedto the predic- Long-Term Action Anticipation
 i 
 ∈ 
 tion. Objects with alargeconfidencevalue are deemed 
 tobelikelynext-active. Long-term action anticipation aims to predict further into
 the future. Rather than predict the next action at a given
 The model isallowedtoperform N predictions for each timestamp,models will berequiredtopredict the sequence
 observed example (with N arbitrary) both to account for of Z future actions which the camera-wearer is likely to
 thepresenceofmultiplenext-active-objects and tohandle perform. Thisisimportant for long-horizonplanningwhere
 the multi-modality of future predictions. Each of the N asequenceofactionsisrequiredtobeper for medinaspecific
 predictionsisintendedasaplausiblefutureobjectinteraction. ordertoachieveagoal. Critically,theseactionsoccurover
 68 

 
 
 
 
 
 
 𝑛ො ,𝑣ො , 𝑛ො ,𝑣ො , 𝑛ො ,𝑣ො , 𝑛ො ,𝑣ො 
 1,𝑖 1,𝑖 2,𝑖 2,𝑖 3,𝑖 3,𝑖 4,𝑖 4,𝑖 
 𝑖𝑡ℎprediction: kneaddough → putdough → packspice → pourspice
 
 
 
 
 Input video 𝑡 
 
 Figure 52.Exampleoflong-termactionanticipation.Afterobservingavideouptoaparticulartimestept,amethodshould beableto
 predict the sequenceofactions that willlikelyoccur,inthecorrectorder(e.g.,first“takedough”,next“putdough”etc.)
 
 long time horizons, may be of variable length and do not focuson the handmanipulation. Weconsidervideos from
 occuruni for mlyacrosstime(e.g.,anactionevery 5 s). Thus, glass-mountedcamerasofwhichfieldofviewapproximately
 the task is defined at a more abstract level — models are aligns with the firstperson.(4)3 Dreconstruction and ground
 required to predict sequences of action classes (verb and planeneedtobeaccurate. Afterrunningstructurefrommo-
 noun), rather than time to action or to next active objects tion,weensure 3 Dreconstruction from the videosachieves
 boundingboxesin the currentframe. reasonablequalitybychecking 2 Dreprojectionof the point
 More for mally,givenanuntrimmedvideo V andastop- cloud and groundplane. Givenasetof the sevideoclips,we
 pingtimetasdescribedabove,thelong-termactionanticipa- chooseframes for training/testing data for everysecond.
 tion model mustobserve V andpredict N setsofsequences 
 :t 
 of Z plausiblefutureactions: 
 Remaining Tasks 
 (nˆ ,vˆ ) Z N (28) 
 {{ z,i z,i }z=1}i=1 For the remaining tasks we first manually ranked the sce-
 narios base dontheirapplicabilityto the forecastingtasks.
 where: 
 Forinstance,scenarioslikecarpentery were highpriority for
 • nˆ is the predicted noun and vˆ is the forecasting whereas walking in the park was low-priority.
 z,i z,i 
 ∈ N ∈ V 
 predictedverbofthez-thfutureaction. Wescoredallscenarios from 1-3 basedon this priority. We
 imposeconstraintson the minimumnumberof hours and
 • { (nˆ z,i ,vˆ z,i ) } Z z=1 represents the sequenceoffutureac- participantstosub-selectscenarios that havesufficient data
 tionssortedbythepredictedorderinwhich the ywill for training(eachparticipantshould have contributedatleast
 appearin the video. 15 minutes;and the reshould beatleast 20 minutesofvideos
 for that scenario). Next,wechunk our videosinto 5 minute
 Like the short-termobjectinteractionanticipation task, 
 clips,anduse the followingalgorithmtoselectclipstobe
 the model is allowed to generate N sets of predictions to 
 labeled. Toensuregeographicaldiversity,wedistribute the
 account for the multi-modalnatureoffutureprediction. Fig- 
 totalh our soveruniversitiesandr and omlyselectclips from
 ure 52 illustrates the proposed task. 
 each to fill the hours allocated to that university. If there
 are universities that contributed less, then their hours are
 J.2 Data Selection 
 distributedacrosstheo the runiversities. Toselect the clips
 Future Locomotion Movements Prediction givenauniversity and the hoursallocated; wewouldfirst
 sampleaparticipant,thensampleavideo for thatparticipant,
 Egocentricvideos for locomotion and hand-objectinterac- 
 andsample 1 clip from thatvideo. Forcertainrepetitivesce-
 tion are nearlymutuallyexclusive. Among the sevideos,we 
 narios(likebrickmaking),wereject this clipifwealready
 skim through each video to manually identify video clips 
 haveselectedatleast 2 clips from the samevideo. Werepeat
 (beginning and endframes)thatsatisfy the followingselec- 
 theprocessuntil the requirednumberofhours are selected.
 tioncriteria. (1)Locomotion,bydefinition,involvesdiverse 
 activitiesassociated with walking. Theclipshouldinclude 
 J.3 Data Annotation 
 substantialtranslationalmovement. (2)Eachvideoclipmust 
 belongerthan 10 seconds for pasttrajectoryobservation and 
 Future Locomotion Movements Prediction 
 future prediction. (3) The videos must observe surround- 
 ing scenes. This differs from the videos for hand-object We generate the ground truth of future trajectories using
 interactionwhere the cameraisdeliberatelytilteddownto 3 D reconstruction of the camera trajectories. Given a se-
 69 

 
 
 
 
 
 
 
 
  
 
 o 
 x 
 n Offset t 
 
 Ground plane 
 (a) Geometry (b) Futuretrajectoryprediction 
 
 Figure 53.(a)Werepresentthefuturetrajectoryofapersonusing the groundplane.Given the 3 Dreconstructionof the cameratrajectory,
 weprojectitintotheestimatedgroundplanetoform the futuretrajectory.(b)Thegroundtruthfuturetrajectory(blue)and the predicted
 trajectories(red and white)areshownintheegocentricimage with the groundplanecoordinate(magentagrid).Wepredicttop 5 trajectories
 where the toppredictionismarkedinred. 
 
 Data Outdoor Indoor Mixed Total 1 sand 1.5 sbefore the pre-conditiontimestep. Therefore,
 Train 34.1 k 0.41 k 16.7 k 51.3 k 
 foreachinteraction the rewill be 5 keyframeslabeled with
 Val 7.5 k 0.23 k 6 k 13.9 k 
 boundingboxesofhands,including the contactframe. We
 Test 7.4 k 0.18 k 3 k 10.6 k 
 use the bouding box center as the ground truth of hands
 Table 32.Wesplit the image data for locomotionpredictionbased positions. 
 onscenes that includingoutdoor,indoor,andmixed. 
 Short-Term Object Interaction Anticipation 
 quence of egocentric images, we reconstruct the 3 D ego- Eachvideo V ofthe data setislabeled with asetofshortterm
 motionandscenegeometryusingast and ardstructure from object interaction anticipation annotations = S (j)
 S V { V } j 
 motionpipeline with afewmodificationtoh and lealarge indicatingtheoccurrenceofobjectinteractionsin the video.
 numberofimages. With the 3 Dscenepointcloud,wees- Eachannotation 
 timate the ground plane using RANSAC with the ground 
 planenormalprior. The 3 Dreconstructedcameratrajectory S V (j) =(t( s j), { n ( h j) } h ,v(j), { A ( h j) } h , { B h (j) } h ) (29)
 is projected onto the ground plane to form the 2 D future 
 trajectoryasshownin Figure 53. includes: 
 Our image dataset includes locomotion in outdoor, (j) 
 • t s :thetimestampindicatingthebeginningof the inter-
 indoor, and mixed scenes. We split the image data 
 action with the activeobjects. Thisis the firstframein
 into training/validation/testing sets with approximately 
 whichtheusertouchesatleastoneof the activeobjects;
 70%/15%/15%,respectively. Theratioacrossscenesdoes 
 notexactlymatchbecause the splitisper for med base don (j) 
 • n : the set of categories of the h interacted ob-
 the(anonymous)participant ID.Thesummaryof the data { h } h 
 jects; 
 split can befoundin Table 32. 
 • v(j): theclassoftheactioninvolving the activeobjects;
 Future Hands Movements Prediction (j) 
 • A : theboundingboxannotations for the active
 { h } h 
 (j) 
 Forthe the futurehandposition and trajectoryprediction,the objects. The cardinality of { A h } h is equal to the
 annotation will beper for medbylabelingboundingboxes cardinalityof n (j) , i.e., A (j) = n (j) . The
 { h } |{ h } h | |{ h }| 
 around hands in the frame in which the user touches the hthset A (j) containsboundingboxannotations for
 active objects as well as in frames preceding each object { h } h (j) 
 interactions. Hands bounding boxes will be associated to theactiveobjectsofcategoryn h attimestampt s ;
 a label useful to distinguish among left and right hands. (j) 
 • B : the bounding box annotations for the next
 Therefore,givenanobjectinteraction,wewillannotatekey { h } h 
 (j) 
 activeobjects. Thecardinality B isequalto the
 framesprecedingthebeginningof the interaction. Specif- { h } h 
 (j) (j) (j) 
 ically,t c andt p denote the timestepofcontactframe and cardinality of { A h } h , i.e., |{ B h } h | = |{ A h } h | .
 pre-conditionframe,andt ,t ,t ,denotetimesteps 0.5 s, Thejthset B (j) contains the boundingboxannotations
 p 1 p 2 p 3 h 
 70 

 
 
 
 
 
 
 future action short-term annotations S (i) are available (see Section J.3)
 V 
 andavalue for Zhas been chosen,thelong-termannotations
 (j) 
 L can beeasilyobtainedbysampling the first Z actions
 V 
 𝑡 𝑠 (𝑗)−4𝛼 𝑡 𝑠 (𝑗)−3𝛼 𝑡 𝑠 (𝑗)−2𝛼 𝑡 𝑠 (𝑗)−𝛼 𝑡 𝑠 (𝑗) annotatedinvideo V beginningaftertimestampt(j). More
 (j) 
 formally,thefutureactionlabels for L areobtainedas:
 Figure 54. Anexampleofhowframes are sampledtobelabeled V 
 withnextactiveobjectannotations.Foragivenactioni,wesample 
 mframesatregularintervalsα.Ifwesetm=4 andα=0.5,we {(n(iz),v(iz))|(t(iz),{n(iz)} ,v(iz),{A(iz)} ,{B(iz)} )∈S ∧
 label the frameofcontactaswellas 4 framesalongasegmentof 0 s h h h h h h V 
 2 sprecedingthebeginningof the actionataframerateof 2 fps. t( 
 s 
 iz) ≥t(j)∧ 
 t(i 1) ≤...≤t(i Z)∧ 
 s s 
 ofnextactiveobjectsofclassn h . Inparticular, B h (j) (cid:64)S V (j) ∈S V |t(j) ∈/ {i 1 ,...,i Z },t≤t s (j) <t( s i Z)}Z z=1
 containsannotations for the sameobjectinstancesanno- 
 (j) (j) wheren 
 (iz) 
 refersto the primaryinteractedobject from the
 t 
 i 
 a 
 c 
 t 
 a 
 e 
 l 
 d 
 ly, 
 in 
 B 
 A 
 h (j 
 h 
 ) = 
 ,tr 
 { 
 a 
 B 
 ck 
 l ( 
 e 
 , j h 
 d 
 ) | l 
 in 
 = 
 fr 
 1 
 am 
 ,.. 
 e 
 . 
 s 
 ,m 
 pr 
 } 
 e 
 , 
 ce 
 w 
 d 
 h 
 in 
 e 
 g 
 re 
 t s 
 B l ( , 
 . 
 j h ) 
 S 
 i 
 p 
 s 
 e 
 t 
 c 
 h 
 if 
 e 
 - 
 s e e x t am of p i l n e t 
 0 
 e o r f ac h t o e w do lo b n je g c - t t s er { m n
 ( 
 h a 
 iz 
 n 
 ) 
 n } o h t . at F io ig n u s r a e re 56 ob il t l a u i s n t e ra d te f s or a m n
 setofboundingboxannotationsofnextactiveobjectof 
 short-termannotations. 
 classn annotatedattimestampt lα. Heremindi- 
 h s 
 − 
 catesthenumberofframespreceding the beginningof 
 Annotationanalysis 
 theinteractioninwhichobjects are annotated,whereas 
 αisthetemporaldistancebetweenthesampledframes. 
 Datasetstatistics Asdiscussedearlier,oneof our primary
 For instance, setting α = 0.5 s and m = 4, we will 
 objectives when selecting the data to annotate was to
 labeltheframeinwhich the objectisinteractedaswell 
 maximize the diversityintermsofactivities and geographic
 as 4 framesina 2 ssegmentpreceding the interaction. 
 locations. Our data setincludesscenariosspanningawide
 Figure 54 showsanexampleofhowframes are sampled 
 range of everyday activities (e.g., gardening, cleaning,
 with the consideredscheme. 
 fishing,etc.). Inadditiontodiversityacrossscenarios,there
 Figure 55 reportsasampleclip with the discussedannota- isalsogeographicdiversity with inscenarios. Forexample,
 tions. Thetimestampt isselectedas the firstoneinwhich cookingmaylookverydifferentin Italy,India,Saudi Arabia,
 s 
 theusertouches the activeobjects.Theframesfollowing this or Japan. In Figure 38,weshow the resultingscenario and
 timestamp are notlabeled. Activeobjectboundingboxes are universitydistributions. Overall,ourbenchmarkconsistsof
 labeledattimestampt ,whereasnextactiveobjectbounding 120 hoursofannotatedvideocoming from 53 scenarios,7
 s 
 boxes are labeledinframesprecedingt . universities,and 406 participants. 
 s 
 Temporal structure of activities Human activity is goal-
 Long-Term Action Anticipation 
 driven and structured over time, with certain action se-
 Eachvideo V islabeled with asetoflong-termactionan- quencesbeingfavoredoverothers.Wemeasure this temporal
 notations L (j) , corresponding to a stopping time until structureusing Normalized Pointwise Mutual Information
 { V } j 
 which the video can beobserved,andasequenceof Z future (NPMI)[41]overpairsofactionsfollowingpriorwork[92].
 actionlabelsdefinedasfollows: NPMIisameasureofhowlikelyactionsfolloweachother.
 Inour data set,typicalpatternsinclude“pullgrass throw
 L ( V j) =(t(j), { (n( z j),v z (j)) } Z z=1 ) (30) grass(0.87)”,“holdspinach → cutspinach(0.83)”, → “turn-on
 faucet turn-offfaucet(0.68)”,“takecloth foldcloth
 where: → → 
 (0.49)”etc. Severalactionsalsooccurinsequence with high
 • t(j): the timestamp until which the video can be ob- NPMIscoresduetotherepetitivenatureof the activity. For
 served(i.e.,V )beforemakingpredictionsoffuture example, “flippage flippage(0.83)”whilereading, or
 :t(j) → 
 actions; “cutcarrot cutcarrot(0.82)”whilecooking. Finally,we
 → 
 see common action sequences involving multiple objects
 (j) 
 • n z :thenouncategoryof the primaryinteractedobject like“filltire closevalve(0.89)”,or“liftvacuum-cleaner
 inthez-thfutureaction; cleanstair → case(0.87)”. Thisstructureisvaluable and can
 → 
 (j) informlong-termactionanticipationmodels. 
 • v z : theverbdescribinghow the objects will beinter- 
 dataset split To facilitate future research and compar-
 acted with inthez-thfutureaction. 
 isons, we construct training, validation, and test splits
 Foreachvideo,t(j)areselected from the lasttimestampof containing 40%, 30%, and 30% of the data, respectively.
 eachannotatedobjectinteraction.Itisworthnoting that once Wenote,however,thatwedonotrelease the groundtruth
 71 

 
 
 
 
 
 
 𝐵 1 ( , 𝑖 ℎ )={[0.4,0.6,0.6,1.0]} 𝐵 1 ( , 𝑖 ℎ )={[0.5,0.8,0.7,1.0]} 𝐵 1 ( , 𝑖 ℎ )={[0.4,0.5,0.6,0.9], 𝐵 1 ( , 𝑖 ℎ )={[0.3,0.2,0.6,0.7], 𝐴( ℎ 𝑖)={[0.1,0.2,0.5,0.8], First frame of contact
 [0.4,0.9,0.6,1.0]} [0.3,0.7,0.6,1.0]} [0.3,0.6,0.6,1.0]} UNLABELED FRAMES
 
 
 
 𝑛(𝑖)=𝑏𝑢𝑐𝑘𝑒𝑡 
 ℎ 
 𝑡(𝑖)−3𝛼 𝑡(𝑖)−2𝛼 𝑡(𝑖)−𝛼 𝑡(𝑖) 𝑣(𝑖)=𝑡𝑎𝑘𝑒 
 𝑠 𝑠 𝑠 𝑠 
 Figure 55.Exampleofannotations for the short-termobjectinteractionanticipation task.
 Video 𝑉 
 𝑆(1) 𝑆(2) 𝑆(3) 𝑆(4) 𝑆(5) 𝑆(6) 𝑆(7) 𝑆(8) 𝑆(9) 𝑆(10) 𝑆(11) 
 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 
 𝑡(𝑗) 
 Z=3 
 𝐿(𝑗)= 𝑡𝑗, 𝑛𝑗,𝑣𝑗 , 𝑛𝑗,𝑣𝑗 , 𝑛𝑗,𝑣𝑗 
 𝑉 1 1 2 2 3 3 
 Figure 56. An exampleof a long-termannotation L(V:t) for anuntrimmed video V at timestamp t can beobtained fromshort-term
 annotations S(i).Intheexample,Z =3,hencethelongtermannotationisobtainedbyconsidering the firstthreeactionsbeginningafter
 V 
 timestampt. 
 annotations for the test set. Following common practice, is smaller than the error tolerance, it is considered as a
 evaluation on the test set will be supported through the correct trajectory prediction. PCT(cid:15) measures how many
 publicevaluationserver and leaderboard. Weassigndatato trajectoriesamong K retrievedtrajectories are closeto the
 splitsr and omlyat the levelof 5 minuteclips. Thisensures groundtruthtrajectory. 
 that all interactions within a 5 minute clip were labeled 
 byanannotator and providesenoughtemporalcontext for Future Hand Movement Asfor the futureh and smovements
 long-termvideotasks,likelong-termactionanticipation. prediction,weonlyconsider the keyframeprediction,and
 there for eadopt Mean Key Frame Displacement Error Con-
 tact(M.Disp.) Key Frame Displacement Errorasevaluation
 J.4 Evaluationmeasures metrics(C.Disp.): 
 Future Locomotion and Hands Movements Prediction 
 • Mean Key Frame Displacement Error(M.Disp.):
 Future Locomotion Wemeasuretheaccuracyof the predic- 
 tion using two metrics. (1) K best mean trajectory error 
 (K-MTE):wemeasure Kbesttrajectoryerror: D m = n 1 (cid:88) (cid:107)h i −hˆ i (cid:107) (33)
 i∈Ht 
 1 (cid:88) 
 K MTE= argmin (cid:80) v t x t x (cid:98)t , (31) 
 − v (cid:107) − (cid:107) 
 {Xk}K k=1 t t t H t refers to the set of visible hand positions of key
 frames, andnis the lengthofset H . h denotes the
 t i 
 x R 2 isthepredictedlocationattimet,x istheground 
 t (cid:98)t predictedh and positionin the imagecoordinate,while
 ∈ 
 truthlocation,andv t isthevisibility. Thevisibilityindicates hˆidenotesthegroundtruthhandpositions.
 the availability of the ground truth trajectory, i.e., due to 
 severeegocentricvideos,thegroundtruthtrajectoriesmay 
 includemissing data. v =0 indicatesmissing data attime • Contact Key Frame Displacement Error(C.Disp.):
 t 
 t. (2)Probabilityofcorrecttrajectory(PCT):wemeasure 
 thesuccessrateof the correcttrajectoryretrieval: D c =(cid:107)h c −hˆ c (cid:107) (34)
 (cid:32) (cid:33) 
 1 1 (cid:88) 
 PCT(cid:15)= δ (cid:80) v t x t x (cid:98)t <(cid:15) , (32) h c refersto the handpositionsat Contactframe.
 K v (cid:107) − (cid:107) 
 t t t 
 whereδ()isoneifthestatementistrue and zeroo the rwise. Note that all reports are reported on downsampled video
 · 
 (cid:15)isthetrajectoryerrortolerance,i.e.,ifthetrajectoryerror frames with heightof 256 andoriginalaspectratio.
 72 

 
 
 
 
 
 
 Short-Term Object Interaction Anticipation • Noun+Verb Top-Km AP:predictioni and annotation
 j areapossiblematchif the followingconditions are
 Methods will beevaluatedat the timestampsinwhichnext- 
 satisfied: 
 activeobjects have beenannotated,i.e., 
 * IOU(ˆb ,b )>0.5; 
 (cid:110) i j 
 tt = t l α 
 s * nˆ =n ; 
 | − · i j 
 ∀ t s ∈{ t( s j) |∃ h:B h (j) (cid:54) = ∅} j * vˆ i =v j . 
 (cid:111) 
 l 1,...,m (35) • Noun+TTCTop-Km AP:predictioni and annotation
 ∀ ∈{ } 
 j areapossiblematchif the followingconditions are
 (j) (j) satisfied: 
 where t s h:B = j isthesetofalltimestampsin- 
 { |∃ h (cid:54) ∅} 
 dicating the beginningofaninteraction,forwhichatleast * IOU(ˆb ,b )>0.5; 
 i j 
 onenextactiveobjecthas been annotated, andαandmare 
 * nˆ =n ; 
 definedin Appendix J.3. i j 
 Sincedetectingnextactiveobjectsisamajorpartof the * δˆ δ <T . 
 i j δ 
 | − | 
 task,webase our evaluationmeasuresonmean Average Pre- 
 • Overall Top-Km AP:predictioni and annotationjarea
 cision(m AP),asdefinedin the Pascal VOCchallenge[60]. 
 possiblematchif the followingconditions are satisfied:
 As in standard m AP, we first match each of the detected 
 nextactiveobjectstogroundtruthannotations. Apredicted * IOU(ˆb ,b )>0.5; 
 i j 
 andagroundtruthboundingboxes are apossiblematchif 
 their Intersection Over Union(IOU)valueexceeds 0.5 and * nˆ i =n j ; 
 if some matching criteria are met. We will define match- * vˆ =v ; 
 i j 
 ing criteria later. Predictions are matched to ground truth 
 * δˆ δ <T . 
 annotationsbelongingto the sameevaluatedexampleina | i − j | δ 
 greedy fashion, prioritizing predictions with higher confi- 
 Where T is a tolerance threshold, parameter of the
 δ 
 dencescores and choosingmatchescorrespondingtolarger 
 evaluationmeasure. 
 IOUvalues. Agroundtruthannotation can bematchedat 
 most with onepredictedbox. Allmatchedpredictions are The goal of the different measures is to assess the ability
 countedastruepositives,whereasallunmatchedpredictions ofthe model topredictnextobjectinteractionsatdifferent
 arecountedasfalsepositives. Per for manceon the wholetest levelsofgranularity. we use K =5 and T =0.25.
 δ 
 setissummarizedusing the meanof the Average Precision 
 valuesobtained for eachclass. Long-Term Action Anticipation 
 Toaccount for the multi-modalnatureoffuturepredic- 
 tions(i.e.,morethanonenextactiveobject can belikely), Methods will be evaluated at the set of timestamps spec-
 we “discount” the number of false positives obtained in a ified by the end of each annotated object interaction in a
 givenexampleby the numberofavailablegroundtruthan- video V. Let L ( V j) = { (n ( z j) ,v z (j) ) } Z z=1 bethegroundtruth
 notationsin that examplemultipliedby K 1,where K is annotation related to video V at time-stamp t(j) and let
 − 
 a parameter of the evaluation measure. Specifically, if an (nˆ (j) ,vˆ (j) ) Z K be the K predicted sequences of
 {{ z,k z,k }z=1}k=1 
 example contains two ground truth annotation, we ignore Z actions. We will consider single noun/verb/action pre-
 the(K 1) 2 falsepositives with the highestscores. This dictionscorrectfollowing the definitionsdiscussedin Sec-
 − ∗ 
 effectivelyimplementsa“Top-Kmean Average Precision” tion J.4. The K predictedsequences will hencebeevaluated
 criterion which does not penalize methods for predicting using the editdistancemetricasfollows.
 upto K 1 possiblylikelynextactiveobjectswhich are For a given k, this is obtained by evaluating the edit
 not anno − tated. Given a generic prediction (ˆb i ,nˆ i ,vˆ i ,δˆ i sˆ i ) distancebetweenapredictedsequence and the groundtruth
 andagenericgroundtruthannotation(b j ,n j ,v j ,δ j ),wede- sequenceoffutureactions. Theeditdistance
 fine the followingvariantsof this Top-Kevaluationmeasure 
 consideringdifferentmatchingcriteria: ∆ ( (nˆ (j) ,vˆ (j) ) Z , (n(j),v(j)) Z )
 E { z,k z,k }z=1 { z z }z=1 
 • Noun Top-Km AP:predictioni and annotationj area iscomputedas the Damerau-Levenshteindistance[47,133]
 possiblematchif the followingconditions are satisfied: oversequencesofpredictionsofverbs,nouns and actions.
 Thegoalof this measureistoassessper for manceinaway
 * IOU(ˆb ,b )>0.5; 
 i j whichisrobusttosomeerrorin the predictedorderoffuture
 * nˆ =n ; actions. A predicted verb/noun is considered “correct” if
 i j 
 73 

 
 
 
 
 
 
 it matches the ground truth verb label at a specific time- Verb time to contact
 (softmax) (softplus)
 step. Theallowedoperationstocompute the editdistance 
 areinsertions,deletions,substitutions and transpositionsof f f e e a a t t s s 𝛿 𝛿
 any two predicted actions. Following the “best of many” Video Slow Fast feats 𝛿
 feats 𝛿 
 criterion, the K predictions are evaluated considering the feature pooling 
 Pre-Trained Detector 
 smallesteditdistancebetween the groundtruth and anyof Detected 
 the K predictions: Last Frame R F - a C s N te N r Next Active Objects attachlabels
 output 
 ∆ ( (nˆ (j) ,vˆ (j) ) Z K , (n (j) ,v (j) ) Z )= 
 E {{ z,k z,k }z=1}k=1 { z z }z=1 
 min ∆ ( (nˆ (j) ,vˆ (j) ) Z , (n (j) ,v (j) ) Z ) Figure 57.Short-Termobjectinteractionanticipation base line.
 k=1..K E { z,k z,k }z=1 { z z }z=1 
 Note that we consider edit distance over simple accu- Future Hands Movements Prediction
 racy base dmeasures. Treatingpredictions for eachfuture 
 Baseline Description Theproposedfutureh and movement
 time-stepindependently and calculatingaccuracydoesnot 
 prediction task can be factorized as a regression problem.
 account for thesequentialnatureof the prediction task where 
 To address this task, we adopt a baseline that utilizes the
 theorderofpredictionsisimportant. Weevaluateeachmet- 
 I 3 Dnetworkasthebackbonetoextract the spatial-temporal
 ric independently for verbs, nouns and actions (verb and 
 videorepresentationsof the inputvideosequence,andthen
 nountogether). Wereporteditdistanceat Z =20(ED@20) 
 usealinearmappingfunctionas the regressortopredict the
 anduse K = 5 inourexperiments. Weselect Z = 20 as 
 futurekeyframeh and positions. Weadopt the smootherl 1
 baselinesbegintopredictactionsatr and omforhighervalues 
 lossas the objectivefunction: 
 of Z. 
 (cid:40) 
 0.5 w (h hˆ)2/β, if h hˆ <β 
 J.5 Baselinedefinitions and implementationdetails L h = w ∗ (h ∗ hˆ − 0.5 β), oth | er − wise | (38)
 ∗ | − |− ∗ 
 Future Locomotion Movements Prediction 
 whereh R 20 isavector that representsthex,ycoordinates
 Wemakeuseof the methodby Parketal.[175]fora base line ofbothle ∈ ftandrighth and sin the aforementionedfivefuture
 algorithm. The method models the trajectory prediction key frames. If the hand is not observed in the keyframe,
 function in Equation (26) using KNN classification with we pad 0 into the hˆ, and adopt a binary mask w to pre-
 CNNimageencoding,i.e., ventthegradientspropagationof the seunobservedinstances.
 =KNN( φ( ) ,φ( )) (36) 
 i Training Details Weadopt the I 3 Dmodelas the backbone
 {X} { I } I 
 network and a regression header, composed of two linear
 where KNN(A,B)finds the Kne are stneighborof Bgiven 
 operations, to predict the hand positions in the future key
 the set A, and φ( ) Rn is a function that extracts the 
 I ∈ frames. For our experiments, we set observation time T o
 image feature of . We use the Alex Net image feature 
 I as 2 s. For training, we applied several data augmentation
 extractorforφ. 
 techniques, including random flipping, rotation, cropping
 Notably, the baseline algorithm leverages a polar co- 
 andcolorjitteringtoavoidoverfitting. Our base line model
 ordinate system to represent the trajectory, i.e., X 2 D = 
 j was trained with a batch size of 64 for 25 epochs using
 (cid:2) (cid:3)T 
 r j θ j isa 2 Dtrajectoryon the groundpla new herer i a cosine learning rate decay with a initial learning rate of
 andθ i arethepolarcoordinatesof the trajectoryrepresented 0.0375. Wesetβ to 5 intheweightedsmoothed L 1 lossas
 intheegocentriccoordinatesystem,i.e.,distance(radial)and introducedin Eq.38. 
 direction(angle)withrespectto the person’sfeetlocationas 
 shownin Figure 53: Short-Term Object Interaction Anticipation 
 X 2 D =cart 2 polar(r TX ,r TX ) (37) Data and annotationsused for the experiments Weper-
 j 1 j 2 j 
 formed our experimentsonasubsetof the data and annota-
 wherer andr arethetwospanningvectorsof the ground tionstoobtainverb and nountaxonomiesconsistent with the
 1 2 
 plane that are aligned with the rotation matrix R . r is Short-Term Object-Interaction Anticipation task. Westarted
 t 1 
 the facing direction and r is lateral direction. Both are by considering all annotated actions for which a contact
 2 
 perpendicular to the ground plane normal n as shown in framehas been specifiedby the annotators. Note that these
 Figure 53. cart 2 polar is a coordinate transform from constituteabout 30%ofthewholesetofannotatedactions
 cartesiantopolarcoordinates. andthat the notionofacontactframeisfundamentalto our
 74 

 
 
 
 
 
 
 task. Wethenga the redallannotatedframes and referenced on all frames with annotated next active objects. We use
 themto the irrespectivecontactframes,computing the time the Faster RCNNdetector base don Res Net 50 using the“3 x”
 toactiontargets. Wediscardedallthoseannotationswhich training schedule provided with the Detectron 2 library 19.
 comprisedaverboranounclassmarkedby the annotator After this stage,theweightsof the Faster R-CNNcomponent
 as“null”. Wefur the rdiscardedannotationsrelatedtonouns arenotupdatedanymore. Wehencetraina Slow Fast model
 whichhad been labeledinconsistently and non-objectclasses basedon Res Net 50. Wefollow the configurationprovided
 such as “wall” or “wallpaper”. We similarly removed all inthe Py Slow Fastlibrary 20 totackle the AVAdetection task
 annotationsrelatedto the verb“talk”whichdonotinvolve (“SLOWFAST 32 x 2 R 50 SHORT.yaml”). The Slow Fast
 interactions with objects. modeltakesasinputvideoclipsof 32 framessampled with
 Toavoidhavinganover-specificnountaxonomy,weclus- a temporal stride of 1 frame. During training, we match
 teredselectednounclassesintohomogeneousgroups. For eachdetectedobjectto the groundtruthinstance with largest
 instance the nouns“okra”,“apple”,“celery”and“avocado” Intersection Over Union(IOU),provided that itislargerthan
 haveall been groupedunder the“vegetable fruit”class. We 0.5. Wehenceattach the verb and timetocontactlabelsof
 alsogroupedverbswhich have similarsemanticwhenantici- thegroundtruthboxesto the matchedones. Wethentrain
 pated. Forinstance,theverbs“take”,“carry”,“lift”,“pull” the model applying the followinglossonlytoboxeswhich
 and “remove” have all been grouped in the “take” cluster. have been matchedtogroundtruthinstances:
 Note that while the seactionsmaybevisuallydifferent,they 
 = +λ (39) 
 v ttc 
 all have similar effects on objects, which makes them in- L L L 
 distinguishablewhenanticipated. Wefur the rremovedall where isthecrossentropyloss for verbprediction, is
 v ttc 
 L L 
 annotations related to nouns appearing less than 50 times thesmooth L 1 loss[87]appliedtotimetocontactprediction,
 inthetestset(wefollow the commonsplitdefined for this andwesetλ = 10 tocontrolthecontributionsof the two
 benchmark). Wechoosetoretainonlynounsappearingat losses. Toregulate the numberofframesprocessedby the
 least 50 timesin the testsettoallow for areliableevaluation slowbranch,wesetα=8.Wetrain the modelon 4 NVIDIA
 throughthem APmeasure. V 100 GPUs with abatchsizeof 64 for 50 epochsusinga
 Thefinalsetof data includes 64,798 annotatedexamples cosinelearningratepolicy with abaselearningrateof 0.001.
 in total with 87 nouns and 74 verbs. Our taxonomy is Wevalidatethemodelat the endofeachepoch and consider
 adapted from the one presented in Figure 39. Figure 58 theweightswhichachieved the bestoveralltop-5 m AP on
 and Figure 59 report the distributions of verb and noun thevalidation. 
 annotations in the selected data. Among the 64,798 
 annotations, 27,801 are in the training set, 17,217 are in Long-Term Action Anticipation
 thevalidationset,and 19,780 arein the testset. 
 Baseline Description Thegoalof the baseline model isto
 takeasinputatrimmedvideoofarbitrarylength,andpre-
 Baseline Description Figure 57 illustrates the proposed 
 dict N differentplausiblesequencesoffutureactions. The
 baseline for short-termobjectinteractionanticipation. The 
 baseline model sthusconsistofthreecomponents: (1)the
 baselineincludestwomaincomponents. AFaster R-CNN 
 encoderbackbone for obtainingcliplevelfeatures,(2)the
 objectdetector[87]isusedtodetectnextactiveobjectsin 
 aggregation module for combining the obtained features
 thelastframeof the inputvideoclipprocessedat full reso- 
 from different clips, and (3) the decoder network for de-
 lution. ASlow Fast 3 DCNN[71]ishenceusedtopredict 
 coding the plausible sequences of future actions. For en-
 averblabel and atimetoaction for eachpredictedobject. 
 coderbackbones,weconsiderstateof the artvideorecog-
 This is done by obtained a fixed-length representation of 
 nition networks from both convolutional model, namely,
 each object through ROI pooling [87]. Two linear layers 
 Slow Fast [71] and the newly proposed video trans for mer
 are hence used to predict a probability distribution over 
 models,namely,MVi T[63]. Foraggregationmodule,we
 verbs and apositivequantity for timetocontactprediction 
 experiment with simple concatenation operators that con-
 respectively. Verb probability distributions are obtained 
 catenates the obtainedclipfeatures from multipleinputclips
 using a softmax layer, whereas a softplus activation is 
 aswellastrans for mer base dself-attentionmodules. Forthe
 used for time to contact prediction to make sure that the 
 decodernetworksweconsider the followingoptions:
 prediction is a positive number. The final output of the 
 modelisobtainedbyattaching the predictedverb and time • No Change: Asimplerecognition base line that assumes
 to contact to each detected next active object. The noun nofuturechangein the currentaction and simplypredicts
 label and confidencescores are copied from the outputof thecurrentlyobservedactionasaduplicatedstaticfuture
 the Faster R-CNNcomponent. sequence for Z steps. 
 19 https://github.com/facebookresearch/detectron 2
 Training Details Wefirsttrain the Faster R-CNNcomponent 20 https://github.com/facebookresearch/Slow Fast
 75 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 ekat tup dloh naelc evom tuc tsujda nepo hcuot esolc nrut ward_tniap hcatta hsup_sserp pid ylppa etarepo tih dnas edivid ffo_nrut llor egnarra no_nrut poocs nethgit nori elffuhs worht riaper kcap tresni htooms eparcs kram hcated daenk llird ruop leep dlof erusaem nesool wes dlom tcepsni wercs eit dlew ezeeuqs wercsnu etirw ekahs elif kcits gnah gid evres tcepsni_hcraes llif evig evird etarg yarps yalp pmup emusnoc llorcs kcik retaw tnalp bmilc kcol gniws
 25000 
 20000 
 15000 
 10000 
 5000 
 0 
 Figure 58.Verbdistributionin the Short-Term Object-Interaction Anticipation data.
 
 
 
 
 
 
 doow tiurf_elbategev tnemudni hguod tnalp reniatnoc tecuaf repap gab hsurb wercs eriw dil leehw doof koob tnemec dlom elttob etalp latem kcirb enohp lwob hcnerw revirdwercs efink puc egnops rood gnirts nikpan dor noops reward yart tekcub top guj nap tniap elbat llird roolf rac elkcis egdirf tun tenibac llaw nep lewot telbat rewolf tam remmah eohs erusaem_epat ksam enigne llab rettuc rewom eveis elcycib sdrac_gniyalp krowtra ssalg dnab_rebbur srossics eldeen epor enots ladep hsart llebbmud eulg moorb rekooc was pom retupmoc pmup repilac elit flehs alutaps
 5000 
 4000 
 3000 
 2000 
 1000 
 0 
 Figure 59.Noundistributionin the Short-Term Object-Interaction Anticipation data.
 • Multi Head: This model trains Z independent heads in 
 parallel,one for eachfuturetimestep. Thefinalsequence 
 issimply the conjoinedpredictedactionsofeachhead. 
 Finally, to generate N plausible future sequences for 
 constructing multimodal baselines, we simply sample the 
 predictedfutureactiondistribution N times. Theframework 
 for a particular instantiation of the Multi Head baseline is 
 illustratedin Figure 60. 
 Figure 60.Long-Term Action Anticipation base line.Abaseline
 Training Details Foreachvideo,wesamplemultipleinput 
 modelwitha Slow Fastbackbone,and Z =3 isshownhere.Blue
 clipstoprocess with ourbackbonenetwork. Asingleclip box:clipencodernetwork.Yellowbox:multipleclassifierheads,
 length for both the backbones, Slow Fast and MVi T, com- one for eachfutureaction.See Sec.J.5 formoredetails.
 prises of 16 frames sampled 4 frames apart. Each clip is 
 processedindependentlyby the sameencoderweights and 
 combined with the aggregationmodule. Theaggregatedfea- 
 tureisdecoded with thedecodermodulewhere the output (cid:88) Z 
 = ((pn,pv),(n ,v )) (40) 
 behaviorchangesduringtraining and testing. Intraining,the L lta L v z z z z 
 decoderpredicts the nextactionprobabilitydistributions for z=1 
 each future step. We calculate the sum of losses for each where is cross entropy loss, p∗ refers to the predicted
 L v z 
 predictionas our totalloss: probabilitydistributionoververbs and nouns,and(n ,v )
 z z 
 76 

 
 
 
 
 
 
 Set Metric Mean Median Left Hand Right Hand 
 Set Method 
 Val 5-MTE 5.11 m 2.53 m M.Disp.↓ C.Disp.↓ M.Disp.↓ C.Disp.↓
 Val 3-MTE 6.19 m 2.99 m Val I 3 D+Reg 54.11 57.29 54.73 57.94 
 Val 1-MTE 8.81 m 4.63 m Test I 3 D+Reg 52.98 56.37 53.68 56.17 
 Test 5-MTE 4.84 m 2.69 m 
 Test 3-MTE 5.54 m 3.24 m Table 35.Resultsoffutureh and movementprediction task.Note
 Test 1-MTE 7.66 m 4.73 m that the leftandrighth and smovements are evaluatedseparately.↓
 indicateslowerisbetter 
 Table 33. Resultsof the locomotionprediction task. Wereport 
 mean/median for 7-15 secondpredictions.we use K =1,3,5. 
 Set Method Noun Noun+Verb Noun+TTC Overall
 Set (cid:15)=1 m (cid:15)=2 m (cid:15)=3 m (cid:15)=4 m (cid:15)=5 m (cid:15)=6 m Val FRCNN+Rnd. 17.55 1.56 3.21 0.34
 Val 0.14 0.29 0.39 0.46 0.51 0.54 Val FRCNN+SF 17.55 5.19 5.37 2.07 
 Test 0.16 0.31 0.40 0.47 0.53 0.58 Test FRCNN+Rnd. 20.45 2.22 3.86 0.44 
 Test FRCNN+SF 20.45 6.78 6.17 2.45 
 Table 34. Resultsof the locomotionprediction task. Wereport 
 the probability of correct trajectory (PCT) as varying the error Table 36.Resultsof the short-termobjectinteractionanticipation
 threshold(cid:15). task.Seetext for discussion. 
 referto the groundtruthfutureactionlabels. 
 placementerror(C.Disp.) onbothvalidation and testsets
 During testing, we sample action class labels (nˆ ,vˆ ) 
 z z in Table 35. Our baseline model achieves M.Disp. of
 from the predicteddistributionindependently for eachfuture 
 (52.98/53.68) and C.Disp. of (56.37/56.17) for left/right
 step. We repeat this sampling procedure N times to gen- 
 handpositionpredictionon the testset. Itisworthnoting
 erate multiple cancidate sets of predictions for evaluation 
 thatpredictingh and positionsoncontactframeismorechal-
 describedin Section J.4. 
 lengingthanono the rkeyframes. Thisisbecause,bythe
 We use the taxonomy presented in Figure 39 for our 
 definitionofcontactframe and pre-conditionframe,thean-
 experiments. Wefinetunea Kinetics-400[109]pretrained 
 ticipationtemporalfootprintofcontactframeislargerthan
 encoder backbones on Ego 4 D action recognition and use 
 otherkeyframes. Wefur the rprovidequalitativeresultsof
 this model for all base linestoextract the cliplevelfeatures. 
 our baseline method in Fig. 61. Notably, the model can
 Theaggregationmodule and decodernetworks are trained 
 makereasonablepredictionsonfutureh and positions. How-
 fromr and ominitializationdirectlyon the forecasting task. 
 ever,the model ismorelikelytofailwhen the reisdrastic
 Theencoderweights are keptunchangedduring the decoder 
 embodiedmotions. 
 network training. We set Z = 20 for long horizon future 
 evaluation and K = 5 as the number of plausible future 
 sequences predicted by the model. For all baselines, we 
 sample 2 inputclipstocapturepastcontextunlesso the rwise 
 Short-Term Object Interaction Anticipation 
 specified. We train the model on 8 NVIDIA V 100 GPUs 
 withabatchsizeof 64 for 30 epochs and abaselearningrate 
 Table 36 reportstheresults for the short-termobjectinterac-
 of 0.0001. 
 tionanticipation task onboth the validation and testsets. We
 comp are the proposedbaseline base don Faster RCNN and
 J.6 Results Slow Fast(“FRCNN+SF”inthetable)withasimpler base-
 linewhichuses Faster RCNNtodetectobject and predict
 Future Locomotion Movements Prediction 
 theirclasses,butdrawsverb and TTCpredictionsr and omly
 Weevaluate the KNNbased base linealgorithmbymeasuring from the training set distribution (“FRCNN+Rnd.” in the
 meantrajectoryerror(K-MTE)andprobabilityofcorrect table). Results are reportedin Top-5 m AP%accordingto the
 trajectory (PCT) given an error tolerance. The trajectory differentmatchingcriteriadiscussedin Appendix J.4.Ascan
 length ranges from 7 to 15 seconds (70-150 points in a benoted,theproposed base lineoutper for msr and ompredic-
 trajectorygiven 10 FPS).Our base lineachievesmeanerror tionbybigmarginswhenverbs and TTCs are predictedon
 8.81 mfor 1 MTEand 0.39 for PCT . Theresultis both the validation and testsets. Thissuggests that,despite
 (cid:15)=3 m 
 − 
 summarizedin Table 33 and 34. beingsimple,the base line can leverage the observedvideo
 to anticipate future object interactions. Figure 62 reports
 some qualitative examples of the baseline. The model is
 Future Hands Movements Prediction 
 sometimesabletodetect the nextactiveobjects and predict
 For future hands movements prediction task, we report suitableverbs and TTCs,butper for mancetendstobelimited
 meandisplacementerror(M.Disp.) andcontactframedis- especiallyincomplexscenarios.
 77 

 
 
 
 
 
 
 1.5 sec before PRE 1.0 sec before PRE 0.5 sec before PRE PRE Frame Contact Frame
 
 
 
 
 
 
 
 
 
 
 
 : Ground Truth : Prediction 
 
 Figure 61.Qualitativeexamplesoffutureh and smovementspredictionusing the proposed base line.Thegroundtruthh and spositions are
 plottedasgreencrosses,while the predictedh and spositions are plottedasredcrosses. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 62.Qualitativeexamplesofshort-termobjectinteractionanticipationusing the proposed base line.Thenumbersinbracketsrepresent
 theconfidencescoresassociatedto the predictions. Thegroundtruthnext-activeobjectishighlightedusingadashedredline,whereas
 modelpredictions are reportedinbluesolidlines. 
 
 Long-Term Action Anticipation architecture from Slow Fastto MVi Tgreatlyimprovesverb
 forecasting prediction per for mance, but deteriorates noun
 forecastingper for mance,highlighting the trade-offbetween
 Table 37 shows our resultsonboth the validation and testsets. 
 thetwodespitesimilaractionclassificationper for manceon
 The No Change base linesimplypredicts the currentactionas 
 Kinetics. Finally,includinglargervideocontextin for mation
 thenext Z actions,andper for mspoorlyatpredictingfuture 
 inthe for mofmultipleinputclipsbyusing the trans for mer
 actions. Explicitlytrainingmultipleheadsimprovesper for- 
 basedaggregatormoduleresultsin the bestper for mance.
 manceonverbs,nouns and actions. Changing the backbone 
 78 

 
 
 
 
 
 
 Valset ED@(Z=20) Future Locomotion Movements Prediction 
 Backbone Aggregator Decoder Verb Noun Action 
 Slow Fast Concat No Change 0.766 0.830 0.960 The base linequantitativeresultson the locomotionpredic-
 Slow Fast Concat Multi Head 0.747 0.808 0.952 
 tion task imply that the visualcues,e.g.,sidewalk,obstacles,
 MVi T Concat Multi Head 0.707 0.901 0.972 
 Slow Fast Trans for mer Multi Head 0.745 0.779 0.941 androad,inegocentricimages are highlyindicativeoffu-
 turemovement. However,the base linemethod that encodes
 test set ED@(Z=20) the visual semantics of an image with a global feature is
 Backbone Aggregator Decoder Verb Noun Action 
 notdetailedenoughto model complexwalkingmovement,
 Slow Fast Concat No Change 0.761 0.810 0.959 
 e.g., avoiding pedestrians. This opens an opportunity for
 Slow Fast Concat Multi Head 0.743 0.791 0.948 
 MVi T Concat Multi Head 0.697 0.904 0.969 challengeparticipantstoincorporateafine-grainedvisual
 Slow Fast Trans for mer Multi Head 0.739 0.780 0.943 representation. 
 Table 37.Resultsof the long-termactionanticipation task.Lower 
 isbetter.Seetext for discussion. Future Hands Movements Prediction 
 Our base line model for futureh and smovementsprediction
 suffers from the drasticheadmovementsinegocentricvideo
 Figure 63 showssomequalitativeresultsof our method. and the stochasticnatureoffuture for ecasting. Wespeculate
 Ineachrow,thegroundtruthfutureactions are shownalong thatexplicitly model ing the headmovements and next-active
 with the predictions from ourmodel(for 5 time-steps). Cor- objectsmaycomplement the videorepresentations for pre-
 rectpredictions are highlightedingreen,whilevalidactions dictingfutureh and smovements.
 that areincorrectly ordered(or paritallycorrect) arehigh- 
 lightedinblue. Note that thoughnotperfectlyaligned,in- 
 Short-Term Object Interaction Anticipation 
 correctlyorderedsequences are givenpartialcreditvia the 
 edit-distancemetric. Theshort-termobjectinteractionanticipationresultshigh-
 light that the proposed task ischallenging,with the baseline
 achievinganoverall Top-5 m AP of 2.07%onthevalidation
 setand 2.45%onthetestset. Thekeychallenges are likely
 dueto the uncertainnatureoffuturepredictionsaswellas
 J.7 Discussion 
 totheinabilityof the objectdetectortocorrectlydetectnext
 activeobjects and ignore the others. Never the less,thepro-
 Data Annotation 
 posed baseline, even if simple, allows to greatly improve
 overacombinationofanobjectdetectorandar and ompre-
 Annotating the videos for for ecasting task sposedanumber dictionofverbs and timetocontactquantities. Thissuggests
 ofinterestingchallenges. First,wefound the diversityof the thatmethods can learntoanalyze the inputvideoinorderto
 dataledtoalarge and diversetaxonomy,whichsomeanno- makereasonablepredictionsabout the future.
 tatorsfoundhardtonavigate. Hence,wefoundanumberof 
 annotatorsused the”OTHER”option,whichweeventually 
 Long-Term Action Anticipation 
 manuallymappedto the taxonomywherepossible. Infuture 
 annotations, we plan to ask annotators to always pick the Wediscussseveralimportantaspectsof the long-termaction
 closesttaxonomyitemevenifwritinginafree-form OTHER forecastingproblemthrough our experiments and ablation
 label,toenc our agethemtostickto the taxonomyasmuch studies. All ablations are run with Slow Fast backbone
 aspossible. Second,wenoticedannotatorsstruggled with networks,and models are trained for 30 epochs.
 definingboundingboxesover“stuff”categories. Forexam- 
 ple,whenlabeling“cuttinggrass”,itwasoftenchallenging Howimportantis Ego 4 Dactionrecognitionpre-training?
 to draw a box that covers the full extent of the object of Table 38 shows the per for mance of our models when
 change(i.e.“grass”). Finally,itwassometimeschallenging pretrained only on Kinetics-400 action recognition (as
 todefi new hat the objectofchangewas,whenusinglarge opposed to further fine-tuning on Ego 4 D action recogni-
 tools. For example, if using a lawn mower to clear grass, tion). All models benefit greatly from training on Ego 4 D
 doesoneconsiderthemowerasthetool and hence the grass data in two ways. First, there is a large domain gap
 astheobjectofchange,orthelevers and buttonsinside the between Kinetics and Ego 4 D both in terms of visuals
 mower as the object of change. We chose to rely on the (third-person vs. egocentric viewpoint) and the diver-
 narratorstodefi new hichinteractiontolabel(i.e.pushing sity of activities they contain, which pre-training helps
 thelever/buttonvscuttinggrass),andasked the annotators account for. Second, action recognition models benefit
 tolabeltools and objectsaccordingly. frombiasesin the labelstructureoffutureactionsasseen
 79 

 
 
 
 
 
 
 ED@5: 0.60 
 take → hold → cut → put → take 
 GT: 
 sickle spinach spinach sickle rubber band 
 take → cut → hold → cut → throw 
 PRED: 
 sickle spinach spinach spinach sickle 
 ED@5: 0.80 smooth → remove → smooth → sand → remove 
 GT: 
 wood sander wood wood sander 
 hold → sand → hold → sand → sand 
 PRED: 
 sander wood sander wood wood 
 Figure 63.Longtermactionanticipation-qualitativeresults.Actionsingreenrepresentcorrectpredictions(correctaction,atthecorrect
 position).Actionsinbluerepresentincorrectorderingofvalidactions.Ouredit-distancemetricaccounts for bothcases.
 Val Set ED@(Z=20) 
 Init Backbone Aggregator Verb Noun Action 0.800 
 K 400 Slow Fast Concat 0.752 0.820 0.958 0.775 
 +Ego 4 D Slow Fast Concat 0.747 0.808 0.952 
 0.750 
 K 400 Slow Fast Trans for mer 0.746 0.809 0.953 
 +Ego 4 D Slow Fast Trans for mer 0.745 0.779 0.941 0.725 
 0.700 
 Table 38.Longtermanticipation-varyingpretraining data.Mul- 
 0.675 
 ti Head decoder used for all models. Ego 4 D action recognition 
 0.650 pretraininggreatlyimprovesdownstreamforecastingper for mance.
 0.625 
 Val Set ED@(Z=20) 1 2 3 4 5 6 7 8 91011121314151617181920
 Z 
 #clips Backbone Aggregator Verb Noun Action 
 2 Slow Fast Trans for mer 0.743 0.790 0.946 
 4 Slow Fast Trans for mer 0.744 0.796 0.947 
 8 Slow Fast Trans for mer 0.745 0.779 0.941 
 Table 39.Longtermanticipation-varyingnumberofinputclips. 
 Multi Headdecoderused for allmodels. Per for manceincreases 
 withmoreinputcontext. 
 from the per for manceof the No Change base linein Table 37. 
 How important is past context for trans for mer based 
 models? Our trans for mer aggregation modules aggregate 
 informationacrossalargertemporalhistorycontrolledby 
 thenumberofinputclipsto the model. Table 39 shows the 
 sensitivity of these models to the amount of past context 
 video that ithasaccessto. Overall,per for manceincreasesas 
 morecontextin for mationisprovidedto the model,however 
 thisincreasecomesat the costofmemoryconsumption— 
 8 is the maximum number of clips that can be fit in GPU 
 memory. 
 Howfarinto the future can modelspredict? Asmentioned 
 in Section J.4 wereportresults for predictionsat Z = 20 
 as baselines begin to predict actions at random for higher 
 values of Z. Figure 64 shows the plot of edit distance vs. 
 Z@DE 
 verbs 
 nouns 
 Figure 64.Per for mancevs.numberoffutureactions Z.Predicting
 furtherinto the futureisnaturallymoredifficult.Modelsbeginto
 predictclosetor and omactions for veryhighvaluesof Z.
 Z forour base linemodels. Asexpected,itisf are asierto
 anticipateactions that occurimmediatelynext,whichgets
 moredifficultas Z increases,andsteadilyplateaus.
 Howtogeneratemultiple can didatepredictions? Asmen-
 tioned in Section J.4 we evaluate the best of K = 5 pre-
 dictions to arrive at our final results. To generate the K
 predictions,wesampleeachclassifierheadindependently,
 however the reareseveralmethodstoimprove this includ-
 ingheuristicsearchalgorithms(likebeamsearch). Ideally,
 the multi-modal nature of future prediction should be ac-
 countedforin the modeldesignitself. Moreover,decoder
 models that takeintoaccount the sequentialnatureduring
 inferenceshould beconsidered. Theseincludetrans for mer
 baseddecoders that are popularinrecentlanguagemodels
 (e.g.,BERT,GPT)Thisisanimportantfuturedirectionof
 research. 
 80 

 
 
 
 
 
 
 J.8 Contributionsstatement 
 Giovanni Maria Farinellaled the Forecasting Benchmark 
 workingonthedefinitionof the proposedtasks,onthecol- 
 lection,andwriting the paper. 
 Rohit Girdharco-ledthe Forecasting Benchmarkworking 
 onthedefinitionof the proposedtasks,onthecollection,and 
 writing the paper. 
 Antonino Furnari contributed to the definition of the pro- 
 posedbenchmarktasks and inparticularto the Short-Term 
 Object Interaction Anticipation task and has been key 
 driver of implementation, collection, annotation develop- 
 mentthroughout the project,andwriting the paper. 
 Ilija Radosavovicworkedon the definitionoftasks and has 
 been key driver of implementation, collection, annotation 
 developmentthroughout the project,andwriting the paper. 
 Tushar Nagarajancontributedtothedefinitionof the pro- 
 posedbenchmarktasks and inparticularto the Long-Term 
 Action Anticipation task and has been keydriverofimple- 
 mentation,collection,annotationdevelopmentthroughout 
 theproject,andwriting the paper. 
 Tullie Murrell worked on baseline implementation of the 
 Long-Term Action Anticipation task. 
 Karttikeya Mangalamworkedon base lineimplementation, 
 experiments and writing the Long-Term Action Anticipation 
 task. 
 Christoph Feichtenhofer oversaw the development of the 
 task,baselines and implementationof the Long-Term Action 
 Anticipation task. 
 Miao Liuworkedon the definitionof Future Hands Move- 
 ment Prediction task and has been keydriverofimplemen- 
 tation, collection, annotation development throughout the 
 project,andwriting the paper. 
 Wenqi Jiaworkedon base lineimplementationof the Future 
 Hands Movement Prediction task. 
 Zachary Chavisworkedon the Locomotion Forecasting task 
 andhas been keydriverofimplementation,collection,and 
 annotationdevelopmentthroughout the project. 
 Hyun Soo Park worked on the definition of Locomotion 
 Forecasting tasks, collection, annotation, and writing the 
 paper. 
 
 
 
 
 
 
 
 
 
 
 81 
 
 
 

 
 
 
 
 
 
 K.Societal Impact 
 Ourcontribution can positivelyimpactvideounderst and- 
 ing. Itoffers the researchcommunityalarge-scaleresource 
 captured with rigorousprivacyandethicsst and ards(detailed 
 in Appendix Aand B)toge the rwithadiversityofsubjects, 
 and the benchmarks will promotereproducibletechnicalad- 
 vances. Morebroadly,egocentricperceptionhas the poten- 
 tialtopositivelyimpactsocietyinmanyapplicationdomains, 
 includingassistivetechnology,education,fitness,entertain- 
 ment and gaming,elderc are,robotics,andaugmentedreality. 
 None the less, future research in this area must guard 
 against the potentialnegativesocietalimpactiftechnology 
 foregocentricvision were misused. 
 First, there are riskssurroundingprivacy. Aswebegin 
 toseeaproliferationofwearablecamerasinpublicspaces, 
 producersof the sewearabledevices will needtodevelop and 
 implement protocols for notice and consent regarding the 
 collectionof data inpublicspaces,aswellasusercontrols 
 for how such data may be used, stored, and shared with 
 any third parties. Similarly, models that may be used to 
 transcribespeechorperformo the rtasksrelatedtofootage 
 should include robust user controls such as the ability to 
 removeorobscurepersonal data orsensitivecontent. 
 Note that for all our audio-visual and socialbenchmarking 
 work, the data usedhas full consent from the participants 
 inthevideo,i.e.,touse the irunblurredfaces and audioof 
 their conversation. To date, the research community has 
 lacked any large-scale data resource with which to study 
 thesekindsofproblems;Ego 4 Dwillhelp the communityto 
 consider new solutionswhileleveragingreal-world,diverse 
 data that respects the privacyprotocolsofdifferentcountries. 
 Fur the rmore,the Ego 4 Ddataisavailableonly for userswho 
 signalicense that enumeratestheallowableusesof the data, 
 whichisintendedtohinderpotentialnegativeapplications. 
 Second,thereisarisk that ourlarge-scalecollectioncould 
 inspirefuturecollectionef for tswithout the samelevelofc are 
 orattentionto the privacy and ethicalconcernsas were taken 
 in Ego 4 D.Tomitigate this risk,wehaveaimedtobecompre- 
 hensiveinourdescriptionsofallpartsof our procedures,and 
 wewillinclude our bestpracticesrecommendationswhen 
 publiclydisseminatingtheresultsof the project. 
 Finally,despite our bestef for tsasdiscussedin the main 
 paper,there are stillsomeimbalancesin the dataset. Forex- 
 ample,thedata from Rwandaisrelativelysmall,andthough 
 74 citiesrepresentsaleapincoverage,theydonotcapture 
 allpossibledemographics. Weacknowledge that nomatter 
 howfaronegoes,fullglobalcoverageofdailylifeactivity 
 iselusive. Still,we canmitigate this riskbycontinuingto 
 growglobalcollaborations with researchers and participants 
 inunderrepresentedareas. 
 82 
 
 
 

 
 
 
 
 
 
 References 
 [16] Sven Bambach,Stefan Lee,David J.Crandall,and Chen Yu.
 Lendingah and:Detectinghands and recognizingactivities
 [1] Github repository of the ESPNet model zoo. https: 
 incomplexegocentricinteractions. In The IEEEInterna-
 //github.com/espnet/espnet_model_zoo. We 
 tional Conferenceon Computer Vision(ICCV),December
 used the Shinji Watanabe/gigaspeech_asr_ 
 2015. 3 
 train_asr_raw_en_bpe 5000_valid.acc.ave 
 [17] Mark ABee and Christophe Micheyl. Thecocktailparty
 model. 60,61 
 problem:whatisit?how can itbesolved?andwhyshould
 [2] Kaldi English GLM file. https://github.com/ 
 animalbehavioristsstudyit? Journalofcomparativepsy-
 kaldi-asr/kaldi/blob/master/egs/ami/s 5/ 
 chology,122(3):235,2008. 52 
 local/english.glm. 61 
 [18] Keni Bernardin,Alexander Elbs,and Rainer Stiefelhagen.
 [3] NISTSRE 2000 Evaluation Plan. https://www.nist. 
 Multipleobjecttrackingper for mancemetrics and evaluation
 gov/sites/default/files/documents/2017/ 
 inasmartroomenvironment. In Sixth IEEEInternational
 09/26/spk-2000-plan-v 1.0.htm_.pdf. 56 
 Workshopon Visual Surveillance,inconjunction with ECCV,
 [4] Yazan Abu Farha, Alexander Richard, and Juergen Gall. volume 90.Citeseer,2006. 8,53
 When will youdowhat?-anticipatingtemporaloccurrences 
 [19] Keni Bernardin and Rainer Stiefelhagen. Evaluatingmul-
 ofactivities. In Computer Vision and Pattern Recognition, 
 tiple object tracking per for mance: the clear mot metrics.
 pages 5343–5352,2018. 3 
 EURASIPJ our nalon Image and Video Processing,2008:1–
 [5] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, 10,2008. 8,53 
 Oriol Vinyals,and Andrew Zisserman. Deepaudio-visual 
 [20] Gedas Bertasius,Hyun Soo Park,Stella X.Yu,and Jianbo
 speechrecognition. IEEEtransactionsonpatternanalysis 
 Shi. First-personaction-objectdetectionwi the gonet. In
 andmachineintelligence,2018. 8,51 
 Proceedingsof Robotics:Science and Systems,July 2017. 9
 [6] Triantafyllos Afouras,Joon Son Chung,and Andrew Zisser- 
 [21] Cigdem Beyan,Francesca Capozzi,Cristina Becchio,and
 man. Theconversation:Deepaudio-visualspeechenhance- 
 Vittorio Murino. Predictionof the leadershipstyleofan
 ment. In Interspeech,2018. 51 
 emergentleaderusingaudio and visualnonverbalfeatures.
 [7] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, IEEETransactionson Multimedia,20(2):441–456,2018. 9
 and Andrew Zisserman. Self-supervised Learningof Audio- 
 [22] Goutam Bhat,Martin Danelljan,Luc Van Gool,and Radu
 Visual Objects from Video. In Proceedingsof the European 
 Timofte. Know Your Surroundings:Exploiting Scene Infor-
 Conferenceon Computer Vision(ECCV 20),volume 12363 
 mation for Object Tracking. ar Xiv:2003.11014[cs],May
 LNCS,pages 208–224,2020. 9 
 2020. 35,36 
 [8] Jean-Baptiste Alayrac,Josef Sivic,Ivan Laptev,and Simon 
 [23] Eric Brachmann, Alexander Krull, Frank Michel, Stefan
 Lacoste-Julien. Jointdiscoveryofobjectstates and manipu- 
 Gumhold, Jamie Shotton, and Carsten Rother. Learning
 lationactions. ICCV,2017. 7,44,45 
 6 dobjectposeestimationusing 3 dobjectcoordinates. In
 [9] Humam Alwassel,Fabian Caba Heilbron,Victor Escorcia, Europeanconferenceoncomputervision,pages 536–551.
 and Bernard Ghanem. Diagnosingerrorintemporalaction Springer,2014. 7 
 detectors. In Proceedingsof the European Conferenceon 
 [24] Tom B.Brown,Benjamin Mann,Nick Ryder,Melanie Sub-
 Computer Vision(ECCV),2018. 41,42 
 biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
 [10] Xavier Anguera,Simon Bozonnet,Nicholas Evans,Corinne tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
 Fredouille,Gerald Friedl and,and Oriol Vinyals. Speaker hini Agarwal,Ariel Herbert-Voss,Gretchen Krueger,Tom
 diarization:Areviewofrecentresearch. IEEETransactions Henighan,Rewon Child,Aditya Ramesh,Daniel M.Ziegler,
 onaudio,speech,andlanguageprocessing,20(2):356–370, Jeffrey Wu,Clemens Winter,Christopher Hesse,Mark Chen,
 2012. 8,53,56 Eric Sigler,Mateusz Litwin,Scott Gray,Benjamin Chess,
 [11] Xavier Anguera Miro´. Robustspeakerdiarization for meet- Jack Clark,Christopher Berner,Sam Mc Candlish,Alec Rad-
 ings. Universitat Polite`cnicade Catalunya,2006. 8,54 ford,Ilya Sutskever,and Dario Amodei. Languagemodels
 [12] Stanislaw Antol,Aishwarya Agrawal,Jiasen Lu,Margaret arefew-shotlearners,2020. 9
 Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi [25] Ian MBullock, Thomas Feix, and Aaron MDollar. The
 Parikh. VQA:Visual Question Answering. In International yalehumangrasping data set:Grasp,object,and task datain
 Conferenceon Computer Vision(ICCV),2015. 7 household and machinesh open vironments. IJRR,2015. 45
 [13] Mehmet Ali Arabacı,Fatih O¨zkan,Elif Surer,Peter Jancˇovicˇ, [26] Minjie Cai,Kris MKitani,and Yoichi Sato. Underst and-
 and Alptekin Temizel. Multi-modal egocentric activity ingh and-objectmanipulation with grasptypes and object
 recognition using audio-visual features. ar Xiv preprint attributes. In RSS,2016. 3
 ar Xiv:1807.00612,2018. 51 [27] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas
 [14] Relja Arandjelovic´ and Andrew Zisserman. Objects that Usunier,Alexander Kirillov,and Sergey Zagoruyko. End-
 sound. In ECCV,2018. 8,51 to-end object detection with trans for mers. In European
 [15] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Conferenceon Computer Vision,pages 213–229.Springer,
 and Michael Auli. wav 2 vec 2.0: A framework for self- 2020. 49 
 supervisedlearningofspeechrepresentations.ar Xivpreprint [28] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike
 ar Xiv:2006.11477,2020. 60 Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec,
 83 

 
 
 
 
 
 
 Vasilis Karaiskos,Wessel Kraaij,Melissa Kronenthal,etal. [41] Kenneth Church and Patrick Hanks.Wordassociationnorms,
 The AMI meeting corpus: A pre-announcement. In In- mutualin for mation,andlexicography. Computationallin-
 ternationalworkshoponmachinelearning for multimodal guistics,16(1):22–29,1990. 71
 interaction,pages 28–39.Springer,2006. 56 [42] Dima Damen, Hazel Doughty, Giovanni Farinella, Sanja
 [29] Joao Carreira and Andrew Zisserman. Quovadis, action Fidler,Antonino Furnari,Evangelos Kazakos,Davide Molti-
 recognition?anew model and the kinetics data set. Inpro- santi,Jonathan Munro,Toby Perrett,Will Price,etal. The
 ceedingsof the IEEEConferenceon Computer Vision and epic-kitchens data set:Collection,challenges and baselines.
 Pattern Recognition,pages 6299–6308,2017. 48,49 IEEETransactionson Pattern Analysis&Machine Intelli-
 gence,(01):1–1,2020. 52 
 [30] Chien-Yi Chang,De-An Huang,Danfei Xu,Ehsan Adeli, 
 Li Fei-Fei,and Juan Carlos Niebles. Procedureplanning [43] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
 ininstructionalvideos. ar Xivpreprintar Xiv:1907.01172, ,Antonino Furnari,Jian Ma,Evangelos Kazakos,Davide
 2019. 45 Moltisanti,Jonathan Munro,Toby Perrett,Will Price,and
 Michael Wray. Rescalingegocentricvision. IJCV,2021. 2,
 [31] Sourish Chaudhuri,Joseph Roth,Daniel PWEllis,Andrew 
 3,45 
 Gallagher,Liat Kaver,Radhika Marvin,Caroline Pantofaru, 
 Nathan Reale,Loretta Guarino Reid,Kevin Wilson,etal. [44] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
 Ava-speech:Adenselylabeled data setofspeechactivityin Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-
 movies. ar Xivpreprintar Xiv:1808.00606,2018. 8,52 vide Moltisanti,Jonathan Munro,Toby Perrett,Will Price,
 and Michael Wray. Scaling egocentric vision: The epic-
 [32] C. Chen, U. Jain, C. Schissler, S. V. Amengual Gari, 
 kitchens dataset. In European Conference on Computer
 Z. Al-Halah, V. Ithapu, P. Robinson, and K. Grauman. 
 Vision(ECCV),2018. 2,3,5,20,52 
 Soundspaces:Audio-visualnavigationin 3 denvironments. 
 [45] Dima Damen,Teesid Leelasawassuk,Osian Haines,Andrew
 In ECCV,2020. 62 
 Calway,and Walterio Mayol-Cuevas.You-Do,I-Learn:Dis-
 [33] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi- 
 covering task relevantobjects and the irmodesofinteraction
 cenc Amengual Gari,Ziad Al-Halah,Vamsi Krishna Ithapu, 
 frommulti-useregocentricvideo. In BMVC,2014. 44,45
 Philip Robinson,and Kristen Grauman. Audio-visualem- 
 [46] Dima Damen,Teesid Leelasawassuk,and Walterio Mayol-
 bodiednavigation. environment,97:103,2019. 8 
 Cuevas. You-do,i-learn:Egocentricunsuperviseddiscovery
 [34] Guoguo Chen,Shuzhou Chai,Guanbo Wang,Jiayu Du,Wei- 
 of objects and their modes of interaction towards video-
 Qiang Zhang,Chao Weng,Dan Su,Daniel Povey,Jan Trmal, 
 basedguidance. CVIU,2016. 3 
 Junbo Zhang,etal. Gigaspeech:Anevolving,multi-domain 
 [47] Fred JDamerau. Atechnique for computerdetection and
 asrcorpus with 10,000 hoursoftranscribedaudio. ar Xiv 
 correctionofspellingerrors. Communicationsof the ACM,
 preprintar Xiv:2106.06909,2021. 60 
 1964. 73 
 [35] Xinlei Chen,Hao Fang,Tsung-Yi Lin,Ramakrishna Vedan- 
 [48] Ana Garcia Del Molino,Cheston Tan,Joo-Hwee Lim,and
 tam,Saurabh Gupta,Piotr Dolla´r,and CLawrence Zitnick. 
 Ah-Hwee Tan. Summarizationofegocentricvideos:Acom-
 Microsoft coco captions: Data collection and evaluation 
 prehensivesurvey. IEEETransactionson Human-Machine
 server. ar Xivpreprintar Xiv:1504.00325,2015. 7 
 Systems,47(1),2016. 3 
 [36] Eunji Chong, Elysha Clark-Whitney, Audrey Souther- 
 [49] Jia Deng,Wei Dong,Richard Socher,Li-Jia Li,Kai Li,and
 land, Elizabeth Stubbs, Chanel Miller, Eliana L Ajodan, 
 Li Fei-Fei. Image Net: A large-scale hierarchical image
 Melanie RSilverman, Catherine Lord, Agata Rozga, Re- 
 data base. In CVPR,2009. 1,3 
 becca MJones,and James MRehg.Detectionofeyecontact 
 [50] Daniel De Tone, Tomasz Malisiewicz, and Andrew Rabi-
 withdeepneuralnetworksisasaccurateashumanexperts. 
 novich. Superpoint:Self-supervisedinterestpointdetection
 Nature Communications,11(1):6386,dec 2020. 9 
 anddescription. In CVPRWorkshop,2018. 38
 [37] Eunji Chong,Yongxin Wang,Nataniel Ruiz,and James M. 
 [51] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina
 Rehg. Detecting Attended Visual Targetsin Video. In Pro- 
 Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans-
 ceedingsof the IEEEConferenceon Computer Vision and 
 formers for language underst and ing. ar Xiv:1810.04805,
 Pattern Recognition(CVPR 20),pages 5395–5405,Seattle, 
 2018. 29 
 WA,2020. 9,66 
 [52] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina
 [38] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae 
 Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans-
 Lee,Hee Soo Heo,Soyeon Choe,Chiheon Ham,Sunghwan 
 formers for languageunderst and ing. In Proceedingsof the
 Jung,Bong-Jin Lee,and Icksang Han. Indefenceofmetric 
 2019 Conferenceof the North Ameri can Chapterof the As-
 learning for speakerrecognition. In Interspeech,2020. 64 
 sociation for Computational Linguistics:Human Language
 [39] Joon Son Chung,Jaesung Huh,Arsha Nagrani,Triantafyl- Technologies, Volume 1 (Long and Short Papers), pages
 los Afouras, and Andrew Zisserman. Spot the conver- 4171–4186,Minneapolis,Minnesota,June 2019.Associa-
 sation: speaker diarisation in the wild. ar Xiv preprint tion for Computational Linguistics. 40
 ar Xiv:2007.01216,2020. 8,52 [53] Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark
 [40] J.S.Chung, A.Nagrani, and A.Zisserman. Vox Celeb 2: Broyles,Hao Jiang,Jie Shen,Maja Pantic,Vamsi Krishna
 Deep Speaker Recognition. In INTERSPEECH,2018. 52, Ithapu,and Ravish Mehra. Easycom:Anaugmentedreality
 57 dataset to support algorithms for easy communication in
 84 

 
 
 
 
 
 
 noisyenvironments.ar Xivpreprintar Xiv:2107.04174,2021. [66] Alireza Fathi,Jessica K.Hodgins,and James M.Rehg. So-
 8,52 cialinteractions:Afirst-personperspective. In CVPR,2012.
 [54] Bardia Doosti, Ching-Hui Chen, Raviteja Vemulapalli, 3 
 Xuhui Jia,Yukun Zhu,and Bradley Green. Boostingimage- [67] A.Fathi,J.K.Hodgins,and J.M.Rehg. Socialinteractions:
 basedmutualgazedetectionusingpseudo 3 dgaze.In Thirty- Afirst-personperspective. In Proceedingsof the IEEECon-
 Fifth AAAIConferenceon Artificial Intelligence,pages 1273– ferenceon Computer Vision and Pattern Recognition(CVPR
 1281,2021. 9 12),pages 1226–1233.IEEE,jun 2012. 9 
 [68] A. Fathi and J. Rehg. Modeling actions through state
 [55] Hazel Doughty,Ivan Laptev,Walterio Mayol-Cuevas,and 
 changes. In CVPR,2013. 7 
 Dima Damen. Actionmodifiers: Learning from adverbs 
 [69] Alireza Fathi and James MRehg. Modelingactionsthrough
 ininstructionalvideos. ar Xivpreprintar Xiv:1912.06617, 
 statechanges. In CVPR,2013. 44,45 
 2019. 45 
 [70] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
 [56] Matteo Dunnhofer, Antonino Furnari, Giovanni Maria 
 Kaiming He. Slowfastnetworks for videorecognition. In
 Farinella, and Christian Micheloni. Isfirstpersonvision 
 ICCV,2019. 3,5,40,41,48,49 
 challenging for object tracking? In IEEE/CVF Interna- 
 [71] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
 tional Conferenceon Computer Vision Workshops(ICCVW) 
 Kaiming He. Slowfastnetworks for videorecognition. In
 -Visual Object Tracking Challenge,2021. 3 
 Proceedingsof the IEEE/CVFinternationalconferenceon
 [57] Ariel Ephrat,Inbar Mosseri,Oran Lang,Tali Dekel,Kevin 
 computervision,pages 6202–6211,2019. 40,75
 Wilson, Avinatan Hassidim, William T Freeman, and [72] Jonathan Fiscus. NIST sclite sscoring toolkit. https:
 Michael Rubinstein. Lookingtolistenat the cocktailparty: //github.com/usnistgov/SCTK. 61
 Aspeaker-independentaudio-visual model for speechsepa- 
 [73] Jianglin Fu,Ivan VBajic´,and Rodney GVaughan. Datasets
 ration. In SIGGRAPH,2018. 8,51 
 forface and objectdetectioninfisheyeimages.Datainbrief,
 [58] Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops! 27:104752,2019. 57 
 predictingunintentionalactioninvideo. In Arxiv,2019. 44 [74] Antonino Furnari, Sebastiano Battiato, Kristen Grauman,
 [59] N.Ryantet.al.The Second DIHARDDiarization Challenge: and Giovanni Maria Farinella. Next-active-objectprediction
 dataset,task,and base lines. In Proceedingsof Interspeech, fromegocentricvideos. Journalof Visual Communication
 2019. 56 and Image Representation,49:401–411,2017. 9
 [60] Mark Everingham,Luc Van Gool,Christopher KIWilliams, [75] Antonino Furnari and Giovanni Farinella. Rolling-unrolling
 John Winn,and Andrew Zisserman.Thepascalvisualobject lstms for actionanticipation from first-personvideo. IEEE
 classes(voc)challenge. Internationalj our nalofcomputer Transactionson Pattern Analysis and Machine Intelligence,
 vision,88(2):303–338,2010. 1,73 2020. 3 
 [76] Antonino Furnari and Giovanni Maria Farinella.Whatwould
 [61] Bernard Ghanem Fabian Caba Heilbron,Victor Escorcia and 
 you expect? anticipating egocentric actions with rolling-
 Juan Carlos Niebles.Activitynet:Alarge-scalevideobench- 
 unrolling lstms and modality attention. In International
 mark for human activity underst and ing. In Proceedings 
 Conferenceon Computer Vision,2019. 9 
 ofthe IEEEConferenceon Computer Vision and Pattern 
 [77] Jiyang Gao,Zhenheng Yang,and Ram Nevatia. Red: Re-
 Recognition,pages 961–970,2015. 1,3,30,32 
 inforcedencoder-decodernetworks for actionanticipation.
 [62] Heng Fan,Haibin Ling,Liting Lin,Fan Yang,Peng Chu, 
 BMVC,2017. 9 
 Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, and Chunyuan 
 [78] R.Gao, R.Feris, and K.Grauman. Learningtoseparate
 Liao. La SOT:AHigh-Quality Benchmark for Large-Scale 
 objectsoundsbywatchingunlabeledvideo. In ECCV,2018.
 Single Object Tracking. In 2019 IEEE/CVFConferenceon 
 8 
 Computer Vision and Pattern Recognition(CVPR),pages 
 [79] Ruohan Gao,Rogerio Feris,and Kristen Grauman.Learning
 5369–5378,Long Beach,CA,USA,June 2019.IEEE. 35 
 toseparateobjectsoundsbywatchingunlabeledvideo. In
 [63] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao 
 ECCV,2018. 51 
 Li, Zhicheng Yan, Jitendra Malik, and Christoph Feicht- 
 [80] Ruohan Gaoand Kristen Grauman. 2.5 dvisualsound. In
 enhofer. Multi scale vision trans for mers. ar Xiv preprint 
 CVPR,2019. 8,51 
 ar Xiv:2104.11227,2021. 75 
 [81] Ruohan Gaoand Kristen Grauman. Co-separatingsounds
 [64] Yue Fan,JWKang,LTLi,KCLi,HLChen,STCheng,PY ofvisualobjects. In ICCV,2019. 51
 Zhang,ZYZhou,YQCai,and Dong Wang. CN-CELEB:a [82] R.Gaoand K.Grauman. Visual Voice:Audio-visualspeech
 challenging Chinesespeakerrecognition data set.In ICASSP separation with cross-modalconsistency. In CVPR,2021. 8,
 2020-2020 IEEE International Conference on Acoustics, 51 
 Speech and Signal Processing(ICASSP),pages 7604–7608. [83] I.Gebru,S.Ba,X.Li,and R.Horaud. Audio-visualspeaker
 IEEE,2020. 57 diarization base donspatiotemporalbayesianfusion. PAMI,
 [65] Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, 2018. 8,51 
 Li Song,and Guangtao Zhai. Dual Attention Guided Gaze [84] Israel D.Gebru,Sile`ye Ba,Xiaofei Li,and Radu Horaud.
 Target Detectionin the Wild. In Proceedingsof the IEEE Audio-visualspeakerdiarization base donspatiotemporal
 Conferenceon Computer Vision and Pattern Recognition bayesianfusion.IEEETransactionson Pattern Analysis and
 (CVPR 21),2021. 9 Machine Intelligence,39,2017. 52 
 85 

 
 
 
 
 
 
 [85] Georgios Georgakis,Md Alimoor Reza,Arsalan Mousavian, [99] Lianghua Huang,Xin Zhao,and Kaiqi Huang. GOT-10 k:A
 Phi-Hung Le,and Jana Kosˇecka´. Multiviewrgb-ddataset Large High-Diversity Benchmark for Generic Object Track-
 forobjectinstancedetection. In 2016 Fourth International ingin the Wild. IEEETransactionson Pattern Analysis and
 Conferenceon 3 DVision(3 DV),pages 426–434.IEEE,2016. Machine Intelligence,43(5):1562–1577,May 2021. 35
 7 [100] Noureldien Hussein, Efstratios Gavves, and Arnold WM
 [86] Rohit Girdhar and Kristen Grauman. Anticipative video Smeulders. Timeception for complexactionrecognition. In
 trans for mer. In ICCV,2021. 3,9 CVPR,2019. 7 
 [101] Go Irie,Mirela Ostrek,Haochen Wang,Hirokazu Kameoka,
 [87] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE 
 Akisato Kimura,Takahito Kawanishi,and Kunio Kashino.
 internationalconferenceoncomputervision,pages 1440– 
 Seeingthroughsounds:Predictingvisualsemanticsegmen-
 1448,2015. 75 
 tationresults from multichannelaudiosignals. In ICASSP
 [88] Georgia Gkioxari and Jitendra Malik. Findingactiontubes. 
 2019-2019 IEEE International Conference on Acoustics,
 In 2015 IEEEConferenceon Computer Vision and Pattern 
 Speech and Signal Processing(ICASSP),pages 3961–3964.
 Recognition(CVPR),pages 759–768,Boston,MA,USA, 
 IEEE,2019. 51 
 June 2015.IEEE. 31 
 [102] Phillip Isola,Joseph J.Lim,and Edward H.Adelson. Dis-
 [89] P.Gollwitzer. Actionphases and mind-sets,Handbookof 
 coveringstates and trans for mationsinimagecollections. In
 motivation and cognition:Foundationsofsocialbehavior. 
 CVPR,2015. 7 
 1990. 45 
 [103] Phillip Isola,Joseph JLim,and Edward HAdelson. Dis-
 [90] Raghav Goyal,Samira Ebrahimi Kahou,Vincent Michal- coveringstates and trans for mationsinimagecollections. In
 ski,Joanna Materzynska,Susanne Westphal,Heuna Kim, CVPR,2015. 45 
 Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz [104] Koji Iwano, Tomoaki Yoshinaga, Satoshi Tamura, and
 Mueller-Freitag,etal. The”somethingsomething”video Sadaoki Furui. Audio-visualspeechrecognitionusinglip
 data base for learning and evaluatingvisualcommonsense. information extracted from side-face images. EURASIP
 In ICCV,2017. 45 Journalon Audio,Speech,and Music Processing,2007:1–9,
 [91] Alex Graves,Santiago Ferna´ndez,and Ju¨rgen Schmidhuber. 2007. 8,51 
 Bidirectionallstmnetworks for improvedphonemeclassi- [105] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew
 fication and recognition. In International conference on Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver:
 artificialneuralnetworks,pages 799–804.Springer,2005. Generalperception with iterativeattention. ar Xivpreprint
 48,49 ar Xiv:2103.03206,2021. 48,49 
 [92] Chunhui Gu,Chen Sun,David ARoss,Carl Vondrick,Car- [106] Baoxiong Jia,Yixin Chen,Siyuan Huang,Yixin Zhu,and
 oline Pantofaru,Yeqing Li,Sudheendra Vijayanarasimhan, Song-Chun Zhu. Amulti-view data set for learningmulti-
 George Toderici,Susanna Ricco,Rahul Sukthankar,etal. agentmulti-taskactivities. In ECCV,2020. 3
 Ava:Avideo data setofspatio-temporallylocalizedatomic [107] Hao Jiang and Kristen Grauman. Seeinginvisibleposes:
 visualactions. In Proceedingsof the IEEEConferenceon Estimating 3 dbodypose from egocentricvideo. In CVPR,
 Computer Vision and Pattern Recognition,pages 6047–6056, 2017. 3 
 2018. 1,3,71 [108] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
 [93] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- Chloe Hillier,Sudheendra Vijayanarasimhan,Fabio Viola,
 mar,Yu Zhang,Jiahui Yu,Wei Han,Shibo Wang,Zheng- Tim Green,Trevor Back,Paul Natsev,etal. Thekineticshu-
 dong Zhang,Yonghui Wu,etal. Con for mer:Convolution- manactionvideo data set. ar Xivpreprintar Xiv:1705.06950,
 augmented trans for mer for speech recognition. ar Xiv 2017. 1,3,4,41 
 preprintar Xiv:2005.08100,2020. 61 [109] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
 Chloe Hillier,Sudheendra Vijayanarasimhan,Fabio Viola,
 [94] Kaiming He,Georgia Gkioxari,Piotr Dolla´r,and Ross Gir- 
 Tim Green,Trevor Back,Paul Natsev,etal. Thekineticshu-
 shick. Mask R-CNN. ar Xiv:1703.06870[cs],Jan.2018. 
 manactionvideo data set. ar Xivpreprintar Xiv:1705.06950,
 33 
 2017. 77 
 [95] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. 
 [110] Evangelos Kazakos,Arsha Nagrani,Andrew Zisserman,and
 Deep residual learning for image recognition. In CVPR, 
 Dima Damen. Epic-fusion:Audio-visualtemporalbinding
 2016. 48,49 
 for egocentric action recognition. In Proceedings of the
 [96] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. 
 IEEEInternational Conferenceon Computer Vision,pages
 Deep residual learning for image recognition. In CVPR, 
 5492–5501,2019. 3,7,8,51 
 2016. 57 
 [111] Petr Kellnhofer,Simon Stent,Wojciech Matusik,and An-
 [97] Farnoosh Heidarivincheh, Majid Mirmehdi, and Dima tonio Torralba. Gaze 360: Physically Unconstrained Gaze
 Damen. Detecting the momentofcompletion: Temporal Estimationin the Wild. In Proceedingsof the IEEEInterna-
 models for localisingactioncompletion. In BMVC,2018. tional Conferenceon Computer Vision(ICCV 19),2019. 9,
 44 63,64,66 
 [98] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, [112] Suyoun Kim,Takaaki Hori,and Shinji Watanabe. Jointctc-
 and Adriane Boyd. spa Cy:Industrial-strength Natural Lan- attention base dend-to-endspeechrecognitionusingmulti-
 guage Processingin Python,2020. 20 tasklearning. In 2017 IEEEinternationalconferenceon
 86 

 
 
 
 
 
 
 acoustics,speech and signalprocessing(ICASSP),pages [126] Kevin Lai,Liefeng Bo,and Dieter Fox. Unsupervisedfea-
 4835–4839.IEEE,2017. 61 ture learning for 3 d scene labeling. In 2014 IEEE Inter-
 [113] Kris M.Kitani,Brian Ziebart,James D.Bagnell,and Martial national Conferenceon Robotics and Automation(ICRA),
 Hebert. Activity for ecasting. In ECCV,2012. 9 pages 3050–3057.IEEE,2014. 7 
 [114] Dietrich Klakow and Jochen Peters. Testing the correlation [127] Tian Lan,Tsung-Chuan Chen,and Silvio Savarese.Ahierar-
 ofworderrorrate and perplexity. Speech Communication, chicalrepresentation for futureactionprediction. In ECCV,
 38(1-2):19–28,2002. 8,54 2014. 9 
 [115] Mark L. Knapp, Judith A. Hall, and Terrence G. Hor- [128] Federico Landini,Ja´n Profant,Mireia Diez,and Luka´sˇBur-
 gan. Nonverbal Communication in Human Interaction. get.Bayesianhmmclusteringofx-vectorsequences(vbx)in
 Wadsworth Cengage Learning,8 thedition,2014. 8 speakerdiarization:theory,implementation and analysison
 [116] Ross A Knepper, Todd Layton, John Romanishin, and standardtasks. Computer Speech&Language,71:101254,
 Daniela Rus. Ikeabot: Anautonomousmulti-robotcoor- 2022. 56 
 dinatedfurnitureassemblysystem. In 2013 IEEEInterna- [129] Y.J.Lee,J.Ghosh,and K.Grauman.Discoveringimportant
 tionalconferenceonrobotics and automation,pages 855– people and objects for egocentricvideosummarization. In
 862.IEEE,2013. 44 CVPR,2012. 2,3 
 [117] Andrew JKolarik,Brian CJMoore,Pavel Zahorik,Silvia [130] Y.J.Lee,J.Ghosh,and K.Grauman.Discoveringimportant
 Cirstea, and Shahina Pardhan. Auditorydistancepercep- people and objects for egocentricvideosummarization. In
 tioninhumans: areviewofcues,development,neuronal Proceedingsof the IEEEConferenceon Computer Vision
 bases,andeffectsofsensoryloss. Attention,Perception,& and Pattern Recognition(CVPR),2012. 45
 Psychophysics,78(2):373–395,2016. 51 
 [131] Yong Jae Leeand Kristen Grauman. Predictingimportant
 [118] Hema S.Koppula and Ashutosh Saxena.Anticipatinghuman objects for egocentricvideosummarization. IJCV,2015. 3
 activitiesusingobjectaffordances for reactiveroboticre- 
 [132] Bruno Lepri,Ramanathan Subramanian,Kyriaki Kalimeri,
 sponse.Pattern Analysis and Machine Intelligence,38(1):14– 
 Jacopo Staiano, Fabio Pianesi, and Nicu Sebe. Connect-
 29,2016. 9 
 ingmeetingbehaviorwi the xtraversion-asystematicstudy.
 [119] Ranjay Krishna,Kenji Hata,Frederic Ren,Li Fei-Fei,and 
 IEEETransactionson Affective Computing,3(4):443–455,
 Juan Carlos Niebles. Dense-captioningeventsinvideos. In 
 2012. 9 
 International Conferenceon Computer Vision(ICCV),2017. 
 [133] Vladimir ILevenshteinetal. Binarycodescapableofcor-
 7 
 rectingdeletions,insertions,andreversals. In Sovietphysics
 [120] Alexei A.Efros Krishna Kumar Singh,Kayvon Fatahalian. 
 doklady,1966. 73 
 Krishnacam:Usingalongitudinal,single-person,egocentric 
 [134] Cheng Li and Kris Kitani. Model recommendation with
 dataset for sceneunderst and ingtasks. In IEEEWinter Con- 
 virtualprobes for ego-centrich and detection.In ICCV,2013.
 ferenceon Applicationsof Computer Vision(WACV),2016. 
 3 
 9 
 [135] Yin Li, Alireza Fathi, and James M. Rehg. Learning to
 [121] Matej Kristan, Ales Leonardis, Jiri Matas, Michael 
 predict gaze in egocentric video. In Proceedings of the
 Felsberg, Roman Pflugfelder, Joni-Kristian Kamarainen, 
 Luka Cˇehovin Zajc,Martin Danelljan,Alan Lukezic,On- IEEEInternational Conferenceon Computer Vision,pages
 3216–3223,2013. 65 
 drej Drbohlav,Linbo He,Yushan Zhang,Song Yan,Jinyu 
 [136] Y.Li,M.Liu,and J.Rehg. Intheeyeofbeholder: Joint
 Yang,Gustavo Fernandez,andetal.Theeighthvisualobject 
 learningofgaze and actionsinfirstpersonvideo. In ECCV,
 tracking VOT 2020 challengeresults,2020. 31 
 2018. 45 
 [122] Taku Kudo and John Richardson. Sentencepiece: A 
 [137] Yin Li,Miao Liu,and Jame Rehg.Inthe Eyeof the Beholder:
 simple and languageindependentsubwordtokenizer and 
 detokenizer for neural text processing. ar Xiv preprint Gaze and Actionsin First Person Video. IEEETransactions
 ar Xiv:1808.06226,2018. 61 on Pattern Analysis and Machine Intelligence,2021. 65
 [123] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui- [138] Yin Li,Miao Liu,and James MRehg.Intheeyeofbeholder:
 jlings,Ivan Krasin,Jordi Pont-Tuset,Shahab Kamali,Stefan Jointlearningofgaze and actionsinfirstpersonvideo. In
 Popov,Matteo Malloci,Alexander Kolesnikov,etal. The Proceedingsof the European Conferenceon Computer Vi-
 openimages data setv 4. International Journalof Computer sion(ECCV),pages 619–635,2018. 2,3
 Vision,128(7):1956–1981,2020. 57 [139] Yanghao Li,Tushar Nagarajan,Bo Xiong,and Kristen Grau-
 [124] F.Dela Torre,J.Hodgins,J.Montano,S.Valcarcel,R.For- man. Ego-exo: Transferring visual representations from
 cada,and J.Macey. Guideto the carnegiemellonuniversity third-persontofirst-personvideos. In CVPR,2021. 3,7
 multimodalactivity(cmu-mmac)data base. In Tech.report [140] Tianwei Lin,Xiao Liu,Xin Li,Errui Ding,and Shilei Wen.
 CMU-RI-TR-08-22,Robotics Institute,Carnegie Mellon Uni- Bmn:Boundary-matchingnetwork for temporalactionpro-
 versity,2009. 3 posalgeneration. In Proceedingsof the IEEE/CVFInterna-
 [125] Loic Lacheze,Yan Guo,Ryad Benosman,Bruno Gas,and tional Conferenceon Computer Vision,pages 3889–3898,
 Charlie Couverture. Audio/videofusion for objectsrecog- 2019. 48,49 
 nition. In 2009 IEEE/RSJInternational Conferenceon In- [141] Tianwei Lin,Xu Zhao,Haisheng Su,Chongjing Wang,and
 telligent Robots and Systems,pages 652–657.IEEE,2009. Ming Yang. Bsn:Boundarysensitivenetwork for temporal
 8 actionproposalgeneration. In Proceedingsof the European
 87 

 
 
 
 
 
 
 Conferenceon Computer Vision(ECCV),pages 3–19,2018. Conferenceon Applicationsof Computer Vision(WACV),
 7,25,32,41 pages 1507–1516,January 2021. 7 
 [142] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He, [156] Christophe Micheyl,Christian Kaernbach,and Laurent De-
 Bharath Hariharan,and Serge Belongie. Feature Pyramid many. Anevaluationofpsychophysical model sofauditory
 Networks for Object Detection.ar Xiv:1612.03144[cs],Apr. changeperception.Psychologicalreview,115(4):1069,2008.
 2017. 33 51 
 [143] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, [157] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
 Pietro Perona,Deva Ramanan,Piotr Dolla´r,and CLawrence Makar and Tapaswi, Ivan Laptev, and Josef Sivic.
 Zitnick. Microsoft COCO:Commonobjectsincontext. In How To 100 M:Learninga Text-Video Embeddingby Watch-
 ECCV,2014. 1,3,47 ing Hundred Million Narrated Video Clips. In ICCV,2019.
 [144] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Fore- 3,6 
 castinghuman-objectinteraction:jointpredictionofmotor [158] Ishan Misra,Abhinav Gupta,and Martial Hebert. Fromred
 attention and actionsinfirstpersonvideo. In ECCV,2020. winetoredtomato: Composition with context. In CVPR,
 3 2017. 45 
 [145] Wen Liu,Weixin Luo,Dongze Lian,and Shenghua Gao.Fu- 
 [159] Yu Mitsuzumi, Atsushi Nakazawa, and Toyoaki Nishida.
 tureframeprediction for anomalydetection–anew base line. 
 Deepeyecontactdetector:Robusteyecontactbiddetection
 In Proceedingsof the IEEEConferenceon Computer Vision 
 usingconvolutionalneuralnetwork. In BMVC,2017. 9
 and Pattern Recognition,pages 6536–6545,2018. 9 
 [160] Davide Moltisanti,Michael Wray,Walterio Mayol-Cuevas,
 [146] William Lotter, Gabriel Kreiman, and David Cox. Deep 
 and Dima Damen. Trespassing the boundaries: Labelling
 predictivecodingnetworks for videoprediction and unsu- 
 temporalbounds for objectinteractionsinegocentricvideo.
 pervisedlearning. ar Xivpreprintar Xiv:1605.08104,2016. 
 In ICCV,2017. 44,45 
 9 
 [161] Pedro Morgado,Nono Vasconcelos,Timothy Langlois,and
 [147] Cewu Lu,Renjie Liao,and Jiaya Jia. Personalobjectdis- 
 Oliver Wang.Self-supervisedgenerationofspatialaudio for
 coveryinfirst-personvideos. TIP,2015. 3 360◦video. In Neur IPS,2018. 8,51 
 [148] Zheng Luand Kristen Grauman. Story-drivensummariza- 
 [162] Matthias Mu¨ller,Adel Bibi,Silvio Giancola,Salman Alsub-
 tion for egocentricvideo. In CVPR,2013. 3 
 aihi, and Bernard Ghanem. Tracking Net: ALarge-Scale
 [149] Tahmida Mahmud, Mahmudul Hasan, and Amit K Roy- 
 Dataset and Benchmark for Object Trackingin the Wild. In
 Chowdhury. Jointpredictionofactivitylabels and starting 
 Vittorio Ferrari,Martial Hebert,Cristian Sminchisescu,and
 times in untrimmed videos. In Proceedings of the IEEE 
 Yair Weiss,editors,Computer Vision–ECCV 2018,volume
 International Conferenceon Computer Vision,pages 5773– 
 11205,pages 310–327.Springer International Publishing,
 5782,2017. 9 
 Cham,2018. 35 
 [150] Manuel JMarin-Jimenez,Vicky Kalogeiton,Pablo Medina- 
 [163] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen
 Suarez,and Andrew Zisserman. Laeo-net:revisitingpeople 
 Grauman.Groundedhuman-objectinteractionhotspots from
 lookingateacho the rinvideos. In Proceedingsof the IEEE 
 video. ICCV,2019. 3 
 Conferenceon Computer Vision and Pattern Recognition, 
 [164] Tushar Nagarajan and Kristen Grauman. Attributesasop-
 pages 3477–3485,2019. 9 
 erators: factorizing unseen attribute-object compositions.
 [151] Manuel Jesu´s Mar´ın-Jime´nez,Andrew Zisserman,Marcin 
 In Proceedingsof the European Conferenceon Computer
 Eichner,and Vittorio Ferrari. Detectingpeoplelookingat 
 Vision(ECCV),pages 169–185,2018. 7,45 
 eacho the rinvideos. International Journalof Computer 
 [165] A. Nagrani, J. S. Chung, and A. Zisserman. Vox Celeb:
 Vision,106(3):282–296,2014. 9 
 a large-scale speaker identification dataset. In INTER-
 [152] Manuel JMar´ın-Jime´nez,Andrew Zisserman,and Vittorio 
 SPEECH,2017. 52,57 
 Ferrari.Here’slookingatyou,kid.Detectingpeoplelooking 
 ateacho the rinvideos.In BMVC,5,2011. 9 [166] Katsuyuki Nakamura,Serena Yeung,Alexandre Alahi,and
 Li Fei-Fei. Jointlylearningenergyexpenditures and activ-
 [153] Michael Mathieu,Camille Couprie,and Yann Le Cun. Deep 
 itiesusingegocentricmultimodalsignals. In CVPR,2017.
 multi-scale video prediction beyond mean square error. 
 3 
 ar Xivpreprintar Xiv:1511.05440,2015. 9 
 [154] Iain Mc Cowan,Jean Carletta,Wessel Kraaij,Simone Ashby, [167] Lukas Neumann,Andrew Zisserman,and Andrea Vedaldi.
 Sebastien Bourban,Mike Flynn,Mael Guillemot,Thomas Futureeventprediction:Ifandwhen. In Proceedingsof the
 Hain,Jaroslav Kadlec,Vasilis Karaiskos,Melissa Kronen- IEEEConferenceon Computer Vision and Pattern Recogni-
 thal,Guillaume Lathoud,Mike Lincoln,Agnes Lisowska, tion Workshops,pages 0–0,2019. 9
 Wilfried Post, Dennis Reidsma, and Pierre Wellner. The [168] Evonne Ng,Donglai Xiang,Hanbyul Joo,and Kristen Grau-
 AMImeetingcorpus. In Proceedingsof Measuring Behav- man. You 2 me:Inferringbodyposeinegocentricvideovia
 ior 2005,the 5 th International Conferenceon Methods and first and secondpersoninteractions. In CVPR,2020. 3
 Techniquesin Behavioral Research,pages 137–140,2005. 9 [169] Joonas Nikunen and Tuomas Virtanen. Directionofarrival
 [155] Jean-Philippe Mercier,Mathieu Garon,Philippe Giguere, basedspatialcovariance model for blindsounds our cesep-
 and Jean-Francois Lalonde. Deep template-based object aration. IEEE/ACMTransactionson Audio, Speech, and
 instancedetection. In Proceedingsof the IEEE/CVFWinter Language Processing,22(3):727–739,2014. 51
 88 

 
 
 
 
 
 
 [170] C.Northcutt,S.Zha,S.Lovegrove,and R.Newcombe.Ego- inanindustrial-likedomain. In IEEEWinter Conferenceon
 com: Amulti-personmulti-modalegocentriccommunica- Applicationof Computer Vision(WACV),2021. 3
 tions data set. PAMI,2020. 3 [185] Rene´Ranftl,Alexey Bochkovskiy,and Vladlen Koltun. Vi-
 [171] Andrew Owens and Alexei AEfros.Audio-visualsceneanal- siontransformers for denseprediction.In Proceedingsof the
 ysis with self-supervisedmultisensoryfeatures. In ECCV, IEEE/CVFInternational Conferenceon Computer Vision,
 2018. 51 pages 12179–12188,2021. 38,39 
 [172] Cristina Palmero,Elsbeth Avan Dam,Sergio Escalera,Mike [186] Adria Recasens,Aditya Khosla,Carl Vondrick,and Antonio
 Kelia,Guido FLichtert,Lucas PJJNoldus,Andrew JSpink, Torralba. Where are the ylooking? In Advancesin Neural
 and Astridvan Wieringen. Automaticmutualgazedetec- Information Processing Systems,pages 199–207,2015. 9
 tioninface-to-facedyadicinteractionvideos. Measuring [187] Joseph Redmon and Ali Farhadi. Yolov 3:Anincremental
 Behavior 2018,2018. 9 improvement. ar Xivpreprintar Xiv:1804.02767,2018. 57
 [173] Daniel SPark,William Chan,Yu Zhang,Chung-Cheng Chiu, [188] James M. Rehg, Agata Rozga, Gregory D. Abowd, and
 Barret Zoph,Ekin DCubuk,and Quoc VLe. Specaugment: Matthew S. Goodwin. Behavioral Imaging and Autism.
 Asimple data augmentationmethod for automaticspeech IEEEPervasive Computing,13(2):84–87,2014. 8
 recognition. ar Xivpreprintar Xiv:1904.08779,2019. 60 [189] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun.
 [174] H.S.Park,J.-J.Hwang,Y.Niu,and J.Shi.Egocentricfuture Fasterr-cnn:Towardsreal-timeobjectdetection with region
 localization. In CVPR,2016. 9 proposalnetworks. In Neur IPS,2015. 33 
 [175] H.S.Park,J.-J.Hwang,Y.Niu,and J.Shi.Egocentricfuture [190] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun.
 localization. In Conferenceon Computer Vision and Pattern Faster r-cnn: Towards real-time object detection with re-
 Recognition(CVPR),2016. 74 gionproposalnetworks. Advancesinneuralin for mation
 [176] Hyun Soo Park,Eakta Jain,and Yaser Sheikh. 3 Dsocial processingsystems,28:91–99,2015. 49
 saliency from head-mountedcameras.In Advancesin Neural [191] Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, and
 Information Processing Systems,volume 1,pages 422–430, Giovanni Maria Farinella. Predicting the future from first
 2012. 9 person(egocentric)vision:Asurvey. Computer Vision and
 [177] Tae Jin Park,Naoyuki Kanda,Dimitrios Dimitriadis,Kyu J Image Underst and ing,2021. 9
 Han,Shinji Watanabe,and Shrikanth Narayanan. Areview [192] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad-
 ofspeakerdiarization:Recentadvances with deeplearning. hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh
 ar Xivpreprintar Xiv:2101.09624,2021. 53,56 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid,
 [178] David R Perrott and Kourosh Saberi. Minimum audible Zhonghua Xi, et al. Ava-activespeaker: An audio-visual
 anglethresholds for sourcesvaryinginbo the levation and dataset for active speaker detection. ar Xiv preprint
 azimuth. The Journalof the Acoustical Societyof America, ar Xiv:1901.01342,2019. 8,52,53
 87(4):1728–1731,1990. 51 [193] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad-
 [179] Hamed Pirsiavash and Deva Ramanan. Detecting activi- hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh
 tiesofdailylivinginfirst-personcameraviews. In 2012 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid,
 IEEEconferenceoncomputervision and patternrecogni- Zhonghua Xi,and Caroline Pantofaru. Ava Active Speaker:
 tion,pages 2847–2854.IEEE,2012. 2,3 An Audio-Visual dataset for Active Speaker Detection.
 [180] H.Pirsiavash and D.Ramanan. Detectingactivitiesofdaily In ICASSP, IEEE International Conference on Acoustics,
 livinginfirst-personcameraviews. In Computer Vision and Speech and Signal Processing-Proceedings,volume 2020-
 Pattern Recognition(CVPR),2012. 45 May,pages 4492–4496,2020. 9 
 [181] Daniel Povey,Arnab Ghoshal,Gilles Boulianne,Lukas Bur- [194] M.S.Ryoo and L.Matthies. First-personactivityrecogni-
 get,Ondrej Glembek,Nagendra Goel,Mirko Hannemann, tion: What are the ydoingtome? In IEEEConferenceon
 Petr Motlicek,Yanmin Qian,Petr Schwarz,Jan Silovsky, Computer Vision and Pattern Recognition(CVPR),2013. 3
 Georg Stemmer,and Karel Vesely. The Kaldispeechrecog- [195] Paul-Edouard Sarlin,Daniel De Tone,Tomasz Malisiewicz,
 nitiontoolkit. In IEEE 2011 Workshopon Automatic Speech and Andrew Rabinovich.Superglue:Learningfeaturematch-
 Recognition and Underst and ing,2011. 56 ing with graph neural networks. In Proceedings of the
 [182] Senthil Purushwalkam,Maximilian Nickel,Abhinav Gupta, IEEE/CVFconferenceoncomputervision and patternrecog-
 and Marc’Aurelio Ranzato. Task-drivenmodularnetworks nition,pages 4938–4947,2020. 36,38
 forzero-shotcompositionallearning. In Proceedingsof the [196] Johannes LSchonberger and Jan-Michael Frahm. Structure-
 IEEEInternational Conferenceon Computer Vision,pages from-motion revisited. In Proceedings of the IEEE con-
 3593–3602,2019. 45 ferenceoncomputervision and patternrecognition,pages
 [183] F.Ragusa,A.Furnari,S.Battiato,G.Signorello,and G.M. 4104–4113,2016. 36 
 Farinella. Egocentricvisitorslocalizationinculturalsites. [197] A.Senocak,T.-H.Oh,J.Kim,M.Yang,and I.S.Kweon.
 Journal on Computing and Cultural Heritage (JOCCH), Learningtolocalizesounds our cesinvisualscenes:Analysis
 2019. 3 andapplications. TPAMI,2019. 8,51 
 [184] Francesco Ragusa, Antonino Furnari, Salvatore Livatino, [198] Dandan Shan,Jiaqi Geng,Michelle Shu,and David Fouhey.
 and Giovanni Maria Farinella. Themec can odataset:Under- Understandinghumanh and sincontactatinternet scale. In
 standinghuman-objectinteractions from egocentricvideos CVPR,2020. 45,46 
 89 

 
 
 
 
 
 
 [199] Dandan Shan,Jiaqi Geng,Michelle Shu,and David Fouhey. [214] Twenty BN. The 20 BN-jester dataset V 1. https://
 Understandinghumanh and sincontactatinternet scale. In 20 bn.com/datasets/jester. 45
 CVPR,2020. 49 [215] Joost Van Amersfoort,Anitha Kannan,Marc’Aurelio Ran-
 [200] Mohit Sharma,Kevin Zhang,and Oliver Kroemer. Learning zato, Arthur Szlam, Du Tran, and Soumith Chintala.
 semanticembeddingspaces for slicingvegetables. ar Xiv Trans for mation-based model sofvideosequences. ar Xiv
 preprintar Xiv:1904.00303,2019. 44 preprintar Xiv:1701.08435,2017. 9,40 
 [201] Gunnar ASigurdsson,Abhinav Gupta,Cordelia Schmid,Ali [216] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko-
 Farhadi,and Karteek Alahari. Charades-ego:Alarge-scale reit,Llion Jones,Aidan NGomez,Łukasz Kaiser,and Illia
 datasetofpairedthird and firstpersonvideos.ar Xivpreprint Polosukhin.Attentionisallyouneed.In Advancesinneural
 ar Xiv:1804.09626,2018. 2,3,45 inprocessingsystems,pages 5998–6008,2017. 49
 [202] Nathan Silberman,Derek Hoiem,Pushmeet Kohli,and Rob 
 [217] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko-
 Fergus. Indoorsegmentation and supportinference from 
 reit,Llion Jones,Aidan NGomez,Łukasz Kaiser,and Illia
 rgbdimages. In Europeanconferenceoncomputervision, 
 Polosukhin.Attentionisallyouneed.In Advancesinneural
 pages 746–760.Springer,2012. 38 
 informationprocessingsystems,pages 5998–6008,2017. 61
 [203] Silero Team. Silero vad: Pre-trained enterprise-grade 
 [218] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu
 voice activity detector (VAD), number detector and lan- 
 Lin, and Honglak Lee. Decomposing motion and con-
 guageclassifier. https://github.com/snakers 4/ 
 tent for naturalvideosequenceprediction. ar Xivpreprint
 silero-vad,2021. 59 
 ar Xiv:1706.08033,2017. 9 
 [204] Michel Silva,Washington Ramos,Joa˜o Ferreira,Felipe Cha- 
 [219] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
 mone, Mario Campos, and Erickson R. Nascimento. A 
 Anticipatingvisualrepresentations from unlabeledvideo. In
 weightedsparsesampling and smoothingframetransition 
 CVPR,2016. 9 
 approach for semanticfast-forwardfirst-personvideos. In 
 2018 IEEE/CVFConferenceon Computer Vision and Pat- [220] He Wang,So¨ren Pirk,Ersin Yumer,Vladimir GKim,Ozan
 tern Recognition(CVPR),2018. 3 Sener,Srinath Sridhar,and Leonidas JGuibas. Learninga
 [205] Krishna Kumar Singh, Kayvon Fatahalian, and Alexei A generative model for multi-stephuman-objectinteractions
 Efros. Krishnacam: Using a longitudinal, single-person, fromvideos. In Eurographics,2019. 45
 egocentric data set for sceneunderst and ingtasks. In WACV, [221] Limin Wang,Yuanjun Xiong,Zhe Wang,Yu Qiao,Dahua
 2016. 2,3 Lin,Xiaoou Tang,and Luc Van Gool.Temporalsegmentnet-
 [206] David Snyder,Daniel Garcia-Romero,Gregory Sell,Daniel works:Towardsgoodpractices for deepactionrecognition.
 Povey,and Sanjeev Khudanpur. X-vectors: Robust DNN In ECCV,2016. 3,7 
 embeddings for speakerrecognition. In 2018 IEEEInterna- [222] Qiang Wang,Li Zhang,Luca Bertinetto,Weiming Hu,and
 tional Conferenceon Acoustics,Speech and Signal Process- Philip H.S.Torr. Fastonlineobjecttracking and segmenta-
 ing(ICASSP),2018. 57 tion:Aunifyingapproach,2019. 17 
 [207] Khurram Soomro,Amir Roshan Zamir,and Mubarak Shah. [223] Xiaolong Wang,Ali Farhadi,and Abhinav Gupta. Actions˜
 Ucf 101:Adatasetof 101 humanactionclasses from videos trans for mations. In CVPR,2016. 45
 inthewild. In CRCV-TR-12-01,2012. 3 
 [224] Xiaolong Wang,Ross Girshick,Abhinav Gupta,and Kaim-
 [208] Emiliano Spera,Antonino Furnari,Sebastiano Battiato,and 
 ing He. Non-localneuralnetworks. In CVPR,2018. 3
 Giovanni Maria Farinella. Egocentricshoppingcartlocal- 
 [225] Yuxin Wu,Alexander Kirillov,Francisco Massa,Wan-Yen
 ization. In International Conferenceon Pattern Recognition 
 Lo,and Ross Girshick. Detectron 2. 35 
 (ICPR),2018. 3 
 [226] Fanyi Xiao,Yong Jae Lee,Kristen Grauman,Jitendra Malik,
 [209] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, 
 and Christoph Feichtenhofer.Audiovisualslowfastnetworks
 Erik Wijmans,Simon Green,Jakob JEngel,Raul Mur-Artal, 
 for video recognition. ar Xiv preprint ar Xiv:2001.08740,
 Carl Ren,Shobhit Verma,etal.Thereplica data set:Adigital 
 2020. 8,51 
 replicaofindoorspaces. ar Xivpreprintar Xiv:1906.05797, 
 2019. 14 [227] SHIXingjian,Zhourong Chen,Hao Wang,Dit-Yan Yeung,
 [210] Yu-Chuan Suand Kristen Grauman. Detectingengagement Wai-Kin Wong,and Wang-chun Woo. Convolutionallstm
 inegocentricvideo. In ECCV,2016. 2,3,45 network: A machine learning approach for precipitation
 [211] Ruijie Tao,Zexu Pan,Rohan Kumar Das,Xinyuan Qian, nowcasting. In Advancesinneuralin for mationprocessing
 Mike Zheng Shou,and Haizhou Li. Issomeonespeaking? systems,pages 802–810,2015. 9
 exploringlong-termtemporalfeatures for audio-visualactive [228] Jun Xu,Tao Mei,Ting Yao,and Yong Rui. Msr-vtt:Alarge
 speakerdetection. ar Xivpreprintar Xiv:2107.06592,2021. videodescription data set for bridgingvideo and language.
 53,58,59 IEEE International Conference on Computer Vision and
 [212] Y.Tian,J.Shi,B.Li,Z.Duan,and C.Xu. Audio-visual Pattern Recognition(CVPR),June 2016. 7
 eventlocalizationinunconstrainedvideos. In ECCV,2018. [229] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet,
 8,51 and Bernard Ghanem. G-tad: Sub-graphlocalization for
 [213] E.Tulving. Episodi can dsemanticmemory. In E.Tulv- temporalactiondetection. In Proceedingsof the IEEE/CVF
 ing and W. Donaldson, editors, Organization of memory. Conferenceon Computer Vision and Pattern Recognition,
 Academic Press,1972. 6 pages 10156–10165,2020. 7,25,32,41 
 90 

 
 
 
 
 
 
 [230] Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, and 
 Yoichi Sato.Futurepersonlocalizationinfirst-personvideos. 
 In The IEEEConferenceon Computer Vision and Pattern 
 Recognition(CVPR),June 2018. 9 
 [231] Ryo Yonetani,Kris M.Kitani,and Yoichi Sato.Recognizing 
 micro-actions and reactions from pairedegocentricvideos. 
 In CVPR,2016. 3 
 [232] Ryo Yonetani,Kris MKitani,and Yoichi Sato. Visualmotif 
 discoveryviafirst-personvision. In ECCV,2016. 3 
 [233] Fisher Yu,Dequan Wang,Evan Shelhamer,and Trevor Dar- 
 rell.Deeplayeraggregation.In Proceedingsof the IEEEcon- 
 ferenceoncomputervision and patternrecognition,pages 
 2403–2412,2018. 49 
 [234] Hua Zhang,Xiaochun Cao,and Rui Wang. Audiovisual 
 attributediscovery for fine-grainedobjectrecognition. In 
 Proceedings of the AAAI Conference on Artificial Intelli- 
 gence,volume 32,2018. 8 
 [235] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. 
 Span-basedlocalizingnetwork for naturallanguagevideo 
 localization. In Proceedingsof the 58 th Annual Meetingof 
 the Association for Computational Linguistics,pages 6543– 
 6554, Online, July 2020. Association for Computational 
 Linguistics. 40 
 [236] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo 
 Luo. Learning 2 dtemporaladjacentnetworks for moment 
 localization with naturallanguage. In AAAI,2020. 26,31, 
 32,39,40 
 [237] Chen Zhao, Ali K Thabet, and Bernard Ghanem. Video 
 self-stitchinggraphnetwork for temporalactionlocalization. 
 In Proceedingsof the IEEE/CVFInternational Conference 
 on Computer Vision,pages 13658–13667,2021. 7,25,32, 
 40,41 
 [238] Hang Zhao,Chuang Gan,Andrew Rouditchenko,Carl Von- 
 drick,Josh Mc Dermott,and Antonio Torralba. Thesound 
 ofpixels. In ECCV,2018. 51 
 [239] Bolei Zhou,Alex Andonian,Aude Oliva,and Antonio Tor- 
 ralba. Temporalrelationalreasoninginvideos. In ECCV, 
 2018. 3 
 [240] Hao Zhou,Chongyang Zhang,Yan Luo,Yanjun Chen,and 
 Chuanping Hu. Embracinguncertainty: Decoupling and 
 de-bias for robusttemporalgrounding. In Proceedingsof 
 the IEEE/CVFConferenceon Computer Vision and Pattern 
 Recognition,pages 8445–8454,2021. 33 
 [241] Xingyi Zhou,Dequan Wang,and Philipp Kra¨henbu¨hl. Ob- 
 jectsaspoints. ar Xivpreprintar Xiv:1904.07850,2019. 49 
 [242] Y.Zhou and T.Berg. Learningtemporaltrans for mations 
 fromtime-lapsevideos. In ECCV,2016. 7 
 [243] Yipin Zhou and Tamara LBerg. Temporalperception and 
 predictioninego-centricvideo. In ICCV,2015. 3,7 
 [244] Yipin Zhou and Tamara LBerg. Learningtemporaltrans for- 
 mations from time-lapsevideos. In ECCV,2016. 45 
 [245] Hao Zhu,Man-Di Luo,Rui Wang,Ai-Hua Zheng,and Ran 
 He. Deepaudio-visuallearning: Asurvey. International 
 Journalof Automation and Computing,pages 1–26,2021. 8 
 91 