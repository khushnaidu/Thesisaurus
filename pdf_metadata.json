[
  {
    "paper_id": "rialto",
    "title": "Reconciling Reality through Simulation: A",
    "authors": "Reconciling Reality",
    "year": 2024,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/rialto.pdf",
    "abstract": "Reconciling Reality through Simulation: A\nReal-to-Sim-to-Real Approach for Robust\nManipulation\nMarcel Torne1 Anthony Simeonov1,4 Zechu Li1,3,4 April Chan1,4\nTao Chen1,4 Abhishek Gupta2∗ Pulkit Agrawal1,4∗\n1Massachusets Institute of Technology 2University of Washington 3TU Darmstadt\n4 Improbable AI Lab\nAbstract—Imitation learning methods need significant human data across a massive range of scenes since content creation\nsupervision to learn policies robust to changes in object poses, can be challenging in simulation and data collection can be\nphysical disturbances, and visual distractors. Reinforcement\nchallenging for the real world, (2) a widely general, robust\nlearning, on the other hand, can explore the environment\npolicy may be overly conservative, lowering its performance\nautonomously to learn robust behaviors but may require im-\npractical amounts of unsafe real-world data collection. To learn on the specific target domains encountered on deployment.\nperformant, robust policies wit",
    "num_pages": 23
  },
  {
    "paper_id": "RLGSBridge",
    "title": "RL-GSBridge: 3D Gaussian Splatting Based",
    "authors": "Gaussian Splatting Based\nReal",
    "year": 2025,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/RLGSBridge.pdf",
    "abstract": "RL-GSBridge: 3D Gaussian Splatting Based\nReal2Sim2Real Method for Robotic Manipulation Learning\nYuxuan Wu∗, Lei Pan∗, Wenhua Wu, Guangming Wang, Yanzi Miao, Fan Xu# and Hesheng Wang#\nAbstract—Sim-to-Real refers to the process of transferring 1. Real2Sim by\nSoft Mesh Binding GS\npolicieslearnedinsimulationtotherealworld,whichiscrucial\nfor achieving practical robotics applications. However, recent\nSim2real methods either rely on a large amount of augmented\ndata or large learning models, which is inefficient for specific\ntasks. In recent years, with the emergence of radiance field\nreconstruction methods, especially 3D Gaussian splatting, it\nhasbecomepossibletoconstructrealisticreal-worldscenes.To Dynamics-based GS Editing Grasp Pick&place\nthis end, we propose RL-GSBridge, a novel real-to-sim-to-real\nframework which incorporates 3D Gaussian Splatting into the\nconventional RL simulation pipeline, enabling zero-shot sim-\nto-real transfer for vision-based deep reinforcement learning. Render im",
    "num_pages": 7
  },
  {
    "paper_id": "egomimic",
    "title": "Overleaf Example",
    "authors": "Scaling Imitation Learning",
    "year": 2024,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/egomimic.pdf",
    "abstract": "EgoMimic: Scaling Imitation Learning via Egocentric Video\nSimar Kareer1, Dhruv Patel1∗, Ryan Punamiya1∗, Pranay Mathur1∗, Shuo Cheng1\nChen Wang2, Judy Hoffman1†, Danfei Xu1†\nFig. 1: EgoMimic unlocks human embodiment data—egocentric videos paired with 3D hand tracks—as a new scalable data source for\nimitation learning. We can capture this data anywhere, without a robot, by wearing a pair of Project Aria glasses while performing\nmanipulation tasks with our own hands. EgoMimic bridges kinematic, distributional, and appearance differences between human\nembodiment data (left) and traditional robot teleoperation data (right) to learn a unified policy. We find that human embodiment data\nboosts task performance by 34-228% over using robot data alone, and enables generalization to new objects or even scenes.\nAbstract—The scale and diversity of demonstration data To scale up data for robotics, there have been recent ad-\nrequired for imitation learning is a significant challenge. We vancesindatac",
    "num_pages": 12
  },
  {
    "paper_id": "ego4d",
    "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
    "authors": "Egocentric Video\nKristen",
    "year": 2022,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/ego4d.pdf",
    "abstract": "Ego4D: Around the World in 3,000 Hours of Egocentric Video\nKristenGrauman1,2,AndrewWestbury1,EugeneByrne∗1,ZacharyChavis∗3,AntoninoFurnari∗4,\nRohitGirdhar∗1,JacksonHamburger∗1,HaoJiang∗5,MiaoLiu∗6,XingyuLiu∗7,MiguelMartin∗1,\nTusharNagarajan∗1,2,IlijaRadosavovic∗8,SanthoshKumarRamakrishnan∗1,2,FionaRyan∗6,\nJayantSharma∗3,MichaelWray∗9,MengmengXu∗10,EricZhongcongXu∗11,ChenZhao∗10,\nSiddhantBansal17,DhruvBatra1,VincentCartillier1,6,SeanCrane7,TienDo3,MorrieDoulaty13,\nAkshayErapalli13,ChristophFeichtenhofer1,AdrianoFragomeni9,QichenFu7,\nAbrhamGebreselasie12,CristinaGonza´lez14,JamesHillis5,XuhuaHuang7,YifeiHuang15,\nWenqiJia6,WeslieKhoo16,Ja´chymKola´ˇr13,SatwikKottur13,AnuragKumar5,FedericoLandini13,\nChaoLi5,YanghaoLi1,ZhenqiangLi15,KarttikeyaMangalam1,8,RaghavaModhugu17,\nJonathanMunro9,TullieMurrell1,TakumiNishiyasu15,WillPrice9,PaolaRuizPuentes14,\nMereyRamazanova10,LedaSari5,KiranSomasundaram5,AudreySoutherland6,YusukeSugano15,\nRuijieTao11,MinhVo5,YuchenWang16,XindiWu7,TakumaYagi15,ZiweiZ",
    "num_pages": 91
  },
  {
    "paper_id": "openvla",
    "title": "An Open-Source Vision-Language-Action Model",
    "authors": "An Open",
    "year": 2024,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/openvla.pdf",
    "abstract": "OpenVLA:\nAn Open-Source Vision-Language-Action Model\nMooJinKim∗,1 KarlPertsch∗,1,2 SiddharthKaramcheti∗,1,3\nTedXiao4 AshwinBalakrishna3 SurajNair3 RafaelRafailov1 EthanFoster1 GraceLam\nPannagSanketi4 QuanVuong5,† ThomasKollar3 BenjaminBurchfiel3 RussTedrake3,6 DorsaSadigh1\nSergeyLevine2 PercyLiang1 ChelseaFinn1\nhttps://openvla.github.io\nLarge-Scale Robot OpenVLA Closed-Loop\nTraining Data Robot Control Policy\nVision-Language-Action Model\nUser: Wipe the table.\nFine-tune VLM w/ Robot Actions:\nOpenVLA:\n970k Robot [Δx, Δθ, ΔGrip] = …\nLlama 2 7B\nEpisodes\nViT\nBase VLM\nMulti-Robot Control & Efficient Fine-Tuning Fully\nOpen-Source\nData\nWeights\nCode\nFigure1: WepresentOpenVLA,a7B-parameteropen-sourcevision-language-actionmodel(VLA),trained\non970krobotepisodesfromtheOpenX-Embodimentdataset[1]. OpenVLAsetsanewstateoftheartfor\ngeneralistrobotmanipulationpolicies.Itsupportscontrollingmultiplerobotsoutoftheboxandcanbequickly\nadaptedtonewrobotdomainsviaparameter-efficientfine-tuning. TheOpenVLAcheckpoi",
    "num_pages": 37
  },
  {
    "paper_id": "openxembodiment",
    "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
    "authors": "Robotic Learning Datasets and",
    "year": 2025,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/openxembodiment.pdf",
    "abstract": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models\nOpen X-Embodiment Collaboration0\nrobotics-transformer-x.github.io\nAbbyO’Neill34,AbdulRehman37,AbhinavGupta4,AbhiramMaddukuri45,AbhishekGupta46,AbhishekPadalkar10,AbrahamLee34,\nAcornPooley11,AgrimGupta28,AjayMandlekar22,AjinkyaJain15,AlbertTung28,AlexBewley11,AlexHerzog11,AlexIrpan11,\nAlexanderKhazatsky28,AnantRai23,AnchitGupta19,AndrewWang34,AndreyKolobov20,AnikaitSingh11,34,AnimeshGarg9,\nAniruddhaKembhavi1,AnnieXie28,AnthonyBrohan11,AntoninRaffin10,ArchitSharma28,ArefehYavary35,ArhanJain46,AshwinBalakrishna32,\nAyzaanWahid11,BenBurgess-Limerick25,BeomjoonKim17,BernhardScho¨lkopf18,BlakeWulfe32,BrianIchter11,CewuLu27,8,CharlesXu34,\nCharlotteLe34,ChelseaFinn11,28,ChenWang28,ChenfengXu34,ChengChi5,28,ChenguangHuang38,ChristineChan11,\nChristopherAgia28,ChuerPan28,ChuyuanFu11,ColineDevin11,DanfeiXu9,DanielMorton28,DannyDriess11,DaphneChen46,DeepakPathak4,\nDhruvShah34,DieterBu¨chler18,DineshJayaraman42,DmitryKalashnikov11,DorsaSadi",
    "num_pages": 12
  },
  {
    "paper_id": "rt1",
    "title": "RT-1: ROBOTICS TRANSFORMER",
    "authors": "Unknown",
    "year": 2023,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/rt1.pdf",
    "abstract": "Bytransferringknowledgefromlarge,diverse,task-agnosticdatasets,modernma- chinelearningmodelscansolvespecificdownstreamtaskseitherzero-shotorwith smalltask-specificdatasetstoahighlevelofperformance. Whilethiscapability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the dif- ficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training,combinedwithhigh-capacityarchitecturesthatcanabsorballofthedi- verse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusionsinastudyofdifferentmodelclassesandtheirabilitytogeneralizeas a function of the data size, model size, and data diversity based on a large-sca",
    "num_pages": 31
  },
  {
    "paper_id": "rt2",
    "title": "https://robotics-transformer2.github.io",
    "authors": "Action Models Transfer\nWeb Knowledge",
    "year": 2023,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/rt2.pdf",
    "abstract": "https://robotics-transformer2.github.io\n2023-8-1\nRT-2: Vision-Language-Action Models Transfer\nWeb Knowledge to Robotic Control\nAnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski,\nTianliDing,DannyDriess,AvinavaDubey,ChelseaFinn,PeteFlorence,ChuyuanFu,\nMontseGonzalezArenas,KeerthanaGopalakrishnan,KehangHan,KarolHausman,AlexanderHerzog,\nJasmineHsu,BrianIchter,AlexIrpan,NikhilJoshi,RyanJulian,DmitryKalashnikov,YuhengKuang,\nIsabelLeal,LisaLee,Tsang-WeiEdwardLee,SergeyLevine,YaoLu,HenrykMichalewski,IgorMordatch,\nKarlPertsch,KanishkaRao,KristaReymann,MichaelRyoo,GreciaSalazar,PannagSanketi,\nPierreSermanet,JaspiarSingh,AnikaitSingh,RaduSoricut,HuongTran,VincentVanhoucke,QuanVuong,\nAyzaanWahid,StefanWelker,PaulWohlhart,JialinWu,FeiXia,TedXiao,PengXu,SichunXu,TianheYu,\nandBriannaZitkovich\nGoogleDeepMind.Authorslistedinalphabeticalorder,withcontributionslistedinAppendixA.\nWestudyhowvision-languagemodelstrainedonInternet-scaledatacanbeincorporateddirectlyinto\nend-to",
    "num_pages": 26
  },
  {
    "paper_id": "inthewild",
    "title": "Learning Generalizable Robotic Reward Functions",
    "authors": "Learning Generalizable Robotic Reward Functions",
    "year": 2021,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/inthewild.pdf",
    "abstract": "Learning Generalizable Robotic Reward Functions\nfrom “In-The-Wild” Human Videos\nAnnie S. Chen, Suraj Nair, Chelsea Finn\nStanford University\nAbstract—We are motivated by the goal of generalist robots ”In-the-wild” Human Videos Robot Videos\nthat can complete a wide range of tasks across many en- ManyEnvironments, ManyTasks OneEnvironment, FewTasks\nvironments. Critical to this is the robot’s ability to acquire\nsome metric of task success or reward, which is necessary for\nreinforcement learning, planning, or knowing when to ask for\nhelp. For a general-purpose robot operating in the real world,\nthis reward function must also be able to generalize broadly\nacross environments, tasks, and objects, while depending only\non on-board sensor observations (e.g. RGB images). While deep\nlearning on large and diverse datasets has shown promise as a\nDVD Reward\npathtowardssuchgeneralizationincomputervisionandnatural Function\nlanguage,collectinghighqualitydatasetsofroboticinteractionat\nTraining\nscaleremai",
    "num_pages": 16
  },
  {
    "paper_id": "dexcap",
    "title": "DexCap: Scalable and Portable Mocap Data",
    "authors": "Portable Mocap Data\nCollection System",
    "year": 2024,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/dexcap.pdf",
    "abstract": "DexCap: Scalable and Portable Mocap Data\nCollection System for Dexterous Manipulation\nChen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu\nStanford University\nhttps://dex-cap.github.io\n(a)DexCap: Portable motion capturesystem (b)Mocapdataand3Dscene (c)DexIL: Dexterous imitation learning\nFig. 1: DEXCAP facilitates the in-the-wild collection of high-quality human hand motion capture data and 3D observations.\nLeveraging this data, DEXIL adapts it to the robot embodiment and trains control policy to perform the same task.\nAbstract—Imitation learning from human hand motion data supervised training using human demonstration data. One\npresentsapromisingavenueforimbuingrobotswithhuman-like commonly used way to collect data is to teleoperate robot\ndexterityinreal-worldmanipulationtasks.Despitethispotential,\nhands to perform the tasks. However, due to the requirement\nsubstantialchallengespersist,particularlywiththeportabilityof\nofarealrobotsystemandslowrobotmotion,thisapp",
    "num_pages": 20
  },
  {
    "paper_id": "habitat",
    "title": "Habitat: A Platform for Embodied AI Research",
    "authors": "Research\nManolis",
    "year": 2019,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/habitat.pdf",
    "abstract": "Theembodimenthypothesisistheideathatintelligenceemerges intheinteractionofanagentwithanenvironmentandasaresult WepresentHabitat,aplatformforresearchinembodied ofsensorimotoractivity. artificialintelligence(AI).Habitatenablestrainingembod- SmithandGasser[26] iedagents(virtualrobots)inhighlyefficientphotorealistic 3Dsimulation. Specifically,Habitatconsistsof: Imaginewalkinguptoahomerobotandasking‘Hey– (i) Habitat-Sim: a flexible, high-performance 3D sim- canyougocheckifmylaptopisonmydesk?Andifso,bring ulator with configurable agents, sensors, and generic 3D ittome.’ Inordertobesuccessful,sucharobotwouldneed datasethandling. Habitat-Simisfast–whenrendering arangeofskills–visualperception(torecognizescenesand a scene from Matterport3D, it achieves several thousand objects),languageunderstanding(totranslatequestionsand framespersecond(fps)runningsingle-threaded,andcan instructionsintoactions),andnavigationincomplexenviron- reachover10,000fpsmulti-processonasingleGPU. ments(tomoveandfindthin",
    "num_pages": 17
  },
  {
    "paper_id": "OKAMI",
    "title": "OKAMI: Teaching Humanoid Robots Manipulation",
    "authors": "Teaching Humanoid Robots Manipulation\nSkills",
    "year": 2024,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/OKAMI.pdf",
    "abstract": "OKAMI: Teaching Humanoid Robots Manipulation\nSkills through Single Video Imitation\nJinhanLi1† YifengZhu1∗ YuqiXie1,2∗ ZhenyuJiang1,2∗ MingyoSeo1\nGeorgiosPavlakos1 YukeZhu1,2\nUTAustin1 NVIDIAResearch2\nAbstract:Westudytheproblemofteachinghumanoidrobotsmanipulationskills\nbyimitatingfromsinglevideodemonstrations. WeintroduceOKAMI,amethod\nthat generates a manipulation plan from a single RGB-D video and derives a\npolicy for execution. At the heart of our approach is object-aware retargeting,\nwhich enables the humanoid robot to mimic the human motions in an RGB-D\nvideo while adjusting to different object locations during deployment. OKAMI\nuses open-world vision models to identify task-relevant objects and retarget the\nbody motions and hand poses separately. Our experiments show that OKAMI\nachievesstronggeneralizationsacrossvaryingvisualandspatialconditions,out-\nperforming the state-of-the-art baseline on open-world imitation from observa-\ntion. Furthermore,OKAMIrollouttrajectoriesareleveraged",
    "num_pages": 19
  },
  {
    "paper_id": "humansin4d",
    "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
    "authors": "Tracking Humans",
    "year": 2023,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/humansin4d.pdf",
    "abstract": "Humans in 4D: Reconstructing and Tracking Humans with Transformers\nShubhamGoel GeorgiosPavlakos JathushanRajasegaran AngjooKanazawa JitendraMalik\n∗ ∗\nshubham-goel, pavlakos, jathushan, kanazawa @berkeley.edu, malik@eecs.berkeley.edu\n{ }\nUniversityofCalifornia,Berkeley\nFigure 1: A “transformerized” view of Human Mesh Recovery. We describe HMR2.0, a fully transformer-based approach for 3D\nhumanposeandshapereconstructionfromasingleimage.Besidesimpressiveperformanceacrossawidevarietyofposesandviewpoints,\nHMR2.0alsoactsasthebackboneofanimprovedsystemforjointlyreconstructingandtrackingHumansin4D(4DHumans). Here,we\nseeoutputreconstructionsfromHMR2.0foreach2Ddetectionintheleftimage.\nAbstract 1.Introduction\nIn this paper, we present a fully transformer-based ap-\nWepresentanapproachtoreconstructhumansandtrack proachforrecovering3Dmeshesofhumanbodiesfromsin-\nthem over time. At the core of our approach, we propose gleimages,andtrackingthemovertimeinvideo.Weobtain\na fully “transformerized” version ",
    "num_pages": 18
  },
  {
    "paper_id": "GSLTS",
    "title": "GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling",
    "authors": "Gaussian Splatting",
    "year": 2025,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/GSLTS.pdf",
    "abstract": "GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling\nfor Long-Term Service Robots\nBin Fu1 and Jialin Li1 and Bin Zhang1 and Ruiping Wang1,(cid:12) and Xilin Chen1\nAbstract—3D Gaussian Splatting (3DGS) has garnered sig-\nnificant attention in robotics for its explicit, high fidelity\ndense scene representation, demonstrating strong potential for\nroboticapplications.However,3DGS-basedmethodsinrobotics\nprimarily focus on static scenes, with limited attention to the\ndynamic scene changes essential for long-term service robots.\nThese robots demand sustained task execution and efficient\nscene updates—challenges current approaches fail to meet.\nTo address these limitations, we propose GS-LTS (Gaussian\nSplatting for Long-Term Service), a 3DGS-based system en-\nabling indoor robots to manage diverse tasks in dynamic\nenvironments over time. GS-LTS detects scene changes (e.g.,\nobject addition or removal) via single-image change detection,\nemploys a rule-based policy to autonomously collect multi-\nA",
    "num_pages": 8
  },
  {
    "paper_id": "robgsim",
    "title": "RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator",
    "authors": "Real Robotic Gaussian Splatting Simulator\nXinhai",
    "year": null,
    "venue": "Science",
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/robgsim.pdf",
    "abstract": "RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator\nXinhaiLi1*, JialinLi2*, ZihengZhang3†, RuiZhang4, FanJia3, TiancaiWang3,\nHaoqiangFan3, Kuo-KunTseng1‡, RuipingWang2‡\n1HarbinInstituteofTechnology,Shenzhen\n2InstituteofComputingTechnology,ChineseAcademyofSciences\n3MEGVIITechnology 4ZhejiangUniversity\nNovel View Synthesis Novel Scene Synthesis\nClosed-Loop Evaluation Novel Object Synthesis\nFigure1. RoboGSimisanefficient,low-costinteractiveplatformwithhigh-fidelityrendering. Itachievesdemonstrationsynthesiswith\nnovel scenes, novel objects, and novel views, facilitating data scaling for policy learning. Additionally, it can perform the closed-loop\nsimulationforsafe,fairandrealisticevaluationondifferentpolicymodels.\nAbstract talTwinsBuilder,SceneComposer,andInteractiveEngine.\nIt can synthesize the simulated data with novel views, ob-\nEfficientacquisitionofreal-worldembodieddatahasbeen jects, trajectories, and scenes. RoboGSim also provides\nincreasinglycritical. However, large-sca",
    "num_pages": 12
  },
  {
    "paper_id": "humanhumanoidph2d",
    "title": "Humanoid Policy ~ Human Policy",
    "authors": "Humanoid Policy",
    "year": null,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/humanhumanoidph2d.pdf",
    "abstract": "Humanoid Policy ∼ Human Policy\nRi-ZhaoQiu*,1 ShiqiYang*,1 XuxinCheng*,1 ChaitanyaChawla*,2 JialongLi1\nTairanHe2 GeYan4 DavidYoon3 RyanHoque3 LarsPaulsen1\nGeYang5 JianZhang3 ShaYi1 GuanyaShi2 XiaolongWang1\n1UCSanDiego,2CMU,3Apple,4UniversityofWashington,5MIT\nhttps://human-as-robot.github.io/\nEgocentric Vision Unified State-Action Space Robot Policies\nSmall-scale\nHumanoid Data\n1.5k demos\nFingers / Wrist\nLarge-scale\nHuman Data\n27k demos\nFigure 1: This paper advocates high-quality human data as a data source for cross-embodiment\nlearning-task-orientedegocentrichumandata. Wecollectalarge-scaledataset,PhysicalHuman-\nHumanoid Data (PH2D), with hand-finger 3D poses from consumer-grade VR devices on well-\ndefinedmanipulationtasksdirectlyalignedwithrobots. Withoutrelyingonmodularperception,we\ntrain a Human Action Transformer (HAT) manipulation policy by directly modeling humans as a\ndifferenthumanoidembodimentinanend-to-endmanner.\nAbstract: Trainingmanipulationpoliciesforhumanoidrobotswithdivers",
    "num_pages": 19
  },
  {
    "paper_id": "ReBot",
    "title": "ReBot: Scaling Robot Learning with",
    "authors": "Teaser\nReal",
    "year": 2025,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/ReBot.pdf",
    "abstract": "Teaser\nReal-to-Sim\nTrajectory\nReBot: Scaling Robot Learning with\nReplay\nReal-toRea-l-Swoirldm-to-Real Robotic Video Synthesis\nBackground\nInpainting\nReal-world Real-to-Sim Trajectory Replay\nYu Fang1, Yue Yang1, X Baicnkggrhouanod InZpahinuti 2 ng, Kaiyuan Zheng3, Gedas Bertasius1, Daniel Szafir1, Mingyu Ding1\nSim-to-Real VLA\nAbstract—Vision-language-action Rea ( l V -to L -S A im ) models present a OpenVLA Video Models\nTrajectory Replay Synthesis\npromising paradigm by training policies directly on real robot\nOcto w/o ReBot\ndatasets like Open X-EmbodimenRte.alH-woorwldever, the high cost w/ ReBot\nof real-world data collection Bhacikngdroeurnsd Infpuarintthinegr data scaling, BridgeData V2 DROID ReBot Finetuned\nthereby restricting the generalizability of VLAs. In this paper, Real Robot Datasets VLA performance VLA Models\nwe introduce ReBot, a novel real-to-sim-Rteoa-lr-teoa-Slimap proach for\nTrajectory Replay\nscaling real robot datasets and adapting VLA models to\nEpisode: Move the yellow ",
    "num_pages": 8
  },
  {
    "paper_id": "learningfirsperson",
    "title": "Learning Robot Activities from First-Person Human Videos",
    "authors": "Learning Robot Activities",
    "year": 2017,
    "venue": null,
    "arxiv_id": null,
    "pdf_path": "/Users/khushnaidu/NLP_Project/papers/learningfirsperson.pdf",
    "abstract": "Learning Robot Activities from First-Person Human Videos\nUsing Convolutional Future Regression\nJangwon Lee and Michael S. Ryoo\nAbstract—We design a new approach that allows robot a limiting aspect particularly when we want to teach a robot\nlearning of new activities from unlabeled human example new (i.e., previously unseen) activities.\nvideos.Givenvideosofhumansexecutingthesameactivityfrom\nIn this paper, we present a new CNN-based approach\nahuman’sviewpoint(i.e.,first-personvideos),ourobjectiveisto\nthat enables robot learning of its activities from ‘human’\nmaketherobotlearnthetemporalstructureoftheactivityasits\nfutureregressionnetwork,andlearntotransfersuchmodelfor example videos. Human activity videos can be attractive\nitsownmotorexecution.Wepresentanewdeeplearningmodel: training resources because it does not require any hardware\nWe extend the state-of-the-art convolutional object detection or professional software for teaching robots, even though\nnetwork for the representation/estima",
    "num_pages": 8
  }
]