 
 
 
 
 
 
 
 
 Robo GSim: A Real 2 Sim 2 Real Robotic Gaussian Splatting Simulator 
 
 
 Xinhai Li 1*, Jialin Li 2*, Ziheng Zhang 3†, Rui Zhang 4, Fan Jia 3, Tiancai Wang 3,
 Haoqiang Fan 3, Kuo-Kun Tseng 1‡, Ruiping Wang 2‡ 
 1 Harbin Instituteof Technology,Shenzhen 
 2 Instituteof Computing Technology,Chinese Academyof Sciences 
 3 MEGVIITechnology 4 Zhejiang University 
 
 Novel View Syn the sis Novel Scene Syn the sis 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Closed-Loop Evaluation Novel Object Syn the sis 
 
 Figure 1. Robo GSimisanefficient,low-costinteractiveplat for mwithhigh-fidelityrendering. Itachievesdemonstrationsyn the siswith
 novel scenes, novel objects, and novel views, facilitating data scaling for policy learning. Additionally, it can perform the closed-loop
 simulation for safe,fair and realisticevaluationondifferentpolicymodels. 
 Abstract tal Twins Builder,Scene Composer,and Interactive Engine.
 It can syn the size the simulated data with novel views, ob-
 Efficientacquisitionofreal-worldembodied data has been jects, trajectories, and scenes. Robo GSim also provides
 increasinglycritical. However, large-scaledemonstrations an online, reproducible, and safe evaluation for different
 captured by remote operation tend to take extremely high manipulation policies. The real 2 sim and sim 2 real trans-
 costs and failtoscaleup the datasizeinanefficientman- ferexperimentsshowahighconsistencyin the texture and
 ner. Sampling the episodesunderasimulatedenvironment physics. We compared the test results of Robo GSim data
 is a promising way for large-scale collection while exist- andrealrobot data onboth Robo GSim and realrobotplat-
 ingsimulatorsfailtohigh-fidelity model ingontexture and forms. The experimental results show that the Robo GSim
 physics. To address these limitations, we introduce the data model can achieve zero-shot per for mance on the real
 Robo GSim, a real 2 sim 2 real robotic simulator, powered by robot, with results comparable to real robot data. Addi-
 3 DGaussian Splatting and the physicsengine. Robo GSim tionally, in experiments with novel perspectives and novel
 mainlyincludesf our parts: Gaussian Reconstructor, Digi- scenes, the Robo GSim data model per for med even better
 on the real robot than the real robot data model. This not
 *Equal contribution only helps reduce the sim 2 real gap but also addresses the
 †Project leader limitationsofrealrobot data collection,suchasitssingle-
 ‡Corresponding authors 
 5202 
 gu A 
 3 
 ]OR.sc[ 
 2 v 93811.1142:vi Xra 

 
 
 
 
 
 
 source and high cost. We hope Robo GSim serves as a ulator that unifies the demonstration syn the sis and closed-
 closed-loopsimulator for faircomparisononpolicylearn- loopevaluation. Robo GSim can generaterealisticmanipu-
 ing. More information can be found on our project page lateddemonstrations with novelscenes,views,andobjects
 https://robogsim.github.io/. forpolicylearning. Itcanalsoper for mclosed-loopevalua-
 tion for differentpolicynetworks,ensuringfaircomparison
 under are alisticenvironment. Inconclusion,ourcorecon-
 1.Introduction tributions can beconcludedas: 
 • Realistic 3 DGS-Based Simulator:Wedevelopa 3 DGS-
 Collecting large-scale manipulated data is of great impor- 
 based simulator that reconstructs scenes and objects
 tance for efficient policy learning. Some methods propose 
 with realistic textures from multi-view RGB videos.
 tocapturethedemonstrationsaswellas the actionsthrough 
 Robo GSimisoptimized for somechallengingconditions
 theremoteoperation[11,37,39].Whilesuchoperationrel- 
 likeweaktextures,lowlight,andreflectivesurfaces.
 atively improves the collection efficiency, it tends to bring 
 • Digital Twin System:Weintroduce the layoutalignment
 extremelylargecosts with the increasing data size.Tosolve 
 modulein the system. With the layout-aligned Isaac Sim,
 this problem, some works [14, 34] attempt to generate the 
 Robo GSim maps the physical interactions between ob-
 syn the tic data under the simulated environment, which is 
 jects and roboticarms from Real 2 Simspaces. 
 further used to learn the manipulation policy. However, 
 • Syn the sizer and Evaluator: Robo GSim can synthe-
 those Sim 2 Real approaches suffer from the large domain 
 size the realistic manipulated demonstrations with novel
 gapbetweensimulated and real-worldenvironments,mak- 
 scenes,views,andobjects for policylearning. Itcanalso
 ing the learnedpolicyinvalid. 
 work as the Evaluator to perform model evaluation in a
 Recently, some works introduce the Real 2 Sim 2 Real 
 physics-consistent manner.The experiment results show
 (R 2 S 2 R) paradigm for robotic learning [3, 21]. The core 
 that our generated data can achieve the sameper for mance
 insight is to perform realistic reconstruction via radiance 
 as real robot data, which in a way solves the sim 2 real
 field methods, such as Ne RF [25] and 3 D Gaussian Splat- 
 gap problem. At the same time, in experiments with
 ting (3 DGS) [15], and insert learned representations into 
 novel scenes and novel perspectives, our generated data
 thesimulator. Amongthosemethods,thetypicalapproach, 
 is more effective than real robot data, even with the 2 D
 Robo-GS[21],presentsa Real 2 Simpipeline and introduces 
 Aug method. Our evaluator also partially verifying the
 a hybrid representation to generate digital assets enabling 
 per for manceof the realrobot model inaclosed-loop.
 high-fidelity simulation. However, it lacks the demonstra- 
 tion syn the sis on novel scenes, views, and objects, as well 
 2.Related Work 
 asverificationaspolicylearning data. Moreover,itfailsto 
 per for mclosed-loopevaluation for differentpoliciesdueto 
 2.1.Sim 2 Realin Robotics 
 themisalignmentbetween the latentrepresentation,simula- 
 tion,andreal-worldspaces. The Real 2 Sim 2 Real approach fundamentally seeks to ad-
 In this paper, we develop a Real 2 Sim 2 Real simulator, dress the Sim 2 Realgap,whichremainsapersistentobstacle
 called Robo GSim,forbothhigh-fidelitydemonstrationsyn- inthetrans for mation from simulationtorealworld[8,27].
 thesis and physics-consistent closed-loop evaluation. It In order to bridge the Sim 2 Real gap as much as possible,
 mainly includes four parts: Gaussian Reconstructor, Digi- manyfeature-richsimulators have emergedinrecentyears,
 tal Twins Builder,Scene Composer and Interactive Engine. including [7, 23, 28, 35, 38]. To this end, various datasets
 Given the multi-view RGBimagesequences and MDH [6] andbenchmarks have also been proposed for effectivepol-
 parameters of the robotic arm, Gaussian Reconstructor is icylearning[12,13,16,26].
 built upon 3 DGS [43] and reconstructs the scene and ob- Previous Sim 2 Real methods can be broadly classified
 jects.Then,the Digital Twins Builderperforms the meshre- intothreecategories: domainr and omization,domainadap-
 construction and createsadigitaltwinin Isaac Sim. In Dig- tation, and learning with disturbances [40]. Domain ran-
 ital Twins Builder, we propose the layout alignment mod- domizationmethods are designedtoexp and the operational
 ule to align the space between the simulation, real-world, envelope of a robot in a simulator by introducing random-
 and GS representation. After that, the Scene Composer ness. Thesimulationenvironmentshould becapableofmi-
 combines the scene,roboticarm and objectsinsimulation, grationof the aforementionedcapabilitiesinreal-worldset-
 and renders the images from new perspective. Finally, in tings[1,10,14,34]. Domainadaptationapproachesaimto
 the Interactive Engine,Robo GSimworksas the Syn the sizer unify the featurespaceofsimulated and realenvironments,
 and Evaluatortoper for msthedemonstrationsyn the sisand facilitatingthetraining and migrationwithin the unifiedfea-
 closed-looppolicyevaluation. ture space [2, 19, 41]. The objective of learning methods
 Robo GSim brings many advantages compared to exist- introducethedisturbancesinto the simulatedenvironment,
 ing(Real 2)Sim 2 Realframeworks. Itis the firstneuralsim- inwhich the policyofrobotsislearned. Itdevelopstheca-

 
 
 
 
 
 
 pacitytooperateeffectivelyin the realworld with noise and the manipulated data in simulation using VR/Xbox equip-
 unpredictability[5,36]. mentof the realworld. 
 2.2.3 DGaussian Splattingin Robotics 3.2.Gaussian Reconstructor 
 
 Asasignifi can tadvancementin the fieldof 3 Dreconstruc- We employ the 3 DGS method to reconstruct static scenes,
 tion, 3 DGS [15] represents the scene as a large set of ex- followed by point cloud segmentation of the robotic arm’s
 plicit Gaussianpoints and combinesitwi the fficientraster- joints. Subsequently, we utilize the MDH dynamic model
 izationtoachievehigh-fidelityreal-timerendering,extend- tocontrol the pointcloudscorrespondingtoeachjoint, fa-
 ing the capabilitiesof Ne RF[25]. cilitatingthedynamicrenderingof the roboticarm.
 More recently, a number of studies have explored the 3 D Gaussian Splatting (3 DGS) [15] employs a set of
 use of 3 DGS to perform manipulation tasks within em- multi-view images as input to achieve high-fidelity scene
 bodied simulators and the real world. For example, Mani- reconstruction. 3 DGSrepresents the sceneasasetof Gaus-
 Gaussian [22] introduces a dynamic GS framework along- sians and utilizes a differentiable rasterization rendering
 sidea Gaussianworld model,whichrespectivelyrepresents methodtoenablereal-timerendering.
 Gaussianpointsimplicitly and parameterizes the mtomodel Specifically, for a scene G = {g }N represented by
 i i=1 
 and predict future states and actions. Similarly, Gaussian- N Gaussians, each Gaussian can be represented as g =
 i 
 Grasper[42]utilizes RGB-Dimagesasinputs and embeds (µ ,Σ ,o ,c ). Here, µ ∈ R 3, Σ ∈ R 3×3, o ∈ R and
 i i i i 
 semanti can dgeometricfeaturesinto 3 DGSthroughfeature c ∈ SH(4) denote the mean, covariance matrix, opacity
 distillation and geometric reconstruction, thereby enabling andcolorfactor, representedbysphericalharmoniccoeffi-
 language-guidedgrasping operations. Toeffectively trans- cients,respectively.
 fer the knowledge learned in simulation to the real world During the renderingprocess,thefinalcolorvalue C of
 and reduce the Sim 2 Real gap, recent works [18, 21, 29] thepixel can beobtainedthrough are nderingmethod,sim-
 basedon 3 DGS have appeared.Amongthem,themostsim- ilar to alpha-blending [15]. It utilizes a sequence of N or-
 ilartoours are Robo-GS[21]and Splat Sim[29]. Robo-GS dered Gaussians that overlap with the pixel. Such process
 achieves manipulable robotic arm reconstruction by bind- can beexpressedasfollows:
 ing Gaussian points, grids, and pixels, with a primary fo- 
 cusonhigh-fidelity Real 2 Simtransfer;however,itprovides i−1 
 (cid:88) (cid:89) 
 limited discussion on the Sim 2 Real phase. Splat Sim re- C = c i α i (1−α j ) (1)
 constructs both the robotic arm and objects in the scene i∈N j=1 
 andsimultaneouslyverifiesthefeasibilityof the method for 
 1 
 Sim 2 Realtasks. However, itlacksdiscussionsongenerat- α =o ·exp( δ⊤Σ−1δ ) (2)
 ingdigitaltwinassetsof the objects, which are critical for i i 2 i 2 D i 
 achievingaccuratemanipulation. where α is the opacity of the i-th Gaussian. δ ∈ R 2 de-
 i i 
 notes the offset between 2 D Gaussian center and current
 3.Methods pixel. Σ ∈R 2×2 represents the 2 Dcovariancematrix.
 2 D 
 Modified Denavit-Hartenberg (MDH) [6] convention is
 3.1.Overall Architecture 
 aparameterized model todescribe the kinematicchainofa
 As shown in Fig. 2, Robo GSim mainly includes four manipulator. Each joint and link in the kinematic chain is
 parts: Gaussian Reconstructor, Digital Twins Builder, characterized by a set of parameters. In MDH, a trans for-
 Scene Composer,and Interactive Engine. Givenmulti-view mation matrix can be constructed for each link, achieving
 images and MDH parameters of the robotic arm, Gaus- anaccuraterepresentationof the manipulator’sposeateach
 sian Reconstructor (Sec. 3.2) reconstructs scenes and ob- stageofmotion. Letx ,y ,z denote the coordinatesof the
 i i i 
 jectsusing 3 DGS,segments the roboticarm,andbuildsan origin for the i-th joint. For a manipulator, the i-th joint
 MDH kinematic drive graph structure to enable accurate configuration can berepresentedas:
 motion modeling of the robotic arm. Digital Twin Builder 
 (Sec.3.3)involvesmeshreconstructionof the sceneandob- Θ={β ,a ,d ,θ } (3) 
 i i i i 
 jects. Throughlayoutalignment,theasset data flow can be 
 interconnected,facilitating the subsequentevaluationin In- where β represents the twist angle, which is the rotation
 i 
 teractive Engine. Scene Composer (Sec. 3.4) achieves the aroundthex-axis from the(i−1)-thjointtothei-thjoint.
 syn the sis of novel objects, scenes, and views. Interactive a denotes the linklength,measuring the distancealong the
 i 
 Engine (Sec. 3.5) syn the sizes novel view/scene/object im- x-axis from z to z . d is the link offset, indicating the
 i−1 i i 
 ages for policylearning. Itcanalsoevaluate the policynet- displacementalongthez-axisfromx tox .θ represents
 i−1 i i 
 works in a closed-loop manner. Moreover, we can collect thejointangle,rotationaroundthez-axisfromx tox .
 i−1 i 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 2. Overviewof the Robo GSim Pipeline:(1)Inputs:multi-view RGBimagesequences and MDHparametersof the roboticarm.
 (2)Gaussian Reconstructor: reconstruct the scene and objectsusing 3 DGS,segment the roboticarm and buildan MDHkinematicdrive
 graphstructure for accuratearmmotion model ing. (3)Digital Twins Builder: per for mmeshreconstructionofboth the scene and objects,
 thencreateadigitaltwinin Isaac Sim, ensuringhighfidelityinsimulation. (4)Scene Composer: combine the roboticarm and objects
 inthesimulation, identifyoptimaltestviewpointsusingtracking, andrenderimages from newperspectives. (5)Interactive Engine: (i)
 Thesyn the sizedimages with novelscenes/views/objects are used for policylearning.(ii)Policynetworks can beevaluatedinaclose-loop
 manner.(iii)Theembodied data can becollectedby the VR/Xboxequipment. 
 Thetransformationmatrix for eachlink T ,using MDH turntable and extract matching features with GIM [33] to
 i 
 parameters,can bewrittenas: address issues such as lack of texture and reflections. We
 thenintegrate the COLMAPpipeline[32]toobtain the ini-
   
 cosθ −sinθ cosβ sinθ sinβ a cosθ 
 i i i i i i i tial SFMpointcloud,whichissubsequentlyused for recon-
 T i =    sin 0 θ i cos s θ in i c β o i sβ i −co c s o θ s i β s i inβ i a i s d in i θ i   (4) s o t n ru t c h t e io w n e b b y , w 3 D e G in S it . ia M lly or e e m ov p e l r o , y fo g r en n e o r v a e ti l ve ob 3 j D ect r s ec a o v n a s il t a ru b c le -
 0 0 0 1 
 tionmethods[17,20]toprocure 3 Dgaussians and textured
 meshesof the objects. Subsequently,weutilize the method
 Bysequentiallymultiplying the setrans for mationmatrices, 
 in Gaussian Editor[4]thatapplies the diffusion model[31]
 we canobtainthefinaltrans for mationmatrix from the base 
 tofacilitateobjectreconstructionin 3 DGS. 
 to the end effector. We segment each joint and then treat 
 all Gaussianpoints with inajointasapointmass. Wefur- 
 thermoveall Gaussianpoints with inajointaccordingto T , Layout Alignment:Asshownin Fig.2,sincewefollow the
 i 
 achievingkinematic-driven Controlof the Gaussianpoints. localcoordinatesystemof the roboticarm,theworldcoor-
 dinates and Isaac Sim are axis-aligned. We first measure
 3.3.Digital Twins Builder 
 thereal-worldscenetoalignthesizeof the importedtable
 Digitaltwinsshouldnotonlymapreal-worldassetsbutalso scene in Isaac Sim. In the GS scene, a downward-facing
 involve coordinate alignment. Through Real 2 Sim layout camera is placed 1.6 meters above the base joint to render
 alignment and Sim 2 GSsparsekeypointalignment, we can asegmentationmap. Forcoordinatealignment,weplacea
 digitize the real world, enabling the flow of digital assets downward-facingcamera 1.6 metersabove the basejointin
 between the real, simulated, and GS representation. This Isaac Sim. Bycomparing the renderedscene from the BEV,
 facilitates the conversion of digital assets in all directions, front and sideviewsegmentation,with the views from Isaac
 achievingcomprehensiveassetflooding. Sim,weadjust the shifttoachievelayoutalignment.
 3 D Assets Generation: We employ two methods to gen- 
 erate 3 D object assets. For real-world objects, we cap- Sim 2 GS Alignment: Given the MDH-based transforma-
 ture high-quality multi-view images of the objects using a tion matrices Tgs and simulated trans for mation matrices
 i 

 
 
 
 
 
 
 Tsim,thereexistsatrans for mationmatrix Tsim such that: Σ′ =R ΣR⊤ (14) 
 i gs(i) norm norm 
 Tsim =Tsim·Ti (5) Object Editing: The trans for mation here can extend the
 gs(i) i gs trans for mation from the scene editing mentioned above.
 Tocompute the averagetrans for mationmatrix Tsim,we use However,thedifferenceis that the targetobject’scoordinate
 gs 
 theweightedsum and applynormalization: centerisgivenby Eq.7. Thecoordinatetrans for mation for
 its Gaussianpoints can berepresented: 
 (cid:80)6 w ·Tsim 
 T g s s im = (cid:13) (cid:13)(cid:80) i 6 =1 w i ·T g s s im i (cid:13) (cid:13) (6) µ′ =R(µ−µ 0 )+µ 0 +t (15)
 (cid:13) i=1 i gsi(cid:13) 
 3.5.Interactive Engine 
 wherew istheweightofeachjoint. 
 i 
 For the target object Tsimin Isaac Sim, we can trans- Ourinteractiveengine can workas: Syn the sizer and Evalu-
 obj 
 ator. As Syn the sizer,itproduceslargevolumesof data with
 form it into the GS coordinate system using the following 
 low-cost for downstream policy learning. As Evaluator, it
 formula: 
 Tgs =Tgs ·Tsim (7) canper for msafe,real-time,andreproducibleevaluation.
 obj sim obj 
 Syn the sizer:we use the enginetogeneratenumeroustrain-
 Camera Localization: Totransform the real-worldcoordi- 
 ing trajectories, including robotic arm movements and tar-
 natesysteminto the GScoordinate, weapply the localiza- 
 get object trajectories. These trajectories drive the GS to
 tion approach from GS-SLAM [24]. For a pre-trained GS 
 generate massive and photorealistic simulated datasets for
 model,G = {g }N ,wefroze the attributesof 3 DGSand 
 i i=1 policylearning. Thisdiverse data includesnovelviewren-
 optimize the externalcameraparameters TW. 
 C derings,scenecombinations,andobjectreplacements.
 In camera localization, only the current camera pose is 
 Evaluator: For trained models, testing directly on phys-
 optimized without updates to the map representation. For 
 ical devices may pose safety risks or incur high costs for
 monocular cases, we minimize the following photometric 
 reproduction. Therefore,weconvert the predictedtrajecto-
 residual: 
 riesinto GS-renderedresultstoefficiently and rapidlyeval-
 L pho = 
 (cid:13) 
 (cid:13)I(G,T C 
 W)−I¯(cid:13) 
 (cid:13) 1 , (8) uate the model’s prediction quality. Specifically, the Isaac
 where I(G,TW) represents rendering Gaussians G from Sim [28] outputs an initial state of the target object and
 C 
 TW,and I¯istheobservedimage. robotic arm, and GS renders according to the status. The
 C 
 rendered images are then fed to the policy to predict the
 3.4.Scene Composer next frame’s action. The predicted action is passed to the
 Scene Editing: To merge the point cloud into the robotic simulation for kinematic inverse parsing, collision detec-
 armscene,thetrans for mation T[R|t]ofthemarkedpointis tion,ando the rphysicalinteractions. Then,Isaacsimsends
 firstcalculated. Thenthecoordinatesof the pointcloudin theparsedsix-axisrelativeposeto the GSrenderer, which
 the new scene are projected into the arm coordinate based thensendstherenderedresultasfeedbackto the policynet-
 onthetrans for mation. Exp and ing the 3 DCovarianceΣin work. Thisservesasvisualfeedback for predicting the next
 3 DGSintoscales and rotationquaternionqby: action,andtheprocessiteratesuntil the taskisfinished.
 Σ=qss Tq T (9) 4.Experiments 
 Theratiorof the trans for mation can beisolatedandex- Since the reisnobenchmarksavailable for Real 2 Sim 2 Real,
 tractedasanindependentcomponent: we construct the following four groups of proxy exper-
 iments to comprehensively evaluate the per for mance of
 (cid:113) 
 r = (RRT) (10) Robo GSim under simulation and real-world. We use UR 5
 (0,0) 
 robot arm for all experiments. The robot arm rendering is
 we canfurtheruseittonormalize the rotationmatrix R: partiallybuiltupon the codebaseof Robo-GS[21].
 Real 2 Sim Novel Pose Synthesisverifieswhether the robot
 R 
 R norm = r (11) arm pose captured in the real world can be effectively uti-
 lizedtoachieveprecisecontrolin the simulator.
 The scale attributesof the Gaussianpointsisadjusted: Sim 2 Real Trajectory Replaycheckswhether the trajecto-
 riescollectedin the simulator can beaccuratelyreproduced
 s=s+log(r) (12) 
 bythereal-worldrobotarm. 
 Apply the Trans for mation T to Gaussianpointcoordinates Robo GSim as Syn the sizer demonstrates the ability of
 Robo GSim to generate high-fidelity demonstrations with
 µ′ =Rµ+t (13) novelscenes,views,andobjects,aligning with realworld.

 
 
 
 
 
 
 Grasp Suc. Place Suc. 
 TV NV(MD) NS TV NV NS 
 Real 100% 30% 40% 90% 0% 20% 
 Real+2 DAUG 80% 100% 60% 80% 0% 60% 
 Robo GSim 100% 70% 100% 90% 0% 90% 
 Table 1.Per for manceonring-toss task inrealworld.TVdenotestestview,NVisnovelview and NSis the novelscene.MDmeansminor
 deviation. 
 
 Robo GSimas Evaluatorshows that Robo GSim can effec- 4.3.Robo GSimas Syn the sizer 
 tivelyper for mclosed-loopevaluation for policynetworks. 
 In this section, we use the vision-language-action (VLA)
 model to validate the effectiveness of syn the tic data by
 Method Grasp Suc. Place Suc. 
 Robo GSim. we usethe LLAMA 3-8 B[9]asthe LLMand
 Real-to-Real 100% 90% 
 CLIP [30] as the vision encoder. Two-layer MLP is used
 Real-to-Robo GSim 100% 30% 
 as the projection network. The VLA model is trained on
 Sim-to-Real 80% 0% 
 8 x A 100(80 GB)for 1 epoch.Thetrainingprocessisdivided
 Robo GSim-to-Real 100% 90% 
 into three stages: (1) Pre-training with only the connector
 Table 2. Cross validation between real world and simulation. enabled,using the LAION-558 Kdataset. (2)Training with
 “Sim-to-Real”means that testing the VLA model trained with the LLM unfrozen using the LLa VA 665 K dataset. (3) Super-
 Isaacsimdataon the realworldrobotarm. vised fine tuning(SFT)withroboticimage-action data and
 the CLIPweightisfrozen. 
 By using the real machine distribution to guide the
 4.1.Real 2 Sim Novel Pose Syn the sis 
 Robo GSimdistribution,weaimtoimprove the model’ssuc-
 Theobjectiveofthenovelposesyn the sisistovalidate the cess rate. We perform the experiments on a challenging
 per for mance of Real 2 Sim reconstruction, with a particular ring-toss task (see Fig. 7), which is divided into two sub-
 focusontheaccuracyof the roboticarm’smovements and tasks: picking up the ring and tossing it onto the target.
 the fidelity of the image texture. The static scene is re- The accuracy requirement for the Z-axis when picking up
 constructed using the initial pose of the robotic arm from the ring is within 5 mm. For real data, 1,000 samples are
 thefirstframeof GT.Thetrajectorycollected from the real collected manually. For a fair comparison, we used 1,000
 roboticarmisusedas the drivingforce,andweemploy the syn the tic samples generated by Robo GSim. During test-
 kinematic control for novel pose rendering. As shown in ing, each model was tested 10 times, with three attempts
 Fig. 3, the results demonstrate that our reconstruction ac- allowed per trial for grasping. If all three attempts failed,
 curatelycapturesboththetexture and the physicaldynam- thetrialwasmarkedasunsuccessful.
 icsof the roboticarm,highlighting the fidelityachievedby As shown in Tab. 1, We compared three models: one
 Robo GSim. Tocomp are with the videosequencedrivenby trained with real machine data, one trained with real ma-
 therealrobotunder the newviewpoint,Robo GSimachieves chine data plus 2 DAUG,andonetrained with Robo GSim.
 a 31.3 PSNR and 0.79 SSIMrenderingresult,whileensur- Thecomparisonwasmadeintermsoftestview,novelview,
 ingreal-timerendering with 10 FPS. andnovelscene. Theresultsshowthatin the testview,the
 generated data from Robo GSim can achieve zero-shot ca-
 4.2.Sim 2 Real Trajectory Replay 
 pability, withper for mancecomparableto the realmachine
 To verify whether the trajectories from Issac Sim can per- data(bothat 90%).Inthenovelscenecase,Robo GSimper-
 fectly align with the real machine and Robo GSim, we de- formed much better than real machine data, reaching 90%
 signedanexperimentwhere the trajectoryiscollectedusing compared to the 60% of real machine data. In the novel
 Issac Sim,andthen the trajectoryisusedtodrive GStoren- view experiment, Robo GSim also had less bias compared
 dera Coke-graspingscene,while the sametrajectoryisused to the real machine data model. We also compared the
 todrive the realmachinetograspa Coke can. Asshownin effect of adding 2 D AUG to the real machine data. Af-
 Fig.4, thecomparisonreveals astrongalignment between ter adding AUG, the per for mance inthe test view dropped
 thesimulatedpolicy and the actualphysicalbehaviorof the (90% → 80%), but in the novel scene, it improved (40%
 roboticarm,highlighting the effectivenessof the Sim 2 Real → 60%). However, in the novel view, the bias increased.
 transfer in our system. These results suggest that our sim- Pure 2 DAUGlacksspatialaw are ness,anditsper for mance
 ulation can reliably model real-worlddynamics,facilitating isfarworsethanthatof Robo GSim,whichhasspatialintel-
 successfulpolicytransfer from simulationto the realworld. ligence. a Itshould benoted that manualcollectiontakesa

 
 
 
 
 
 
 Real 
 
 
 
 Robo GSim 
 
 
 Depth 
 
 
 
 Diff 
 
 
 Dr. Robot 
 
 
 PSNR:31.4 SSIM:0.79 FPS:10 
 
 Figure 3. Real 2 Sim Novel Pose Syn the sis: ”Real”representsthecaptureof the realroboticarm from anewviewpoint. ”Robo GSim”
 showstherenderingofthenovelpose from the new viewpointdrivenby the realrecordedtrajectory. ”Depth”shows the renderingdepth
 by GS.”Diff”isthedifferencecalculatedbetween the Real and the rendered RGBimages. Wecomputethepixeldistanceof the same
 pointbetween the Real and Robo GSim,whichis 7.37. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 4. Sim 2 Real Trajectory Replay: The”Sim”rowdisplays the videosequencecollected from Isaac Sim. ”Real”represents the
 demonstrationdrivenby the trajectoryinsimulation.”Robo GSim”isthe GSrenderingresultdrivenby the sametrajectory.”Diff”indicates
 thedifferencesbetween Real and the renderedresults. 
 
 totalof 40 hourswhile Robo GSimonlyrequires 4 hours for doorenvironments.Thehigh-fidelitymulti-viewrenderings
 syn the sis. It is promising to further scale up the data size demonstrate that Robo GSimenables the robotarmtooper-
 ofsynthesis for fur the rper for manceimprovements. Fig.7 ateseamlesslyacrossdiversescenes.
 shows the visualization of some success and failure cases. 
 4.4.Robo GSimas Evaluator 
 Moreover, we also illustrate some more qualitative analy- 
 sis for novel scene syn the sis. As shown in Fig. 5, we dis- Realisticclosed-loopevaluationiscrucial for validation and
 playtheresultsof the physicalmigrationof the UR 5 robot comparisonofpolicynetworks. Inthissection, wemainly
 armto new scenes,includingafactory,ashelf,andtwoout- explore the effectiveness of using Robo GSim as an Evalu-
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 5. Novel Scene Syn the sis: Weshowtheresultsofthephysicalmigrationof the robotarmto new scenes,includingafactory,a
 shelf,andtwooutdoorenvironments.Thehigh-fidelitymulti-viewrenderingsdemonstrate that Robo GSimenables the robotarmtooperate
 seamlesslyacrossdiversescenes. 
 
 
 
 
 
 
 
 
 Figure 6. Robo GSimas Syn the sizer: Renderingof the sametrainingsetinnovelview and novelscenes.
 
 Method L 1 ↓ PSNR↑ tialalignmenttoenables 3 Dassertflow. Withnovelview-
 3 DGS 0.01381 34.19939 point, object, trajectory and scene, our Robo GSim engine
 2 DGS 0.01798 32.36417 can generate high-fidelity syn the sized data. Additionally,
 PGSR 0.01925 31.72386 due to our precise spatial alignment, Robo GSim can serve
 Mip Ne RF 360 0.02618 23.51348 as evaluator that allows real-time online policy evaluation.
 Despiteitsgreatprogress,thecurrentversionof Robo GSim
 Table 3.3 DGSachievesbetterstaticreconstructionthan 2 DGS has several limitations. It can only simulate rigid objects
 andthelighting for syn the sizedobjectsisnotyet full yuni-
 fied with the robotic arm. Moreover, generating geometri-
 ator. It aims to show its high consistency with real-world 
 cally consistent object meshes remains challenging, which
 inference. Given the well-trained VLA model, we deploy 
 isoftenkeytocompletingcomplexmanipulationtasks. In
 itforbothreal-worldrobots and Robo GSimsimulation. As 
 thenearfuture,wewillexploremoreadvancedmeshextrac-
 shownin Fig.9, ourclosed-loopsimulator Robo GSim can 
 tionmethods,furtherexp and the taskcategories and estab-
 reproduceresultssimilartothose from the realworld. For 
 lish the benchmarks to comprehensively evaluate the per-
 similarbadcases,our Robo GSim can avoid the issuesexist- 
 formanceacrossdiversescenarios. 
 ingin the realworld,likeviolations and collisions. There- 
 fore,ourevaluatorprovidesafair,safe,andefficientevalu- 
 ationplatform for policy. 
 6.Acknowledgements 
 5.Conclusion and Discussion 
 Inthispaper,webuilta Real 2 Sim 2 Realsimulator,basedon Theworkwassupportedby the National Science and Tech-
 3 DGS.Wealsointroduce the digitaltwinsystem with spa- nology Major Projectof China(2023 ZD 0121300).

 
 
 
 
 
 
 
 
 
 
 kci P 
 
 
 
 
 
 
 
 
 
 
 ecal P 
 Success 
 Fail 
 Success 
 Fail 
 
 Figure 7. Robo GSimas Syn the sizer: Thefirsttworowsshowrealrobotvideoscaptured from the testviewpoint,illustratingsuccessful
 andfailedcasesof the VLAmodelon the Pick task.Thelasttworowsdisplayrealrobotvideoscaptured from the testviewpoint,showing
 successful and failedcasesof the VLAmodelon the Place task. 
 
 Real 
 
 
 
 
 Robo GSim 
 
 
 
 
 Real Real 
 
 
 Hit someone Break gripper 
 in real world in real world 
 Robo GSim Robo GSim 
 
 
 
 
 Figure 8. Robo GSimas Evaluator: Thefirsttworows,labeled”Real”and”Robo GSim”,showthefootagecaptured from the realrobot
 and Robo GSim,respectively. They are bothdrivenbythetrajectorygeneratedby the same VLAnetwork. Inthethirdrow,theleftside
 shows the real-worldinferencewhere the robotarmexceedsitsoperationallimits,resultinginamanualshutdown. Therightsideshows
 aninstancewhereawrongdecision from the VLAnetwork,causestheroboticarmtocollide with the table. Thef our throwpresents the
 simulationresults from Robo GSim,which can avoiddangerouscollisions. 
 
 
 
 

 
 
 
 
 
 
 sus_mi SG 
 
 
 liaf_mi SG 
 
 
 sus_mi SG 
 
 
 
 liaf_mi SG 
 
 
 sus_lae R 
 
 
 liaf_lae R 
 
 
 sus_lae R 
 
 
 
 liaf_lae R 
 
 
 liaf_mi S 
 
 
 Figure 9. Expon Realrobot: Thefirstrow,GSim sus,representssuccessfulcasesof Robo GSim data under the testview. Thesecond
 row, GSim fail, represents failure cases of Robo GSim data under the novel view. The third row, GSim sus, represents successful
 casesof Robo GSim data under the novelscene. Thef our throw,GSim fail,representsfailurecasesof Robo GSim data under the novel
 scene.Thefifthrow,Real sus,representssuccessfulcasesofreal-world data under the testview.Thesixthrow,Real fail,represents
 successful cases of real-world data under the novel view. The seventh row, Real sus, represents successful cases of real-world data
 under the novelscene. Theeighthrow, Real fail, representsfailurecasesofreal-world data under the novelscene. Theninthrow,
 Sim fail,representsfailurecasesof Isaac Sim data under the testview. 
 
 References Moran,Steven Bohez,Fereshteh Sadeghi,Bojan Vujatovic,
 and Nicolas Heess. Nerf 2 real: Sim 2 realtransferofvision-
 [1] Open AI: Marcin Andrychowicz, Bowen Baker, Maciek 
 guidedbipedalmotionskillsusingneuralradiancefields. In
 Chociej, Rafal Jozefowicz, Bob Mc Grew, Jakub Pachocki, 
 2023 IEEE International Conference on Robotics and Au-
 Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, 
 tomation(ICRA),pages 9362–9369,2023. 2
 etal. Learningdexterousin-handmanipulation. The Inter- 
 [4] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xi-
 national Journalof Robotics Research,39(1):3–20,2020. 2 
 aofeng Yang,Yikai Wang,Zhongang Cai,Lei Yang,Huaping
 [2] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Liu,and Guosheng Lin. Gaussianeditor: Swift and control-
 Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel- lable 3 dediting with gaussiansplatting. In Proceedingsof
 level domain adaptation with generative adversarial net- the IEEE/CVFConferenceon Computer Vision and Pattern
 works. In Proceedingsof the IEEEconferenceoncomputer Recognition(CVPR),pages 21476–21485,2024. 4
 vision and patternrecognition,pages 3722–3731,2017. 2 
 [5] Cheng Chi,Zhenjia Xu,Siyuan Feng,Eric Cousineau,Yilun
 [3] Arunkumar Byravan, Jan Humplik, Leonard Hasenclever, Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song.
 Arthur Brussee, Francesco Nori, Tuomas Haarnoja, Ben Diffusionpolicy: Visuomotorpolicylearningviaactiondif-

 
 
 
 
 
 
 fusion. The International Journal of Robotics Research, [19] Mingsheng Long,Yue Cao,Jianmin Wang,and Michael Jor-
 2024. 3 dan.Learningtransferablefeatures with deepadaptationnet-
 [6] Peter ICorke. Asimple and systematicapproachtoassign- works. In International conference on machine learning,
 ing denavit–hartenberg parameters. IEEE transactions on pages 97–105.PMLR,2015. 2
 robotics,23(3):590–594,2007. 2,3 [20] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
 [7] Erwin Coumans and Yunfei Bai. Pybullet, a python mod- Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,
 uleforphysicssimulation for games,robotics and machine Marc Habermann,Christian Theobalt,etal. Wonder 3 d:Sin-
 learning. http://pybullet.org,2016–2021. 2 gleimageto 3 dusingcross-domaindiffusion.ar Xivpreprint
 [8] Konstantinos Dimitropoulos, Ioannis Hatzilygeroudis, and ar Xiv:2310.15008,2023. 4
 Konstantinos Chatzilygeroudis. Abriefsurveyofsim 2 real [21] Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng
 methods for robot learning. In International Conference Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen
 on Roboticsin Alpe-Adria Danube Region,pages 133–140. Feng,Lu Shi,etal. Robo-gs: Aphysicsconsistentspatial-
 Springer,2022. 2 temporal model for roboticarm with hybridrepresentation.
 [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- ar Xivpreprintar Xiv:2408.14873,2024. 2,3,5
 hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil [22] Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Ji-
 Mathur,Alan Schelten,Amy Yang,Angela Fan,etal. The wen Lu,and Yansong Tang. Manigaussian: Dynamicgaus-
 llama 3 herd ofmodels. ar Xiv preprint ar Xiv:2407.21783, sian splatting for multi-task robotic manipulation. In Eu-
 2024. 6 ropean Conference on Computer Vision, pages 349–366.
 [10] Ioannis Exarchos, Yifeng Jiang, Wenhao Yu, and C Karen Springer,2025. 3
 Liu. Policy transfer via kinematic domain randomization 
 [23] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,
 andadaptation. In 2021 IEEEInternational Conferenceon 
 Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,
 Robotics and Automation(ICRA),pages 45–51.IEEE,2021. 
 Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel
 2 
 State. Isaac gym: High per for mance GPU based physics
 [11] Zipeng Fu, Tony Z. Zhao, and Chelsea Finn. Mobile 
 simulation for robotlearning. In Thirty-fifth Conferenceon
 ALOHA: Learning bimanual mobile manipulation using 
 Neural Information Processing Systems Datasets and Bench-
 low-cost whole-body teleoperation. In 8 th Annual Confer- 
 marks Track(Round 2),2021. 2 
 enceon Robot Learning,2024. 2 
 [24] Hidenobu Matsuki, Riku Murai, Paul H.J. Kelly, and An-
 [12] Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao 
 drew J.Davison. Gaussiansplattingslam. In Proceedingsof
 Dong, and He Wang. Partmanip: Learning cross-category 
 the IEEE/CVFConferenceon Computer Vision and Pattern
 generalizablepartmanipulationpolicy from pointcloudob- 
 Recognition(CVPR),pages 18039–18048,2024. 5
 servations. ar Xivpreprintar Xiv:2303.16958,2023. 2 
 [25] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
 [13] Minho Heo, Youngwoon Lee, Doohyun Lee, and Joseph J. 
 Jonathan T.Barron,Ravi Ramamoorthi,and Ren Ng. Nerf:
 Lim. Furniturebench: Reproducible real-world benchmark 
 representingscenesasneuralradiancefields for viewsyn the-
 for long-horizon complex manipulation. In Robotics: Sci- 
 sis. Commun.ACM,65(1):99–106,2021. 2,3
 ence and Systems,2023. 2 
 [26] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita
 [14] Johann Huber, Franc¸ois He´le´non, Hippolyte Watrelot, 
 Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yun-
 Fa¨ız Ben Amar, and Ste´phane Doncieux. Domain ran- 
 rong Guo,Hammad Mazhar,Ajay Mandlekar,Buck Babich,
 domization for sim 2 realtransferofautomaticallygenerated 
 Gavriel State, Marco Hutter, and Animesh Garg. Orbit: A
 grasping data sets.In 2024 IEEEInternational Conferenceon 
 unified simulation framework for interactive robot learning
 Robotics and Automation(ICRA),pages 4112–4118.IEEE, 
 environments. IEEERobotics and Automation Letters,8(6):
 2024. 2 
 3740–3747,2023. 2 
 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler, 
 [27] Jean-Baptiste Mouret and Konstantinos Chatzilygeroudis.20
 and George Drettakis. 3 d gaussian splatting for real-time 
 radiancefieldrendering.ACMTransactionson Graphics,42 yearsofrealitygap:afewthoughtsaboutsimulatorsinevo-
 lutionary robotics. In Proceedings of the genetic and evo-
 (4),2023. 2,3 
 lutionarycomputationconferencecompanion, pages 1121–
 [16] Vikash Kumar,Rutav Shah,Gaoyue Zhou,Vincent Moens, 
 1124,2017. 2 
 Vittorio Caggiano, Abhishek Gupta, and Aravind Ra- 
 jeswaran. Robohive: Aunifiedframework for robotlearn- [28] NVIDIA. Isaacsim. https://developer.nvidia.
 ing. In Thirty-seventh Conference on Neural Information com/isaac/sim,2024. Softw are. 2,5
 Processing Systems Datasets and Benchmarks Track,2023. [29] Mohammad Nomaan Qureshi,Sparsh Garg,Francisco Yan-
 2 dun, David Held, George Kantor, and Abhishesh Sil-
 [17] Xinhai Li, Huaibin Wang, and Kuo-Kun Tseng. Gaus- wal. Splatsim: Zero-shot sim 2 real transfer of rgb manip-
 siandiffusion: 3 dgaussiansplatting for denoisingdiffusion ulation policies using gaussian splatting. ar Xiv preprint
 probabilistic models with structured noise. ar Xiv preprint ar Xiv:2409.10161,2024. 3
 ar Xiv:2311.11221,2023. 4 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
 [18] Ruoshi Liu, Alper can berk, Shuran Song, and Carl Von- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
 drick. Differentiable robot rendering. In 8 th Annual Con- Amanda Askell,Pamela Mishkin,Jack Clark,etal.Learning
 ferenceon Robot Learning,2024. 3 transferable visual models from natural language supervi-

 
 
 
 
 
 
 sion.In Internationalconferenceonmachinelearning,pages grasping. IEEE Robotics and Automation Letters, 9(9):
 8748–8763.PMLR,2021. 6 7827–7834,2024. 3 
 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, [43] Licheng Zhong,Hong-Xing Yu,Jiajun Wu,and Yunzhu Li.
 Patrick Esser, and Bjo¨rn Ommer. High-resolution image Reconstruction and simulationofelasticobjects with spring-
 syn the sis with latent diffusion models. In Proceedings of mass 3 d gaussians. In European Conference on Computer
 the IEEE/CVF conference on computer vision and pattern Vision,pages 407–423.Springer,2025. 2
 recognition,pages 10684–10695,2022. 4 
 [32] Johannes LSchonberger and Jan-Michael Frahm. Structure- 
 from-motion revisited. In Proceedings of the IEEE con- 
 ference on computer vision and pattern recognition, pages 
 4104–4113,2016. 4 
 [33] Xuelun Shen,Zhipeng Cai,Wei Yin,Matthias Mu¨ller,Zijun 
 Li,Kaixuan Wang,Xiaozhi Chen,and Cheng Wang. Gim: 
 Learninggeneralizableimagematcher from internetvideos. 
 In The Twelfth International Conferenceon Learning Repre- 
 sentations,2024. 4 
 [34] Josh Tobin,Rachel Fong,Alex Ray,Jonas Schneider,Woj- 
 ciech Zaremba, and Pieter Abbeel. Domainr and omization 
 fortransferringdeepneuralnetworks from simulationto the 
 real world. In 2017 IEEE/RSJ international conference on 
 intelligent robots and systems (IROS), pages 23–30. IEEE, 
 2017. 2 
 [35] Emanuel Todorov,Tom Erez,and Yuval Tassa. Mujoco: A 
 physicsengine for model-basedcontrol. In 2012 IEEE/RSJ 
 International Conferenceon Intelligent Robots and Systems, 
 pages 5026–5033.IEEE,2012. 2 
 [36] Jingkang Wang,Yang Liu,and Bo Li. Rein for cementlearn- 
 ing with perturbedrewards.In Proceedingsof the AAAIcon- 
 ferenceonartificialintelligence,pages 6202–6209,2020. 3 
 [37] David Whitney, Eric Rosen, Daniel Ullman, Elizabeth 
 Phillips, and Stefanie Tellex. Rosreality: Avirtualreality 
 frameworkusingconsumer-gradehardw are for ros-enabled 
 robots. In 2018 IEEE/RSJInternational Conferenceon In- 
 telligent Robots and Systems(IROS),pages 1–9,2018. 2 
 [38] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao 
 Zhu,Fangchen Liu,Minghua Liu,Hanxiao Jiang,Yifu Yuan, 
 He Wang,Li Yi,Angel X.Chang,Leonidas J.Guibas,and 
 Hao Su. SAPIEN:Asimulatedpart-basedinteractiveenvi- 
 ronment. In The IEEEConferenceon Computer Vision and 
 Pattern Recognition(CVPR),2020. 2 
 [39] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea 
 Finn. Learning Fine-Grained Bimanual Manipulation with 
 Low-Cost Hardw are. In Proceedings of Robotics: Science 
 and Systems,Daegu,Republicof Korea,2023. 2 
 [40] Wenshuai Zhao, Jorge Pen˜a Queralta, and Tomi Wester- 
 lund. Sim-to-realtransferindeeprein for cementlearning for 
 robotics:asurvey.In 2020 IEEESymposium Serieson Com- 
 putational Intelligence(SSCI),pages 737–744,2020. 2 
 [41] Liming Zheng, Wenxuan Ma, Yinghao Cai, Tao Lu, and 
 Shuo Wang. Gpdan:Graspposedomainadaptationnetwork 
 for sim-to-real 6-dof object grasping. IEEE Robotics and 
 Automation Letters,8(8):4585–4592,2023. 2 
 [42] Yuhang Zheng,Xiangyu Chen,Yupeng Zheng,Songen Gu, 
 Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zeng- 
 mao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen 
 Chen,Xiaoxiao Long,and Meiqing Wang.Gaussiangrasper: 
 3 dlanguagegaussiansplatting for open-vocabularyrobotic 