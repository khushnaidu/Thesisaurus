 
 
 
 https://robotics-trans for mer 2.github.io
 2023-8-1 
 
 RT-2: Vision-Language-Action Models Transfer 
 
 Web Knowledge to Robotic Control 
 
 
 Anthony Brohan,Noah Brown,Justice Carbajal,Yevgen Chebotar,Xi Chen,Krzysztof Choromanski,
 Tianli Ding,Danny Driess,Avinava Dubey,Chelsea Finn,Pete Florence,Chuyuan Fu, 
 Montse Gonzalez Arenas,Keerthana Gopalakrishnan,Kehang Han,Karol Hausman,Alexander Herzog,
 Jasmine Hsu,Brian Ichter,Alex Irpan,Nikhil Joshi,Ryan Julian,Dmitry Kalashnikov,Yuheng Kuang,
 Isabel Leal,Lisa Lee,Tsang-Wei Edward Lee,Sergey Levine,Yao Lu,Henryk Michalewski,Igor Mordatch,
 Karl Pertsch,Kanishka Rao,Krista Reymann,Michael Ryoo,Grecia Salazar,Pannag Sanketi,
 Pierre Sermanet,Jaspiar Singh,Anikait Singh,Radu Soricut,Huong Tran,Vincent Vanhoucke,Quan Vuong,
 Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Jialin Wu,Fei Xia,Ted Xiao,Peng Xu,Sichun Xu,Tianhe Yu,
 and Brianna Zitkovich 
 Google Deep Mind.Authorslistedinalphabeticalorder,withcontributionslistedin Appendix A.
 
 
 Westudyhowvision-language model strainedon Internet-scale data can beincorporateddirectlyinto
 end-to-endroboticcontroltoboostgeneralization and enableemergentsemanticreasoning. Ourgoalis
 toenableasingleend-to-endtrained model tobothlearntomaprobotobservationstoactions and enjoy
 thebenefitsoflarge-scalepretrainingonlanguage and vision-language data from the web. Tothisend,
 weproposetoco-fine-tunestate-of-the-artvision-language model sonbothrobotictrajectory data and
 
 Internet-scalevision-languagetasks,suchasvisualquestionanswering. Incontrasttoo the rapproaches,
 weproposeasimple,generalrecipetoachieve this goal: inordertofitbothnaturallanguageresponses
 androboticactionsinto the same for mat,weexpress the actionsastexttokens and incorporatethem
 directly into the training set of the model in the same way as natural language tokens. We refer to
 suchcategoryof model sasvision-language-actionmodels(VLA)andinstantiateanexampleofsuch
 amodel,whichwecall RT-2. Ourextensiveevaluation(6 kevaluationtrials)shows that ourapproach
 leadstoper for mantroboticpolicies and enables RT-2 toobtainarangeofemergentcapabilities from
 Internet-scaletraining. Thisincludessignifi can tlyimprovedgeneralizationtonovelobjects,theability
 tointerpretcomm and snotpresentin the robottraining data(suchasplacinganobjectontoaparticular
 numberoricon),and the abilitytoper for mrudimentaryreasoninginresponsetousercommands(such
 
 aspickingup the smallestorlargestobject,ortheoneclosesttoano the robject). Wefur the rshow that
 incorporatingchainofthoughtreasoningallows RT-2 toper for mmulti-stagesemanticreasoning,for
 examplefiguringoutwhichobjecttopickup for useasanimprovisedhammer(arock),orwhichtype
 ofdrinkisbestsuited for someo new hoistired(anenergydrink). 
 
 
 1. Introduction 
 
 High-capacity models pretrained on broad web-scale datasets provide an effective and powerful
 platform for awiderangeofdownstreamtasks: largelanguagemodels can enablenotonlyfluenttext
 generation(Aniletal.,2023;Brohanetal.,2022;Open AI,2023)butemergentproblem-solving(Cobbe
 et al., 2021; Lewkowycz et al., 2022; Polu et al., 2022) and creative generation of prose (Brown
 et al., 2020; Open AI, 2023) and code (Chen et al., 2021), while vision-language models enable
 open-vocabulary visual recognition (Kirillov et al., 2023; Minderer et al., 2022; Radford et al., 2021)
 and can evenmakecomplexinferencesaboutobject-agentinteractionsinimages(Alayracetal.,2022;
 Chenetal.,2023 a,b;Driessetal.,2023;Haoetal.,2022;Huangetal.,2023;Wangetal.,2022). Such
 semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously
 useful for generalist robots that must perform a variety of tasks in real-world environments. However,
 
 
 Correspondingauthor(s):chebotar@google.com,tianheyu@google.com,karolhausman@google.com
 ¬© 2023 Google Deep Mind.Allrightsreserved 
 3202 
 lu J 
 82 
 ]OR.sc[ 
 1 v 81851.7032:vi Xra 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 Internet-Scale VQA + Robot Action Data Vision-Language-Action Models for Robot Control Closed-Loop
 Robot Control 
 Q: What is happening Q: What should the robot RT-2 
 in the image? do to <task>? A: ‚Ä¶ Large Language Model 
 A: 311 423 170 55 244 
 A grey donkey walks 
 down the street. 
 Put the strawberry
 Q: Que puis-je faire avec Vi T into the correct bowl
 ces objets? 
 A: 3455 1144 189 25673 
 Faire cuire un g√¢teau. 
 ŒîT = [0.1, -0.2, 0] 
 Q: What should the robot A: 132 114 128 5 25 156 De-Tokenize ŒîR = [10‚àò , 25‚àò , -7‚àò ] Pick the nearly falling bag
 do to <task>? Robot Action 
 A: 132 114 128 5 25 156 
 Œî Translation = [0.1, -0.2, 0] 
 ŒîRotation = [10‚àò , 25 ‚àò, -7 ‚àò ] Co-Fine-Tune Deploy 
 Pick object that is different
 Figure 1|RT-2 overview:werepresentrobotactionsasano the rlanguage,which can becastintotexttokens and
 trainedtoge the rwith Internet-scalevision-language data sets. Duringinference,thetexttokensarede-tokenized
 into robot actions, enabling closed loop control. This allows us to leverage the backbone and pretraining
 of vision-language models in learning robotic policies, transferring some of their generalization, semantic
 underst and ing,andreasoningtoroboticcontrol. Wedemonstrateexamplesof RT-2 executionon the project
 website: robotics-trans for mer 2.github.io. 
 it is unclear how robots should acquire such capabilities. While a brute force approach might entail
 collectingmillionsofroboticinteractiontrials,themostcapablelanguage and vision-languagemodels
 are trained on billions of tokens and images from the web (Alayrac et al., 2022; Chen et al., 2023 a,b;
 Huang et al., 2023) ‚Äì an amount unlikely to be matched with robot data in the near future. On the
 other hand, directly applying such models to robotic tasks is also difficult: such models reason about
 semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such
 as Cartesian end-effector commands. While a number of recent works have sought to incorporate
 language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess
 etal.,2023;Vempralaetal.,2023),suchmethodsgenerallyaddressonly the‚Äúhigherlevel‚Äùaspectsof
 robotic planning, essentially taking the role of a state machine that interprets commands and parses
 them into individual primitives (such as picking and placing objects), which are then executed by
 separate low-level controllers that themselves do not benefit from the rich semantic knowledge of
 Internet-scale models during training. Therefore, in this paper we ask: can large pretrained vision-
 language models be integrated directly into low-level robotic control to boost generalization and
 enable emergent semantic reasoning? 
 To this end, we explore an approach that is both simple and surprisingly effective: we directly
 train vision-language models designed for open-vocabulary visual question answering and visual
 dialogue to output low-level robot actions, along with solving other Internet-scale vision-language
 tasks. Although such models are typically trained to produce natural language tokens, we can train
 them on robotic trajectories by tokenizing the actions into text tokens and creating ‚Äúmultimodal
 sentences‚Äù(Driessetal.,2023)that‚Äúrespond‚Äùtoroboticinstructionspaired with cameraobservations
 by producing corresponding actions. In this way, vision-language models can be directly trained to
 actasinstructionfollowingroboticpolicies. Thissimpleapproachisincontrast with prioralternatives
 for incorporating VLMs into robot policies (Shridhar et al., 2022 a) or designing new vision-language-
 action architectures from scratch (Reed et al., 2022): instead, pre-existing vision-language models,
 with already-amortized significant compute investment, are trained without any new parameters to
 output text-encoded actions. We refer to this category of models as vision-language-action (VLA)
 models. We instantiate VLA models by building on the protocol proposed for RT-1 (Brohan et al.,
 2022), using a similar dataset, but exp and ing the model to use a large vision-language backbone.
 Hence we refer to our model as RT-2 (Robotics Trans for mer 2). We provide an overview in Figure 1.
 2 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 We observe that robotic policies derived from such vision-language models exhibit a range of
 remarkable capabilities, combining the physical motions learned from the robot data with the ability
 to interpret images and text learned from web data into a single model. Besides the expected benefit
 of dramatically improving generalization to novel objects and semantically varied instructions, we
 observe a number of emergent capabilities. While the model‚Äôs physical skills are still limited to the
 distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in
 new ways by interpreting images and language commands using knowledge gleaned from the web.
 Some example highlights are shown in Figure 2. The model is able to re-purpose pick and place
 skills learned from robot data to place objects near semantically indicated locations, such as specific
 numbersoricons,despitethosecuesnotbeingpresentin the robot data. Themodel can alsointerpret
 relations between objects to determine which object to pick and where to place it, despite no such
 relations being provided in the robot demonstrations. Fur the rmore, if we augment the comm and
 with chain of thought prompting, the model is able to make even more complex semantic inferences,
 such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type
 of drink is best suited for someone who is tired (an energy drink). 
 
 Our main contribution is RT-2, a family of models derived from fine-tuning large vision-language
 models trained on web-scale data to directly act as generalizable and semantically aware robotic
 policies. Our experiments investigate models with up to 55 B parameters trained on Internet data
 and instruction-annotated robotic trajectories from previous work (Brohan et al., 2022). Over the
 courseof 6 kroboticevaluations,weshow that RT-2 enablesignifi can timprovementstogeneralization
 over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from
 web-scale vision-language pretraining. 
 
 
 2. Related Work 
 
 Vision-language models. There are several categories of Vision-Language Models (VLMs) (Gan et al.,
 2022), with perhaps two most relevant: (1) representation-learning models, e.g. CLIP (Radford
 et al., 2021), which learn common embeddings for both modalities, and (2) visual language models
 of the form {vision,text} ‚Üí {text} which learn to take vision and language as input and provide
 free-form text. Both categories have been used to provide pretraining for a wide variety of applied
 to downstream applications such as object classification (Radford et al., 2021), detection (Gu et al.,
 2021), and segmentation (Ghiasi et al., 2021). In this work, we focus on the latter category (Alayrac
 et al., 2022; Chen et al., 2023 a,b; Driess et al., 2023; Hao et al., 2022; Li et al., 2023, 2019; Lu
 et al., 2019). These models are generally trained on many different tasks, such as image captioning,
 vision-question answering (VQA), and general language tasks on multiple datasets at the same time.
 While prior works study VLMs for a wide range of problems and settings including in robotics, our
 focus is on how the capabilities of VLMs can be extended to robotics closed-loop control by endowing
 them with the abilitytopredictrobotactions,thusleveraging the knowledgealreadypresentin VLMs
 to enable new levels of generalization. 
 
 Generalization in robot learning. Developing robotic controllers that can broadly succeed in a
 variety of scenarios is a long-standing goal in robotics research (Kaelbling, 2020; Smith and Coles,
 1973). A promising approach for enabling generalization in robotic manipulation is by learning from
 large anddiverse datasets(Dasari etal.,2019;Levineetal.,2018; Pinto and Gupta,2016). Bydoing
 so, prior methods have demonstrated how robots can generalize to novel object instances (Finn and
 Levine,2017;Levineetal.,2018;Mahleretal.,2017;Pinto and Gupta,2016;Youngetal.,2021),to
 tasks involving novel combinations of objects and skills (Dasari and Gupta, 2021; Finn et al., 2017;
 James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang
 et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022 a; Pong et al.,
 
 
 3 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and
 tounseenenvironments(Cuietal.,2022;Duetal.,2023 a;Hansenetal.,2020). Unlikemostofthese
 prior works, we aim to develop and study a single model that can generalize to unseen conditions
 along all of these axes. A key ingredient of our approach is to leverage pre-trained models that have
 been exposed to data that is much broader than the data seen by the robot.
 Pre-training for robotic manipulation. Pre-training has a long history in robotic learning. Most
 works focus on pre-trained visual representations that can be used to initialize the encoder of the
 robot‚Äôs camera observations, either via supervised Image Net classification (Shah and Kumar, 2021),
 data augmentation (Kostrikov et al., 2020; Laskin et al., 2020 a,b; Pari et al., 2021) or objectives
 that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al.,
 2023 b; Nair et al., 2022 b; Xiao et al., 2022 b). Other works have incorporated pre-trained language
 
 models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al.,
 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022 a; Shridhar et al., 2022 b) or
 for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023;
 Singh et al., 2023; Wu et al., 2023). Rather than using pre-training vision models or pre-trained
 language models, we specifically consider the use of pre-trained vision-language models (VLMs),
 which provide rich, grounded knowledge about the world. Prior works have studied the use of VLMs
 for robotics (Driess et al., 2023; Du et al., 2023 b; Gadre et al., 2022; Karamcheti et al., 2023; Shah
 et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this
 work. Thesepriorapproaches use VLMs forvisualstaterepresentations(Karamchetietal.,2023), for
 identifyingobjects(Gadreetal.,2022;Stoneetal.,2023),forhigh-levelplanning(Driessetal.,2023),
 or for providing supervision or success detection (Du et al., 2023 b; Ma et al., 2023; Sumers et al.,
 2023; Xiao et al., 2022 a; Zhang et al., 2023). While CLIPort (Shridhar et al., 2021) and MOO (Stone
 et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both
 incorporate significant structure into the policy that limits their applicability. Notably, our work does
 notrelyon are stricted 2 Dactionspace and doesnotrequireacalibratedcamera. Moreover,acritical
 distinction is that, unlike these works, we leverage VLMs that generate language, and the unified
 output space of our formulation enables model weights to be entirely shared across language and
 action tasks, without introducing action-only model layer components. 
 
 
 3. Vision-Language-Action Models 
 
 In this section, we present our model family and the design choices for enabling training VLMs to
 directly perform closed-loop robot control. First, we describe the general architecture of our models
 and how they can be derived from models that are commonly used for vision-language tasks. Then,
 we introduce the recipe and challenges of fine-tuning large VLMs that are pre-trained on web-scale
 data to directly output robot actions, becoming VLA models. Finally, we describe how to make these
 modelspractical for robottasks,addressingchallenges with modelsize and inferencespeedtoenable
 real-time control. 
 
 
 3.1. Pre-Trained Vision-Language Models 
 
 The vision-language models (Chen et al., 2023 a; Driess et al., 2023) that we build on in this work
 take as input one or more images and produce a sequence of tokens, which conventionally represents
 natural language text. Such models can perform a wide range of visual interpretation and reasoning
 tasks, from inferring the composition of an image to answering questions about individual objects
 and their relations to other objects (Alayrac et al., 2022; Chen et al., 2023 a; Driess et al., 2023;
 Huang et al., 2023). Representing the knowledge necessary to perform such a wide range of tasks
 
 
 4 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 requires large models and web-scale datasets. In this work, we adapt two previously proposed VLMs
 to act as VLA models: Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023). We will refer
 to vision-language-action versions of these models as RT-2-Pa LI-X and RT-2-Pa LM-E. We leverage
 instantiations of these models that range in size from billions to tens of billions of parameters. We
 provide a detailed description of the architecture of these two models in Appendix D.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 2 | RT-2 is able to generalize to a variety of real-world situations that require reasoning, symbol
 underst and ing,andhumanrecognition. Westudy the sechallengingscenariosindetailin Section 4.
 
 
 3.2. Robot-Action Fine-tuning 
 
 To enable vision-language models to control a robot, they must be trained to output actions. We
 take a direct approach to this problem, representing actions as tokens in the model‚Äôs output, which
 are treated in the same way as language tokens. We base our action encoding on the discretization
 proposed by Brohan et al. (2022) for the RT-1 model. The action space consists of 6-Do F positional
 and rotational displacement of the robot end-effector, as well as the level of extension of the robot
 gripper and a special discrete comm and for terminating the episode, which should be triggered by
 the policy to signal successful completion. The continuous dimensions (all dimensions except for
 the discrete termination comm and) are discretized into 256 bins uni for mly. Thus, the robot action
 can be represented using ordinals of the discrete bins as 8 integer numbers. In order to use these
 discretized actions to fine tune a vision-language into a vision-language-action model, we need to
 associate tokens from the model‚Äôs existing tokenization with the discrete action bins. This requires
 
 
 5 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 reserving 256 tokens to serve as action tokens. Which tokens to choose depends on the particular
 tokenization used by each VLM, which we discuss later in this section. In order to define a target
 for VLM fine-tuning we convert the action vector into a single string by simply concatenating action
 tokens for each dimension with a space character: 
 ‚Äúterminate Œîpos Œîpos Œîpos Œîrot Œîrot Œîrot gripper_extension‚Äù. 
 ùë• ùë¶ ùëß ùë• ùë¶ ùëß 
 A possible instantiation of such a target could be: ‚Äú1 128 91 241 5 101 127‚Äù. The two VLMs that
 we fine tune in our experiments, Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023), use
 different tokenizations. For Pa LI-X, integers up to 1000 each have a unique token, so we simply
 associate the action bins to the token representing the corresponding integer. For the Pa LM-E model,
 which does not provide this convenient representation of numbers, we simply overwrite the 256 least
 frequently used tokens to represent the action vocabulary. It is worth noting that training VLMs to
 override existing tokens with action tokens is a form of symbol tuning (Wei et al., 2023), which has
 been shown to work well for VLMs in prior work. 
 
 Taking the action representation described above, we convert our robot data to be suitable for
 VLM model fine-tuning, where our inputs include robot camera image and textual task description
 (usingst and ard VQA for mat‚ÄúQ:whatactionshould the robottaketo[taskinstruction]? A:‚Äù),andour
 output is formatted as a string of numbers/least frequently used tokens representing a robot action.
 Co-Fine-Tuning. As we will show in our experiments, a key technical detail of the training recipe
 that improves robot per for mance is co-fine-tuning robotics data with the original web data instead of
 na√Øvefinetuningonrobotdataonly. Wenoticethatco-fine-tuningleadstomoregeneralizablepolicies
 since the policies are exposedtobothabstractvisualconcepts from webscale data and lowlevelrobot
 actions during fine-tuning, instead of just robot actions. During co-fine-tuning we balance the ratios
 of robot and web data in each training batch by increasing the sampling weight on the robot dataset.
 
 Output Constraint. One important distinction between RT-2 and standard VLMs is that RT-2
 is required to output valid action tokens for execution on the real robot. Thus, to ensure that RT-2
 outputs valid action tokens during decoding, we constrain its output vocabulary via only sampling
 valid action tokens when the model is prompted with a robot-action task, whereas the model is still
 allowed to output the full range of natural language tokens on standard vision-language tasks.
 
 
 3.3. Real-Time Inference 
 The size of modern VLMs can reach tens or hundreds of billions of parameters (Chen et al., 2023 a;
 Driess et al., 2023). The largest model trained in this work uses 55 B parameters. It is infeasible to
 directlyrunsuch model son the standarddesktop-stylemachinesoron-robot GPUscommonlyused for
 real-time robot control. To the best of our knowledge, our model is the largest ever, by over an order
 
 ofmagnitude,used for directclosed-looproboticcontrol,and the reforerequiresa new setofsolutions
 to enable efficient real-time inference. We develop a protocol that allows us to run RT-2 models on
 robots by deploying them in a multi-TPU cloud service and querying this service over the network.
 With this solution,we canachieveasuitablefrequencyofcontrol and alsoservemultiplerobotsusing
 the same cloud service. The largest model we evaluated, the 55 B parameter RT-2-Pa LI-X-55 B model,
 can run at a frequency of 1-3 Hz. The smaller version of that model, consisting of 5 B parameters, can
 run at a frequency of around 5 Hz. 
 
 4. Experiments 
 
 Our experiments focus on real-world generalization and emergent capabilities of RT-2 and aim to
 answer the following questions: 
 
 
 6 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects,
 backgrounds, and environments? 
 2. Can we observe and measure any emergent capabilities of RT-2? 
 3. How does the generalization vary with parameter count and other design decisions?
 4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models?
 
 We evaluate our approach and several baselines with about 6,000 evaluation trajectories in a variety
 of conditions, which we describe in the following sections. Unless specified otherwise, we use a
 7 Do F mobile manipulator with the action space described in Sec. 3.2. We also demonstrate examples
 of RT-2 execution on the project website: robotics-trans for mer 2.github.io. We train two
 specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-Pa LI-X is built from 5 B and
 55 B Pa LI-X (Chen et al., 2023 a), and (2) RT-2-Pa LM-E is built from 12 B Pa LM-E (Driess et al., 2023).
 For training, we leverage the original web scale data from Chen et al. (2023 a) and Driess et al.
 (2023), which consists of visual question answering, captioning, and unstructured interwoven image
 and text examples. We combine it with the robot demonstration data from Brohan et al. (2022),
 which was collected with 13 robots over 17 months in an office kitchen environment. Each robot
 
 demonstration trajectory is annotated with a natural language instruction that describes the task
 per for med,consistingofaverbdescribing the skill(e.g.,‚Äúpick‚Äù,‚Äùopen‚Äù,‚Äúplaceinto‚Äù)andoneormore
 nouns describing the objects manipulated (e.g., ‚Äú7 up can‚Äù, ‚Äúdrawer‚Äù, ‚Äúnapkin‚Äù) (see Appendix B for
 moredetailson the used data sets). Forall RT-2 trainingrunsweadopt the hyperparameters from the
 original Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023) papers, including learning rate
 schedules and regularizations. More training details can be found in Appendix E.
 Baselines. We comp are our method to multiple state-of-the-art baselines that challenge different
 aspects of our method. All of the baselines use the exact same robotic data. To comp are against a
 state-of-the-art policy, we use RT-1 (Brohan et al., 2022), a 35 M parameter trans for mer-based model.
 Tocomp are againststate-of-the-artpretrainedrepresentations,we use VC-1(Majumd are tal.,2023 a)
 and R 3 M (Nair et al., 2022 b), with policies implemented by training an RT-1 backbone to take their
 
 representationsasinput. Tocomp are againsto the rarchitectures for using VLMs,we use MOO(Stone
 et al., 2023), which uses a VLM to create an additional image channel for a semantic map, which is
 then fed into an RT-1 backbone. More information is provided in Appendix C.
 
 4.1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects,
 backgrounds, and environments? 
 
 
 
 
 
 (a) Unseen Objects (b) Unseen Backgrounds (c) Unseen Environments 
 Figure 3 | Examplegeneralizationscenariosused for evaluationin Figures 4 and 6 band Tables 4 and 6.
 
 To evaluate in-distribution per for mance as well as generalization capabilities, we comp are the
 RT-2-Pa LI-X and RT-2-Pa LM-E models to the four baselines listed in the previous sections. For the
 seen tasks category, we use the same suite of seen instructions as in RT-1 (Brohan et al., 2022), which
 include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for
 placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for
 pickingoutof and placingobjectsintodrawers. Note,however,thatthese‚Äúin-distribution‚Äùevaluations
 still vary the placement of objects and factors such as time of day and robot position, requiring the
 skills to generalize to realistic variability in the environment. 
 
 
 7 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 Figure 3 showsexamplegeneralizationevaluations,which are splitintounseencategories(objects,
 backgrounds and environments),and are additionallysplitintoeasy and hardcases. Forunseenobjects,
 hard cases include harder-to-grasp and more unique objects (such as toys). For unseen backgrounds,
 hardcasesincludemorevariedbackgrounds and novelobjects. Lastly,forunseenenvironments,hard
 cases correspond to a more visually distinct office desk environment with monitors and accessories,
 while the easierenvironmentisakitchensink. Theseevaluationsconsistsofover 280 tasks that focus
 primarily on pick and placing skills in many diverse scenarios. The list of instructions for unseen
 categories is specified in Appendix F.2. 
 
 
 
 
 
 
 
 
 
 
 Figure 4 | Overallper for manceoftwoinstantiationsof RT-2 and base linesacrossseentraining task saswellas
 unseenevaluationsmeasuringgeneralizationtonovelobjects,novelbackgrounds,andnovelenvironments.
 Appendix Table 4 details the fullresults. 
 
 The evaluation results are shown in Figure 4 and Appendix Table 4. The per for mance on seen
 tasks is similar between the RT-2 models and RT-1, with other baselines attaining a lower success
 rate. The difference between the RT-2 models and the baseline is most pronounced in the various
 generalization experiments, suggesting that the strength of vision-language-action models lies in
 transferring more generalizable visual and semantic concepts from their Internet-scale pretraining
 data. Here, on average, both instantiations of RT-2 perform similarly, resulting in ‚àº2 x improvement
 over the next two baselines, RT-1 and MOO, and ‚àº6 x better than the other baselines. The Pa LM-E
 version of RT-2 seems to perform better than the RT-2-Pa LI-X in harder versions of generalization
 scenarios while under-per for ming on easier ones, resulting in a similar average per for mance.
 
 Open Source Language Table Benchmark. To provide an additional point of comparison using
 open-source baselines and environments, we leverage the open-source Language-Table simulation
 environment from Lynchetal.(2022). Weco-fine-tuneasmaller Pa LI 3 Bmodelonseveralprediction
 tasks, including in-domain VQA tasks, for the Language-Table dataset, and evaluate the resulting
 policy in simulation. For the action prediction task, we discretize and encode actions as text in the
 format‚ÄúX Y‚Äù,where Xand Yrangebetween{-10,-9,...,+9,+10},andrepresentdelta 2 Dcartesian
 setpointsof the endeffector. Duetoitsreducedsize,theresulting model can runinferenceatasimilar
 rate(5 Hz)astheo the rbaselines. Theresultsof this experiment are presentedin Table 1. Weobserve
 a significant per for mance boost when using our model compared to the baselines, indicating that the
 VLM-based pre-training together with the expressiveness of the large Pa LI model can be beneficial in
 other scenarios, in this case, simulation with a different robot. We also show qualitative real-world
 out-of-distribution behaviors behaviors in Figure 5, demonstrating novel pushing tasks and targeting
 objects not before seen in this environment. More details about the Language Table experiments can
 be found in Appendix B and D. 
 
 
 4.2. Can we observe and measure any emergent capabilities of RT-2? 
 Inadditiontoevaluating the generalizationcapabilitiesofvision-language-actionmodels,wealsoaim
 to evaluate the degree to which such models can enable new capabilities beyond those demonstrated
 
 
 8 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 Model Language-Table 
 BC-Zero(Jangetal.,2021) 72¬±3 
 RT-1(Brohanetal.,2022) 74¬±13 
 LAVA(Lynchetal.,2022) 77¬±4 
 RT-2-Pa LI-3 B(ours) 90¬±10 
 Figure 5 | Real-world out-of-distribution behaviors in the Table 1 | Per for mance on the simulated
 Language Tableenvironment. Identical RT-2-Pa LI-3 Bmodel Language-Table tasks (Lynch and Ser-
 checkpointisusedasin Tab.1. manet,2020). 
 
 intherobot data bytransferringknowledge from the web. Werefertosuchcapabilitiesasemergent,in
 the sense that they emerge by transferring Internet-scale pretraining. We do not expect such transfer
 to enable new robotic motions, but we do expect semantic and visual concepts, including relations
 andnouns,totransfereffectively, evenincaseswherethoseconcepts were notseenin the robot data.
 
 Qualitative Evaluations. First, we experiment with our RT-2-Pa LI-X model to determine various
 emergent capabilities transferred from vision-language concepts. We demonstrate some examples of
 such interactions in Figure 2. We find through our explorations that RT-2 inherits novel capabilities
 in terms of semantic underst and ing and basic reasoning in the context of the scene. For example
 accomplishing the task ‚Äúput strawberry into the correct bowl‚Äù requires a nuanced underst and ing of
 not only what a strawberry and bowl are, but also reasoning in the context the scene to know the
 strawberry should go with the like fruits. For the task ‚Äúpick up the bag about to fall off the table,‚Äù
 RT-2 demonstrates physical underst and ing to disambiguate between two bags and recognize the
 precariously placed object. All the interactions tested in these scenarios have never been seen in the
 robot data, which points to the transfer of semantic knowledge from vision-language data.
 Quantitative Evaluations. Toquantify the seemergentcapabilities,wetake the toptwo base lines
 from the previousevaluations,RT-1 and VC-1,andcomp are the magainst our twomodels: RT-2-Pa LI-X
 and RT-2-Pa LM-E. To reduce the variance of these experiment, we evaluate all of the methods using
 the A/B testing framework (Fisher, 1936), where all four models are evaluated one after another in
 the exact same conditions. 
 
 We‚Äô split the emergent capabilities of RT-2 into three categories covering axes of reasoning and
 semantic underst and ing (with examples of each shown in Appendix Figure 8). The first we term
 symbol underst and ing, which explicitly tests whether the RT-2 policy transfers semantic knowledge
 from vision-language pretraining that was not present in any of the robot data. Example instructions
 in this category are ‚Äúmove apple to 3‚Äù or ‚Äúpush coke can on top of heart‚Äù. The second category we
 termreasoning,whichdemonstratestheabilitytoapplyvariousaspectsofreasoningof the underlying
 VLMtocontroltasks. These task srequirevisualreasoning(‚Äúmove the appletocup with samecolor‚Äù),
 math (‚Äúmove X near the sum of two plus one‚Äù), and multilingual underst and ing (‚Äúmueve la manzana
 al vaso verde‚Äù). We refer to the last category as human recognition tasks, which include tasks such as
 ‚Äúmove the coke can to the person with glasses‚Äù, to demonstrate human-centric underst and ing and
 recognition. The full list of instructions used for this evaluation is specified in Appendix F.2.
 
 Wepresent the resultsof this experimentin Figure 6 awithall the numericalresultsin Appendix H.2.
 We observe that our VLA models signifi can tly outperform the baselines across all categories, with
 our best RT-2-Pa LI-X model achieving more than 3 x average success rate over the next best baseline
 (RT-1). Wealsonote that while the larger Pa LI-X-based model resultsinbettersymbolunderst and ing,
 reasoning and person recognition per for mance on average, the smaller Pa LM-E-based model has
 an edge on tasks that involve math reasoning. We attribute this interesting result to the different
 pre-trainingmixtureusedin Pa LM-E,whichresultsina model that ismorecapableatmathcalculation
 than the mostly visually pre-trained Pa LI-X. 
 
 
 
 9 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 
 
 
 
 
 
 
 
 
 
 (a)Per for mancecomparisononvariousemergentskillevalu-(b)Ablationsof RT-2-Pa LI-Xshowcasing the impactofparam-
 ations(Figure 8)between RT-2 andtwo base lines. etercount and trainingstrategyongeneralization.
 
 Figure 6 | Quantitativeper for manceof RT-2 across(6 a)emergentskills and(6 b)size and trainingablations.
 Appendix Tables 5 and 6 detail the fullnumericalresults. 
 4.3. How does the generalization vary with parameter count and other design decisions?
 
 For this comparison, we use RT-2-Pa LI-X model because of its flexibility in terms of the model size
 (dueto the natureof Pa LM-E,RT-2-Pa LM-Eisrestrictedtoonlycertainsizesof Pa LMand Vi Tmodels).
 In particular, we comp are two different model sizes, 5 B and 55 B, as well as three different training
 routines: training a model from scratch, without using any weights from the VLM pre-training;
 fine-tuning a pre-trained model using robot action data only; and co-fine-tuning (co-training with
 fine-tuning), the primary method used in this work where we use both the original VLM training
 data as well as robotic data for VLM fine-tuning. Since we are mostly interested in the generalization
 aspects of these models, we remove the seen tasks evaluation from this set of experiments.
 
 The results of the ablations are presented in Figure 6 b and Appendix Table 6. First, we observe
 that training a very large model from scratch results in a very poor per for mance even for the 5 B
 model. Given this result, we decide to skip the evaluation of an even bigger 55 B Pa LI-X model when
 trained from scratch. Second, we notice that co-fine-tuning a model (regardless of its size) results in
 a better generalization per for mance than simply fine-tuning it with robotic data. We attribute this to
 the fact that keeping the original data around the fine-tuning part of training, allows the model to
 not for getitspreviousconceptslearnedduring the VLMtraining. Lastly,somewhatunsurprisingly,we
 notice that the increased size of the model results in a better generalization per for mance.
 
 
 4.4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models?
 Inspiredby the chain-of-thoughtpromptingmethodin LLMs(Weietal.,2022),wefine-tuneavariant
 of RT-2 with Pa LM-E for just a few hundred gradient steps to increase its capability of utilizing
 language and actions jointly with the hope that it will elicit a more sophisticated reasoning behavior.
 We augment the data to include an additional ‚ÄúPlan‚Äù step, which describes the purpose of the action
 that the robot is about to take in natural language first, which is then followed by the actual action
 tokens, e.g. ‚ÄúInstruction: I‚Äôm hungry. Plan: pick rxbar chocolate. Action: 1 128 124 136 121 158 111
 255.‚Äù This data augmentation scheme acts as a bridge between VQA datasets (visual reasoning) and
 manipulation datasets (generating actions). 
 
 We qualitatively observe that RT-2 with chain-of-thought reasoning is able to answer more so-
 phisticated commands due to the fact that it is given a place to plan its actions in natural language
 first. This is a promising direction that provides some initial evidence that using LLMs or VLMs as
 planners(Ahnetal.,2022;Driessetal.,2023)can becombined with low-levelpoliciesinasingle VLA
 model. Rollouts of RT-2 with chain-of-thought reasoning are shown in Figure 7 and in Appendix I.
 
 
 10 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 Prompt: Prompt: Prompt: Prompt: 
 Given <img> Instruction: Given <img> Instruction: Given <img> Instruction: Given <img> Instruction:
 Bring me a drink. Move all the objects Pick the object that is Move the green objects
 Prediction: together. different from all other together. 
 Plan: pick 7 up can. Prediction: objects Prediction: 
 Action: 1 143 129 123 145 Plan: move green can near Prediction: Plan: move green can near
 114 115 127 green rice chip bag. Plan: pick rxbar green rice chip bag.
 Action: 1 128 126 127 135 chocolate. Action: 1 128 Action: 1 130 129 121 131
 123 119 127 129 125 131 125 128 127 127 128 127 
 
 
 
 
 
 Prompt: 
 Given <img> I need to 
 hammer a nail, what 
 object from the scene 
 might be useful? 
 Prediction: 
 Rocks. Action: 1 129 138 
 122 132 135 106 127 
 Figure 7 | Rolloutsof RT-2 withchain-of-thoughtreasoning,where RT-2 generatesbothaplan and anaction.
 5. Limitations 
 Even though RT-2 exhibits promising generalization properties, there are multiple limitations of this
 approach. First,althoughweshow that includingweb-scalepretrainingvia VLMsboostsgeneralization
 over semantic and visual concepts, the robot does not acquire any ability to perform new motions
 by virtue of including this additional experience. The model‚Äôs physical skills are still limited to the
 distribution of skills seen in the robot data (see Appendix G), but it learns to deploy those skills in
 new ways. We believe this is a result of the dataset not being varied enough along the axes of skills.
 An exciting direction for future work is to study how new skills could be acquired through new data
 collection paradigms such as videos of humans. 
 Second, although we showed we could run large VLA models in real time, the computation cost
 of these models is high, and as these methods are applied to settings that demand high-frequency
 control,real-timeinferencemaybecomeamajorbottleneck. Anexcitingdirection for futureresearch
 is to explore quantization and distillation techniques that might enable such models to run at higher
 ratesoronlower-costhardw are. Thisisalsoconnectedtoanothercurrentlimitationin that the reare
 onlyasmallnumberofgenerallyavailable VLMmodels that can beusedtocreate RT-2. Wehope that
 more open-sourced models will become available (e.g. https://llava-vl.github.io/) and the
 proprietary ones will open up their fine-tuning APIs, which is a sufficient requirement to build VLA
 models. 
 
 
 6. Conclusions 
 
 In this paper, we described how vision-language-action (VLA) models could be trained by combining
 vision-language model (VLM) pretraining with robotic data. We then presented two instantiations of
 VLAs based on Pa LM-E and Pa LI-X, which we call RT-2-Pa LM-E and RT-2-Pa LI-X. These models are co-
 fine-tuned with robotic trajectory data to output robot actions, which are represented as text tokens.
 We showed that our approach results in very per for mant robotic policies and, more importantly,
 leads to a signifi can tly better generalization per for mance and emergent capabilities inherited from
 
 
 11 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 web-scale vision-language pretraining. We believe that this simple and general approach shows a
 promise of robotics directly benefiting from better vision-language models, which puts the field of
 robot learning in a strategic position to further improve with advancements in other fields.
 
 Acknowledgments 
 
 Wewouldliketoacknowledge Fred Alcober,Jodi Lynn Andres,Carolina Parada,Joseph Dabis,Rochelle
 Dela Cruz, Jessica Gomez, Gavin Gonzalez, John Guilyard, Tomas Jackson, Jie Tan, Scott Lehrer, Dee
 M, Utsav Malla, Sarah Nguyen, Jane Park, Emily Perez, Elio Prado, Jornell Quiambao, Clayton Tan,
 
 Jodexty Therlonge, Eleanor Tomlinson, Wenxuan Zhou, and the greater Google Deep Mind team for
 their feedback and contributions. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 12 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 References 
 M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,K.Gopalakrishnan,K.Hausman,
 
 A.Herzog,etal. Doas Ican,notas Isay: Groundinglanguageinroboticaf for dances. ar Xivpreprint
 ar Xiv:2204.01691, 2022. 
 J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Milli can,
 M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. ar Xiv preprint
 ar Xiv:2204.14198, 2022. 
 
 R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin,A.Passos,S.Shakeri,E.Taropa,P.Bailey,Z.Chen,
 et al. Palm 2 technical report. ar Xiv preprint ar Xiv:2305.10403, 2023. 
 
 A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,
 A. Herzog, J. Hsu, et al. Rt-1: Robotics trans for mer for real-world control at scale. ar Xiv preprint
 ar Xiv:2212.06817, 2022. 
 
 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
 G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information
 processing systems, 33:1877‚Äì1901, 2020. 
 
 D.Cer,Y.Yang,S.Kong,N.Hua,N.Limtiaco,R.S.John,N.Constant,M.Guajardo-Cespedes,S.Yuan,
 C. Tar, Y. Sung, B. Strope, and R. Kurzweil. Universal sentence encoder. Co RR, abs/1803.11175,
 2018. URL http://arxiv.org/abs/1803.11175. 
 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,
 N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. ar Xiv preprint
 ar Xiv:2107.03374, 2021. 
 
 X.Chen,J.Djolonga,P.Padlewski,B.Mustafa,S.Changpinyo,J.Wu,C.R.Ruiz,S.Goodman,X.Wang,
 Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu, M. Joshi,
 B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters,
 G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu,
 K. Rong, A. Kolesnikov, M. Seyedhosseini, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali-x:
 On scaling up a multilingual vision and language model, 2023 a. 
 
 X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner,
 B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue,
 A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner,
 A.Angelova,X.Zhai,N.Houlsby,and R.Soricut. Pali: Ajointly-scaledmultilinguallanguage-image
 model, 2023 b. 
 
 K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
 R. Nakano, et al. Training verifiers to solve math word problems. ar Xiv preprint ar Xiv:2110.14168,
 2021. 
 
 Z.J.Cui,Y.Wang,N.Muhammad,L.Pinto,etal. Fromplaytopolicy: Conditionalbehaviorgeneration
 from uncurated robot data. ar Xiv preprint ar Xiv:2210.10047, 2022. 
 
 S. Dasari and A. Gupta. Trans for mers for one-shot visual imitation. In Conference on Robot Learning,
 pages 2071‚Äì2084. PMLR, 2021. 
 S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn.
 Robonet: Large-scale multi-robot learning. In Conference on Robot Learning, 2019.
 
 
 13 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 M.Dehghani,J.Djolonga,B.Mustafa,P.Padlewski,J.Heek,J.Gilmer,A.Steiner,M.Caron,R.Geirhos,
 I.Alabdulmohsin,R.Jenatton,L.Beyer,M.Tschannen,A.Arnab,X.Wang,C.Riquelme,M.Minderer,
 J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver,
 F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink,
 A.Kolesnikov,F.Pavetiƒá,D.Tran,T.Kipf,M.Luƒçiƒá,X.Zhai,D.Keysers,J.Harmsen,and N.Houlsby.
 Scaling vision trans for mers to 22 billion parameters, 2023. 
 D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,
 T. Yu, et al. Palm-e: An embodied multimodal language model. ar Xiv preprint ar Xiv:2303.03378,
 
 2023. 
 M. Du, S. Nair, D. Sadigh, and C. Finn. Behavior retrieval: Few-shot imitation learning by querying
 unlabeled datasets. ar Xiv preprint ar Xiv:2304.08742, 2023 a. 
 
 Y.Du,K.Konyushkova,M.Denil,A.Raju,J.Landon,F.Hill,N.de Freitas,and S.Cabi. Vision-language
 models as success detectors. ar Xiv preprint ar Xiv:2303.07280, 2023 b. 
 
 C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International
 Conference on Robotics and Automation (ICRA), pages 2786‚Äì2793. IEEE, 2017.
 C.Finn,T.Yu,T.Zhang,P.Abbeel,and S.Levine. One-shotvisualimitationlearningviameta-learning.
 In Conference on robot learning, pages 357‚Äì368. PMLR, 2017. 
 
 R. A. Fisher. Design of experiments. British Medical Journal, 1(3923):554, 1936.
 
 S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Clip on wheels: Zero-shot object
 navigation as object localization and exploration. ar Xiv preprint ar Xiv:2203.10421, 2022.
 Z.Gan,L.Li,C.Li,L.Wang,Z.Liu,J.Gao,etal. Vision-languagepre-training: Basics,recentadvances,
 and future trends. Foundations and Trends¬Æ in Computer Graphics and Vision, 14(3‚Äì4):163‚Äì352,
 2022. 
 
 G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Open-vocabulary image segmentation. ar Xiv preprint
 ar Xiv:2112.12143, 2021. 
 
 K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,
 M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma,
 M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty,
 A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang,
 Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam,
 R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari,
 K. Somasundaram, A. Sou the rland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Z. Zhao,
 Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu,
 C.V.Jawahar,H.Joo,K.Kitani,H.Li,R.Newcombe,A.Oliva,H.S.Park,J.M.Rehg,Y.Sato,J.Shi,
 M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik. Ego 4 d: Around the world in 3,000
 hours of egocentric video, 2022. 
 
 X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language
 knowledge distillation. ar Xiv preprint ar Xiv:2104.13921, 2021. 
 
 N. Hansen, R. Jangir, Y. Sun, G. Aleny√†, P. Abbeel, A. A. Efros, L. Pinto, and X. Wang. Self-supervised
 policy adaptation during deployment. ar Xiv preprint ar Xiv:2007.04309, 2020.
 Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. Language models are
 general-purpose interfaces. ar Xiv preprint ar Xiv:2206.06336, 2022. 
 
 
 14 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep rein for cement
 learning via transfer-learning from text. ar Xiv preprint ar Xiv:2005.09382, 2020.
 S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu,
 et al. Language is not all you need: Aligning perception with language models. ar Xiv preprint
 ar Xiv:2302.14045, 2023. 
 
 W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting
 actionableknowledge for embodiedagents. In International Conferenceon Machine Learning,pages
 9118‚Äì9147. PMLR, 2022. 
 
 S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation
 learning. In Conference on robot learning, pages 783‚Äì795. PMLR, 2018. 
 
 E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-
 shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages
 991‚Äì1002. PMLR, 2021. 
 
 Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,and L.Fan.
 Vima: General robot manipulation with multimodal prompts. ar Xiv preprint ar Xiv:2210.03094,
 2022. 
 L. P. Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915‚Äì916, 2020.
 
 S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang. Language-driven
 representation learning for robotics. ar Xiv preprint ar Xiv:2302.12766, 2023.
 
 A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Roll and, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,
 W.-Y. Lo, et al. Segment anything. ar Xiv preprint ar Xiv:2304.02643, 2023.
 
 I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep
 rein for cement learning from pixels. ar Xiv preprint ar Xiv:2004.13649, 2020.
 M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Rein for cement learning with
 augmented data. Advances in neural information processing systems, 33:19884‚Äì19895, 2020 a.
 
 M.Laskin,A.Srinivas,and P.Abbeel.Curl: Contrastiveunsupervisedrepresentationsforrein for cement
 learning. In International Conference on Machine Learning, pages 5639‚Äì5650. PMLR, 2020 b.
 
 S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,and D.Quillen. Learningh and-eyecoordination for robotic
 grasping with deep learning and large-scale data collection. The International journal of robotics
 research, 37(4-5):421‚Äì436, 2018. 
 
 A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil,
 I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models.
 ar Xiv preprint ar Xiv:2206.14858, 2022. 
 
 J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen
 image encoders and large language models. ar Xiv preprint ar Xiv:2301.12597, 2023.
 L.H.Li,M.Yatskar,D.Yin,C.-J.Hsieh,and K.-W.Chang. Visualbert: Asimple and per for mant base line
 for vision and language. ar Xiv preprint ar Xiv:1908.03557, 2019. 
 
 H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained vision-
 language models. ar Xiv preprint ar Xiv:2210.13431, 2022. 
 
 
 15 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: pretraining task-agnostic visiolinguistic representations
 for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.
 C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. ar Xiv
 preprint ar Xiv:2005.07648, 2020. 
 
 C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,and P.Florence.Interactive
 language: Talking to robots in real time. ar Xiv preprint ar Xiv:2210.06407, 2022.
 
 Y.J.Ma,S.Sodhani,D.Jayaraman,O.Bastani,V.Kumar,and A.Zhang. Vip: Towardsuniversalvisual
 reward and representation via value-implicit pre-training. ar Xiv preprint ar Xiv:2210.00030, 2022.
 Y. J. Ma, W. Liang, V. Som, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. Liv: Language-image
 representations and rewards for robotic control. ar Xiv preprint ar Xiv:2306.00958, 2023.
 
 J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0:
 Deep learning to plan robust grasps with syn the tic point clouds and analytic grasp metrics. ar Xiv
 preprint ar Xiv:1703.09312, 2017. 
 
 A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,J.Malik,
 et al. Where are we in the search for an artificial visual cortex for embodied intelligence? ar Xiv
 preprint ar Xiv:2303.18240, 2023 a. 
 
 A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,J.Malik,
 et al. Where are we in the search for an artificial visual cortex for embodied intelligence? ar Xiv
 preprint ar Xiv:2303.18240, 2023 b. 
 O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation
 learning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205‚Äì11212, 2022.
 
 M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran,
 A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection with vision
 trans for mers. ar Xiv preprint ar Xiv:2205.06230, 2022. 
 
 Y.Mu,Q.Zhang,M.Hu,W.Wang,M.Ding,J.Jin,B.Wang,J.Dai,Y.Qiao,and P.Luo. Embodiedgpt:
 Vision-language pre-training via embodied chain of thought. ar Xiv preprint ar Xiv:2305.15021,
 2023. 
 
 S.Nair,E.Mitchell,K.Chen,S.Savarese,C.Finn,etal. Learninglanguage-conditionedrobotbehavior
 fromoffline data and crowd-sourcedannotation. In Conferenceon Robot Learning,pages 1303‚Äì1315.
 PMLR, 2022 a. 
 S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R 3 m: A universal visual representation for
 robot manipulation. ar Xiv preprint ar Xiv:2203.12601, 2022 b. 
 
 Open AI. Gpt-4 technical report, 2023. 
 
 J.Pari,N.M.Shafiullah,S.P.Arunachalam,and L.Pinto. Thesurprisingeffectivenessofrepresentation
 learning for visual imitation. ar Xiv preprint ar Xiv:2112.01511, 2021. 
 
 L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50 k tries and 700 robot
 hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406‚Äì3413.
 IEEE, 2016. 
 S.Polu,J.M.Han,K.Zheng,M.Baksys,I.Babuschkin,and I.Sutskever. Formalma the maticsstatement
 curriculum learning. ar Xiv preprint ar Xiv:2202.01344, 2022. 
 
 
 16 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-fit: State-covering self-supervised
 rein for cement learning. ar Xiv preprint ar Xiv:1903.03698, 2019. 
 A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
 J. Clark, et al. Learning transferable visual models from natural language supervision. In Interna-
 tional Conference on Machine Learning, pages 8748‚Äì8763. PMLR, 2021. 
 
 S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,Y.Sulsky,
 J. Kay, J. T. Springenberg, et al. A generalist agent. ar Xiv preprint ar Xiv:2205.06175, 2022.
 
 M.Ryoo,A.Piergiovanni,A.Arnab,M.Dehghani,and A.Angelova. Tokenlearner: Adaptivespace-time
 tokenization for videos. Advancesin Neural Information Processing Systems,34:12786‚Äì12797,2021.
 
 D.Shah,B.Osi≈Ñski,b.ichter,and S.Levine. Lm-nav: Roboticnavigation with largepre-trainedmodels
 of language, vision, and action. In K. Liu, D. Kulic, and J. Ichnowski, editors, Proceedings of The 6 th
 Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 492‚Äì
 504.PMLR,14‚Äì18 Dec 2023. URLhttps://proceedings.mlr.press/v 205/shah 23 b.html.
 
 R. Shah and V. Kumar. Rrl: Resnet as representation for rein for cement learning. ar Xiv preprint
 ar Xiv:2107.03380, 2021. 
 M.Shridhar,L.Manuelli,and D.Fox. Cliport: What and wherepathways for roboticmanipulation. In
 Proceedings of the 5 th Conference on Robot Learning (Co RL), 2021. 
 
 M.Shridhar,L.Manuelli,and D.Fox. Cliport: What and wherepathways for roboticmanipulation. In
 Conference on Robot Learning, pages 894‚Äì906. PMLR, 2022 a. 
 
 M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task trans for mer for robotic manipula-
 tion. ar Xiv preprint ar Xiv:2209.05451, 2022 b. 
 
 I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.
 Progprompt: Generating situated robot task plans using large language models. In ICRA, 2023.
 M. H. Smith and L. S. Coles. Design of a low cost, general purpose robot. In IJCAI, pages 324‚Äì336,
 1973. 
 
 A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia,
 C. Finn, et al. Open-world object manipulation using pre-trained vision-language models. ar Xiv
 preprint ar Xiv:2303.00905, 2023. 
 
 T. Sumers, K. Marino, A. Ahuja, R. Fergus, and I. Dasgupta. Distilling internet-scale vision-language
 models into embodied agents. ar Xiv preprint ar Xiv:2301.12507, 2023. 
 
 Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri,
 T. Schuster, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler. Ul 2: Unifying language learning
 paradigms, 2023. 
 
 S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and model
 abilities. Microsoft Auton. Syst. Robot. Res, 2:20, 2023. 
 J.Wang,Z.Yang,X.Hu,L.Li,K.Lin,Z.Gan,Z.Liu,C.Liu,and L.Wang.Git: Agenerativeimage-to-text
 trans for mer for vision and language. ar Xiv preprint ar Xiv:2205.14100, 2022.
 
 J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting
 elicits reasoning in large language models. ar Xiv preprint ar Xiv:2201.11903, 2022.
 
 
 17 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le.
 Symbol tuning improves in-context learning in language models, 2023. 
 J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser.
 Tidybot: Personalizedrobotassistance with largelanguagemodels.ar Xivpreprintar Xiv:2305.05658,
 2023. 
 
 T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson.
 Robotic skill acquisition via instruction augmentation with vision-language models. ar Xiv preprint
 ar Xiv:2211.11736, 2022 a. 
 
 T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control. ar Xiv
 preprint ar Xiv:2203.06173, 2022 b. 
 
 S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. In
 Conference on Robot Learning, pages 1992‚Äì2005. PMLR, 2021. 
 
 K.-T.Yu,M.Bauza,N.Fazeli,and A.Rodriguez. Morethanamillionwaystobepushed.ahigh-fidelity
 experimental dataset of planar pushing. In 2016 IEEE/RSJ international conference on intelligent
 robots and systems (IROS), pages 30‚Äì37. IEEE, 2016. 
 T.Yu,C.Finn,A.Xie,S.Dasari,T.Zhang,P.Abbeel,and S.Levine. One-shotimitation from observing
 humans via domain-adaptive meta-learning. ar Xiv preprint ar Xiv:1802.01557, 2018.
 
 X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision trans for mers. In Proceedings of the
 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104‚Äì12113, 2022.
 
 X.Zhang,Y.Ding,S.Amiri,H.Yang,A.Kaminski,C.Esselink,and S.Zhang. Groundingclassical task
 planners via vision-language models. ar Xiv preprint ar Xiv:2304.08587, 2023.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 18 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 A. Contributions 
 ‚Ä¢ Training and Evaluations (designing and executing procedures for training models, evalu-
 
 ating models in simulation and the real world, running ablations for algorithm design
 choices): Yevgen Chebotar, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey,
 Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han,
 Alexander Herzog, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee, Yao Lu, Henryk Michalewski,
 Igor Mordatch, Karl Pertsch, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Paul
 Wohlhart, Fei Xia, Ted Xiao, and Tianhe Yu. 
 ‚Ä¢ Network Architecture (designing and implementing model network modules, working on
 tokenization of actions, enabling inference of the model networks during experiments):
 Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Danny Driess, Pete Florence, Keerthana
 Gopalakrishnan, Kehang Han, Karol Hausman, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee,
 Henryk Michalewski, Igor Mordatch, Kanishka Rao, Michael Ryoo, Anikait Singh, Quan Vuong,
 Ayzaan Wahid, Jialin Wu, Fei Xia, Ted Xiao, and Tianhe Yu. 
 ‚Ä¢ Data Collection (collecting data on real robots, running real robot evaluations, executing
 operations required for running real robots): Noah Brown, Justice Carbajal, Tianli Ding,
 Krista Reymann, Grecia Salazar, Pierre Sermanet, Jaspiar Singh, Huong Tran, Stefan Welker,
 and Sichun Xu. 
 ‚Ä¢ Leadership (leading the project efforts, managing the project staff, advising on project
 directions): Yevgen Chebotar, Chelsea Finn, Karol Hausman, Brian Ichter, Sergey Levine, Yao
 Lu, Igor Mordatch, Kanishka Rao, Pannag Sanketi, Radu Soricut, Vincent Vanhoucke, and
 Tianhe Yu. 
 ‚Ä¢ Paper (working on the paper manuscript, designing paper visualizations and figures):
 Yevgen Chebotar, Danny Driess, Chelsea Finn, Pete Florence, Karol Hausman, Brian Ichter, Lisa
 Lee,Sergey Levine,Igor Mordatch,Karl Pertsch,Quan Vuong,Fei Xia,Ted Xiao,and Tianhe Yu.
 
 ‚Ä¢ Infrastructure (working on infrastructure and code base backbone needed for training
 models, running experiments, storing and accessing data): Anthony Brohan,Yevgen Chebo-
 tar,Danny Driess,Kehang Han,Jasmine Hsu,Brian Ichter,Alex Irpan,Nikhil Joshi,Ryan Julian,
 Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Yao Lu, Igor
 Mordatch, Quan Vuong, Ayzaan Wahid, Fei Xia, Ted Xiao, Peng Xu, and Tianhe Yu.
 
 
 B. Datasets 
 The vision-language datasets are based on the dataset mixtures from Chen et al. (2023 b) and Driess
 et al. (2023). The bulk of this data consists of the Web LI dataset, which is around 10 B image-text
 
 pairs across 109 languages, filtered to the top 10% scoring cross-modal similarity examples to give
 1 B training examples. Many other captioning and vision question answering datasets are included
 as well, and more info on the dataset mixtures can be found in Chen et al. (2023 b) for RT-2-Pa LI-X,
 and Driessetal.(2023)for RT-2-Pa LM-E.Whenco-fine-tuning RT-2-Pa LI-X,wedonotuse the Episodic
 Web LI dataset described by Chen et al. (2023 a). 
 The robotics dataset is based on the dataset from Brohan et al. (2022). This consists of demon-
 stration episodes collected with a mobile manipulation robot. Each demonstration is annotated with
 anaturallanguageinstruction from oneofsevenskills: "Pick Object","Move Object Near Object",
 "Place Object Upright","Knock Object Over","Open Drawer","Close Drawer","Place Objectinto
 Receptacle", and "Pick Object from Receptacle and place on the counter". Further details can
 be found in Brohan et al. (2022). 
 
 RT-2-Pa LI-X weights the robotics dataset such that it makes up about 50% of the training mixture
 
 
 19 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 for co-fine-tuning. RT-2-Pa LM-E weights the robotics dataset to be about 66% of the training mixture.
 For the resultson Language-Tablein Table 1,our model istrainedon the Language-Table data sets
 from Lynch et al. (2022). Our model is co-fine-tuned on several prediction tasks: (1) predict the
 action, given two consecutive image frames and a text instruction; (2) predict the instruction, given
 image frames; (3) predict the robot arm position, given image frames; (4) predict the number of
 timesteps between given image frames; and (5)predict whether the task wassuccessful, given image
 frames and the instruction. 
 
 
 C. Baselines 
 
 We comp are our method to multiple state-of-the-art baselines that challenge different aspects of our
 method. All of the baselines use the exact same robotic data. 
 
 
 ‚Ä¢ RT-1: Robotics Trans for mer 1 Brohan et al. (2022) is a trans for mer-based model that achieved
 state-of-the-art per for mance on a similar suite of tasks when it was published. The model does
 not use VLM-based pre-training so it provides an important data point demonstrating whether
 VLM-based pre-training matters. 
 ‚Ä¢ VC-1: VC-1 Majumdar et al. (2023 a) is a visual foundation model that uses pre-trained visual
 representations specifically designed for robotics tasks. We use pre-trained representations
 from the VC-1 Vi T-L model. Since VC-1 does not include language conditioning, we add this by
 separatelyembedding the languagecomm and via Universal Sentence Encoder Ceretal.(2018)
 to enable comparison to our method. In particular, we concatenate the resulting language
 embedding tokens to the image tokens produced by VC-1, and pass the concatenated token
 sequences through token learner Ryoo et al. (2021). The token sequences produced by token
 learner are then consumed by an RT-1 decoder-only trans for mer model to predict robot action
 tokens. We train the VC-1 baseline end-to-end and unfreeze the VC-1 weights during training,
 since this led to far better results than using frozen VC-1 weights. 
 ‚Ä¢ R 3 M: R 3 M Nair et al. (2022 b) is a similar method to VC-1 in that R 3 M uses pre-trained
 visual-language representations to improve policy training. In this case the authors use Ego 4 D
 dataset Graumanetal.(2022)ofhumanactivitiestolearn the representation that isusedby the
 policy. Both VC-1 and R 3 M test different state-of-the-art representation learning methods as an
 alternative to using a VLM. To obtain a language-conditioned policy from the R 3 M pretrained
 representation, we follow the same procedure as described above for VC-1, except we use the
 R 3 M Res Net 50 model to obtain the image tokens, and unfreeze it during training.
 ‚Ä¢ MOO: MOO Stone et al. (2023) is an object-centric approach, where a VLM is first used to
 
 specify the objectofinterestina for mofasingle,coloredpixelin the originalimage. Thispixel-
 modified image is then trained with an end-to-end policy to accomplish a set of manipulation
 tasks. This baseline corresponds to a situation where a VLM is used as a separate module that
 enhances perception but its representations are not used for policy learning.
 
 
 D. VLMs for RT-2 
 The Pa LI-X model architecture consists of a Vi T-22 B Dehghani et al. (2023) to process images, which
 canacceptsequencesofùëõimages,leadingtoùëõ√óùëòtokensperimage,whereùëòisthenumberofpatches
 perimage. Theimagetokenspassingoveraprojectionlayeris the nconsumedbyanencoder-decoder
 
 backbone of 32 B parameters and 50 layers, similar to UL 2 Tay et al. (2023), which jointly processes
 text and images as embeddings to generate output tokens in an auto-regressive manner. The text
 
 
 20 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 inputusuallyconsistsof the typeof task and anyadditionalcontext(e.g.,"Generatecaptionin ‚ü®lang‚ü©"
 for captioning tasks or "Answer in ‚ü®lang‚ü©: question" for VQA tasks). 
 The Pa LI-3 B model trained on Language-Table (Table 1) uses a smaller Vi T-G/14 (Zhai et al.,
 2022) (2 B parameters) to process images, and UL 2-3 B (Tay et al., 2023) for the encoder-decoder
 network. 
 
 The Pa LM-E model is based on a decoder-only LLM that projects robot data such as images and
 text into the language token space and outputs text such as high-level plans. In the case of the
 used Pa LM-E-12 B, the visual model used to project images to the language embedding space is
 a Vi T-4 B Chen et al. (2023 b). The concatenation of continuous variables to textual input allows
 Pa LM-E to be fully multimodal, accepting a wide variety of inputs such as multiple sensor modalities,
 object-centric representations, scene representations and object entity referrals.
 
 
 E. Training Details 
 
 We perform co-fine-tuning on pre-trained models from the Pa LI-X (Chen et al., 2023 a) 5 B & 55 B
 model, Pa LI (Chen et al., 2023 b) 3 B model and the Pa LM-E (Driess et al., 2023) 12 B model. For
 RT-2-Pa LI-X-55 B, we use learning rate 1 e-3 and batch size 2048 and co-fine-tune the model for
 80 K gradient steps whereas for RT-2-Pa LI-X-5 B, we use the same learning rate and batch size and
 co-fine-tune the model for 270 K gradient steps. For RT-2-Pa LM-E-12 B, we use learning rate 4 e-4 and
 batch size 512 to co-fine-tune the model for 1 M gradient steps. Both models are trained with the
 next token prediction objective, which corresponds to the behavior cloning loss in robot learning. For
 RT-2-Pa LI-3 B model used for Language-Table results in Table 1, we use learning rate 1 e-3 and batch
 size 128 to co-fine-tune the model for 300 K gradient steps. 
 
 
 F. Evaluation Details 
 
 F.1. Evaluation Scenarios 
 
 Forstudying the emergentcapabilitiesof RT-2 inaquantitativemanner, westudyvariouschallenging
 semanticevaluationscenarios that aimtomeasurecapabilitiessuchasreasoning,symbolunderst and-
 ing, and human recognition. A visual overview of a subset of these scenes is provided in Figure 8,
 and the full list of instructions used for quantiative evalution is shown in Table 3.
 
 
 F.2. Evaluation Instructions 
 Table 2 listsnaturallanguageinstructionsusedin model evaluations for unseenobjects,backgrounds,
 and environments. Each instruction was run between 1-5 times, depending on the number of total
 instructions in that evaluation set. Table 3 lists natural language instructions used to evaluate
 quantitative emergent evals. Each instruction was run 5 times. 
 
 
 
 
 
 
 
 
 
 
 
 21 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 (a) Reasoning 
 
 
 
 
 
 ‚Äúmove coke can to 
 ‚Äúmove apple to cup with ‚Äúmove banna near the ‚Äúd√©placer les frites verts ‚Äúpick a healthy drink‚Äù Taylor Swift‚Äù
 same color‚Äù sum of two plus one‚Äù dans la tasse rouge‚Äù 
 
 
 
 
 ‚Äúmove coke can to 
 person with glasses‚Äù
 ‚Äúmove coke can ‚Äúput coke can close ‚Äúmove banana to ‚Äúmove apple to tree‚Äù
 near Y‚Äù to dog‚Äù android‚Äù (c) Human 
 (b) Symbol Underst and ing Recognition 
 Figure 8 | Anoverviewofsomeoftheevaluationscenariosusedtostudy the emergentcapabilitiesof RT-2.
 They focus on three broad categories, which are (a) reasoning, (b) symbol underst and ing, and (c) human
 recognition. Thevisualizedinstructions are asubsetof the fullinstructions,which are listedin Appendix F.2.
 
 Task Group Tasks 
 Symbol Underst and- movecoke can near X,movecoke can near 3,movecoke can near Y
 ing: Symbol 1 
 Symbol Underst and- moveappletotree,moveappletoduck,moveappletoapple,moveapple
 ing: Symbol 2 tomatchingcard 
 Symbol Underst and- putcoke can closetodog,pushcoke can ontopofheart,placecoke can
 ing: Symbol 3 abovestar 
 
 Reasoning: Math move banana to 2, move banna near the sum of two plus one, move
 banana near the answer of three times two, move banana near the
 smallestnumber 
 Reasoning: Logos movecuptogoogle,movecupto and roid,movecuptoyoutube,move
 cuptoasearchengine,movecuptoaphone 
 Reasoning: Nutrition getmeahealthysnack,pickahealthydrink,pickupasweetdrink,move
 thehealthysnackto the healthydrink,pickupasaltysnack 
 Reasoning: Color and move apple to cup with same color, move apple to cup with different
 Multilingual color,movegreenchipstomatchingcolorcup,moveappletovasoverde,
 Bewegen Sieden Apfelindierote Tasse,movegreenchipstovasorojo,
 muevelamanzanaalvasoverde,d√©placerlesfritesvertsdanslatasse
 rouge 
 Person Recognition: movecoke can totaylorswift,movecoke can totomcruise,movecoke
 Celebrities cantosnoopdog 
 
 Person Recognition: movecoke can toperson with glasses,movecoke can totheman with
 Celeb A whitehair,movecoke can tothebrunettelady 
 
 Table 3 | Naturallanguageinstructionsused for quantitativeemergentevalutions.
 
 
 
 
 22 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 G. Example Failure Cases 
 In Fig.9 weprovideexamplesofanotabletypeoffailurecasein the Language Tablesetting,with the
 
 RT-2 model not generalizing to unseen object dynamics. In these cases, although the model is able
 to correctly attend to the language instruction and move to the first correct object, it is not able to
 control the challenging dynamics of these objects, which are signifi can tly different than the small set
 ofblockobjects that have been seenin this environment Lynchetal.(2022). Thenpensimplyrollsoff
 the table (Fig. 9, left), while the banana‚Äôs center-of-mass is far from where the robot makes contact
 (Fig. 9, right). We note that pushing dynamics are notoriously difficult to predict and control Yu et al.
 (2016). We hypo the size that greater generalization in robot-environment interaction dynamics may
 be possible by further scaling the datasets across diverse environments and objects ‚Äì for example, in
 this case, datasets that include similar types of more diverse pushing dynamics Dasari et al. (2019).
 In addition, despite RT-2‚Äôs promising per for mance on real world manipulation tasks in qualitative
 and quantitative emergent evaluations, we still find numerous notable failure cases. For example,
 
 with the current training dataset composition and training method, RT-2 seemed to perform poorly
 at: 
 
 ‚Ä¢ Grasping objects by specific parts, such as the handle 
 ‚Ä¢ Novel motions beyond what was seen in the robot data, such as wiping with a towel or tool use
 ‚Ä¢ Dexterous or precise motions, such as folding a towel 
 ‚Ä¢ Extended reasoning requiring multiple layers of indirection 
 
 
 Push the red marker to the video game controller Push the banana to the apple
 
 
 
 
 
 Figure 9 | Qualitativeexamplefailurecasesin the real-worldfailingtogeneralizetounseenobjectdynamics.
 
 
 H. Quantitative Experimental Results 
 
 
 H.1. Overall Per for mance, for Section 4.1 
 Table 4 lists our quantitative overall evaluation results. We find that RT-2 performs as well or better
 than baselines on seen tasks and signifi can tly outperforms baselines on generalization to unseen
 objects, backgrounds, and environments. 
 
 Model Seen Tasks Unseen Objects Unseen Backgrounds Unseen Environments Unseen Average
 Easy Hard Easy Hard Easy Hard 
 
 R 3 M(Nairetal.,2022 b) 45 32 14 13 9 0 2 12 
 VC-1(Majumd are tal.,2023 a) 63 34 10 13 3 0 0 10 
 RT-1(Brohanetal.,2022) 92 31 43 71 9 26 14 32 
 MOO(Stoneetal.,2023) 75 58 48 38 41 19 3 35 
 RT-2-Pa LI-X-55 B(ours) 91 70 62 96 48 63 35 62 
 RT-2-Pa LM-E-12 B 1(ours) 93 84 76 75 71 36 33 62 
 Table 4 | Overallper for manceoftwoinstantiationsof RT-2 and base linesacrossseentraining task saswellas
 unseenevaluationsmeasuringgeneralizationtonovelobjects,novelbackgrounds,andnovelenvironments.
 
 23 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 H.2. Emergent Evaluation, for Section 4.2 
 Table 5 lists all of our quantitative emergent evaluation results. We find that RT-2 performs 2 x
 to 3 x better than RT-1 on these new instructions, without any additional robotic demonstrations.
 This showcases how our method allows us to leverage capabilities from pretraining on web-scale
 vision-language datasets. 
 
 
 Model Symbol Underst and ing Reasoning Person Recognition Average
 Symbol 1 Symbol 2 Symbol 3 Average Math Logos Nutrition Color/Multilingual Average Celebrities Celeb A Average
 VC-1(Majumd are tal.,2023 a) 7 25 0 11 0 8 20 13 10 20 7 13 11 
 RT-1(Brohanetal.,2022) 27 20 0 16 5 0 32 28 16 20 20 20 17 
 RT-2-Pa LI-X-55 B(ours) 93 60 93 82 25 52 48 58 46 53 53 53 60 
 RT-2-Pa LM-E-12 B(ours) 67 20 20 36 35 56 44 35 43 33 53 43 40 
 Table 5 | Per for manceof RT-2 and base linesonquantitativeemergentevaluations. 
 
 H.3. Size and Training Ablations, for Section 4.3 
 Table 6 detailsquantitativeresults for ablationsacross model size and trainingapproach. Acrosseach,
 we see that model size plays an important role in per for mance and that co-fine-tuning outperforms
 fine-tuning, which outperforms training from scratch. 
 
 Model Size Training Unseen Objects Unseen Backgrounds Unseen Environments Average
 
 Easy Hard Easy Hard Easy Hard 
 RT-2-Pa LI-X 5 B fromscratch 0 10 46 0 0 0 9 
 RT-2-Pa LI-X 5 B fine-tuning 24 38 79 50 36 23 42 
 RT-2-Pa LI-X 5 B co-fine-tuning 60 38 67 29 44 24 44 
 RT-2-Pa LI-X 55 B fine-tuning 60 62 75 38 57 19 52 
 RT-2-Pa LI-X 55 B co-fine-tuning 70 62 96 48 63 35 63 
 Table 6 | Ablationsof RT-2 showcasing the impactofparametercount and trainingstrategyongeneralization.
 
 
 I. Additional Chain-Of-Thought Reasoning Results 
 
 Wepresentadditionalexamplesofchain-of-thoughtreasoningrolloutsaccomplished with RT-2-Pa LM-
 E, as described in Sec. 4.4, in Figure 10. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 1 Theoriginalpre-training data mixtureusedin Pa LM-E-12 B(asdescribedin Driessetal.(2023))includesrobotimages
 forhigh-level VQAplanningtasks that can besimilartoimagesencounteredingeneralizationscenarios.However,noneof
 thosetrainingexamplesincludelow-levelactions that are evaluatedin this experiment.
 
 24 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 User 
 Pick up the 
 object that is 
 different from all 
 other objects 
 Plan: Pick rxbar 
 chocolate 
 User 
 Move the green 
 objects together 
 Plan: Move green 
 can near green 
 rice chip bag 
 User 
 Move the blue 
 objects together 
 Plan: Move blue 
 chip bag near 
 pepsi 
 User 
 I am sleepy, bring 
 me a drink to 
 help. 
 Plan: Pick redbull 
 can 
 Figure 10 | Additionalexamplesof RT-2 withchain-of-thoughtreasoning 
 
 
 
 
 
 
 
 
 
 
 
 25 
 
 
 

 
 
 
 
 RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control
 
 
 Task Group Tasks 
 Unseen Objects pickbanana,movebanananearcoke can,moveorange can nearbanana,
 (Easy) pickoreo,moveoreonearapple,moveredbull can nearoreo,pickpear,
 pickcoconutwater,movepearnearcoconutwater,movepepsi can near
 pear 
 Unseen Objects pickcoldbrew can,picklargeorangeplate,pickchewtoy,picklargeten-
 (Hard) nisball,pickbirdornament,pickfishtoy,pickgingerlemonkombucha,
 pick egg separator, pick wrist watch, pick green sprite can, pick blue
 microfibercloth,pickyellowpear,pickpretzelchipbag,pickdisinfectant
 wipes,pickpineapplehintwater,pickgreencup,pickpicklesnack,pick
 small blue plate, pick small orange rolling pin, pick octopus toy, pick
 catniptoy 
 Unseen Back- pickgreenjalapenochipbag,pickorange can,pickpepsi can,pick 7 up
 grounds(Easy) can, pick apple, pick blue chip bag, pick orange, pick 7 up can, move
 orangenearsink,pickcoke can,picksponge,pickrxbarblueberry
 Unseen Back- pick wrist watch, pick egg separator, pick green sprite can, pick blue
 grounds(Hard) microfibercloth,pickyellowpear,pickpretzelchipbag,pickdisinfectant
 wipes,pickpineapplehintwater,pickgreencup,pickpicklesnack,pick
 small blue plate, pick small orange rolling pin, pick octopus toy, pick
 catniptoy,pickswedishfishbag,picklargegreenrollingpin,pickblack
 sunglasses 
 Unseen Environ- pickcoke can,pickapple,pickrxbarblueberry,moveapplenearcoke can,
 ments(Easy) moverxbarblueberrynearapple,movecoke can nearrxbarblueberry,
 pickblueplasticbottle,picksponge,pickbluechipbag,movesponge
 near blue plastic bottle, move blue chip bag near sponge, move blue
 plasticbottlenearbluechipbag,movecoke can nearwhitemug,move
 spongenearwhitemug,movecoke can nearyellowbowl,movesponge 
 nearyellowbowl,movecoke can neargreencloth,movespongenear 
 greencloth,movecoke can nearplate,movespongenearplate,move
 coke can near spoon, move sponge near spoon, move coke can near
 orangecup,movespongenearorangecup,pickwhitemug,pickyellow
 bowl,pickgreencloth,movewhitemugnearsponge,moveyellowbowl
 nearsponge,movegreenclothnearsponge,pickplate,pickspoon,pick
 orange cup, move plate near sponge, move spoon near sponge, move
 orangecupnearsponge,putcoke can intosink,dropcoke can intosink,
 push coke can into sink, put sponge into sink, drop sponge into sink,
 pushspongeintosink,putgreenclothintosink,dropgreenclothinto
 sink,pushgreenclothintosink 
 Unseen Environ- pickcoke can,pickapple,pickrxbarblueberry,moveapplenearcoke can,
 ments(Hard) moverxbarblueberrynearapple,movecoke can nearrxbarblueberry,
 movecoke can nearstapler,moveapplenearstapler,movecoke can near
 keyboard, move apple near keyboard, move coke can near tissue box,
 moveappleneartissuebox,movecoke can nearpapers,moveapplenear
 papers,movecoke can nearmouse,moveapplenearmouse,movecoke 
 can near book, move apple near book, pick marker, pick stapler, pick
 mouse,movemarkernearapple,movestaplernearapple,movemouse
 nearapple,pushcoke can totheleft,pushcoke can totheright,push
 spongeto the left,pushspongeto the right,pushtissueboxto the left,
 pushtissueboxto the right,pointatcoke can,pointatsponge,pointat
 tissuebox 
 Table 2 | Natural language instructions used for evaluations testing controlled distribution shifts along the
 dimension of novel objects, novel environments, and novel backgrounds. For each category, we introduce
 evaluationsettings with smallerdistributionshiftsaswellaslargerdistributionshifts. Avisualizationofthese
 scenariosifshownin Figure 3. 
 26 
 
 