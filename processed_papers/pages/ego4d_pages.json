[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n \n \n Ego 4 D: Around the World in 3,000 Hours of Egocentric Video \n \n \n Kristen Grauman 1,2,Andrew Westbury 1,Eugene Byrne∗1,Zachary Chavis∗3,Antonino Furnari∗4,\n Rohit Girdhar∗1,Jackson Hamburger∗1,Hao Jiang∗5,Miao Liu∗6,Xingyu Liu∗7,Miguel Martin∗1,\n Tushar Nagarajan∗1,2,Ilija Radosavovic∗8,Santhosh Kumar Ramakrishnan∗1,2,Fiona Ryan∗6,\n Jayant Sharma∗3,Michael Wray∗9,Mengmeng Xu∗10,Eric Zhongcong Xu∗11,Chen Zhao∗10,\n Siddhant Bansal 17,Dhruv Batra 1,Vincent Cartillier 1,6,Sean Crane 7,Tien Do 3,Morrie Doulaty 13,\n Akshay Erapalli 13,Christoph Feichtenhofer 1,Adriano Fragomeni 9,Qichen Fu 7,\n \n Abrham Gebreselasie 12,Cristina Gonza´lez 14,James Hillis 5,Xuhua Huang 7,Yifei Huang 15,\n Wenqi Jia 6,Weslie Khoo 16,Ja´chym Kola´ˇr 13,Satwik Kottur 13,Anurag Kumar 5,Federico Landini 13,\n Chao Li 5,Yanghao Li 1,Zhenqiang Li 15,Karttikeya Mangalam 1,8,Raghava Modhugu 17,\n Jonathan Munro 9,Tullie Murrell 1,Takumi Nishiyasu 15,Will Price 9,Paola Ruiz Puentes 14,\n Merey Ramazanova 10,Leda Sari 5,Kiran Somasundaram 5,Audrey Southerland 6,Yusuke Sugano 15,\n Ruijie Tao 11,Minh Vo 5,Yuchen Wang 16,Xindi Wu 7,Takuma Yagi 15,Ziwei Zhao 16,Yunyi Zhu 11,\n Pablo Arbela´ez†14,David Crandall†16,Dima Damen†9,Giovanni Maria Farinella†4,\n Christian Fuegen†13,Bernard Ghanem†10,Vamsi Krishna Ithapu†5,C.V.Jawahar†17,Hanbyul Joo†1,\n Kris Kitani†7,Haizhou Li†11,Richard Newcombe†5,Aude Oliva†18,Hyun Soo Park†3,\n James M.Rehg†6,Yoichi Sato†15,Jianbo Shi†19,Mike Zheng Shou†11,Antonio Torralba†18,\n Lorenzo Torresani†1,20,Mingfei Yan†5,Jitendra Malik 1,8 \n \n \n 1 Facebook AIResearch(FAIR),2 Universityof Texasat Austin,3 Universityof Minnesota,4 Universityof Catania,\n 5 Facebook Reality Labs,6 Georgia Tech,7 Carnegie Mellon University,8 UCBerkeley,9 Universityof Bristol,\n 10 King Abdullah Universityof Scienceand Technology,11 National Universityof Singapore,\n 12 Carnegie Mellon University Africa,13 Facebook,14 Universidaddelos Andes,15 Universityof Tokyo,16 Indiana University,\n 17 International Instituteof Information Technology,Hyderabad,18 MIT,19 Universityof Pennsylvania,20 Dartmouth\n \n \n Abstract episodicmemory),present(analyzinghand-objectmanipu-\n lation,audio-visualconversation,andsocialinteractions),\n andfuture(forecastingactivities). Bypubliclysharingthis\n Weintroduce Ego 4 D,amassive-scaleegocentricvideo \n massiveannotateddatasetandbenchmarksuite,weaimto\n datasetandbenchmarksuite. Itoffers 3,670 hoursofdaily- \n pushthefrontieroffirst-personperception. Projectpage:\n lifeactivityvideospanninghundredsofscenarios(house- \n https://ego 4 d-data.org/ \n hold, outdoor, workplace, leisure, etc.) captured by 931 \n uniquecamerawearersfrom 74 worldwidelocationsand 9 \n differentcountries. Theapproachtocollectionisdesigned \n toupholdrigorousprivacyandethicsstandards,withcon- 1.Introduction \n sentingparticipantsandrobustde-identificationprocedures \n whererelevant. Ego 4 Ddramaticallyexpandsthevolumeof Today’scomputervisionsystemsexcelatnamingobjects\n diverse egocentric video footage publicly available to the andactivitiesin Internetphotosorvideoclips. Theirtremen-\n researchcommunity. Portionsofthevideoareaccompanied dousprogressoverthelastdecadehasbeenfueledbymajor\n byaudio, 3 Dmeshesoftheenvironment, eyegaze, stereo, dataset and benchmark efforts, which provide the annota-\n and/orsynchronizedvideosfrommultipleegocentriccam- tionsneededtotrainandevaluatealgorithmsonwell-defined\n erasatthesameevent. Furthermore,wepresentahostof tasks[49,60,61,92,108,143]. \n newbenchmarkchallengescenteredaroundunderstanding Whilethisprogressisexciting,currentdatasetsandmod-\n thefirst-personvisualexperienceinthepast(queryingan elsrepresentonlyalimiteddefinitionofvisualperception.\n 1 \n 2202 \n ra M \n 11 \n ]VC.sc[ \n 3 v 85070.0112:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n 1 2 \n \n 3 4 \n Doing laundry \n Baking \n Geographic diversity Multi-perspective \n \n IMU \n L R \n Shopping \n Sports \n Reading \n Human locomotion \n Stereo vision \n Gardening \n Sewing / Knitting \n 3 D \n Pets \n \n \n Playing games \n Social interaction Video + 3 D scans \n Figure 1.Ego 4 Disamassive-scaleegocentricvideodatasetofdailylifeactivityspanning 74 locationsworldwide.Hereweseeasnapshotof\n thedataset(5%oftheclips,randomlysampled)highlightingitsdiversityingeographiclocation,activities,andmodalities.Thedataincludes\n socialvideoswhereparticipantsconsentedtoremainunblurred.Seehttps://ego 4 d-data.org/fig 1.htmlforinteractivefigure.\n First,today’sinfluential Internetdatasetscapturebrief,iso- theygoaboutdailyactivitiesinthehome,workplace,leisure,\n latedmomentsintimefromathird-person“spectactor”view. social settings, and commuting. Based on self-identified\n However,inbothroboticsandaugmentedreality,theinput characteristics, the camera wearers are of varying back-\n isalong,fluidvideostreamfromthefirst-personor“ego- grounds,occupations,gender,andages—notsolelygraduate\n centric” point of view—where we see the world through students! Thevideo’srichgeographicdiversitysupportsthe\n theeyesofanagentactivelyengagedwithitsenvironment. inclusionofobjects,activities,andpeoplefrequentlyabsent\n Second,whereas Internetphotosareintentionallycaptured fromexistingdatasets. Sinceeachparticipantworeacamera\n byahumanphotographer,imagesfromanalways-onwear- for 1 to 10 hoursatattime,thedatasetofferslong-formvideo\n able egocentric camera lack this active curation. Finally, contentthatdisplaysthefullarcofaperson’scomplexinter-\n first-personperceptionrequiresapersistent 3 Dunderstand- actionswiththeenvironment,objects,andotherpeople. In\n ingofthecamerawearer’sphysicalsurroundings,andmust additionto RGBvideo,portionsofthedatasetalsoprovide\n interpretobjectsandactionsinahumancontext—attentive audio,3 Dmeshes,gaze,stereo,and/orsynchronizedmulti-\n tohuman-objectinteractionsandhigh-levelsocialbehaviors. camera views that allow seeing one event from multiple\n perspectives. Ourdatasetdrawsinspirationfrompriorego-\n Motivated by these critical contrasts, we present the \n centricvideodataefforts[43,44,129,138,179,201,205,210],\n Ego 4 D dataset and benchmark suite. Ego 4 D aims to cat- \n butmakessignificantadvancesintermsofscale,diversity,\n alyzethenexteraofresearchinfirst-personvisualpercep- \n andrealism. \n tion. Ego is for egocentric, and 4 D is for 3 D spatial plus \n temporalinformation. \n Equallyimportanttohavingtherightdataistohavethe\n Ourfirstcontributionisthedataset: amassiveego-video rightresearchproblems. Oursecondcontributionisasuite\n collectionofunprecedentedscaleanddiversitythatcaptures offivebenchmarktasksspanningtheessentialcomponents\n dailylifeactivityaroundtheworld. See Figure 1. Itconsists ofegocentricperception—indexingpastexperiences,ana-\n of 3,670 hoursofvideocollectedby 931 uniqueparticipants lyzingpresentinteractions,andanticipatingfutureactivity.\n from 74 worldwidelocationsin 9 differentcountries. The Toenableresearchonthesefronts,weprovidemillionsof\n vastmajorityofthefootageisunscriptedand“inthewild”, rich annotations that resulted from over 250,000 hours of\n representingthenaturalinteractionsofthecamerawearersas annotatoreffortandrangefromtemporal,spatial,andseman-\n 2 "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n tic labels, to dense textual narrations of activities, natural \n languagequeries,andspeechtranscriptions. \n Ego 4 Distheculminationofanintensivetwo-yeareffort \n by Facebookand 13 universitiesaroundtheworldwhocame \n togetherforthecommongoalofspurringnewresearchin \n egocentricperception. Wearekickstartingthatworkwitha \n formalbenchmarkchallengetobeheldin June 2022. Inthe \n comingyears,webelieveourcontributioncancatalyzenew \n researchnotonlyinvision,butalsorobotics,augmentedreal- \n ity,3 Dsensing,multimodallearning,speech,andlanguage. \n These directions will stem not only from the benchmark \n taskswepropose,butalsoalternativeonesthatthecommu- Figure 2.Ego 4 Dcamerawearerdemographics—age,gender,coun-\n nitywilldevelopleveragingourmassive,publiclyavailable triesofresidence,andoccupations(self-reported).Fontsizereflects\n relativefrequencyoftheoccupation. \n dataset. \n 2.Related Work \n graduatestudentsascamerawearers[43,44,66,129,129,138,\n Large-scalethird-persondatasets Inthelastdecade,an- 168,179,194,210],Ego 4 Dcamerawearersareofamuch\n notateddatasetshavebothpresentednewproblemsincom- wider demographic, as detailed below. Aside from daily\n puter vision and ensured their solid evaluation. Existing lifeactivity,prioregodatasetsfocusonconversation[170],\n collectionslike Kinetics[108],AVA[92],UCF[207],Ac- inter-personinteractions[66,168,194,231],placelocaliza-\n tivity Net [61], How To 100 M [157], Image Net [49], and tion[183,208],multimodalsensordata[124,166,204],hu-\n COCO[143]focusonthird-person Webdata,whichhave man hands [16,134] human-object interaction [106,184],\n thebenefitandbiasofahumanphotographer. Incontrast, andobjecttracking[56]. \n Ego 4 Disfirst-person. Passivelycapturedwearablecamera Ego 4 Disanorderofmagnitudelargerthantoday’slargest\n video entails unusual viewpoints, motion blur, and lacks egocentricdatasetsbothintermsofhoursofvideo(3,670\n temporal curation. Notably, pre-training egocentric video hoursvs.100 in[43])anduniquecamerawearers(931 peo-\n modelswiththird-persondata[70,221,224,239]suffersfrom ple vs. 71 in [201]); it spans hundreds of environments\n thesizeabledomainmismatch[139,201]. (ratherthanoneordozens,asinexistingcollections);and\n itsvideocomesfrom 74 worldwidelocationsand 9 coun-\n Egocentricvideounderstanding Egocentricvideooffers \n tries(vs.justoneorafewcities). The Ego 4 Dannotations\n a host of interesting challenges, such as human-object in- \n are also of unprecedented scale and depth, with millions\n teractions[26,46,163],activityrecognition[110,139,243], \n ofannotationssupportingmultiplecomplextasks. Assuch,\n anticipation[4,75,86,144,205],videosummarization[48, \n Ego 4 Drepresentsastepchangeindatasetscaleanddiversity.\n 129,131,147,148,232],detectinghands[16,134],parsing \n We believe both factors are paramount to pursue the next\n socialinteractions[66,168,231],andinferringthecamera \n generationofperceptionforembodied AI. \n wearer’s body pose [107]. Our dataset can facilitate new \n workinalltheseareasandmore,andourproposedbench- \n 3.Ego 4 DDataset \n marks(andannotationsthereof)widenthetasksresearchers \n canconsidermovingforward. Wedeferdiscussionofhow Nextweoverviewthedataset,whichwearemakingpub-\n priorworkrelatestoourbenchmarktasksto Sec.5. liclyavailableunderan Ego 4 Dlicense.\n Egocentric video datasets Multiple egocentric datasets \n 3.1.Collectionstrategyandcamerawearers \n havebeendevelopedoverthelastdecade. Mostrelevantto \n ourworkarethosecontainingunscripteddailylifeactivity, Notonlydowewishtoamassanego-videocollectionthat\n whichincludes EPIC-Kitchens[43,44],UTEgo[129,210], issubstantialinscale,butwealsowanttoensureitsdiversity\n Activities of Daily Living (ADL) [179], and the Disney ofpeople,places,objects,andactivities. Furthermore,for\n dataset[66]. Thepracticeofgivingcamerastoparticipants realism,weareinterestedinunscriptedfootagecapturedby\n totakeoutofthelab,firstexploredin[66,129,179],inspires peoplewearingacameraforlongperiodsoftime.\n our approach. Others are (semi-)scripted, where camera To this end, we devised a distributed approach to data\n wearers are instructed to perform a certain activity, as in collection. The Ego 4 D project consists of 14 teams from\n Charades-Ego[201]and EGTEA[138]. Whereastoday’s universities and labs in 9 countries and 5 continents (see\n largestegodatasetsfocussolelyonkitchens[44,44,124,138], mapin Figure 1). Eachteamrecruitedparticipantstoweara\n Ego 4 Dspanshundredsofenvironmentsbothindoorsandout- camerafor 1 to 10 hoursatatime,foratotalof 931 unique\n doors. Furthermore,whileexistingdatasetsrelylargelyon camerawearersand 3,670 hoursofvideointhisfirstdataset\n 3 "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n \n \n \n Carpenter > 7 hrsof videos Crafting> 12 hrsof videos Bike Mechanic> 5.5 hrsof videos\n \n \n \n \n \n Figure 3. Scenarios in Ego 4 D. Outer circle shows the 14 most \n commonscenarios(70%ofthedata).Wordleshowsscenariosin \n theremaining 30%.Innercircleiscolorcodedbythecontributing \n partner(seemapcolorlegendin Fig 1). Figure 4. Somevideos(bottom)havecoupled 3 Dmeshes(top)\n from Matterport 3 Dscanners,allowingonetorelatethedynamic\n videotothestatic 3 Denvironment(middle). \n release(Ego 4 D-3 K).Participantsin 74 totalcitieswerere- \n cruitedbywordofmouth,ads,andpostingsoncommunity \n bulletinboards.Someteamsrecruitedparticipantswithoccu- understanding [108]. In this way, we capture unscripted\n pationsthathaveinterestingvisualcontexts,suchasbakers, activitywhilebeingmindfulofthescenarios’coverage.\n carpenters,landscapers,ormechanics. The exception is for certain multi-person scenarios,\n Both the geographic spread of our team as well as our where,inordertoensuresufficientdatafortheaudio-visual\n approachtorecruitingparticipantswerecriticaltoarriveat andsocialbenchmarks, weaskedparticipantsatfivesites\n adiversedemographiccomposition,asshownin Figure 2.1 whohadconsentedtosharetheirconversationaudioandun-\n Participantscoverawidevarietyofoccupations,spanmany blurredfacestotakepartinsocialactivities,suchasplaying\n agebrackets,with 96 ofthemover 50 yearsold,and 45% games. We leverage this portion of Ego 4 D for the audio-\n arefemale. Twoparticipantsidentifiedasnon-binary,and visualandsocialinteractionbenchmarks(Sec.5.3 and 5.4).\n twopreferrednottosayagender. Figure 3 showsthewidedistributionofscenarioscaptured\n inourdataset. Notethatwithineachgivenscenariothereare\n 3.2.Scenarioscomposingthedataset \n typicallydozensofactionstakingplace,e.g.,thecarpentry\n scenario includes hammering, drilling, moving wood, etc.\n What activities belong in an egocentric video dataset? \n Overall,the 931 camerawearersbestowourdatasetwitha\n Ourresearchismotivatedbyproblemsinroboticsandaug- \n glimpseofdailylifeactivityaroundtheworld.\n mentedreality, wherevisionsystemswillencounterdaily \n lifescenarios. Hence,weconsultedasurveyfromthe U.S. 3.3.Camerasandmodalities \n Bureauof Labor Statistics 2 thatcaptureshowpeoplespend \n thebulkoftheirtimeinthehome(e.g.,cleaning,cooking, To avoid models overfitting to a single capture device,\n yardwork),leisure(e.g.,crafting,games,attendingaparty), sevendifferenthead-mountedcamerasweredeployedacross\n transportation (e.g., biking, car), errands (e.g., shopping, thedataset: Go Pro,Vuzix Blade,Pupil Labs,ZShades,OR-\n walkingdog,gettingcarfixed),andintheworkplace(e.g, DRO EP 6, i Vue Rincon 1080, and Weeview. They offer\n talkingwithcolleagues,makingcoffee). tradeoffs in the modalities available (RGB, stereo, gaze),\n Tomaximizecoverageofsuchscenarios,ourapproachis fieldofview, andbatterylife. Thefieldofviewandcam-\n acompromisebetweendirectingcamerawearersandgiving era mounting are particularly influential: while a Go Pro\n noguidanceatall: (1)werecruitedparticipantswhosecol- mounted on the head pointing down offers a high resolu-\n lectivedailylifeactivitywouldnaturallyencompassaspread tionviewofthehandsmanipulatingobjects(Fig.5,right),\n ofthescenarios(asselectedfreelybytheparticipant),and a heads-up camera like the Vuzix shares the vantage of a\n (2)weaskedparticipantstowearthecameraatlength(at person’seyes,butwillmissinteractionsclosetothebody\n leastaslongasthebatterylifeofthedevice)sothattheactiv- (Fig.5,left). \n itywouldunfoldnaturallyinalongercontext. Atypicalraw Inadditiontovideo,portionsof Ego 4 Dofferseveralother\n videoclipinourdatasetlasts 8 minutes—significantlylonger \n datamodalities:3 Dscans,audio,gaze 3,stereo,multiplesyn-\n thanthe 10 secondclipsoftenstudiedinthird-personvideo chronized wearable cameras, and textual narrations. See\n Table 1. Each can support new research challenges. For\n 1 for 64%ofallparticipants;missingdemographicsareduetoprotocols example, having Matterport 3 D scans of the environment\n orparticipantsoptingoutofansweringspecificquestions. \n 2 https://www.bls.gov/news.release/atus.nr 0.htm 3 Eyetrackersweredeployedby Indiana U.and Georgia Techonly.\n 4 "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n Modality: RGBvideo Textnarrations Features Audio Faces 3 Dscans Stereo Gaze IMU Multi-cam\n #hours: 3,670 3,670 3,670 2,535 612 491 80 45 836 224 \n \n Table 1. Modalitiesofdatain Ego 4 Dandtheiramounts. “Narrations”aredense,timestampeddescriptionsofcameraweareractivity\n (cf. Sec.4).“3 Dscans”aremeshesfrom Matterport 3 Dscannersforthefullenvironmentinwhichthevideowascaptured.“Faces”refersto\n videowhereparticipantsconsentedtoremainunblurred.“Multi-cam”referstosynchronizedvideocapturedatthesameeventbymultiple\n camerawearers.“Features”referstoprecomputed Slow Fast[70]videofeatures.Gazecollectedonlyby Indiana U.and Georgia Tech.\n \n coupledwithego-videoclips(Figure 4)offersauniqueop- \n portunityforunderstandingdynamicactivitiesinapersistent \n 3 Dcontext, asweexploitinthe Episodic Memorybench- \n mark(see Sec.5.1). Multiplesynchronizedegocentricvideo \n streams allow accounting for the first and second-person \n viewinsocialinteractions. Audioallowsanalysisofconver- \n sationandacousticscenesandevents. \n Figure 5.Examplenarrations.“C”referstocamerawearer.\n 3.4.Privacyandethics \n Fromtheonset,privacyandethicsstandardswerecritical \n will be at least subtle ways in which the language-based\n tothisdatacollectioneffort. Eachpartnerwasresponsible \n narrationsarebiasedtowardstheirlocalwordchoices.\n fordevelopingapolicy. Whilespecificsvarypersite,this \n generallyentails: 3.6.Datasetaccessibility \n • Comply with own institutional research policy, e.g., At 3,670 hours of video, we are mindful that Ego 4 D’s\n independentethicscommitteereviewwhererelevant scale can be an obstacle for accessibility for some re-\n searchers,dependingontheirstorageandcomputeresources.\n • Obtaininformedconsentofcamerawearers,whocan \n Tomitigatethis,wehavetakenseveralmeasures. First,we\n askquestionsandwithdrawatanytime,andarefreeto \n provide precomputed action features (Slow Fast 8 x 8 with\n reviewandredacttheirownvideo \n Res Net 101 backbonepretrainedfor Kinetics 400)withthe\n • Respect rights of others in private spaces, and avoid \n dataset,anoptionalstartingpointforanydownstreamwork.\n captureofsensitiveareasoractivities \n Second,onlyportionsofthedataconstitutetheformalchal-\n • Follow de-identification requirements for personally lengetrain/testsetsforeachbenchmark—notall 3,670 hours\n identifiableinformation(PII) (see Appendix E).As Ego 4 Dannotationsincrease,wewill\n createstandardizedmini-sets. Finally,weprovidetheoption\n Inshort,thesestandardstypicallyrequirethatthevideobe \n todownloadonlythedatatargetinganindividualbenchmark\n capturedinacontrolledenvironmentwithinformedconsent \n ormodalityofinterest. \n by all participants, or else in public spaces where faces \n andother PIIareblurred. Appendix Kdiscussespotential \n 4.Narrationsof Camera Wearer Activity \n negativesocietalimpact. \n Before any other annotation occurs, we pass all video\n 3.5.Possiblesourcesofbias \n throughanarrationprocedure. Inspiredbythepause-and-\n While Ego 4 D pushes the envelope on massive every- talknarrator[44],annotatorsareaskedtowatcha 5 minute\n dayvideofromgeographicallyanddemographicallydiverse clipofvideo,summarizeitwithafewsentences,andthen\n sources, we are aware of a few biases in our dataset. 74 re-watch,pausingrepeatedlytowriteasentenceabouteach\n locationsisstillalongwayfromcompletecoverageofthe thing the camera wearer does. We record the timestamps\n globe. Inaddition,thecamerawearersaregenerallylocated andtheassociatedfree-formsentences. See Figure 5. Each\n inurbanorcollegetownareas. The COVID-19 pandemic video receives two independent narrations from different\n ledtoamplefootageinstay-at-homescenariossuchascook- annotators. Thenarrationsaretemporallydense: onaverage\n ing,cleaning,crafts,etc.andmorelimitedopportunitiesto wereceived 13.2 sentencesperminuteofvideo,foratotalof\n collectvideoatmajorsocialpublicevents. Inaddition,since 3.85 Msentences. Intotalthenarrationsdescribethe Ego 4 D\n batterylifeprohibitsdaylongfilming,thevideos—though videousing 1,772 uniqueverbs(activities)and 4,336 unique\n unscripted—tendtocontainmoreactiveportionsofapartic- nouns(objects). See Appendix Dfordetails.\n ipant’sday. Finally,Ego 4 Dannotationsaredonebycrowd- The narrations allow us to (1) perform text mining for\n sourcedworkersintwositesin Africa. Thismeansthatthere data-driventaxonomyconstructionforactionsandobjects,\n 5 "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 6. The Ego 4 Dbenchmarksuitecentersaroundthefirst-personvisualexperience—fromrememberingthepast,toanalyzingthe\n present,toanticipatingthefuture. \n \n (2)sortthevideosbytheircontenttomapthemtorelevant \n benchmarks,and(3)identifytemporalwindowswherecer- \n tainannotationsshouldbeseeded. Beyondtheseuses,the \n narrationsarethemselvesacontributionofthedataset,po- \n tentiallyvaluableforresearchonvideowithweaklyaligned \n naturallanguage.Toourknowledge,oursisthelargestrepos- \n itoryofalignedlanguageandvideo(e.g.,How To 100 M[157], \n anexisting Internetrepositorywithnarrations,containsnoisy \n spokennarrationsthatonlysometimescommentontheac- \n tivitiestakingplace). \n Figure 7.Episodic Memory’sthreequerytypes\n 5.Ego 4 DBenchmark Suite \n First-personvisionhasthepotentialtotransformmany \n to France?”), to be distinguished from semantic memory\n applications in augmented reality and robotics. However, \n (“what’sthecapitalof France?”). Anaugmentedrealityas-\n compared to mainstream video understanding, egocentric \n sistantthatprocessestheegocentricvideostreamcouldgive\n perceptionrequiresnewfundamentalresearchtoaccountfor \n ussuper-humanmemoryifitcouldappropriatelyindexour\n long-formvideo,attentioncues,person-objectinteractions, \n visualexperienceandanswerqueries. \n multi-sensorydata,andthelackofmanualtemporalcuration \n inherenttoapassivelyworncamera. Taskdefinition Givenanegocentricvideoandaquery,the\n Inspiredbyallthesefactors,weproposeasuiteofchal- Ego 4 D Episodic Memory task requires localizing where\n lengingbenchmarktasks. Thefivebenchmarkstacklethe the answer can be seen within the user’s past video. We\n past,present,andfutureoffirst-personvideo. See Figure 6. consider three query types. (1) Natural language queries\n Thefollowingsectionsintroduceeachtaskanditsannota- (NLQ),inwhichthequeryisexpressedintext(e.g.,“What\n tions. Thefirstdatasetreleasehasannotationsfor 48-1,000 did I put in the drawer?”), and the output response is the\n hoursofdataperbenchmark,ontopofthe 3,670 hoursof temporalwindowwheretheanswerisvisibleordeducible.\n datathatisnarrated. The Appendicesdescribehowwesam- (2)Visualqueries(VQ),inwhichthequeryisastaticimage\n pledvideosperbenchmarktomaximizerelevancetothetask of an object, and the output response localizes the object\n whilemaintaininggeographicdiversity. thelasttimeitwasseeninthevideo,bothtemporallyand\n Wedevelopedbaselinemodelsdrawingonstate-of-the- spatially. Thespatialresponseisa 2 Dboundingboxonthe\n artcomponentsfromtheliteratureinordertotestdriveall object, and optionally a 3 D displacement vector from the\n Ego 4 Dbenchmarks. The Appendixpresentsthebaseline currentcamerapositiontotheobject’s 3 Dboundingbox.VQ\n modelsandquantitativeresults. Wearerunningaformal captureshowausermightteachthesystemanobjectwith\n Ego 4 Dcompetitionin June 2022 invitingtheresearchcom- animageexample, thenlateraskforitslocation(“Where\n munitytoimproveonthesebaselines. isthis[pictureofmykeys]?”). (3)Momentsqueries(MQ),\n in which the query is the name of a high-level activity or\n 5.1.Episodic Memory “moment”,andtheresponseconsistsofalltemporalwindows\n where the activity occurs (e.g., “When did I read to my\n Motivation Egocentric video from a wearable camera \n children?”). See Figure 7. \n recordsthewho/what/when/whereofanindividual’sdaily \n lifeexperience. Thismakesitidealforwhat Tulvingcalled Annotations Forlanguagequeries,wedevisedasetof 13\n episodic memory [213]: specific first-person experiences template questions meant to span things a user might ask\n (“what did I eat and who did I sit by on my first flight to augment their memory, such as “what is the state of\n 6 "
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n object X?”,e.g.,“did Ileavethewindowopen?”.Annotators \n expressthequeriesinfree-formnaturallanguage,andalso \n provide the slot filling (e.g., X = window). For moments, \n weestablishedataxonomyof 110 activitiesinadata-driven, pre-condition PNR post-condition\n semi-automaticmannerbyminingthenarrationsummaries. \n State-change:Plantremovedfromground\n Momentscapturehigh-levelactivitiesinthecamerawearer’s \n day,e.g.,settingthetableisamoment,whereaspickupis \n anactioninour Forecastingbenchmark(Sec.5.5). \n For NLQ and VQ, we ask annotators to generate lan- \n guage/visual queries and couple them with the “response pre-condition PNR post-condition\n track”inthevideo. For MQ,weprovidethetaxonomyof \n State-change:Woodsmoothed \n labelsandaskannotatorstolabelclipswitheachandevery \n temporalsegmentcontainingamomentinstance. Intotal, Figure 8.Handsand Objects:Exampleobjectstatechangesdefined\n wehave 74 Ktotalqueriesspanning 800 hoursofvideo. bypre-condition,PNR,andpost-conditionframes.\n ∼ \n Evaluationmetricsandbaselines For NLQ,weusetop-k \n recall at a certain temporal intersection over union (t Io U) manactionsbetter,aswellastotrainrobotstolearnfrom\n threshold. MQ adopts a popular metric used in temporal humandemonstrationsinvideo.\n actiondetection:m APatmultiplet Io Uthresholds,aswellas \n Taskdefinitions Weinterpretanobjectstatechangetoin-\n top-kxrecall. VQadoptstemporalandspatio-temporallocal- \n cludevariousphysicalchanges,includingchangesinsize,\n izationmetricsaswellastimelinessmetricsthatencourage \n shape,composition,andtexture. Objectstatechangescanbe\n speedysearches. Appendix Fpresentsthebaselinemodels \n viewedalongtemporal,spatialandsemanticaxes,leadingto\n wedevelopedandreportsresults. \n thesethreetasks: (1)Point-of-no-returntemporallocaliza-\n Relation to existing tasks Episodic Memory has some tion: givenashortvideoclipofastatechange,thegoalisto\n foundationsinexistingvisionproblems,butalsoaddsnew estimatethekeyframethatcontainsthepoint-of-no-return\n challenges. All three queries call for spatial reasoning in (PNR)(thetimeatwhichastatechangebegins);(2)State\n astaticenvironmentcoupledwithdynamicvideoofaper- changeobjectdetection: giventhreetemporalframes(pre,\n son who moves and changes things; current work largely post,PNR),thegoalistoregresstheboundingboxofthe\n treats these two elements separately. The timeliness met- objectundergoingastatechange; (3)Objectstatechange\n ricsencourageworkonintelligentcontextualsearch. While classification: givenashortvideoclip,thegoalistoclassify\n currentliteratureonlanguage+visionfocusesoncaptioning whetheranobjectstatechangehastakenplaceornot.\n and question answering for isolated instances of Internet \n Annotations Weselectthedatatoannotatebasedonactivi-\n data[12,35,119,228],NLQismotivatedbyqueriesabout \n tiesthatarelikelytoinvolvehand-objectinteractions(e.g.,\n thecamerawearer’sownvisualexperienceandoperatesover \n knitting,carpentry,baking,etc.). Westartbylabelingeach\n long-termobservations. VQupgradesobjectinstancerecog- \n narratedhand-objectinteraction. Foreach, welabelthree\n nition [23,85,126,155] to deal with video (frequent Fo V \n momentsintime(pre,PNR,post)andtheboundingboxes\n changes, objects entering/exiting the view) and to reason \n forthehands,tools,andobjectsineachofthethreeframes.\n aboutobjectsinthecontextofa 3 Denvironment. Finally, \n Wealsoannotatethestatechangetypes(remove,burn,etc.,\n MQcanbeseenasactivitydetection[141,229,237]butfor \n see Fig.8),actionverbs,andnounsfortheobjects.\n theactivitiesofthecamerawearer. \n Evaluation metrics and baselines Object state change\n 5.2.Handsand Objects temporallocalizationisevaluatedusingabsolutetemporal\n errormeasuredinseconds. Objectstatechangeclassifica-\n Motivation While Episodic Memory aims to make past \n tion is evaluated by classification accuracy. State change\n video queryable, our next benchmark aims to understand \n objectdetectionisevaluatedbyaverageprecision(AP).Ap-\n the camera wearer’s present activity—in terms of inter- \n pendix Gdetailstheannotationsandpresentsbaselinemodel\n actions with objects and other people. Specifically, the \n resultsforthethree Handsand Objectstasks. \n Handsand Objectsbenchmark captures how the camera \n wearer changes the state of an object by using or manip- Relation to existing tasks Limited prior work considers\n ulatingit—whichwecallanobjectstatechange. Though objectstatechangeinphotos[102,164]orvideo[8,68,242];\n cutting a piece of lumber in half can be achieved through Ego 4 Disthefirstvideobenchmarkdedicatedtothetaskof\n manymethods(e.g.,varioustools,force,speed,grasps,end- understandingobjectstatechanges. Thetaskissimilarto\n effectors),allshouldberecognizedasthesamestatechange. actionrecognition(e.g.,[100,110,139,221,243])becausein\n Thisgeneralizationabilitywillenableustounderstandhu- somecasesaspecificactioncancorrespondtoaspecificstate\n 7 "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n rate (DER) [11] and word error rate (WER) [114] for di-\n arizationandtranscription,respectively. Wepresent AVD\n baselinemodelsandresultsin Appendix H. \n Relation to existing tasks The past few years have seen\n audiostudiedincomputervisiontasks[245]foractionclas-\n sification[110,226],objectcategorization[125,234],source\n localizationandtracking[14,197,212]andembodiednavi-\n gation[33]. Meanwhile,visualinformationisincreasingly\n usedinhistoricallyaudio-onlytaskslikespeechtranscrip-\n tion,voicerecognition,audiospatialization[5,80,104,161],\n speakerdiarization[10,83],andsourceseparation[57,78,82].\n Datasetslike Vox Celeb[39],AVASpeech[31],AVAactive\n Figure 9. Audio-Visualand Socialbenchmarkannotations \n speaker[192],AVDIAR[83],and Easy Com[53]supportthis\n research. However,thesedatasetsaremainlynon-egocentric.\n change. However,asinglestatechange(e.g.,cutting)can Unlike Ego 4 D,theydonotcapturenaturalconversational\n alsobeobservedinmanyforms(variousobject-tool-action characteristics involving a variety of noisy backgrounds,\n combinations). Itisourhopethattheproposedbenchmarks overlapping,interruptingandun-intelligiblespeech,environ-\n will lead to the development of more explicit models of mentvariation,movingcamerawearers,andspeakersfacing\n objectstatechange,whileavoidingapproachesthatsimply awayfromthecamerawearer. \n overfittoactionorobjectobservations. \n 5.4.Social Interactions \n 5.3.Audio-Visual Diarization Motivation Anegocentricvideoprovidesauniquelensfor\n studyingsocialinteractionsbecauseitcapturesutterances\n Motivation Ournexttwotasksaimtounderstandthecam- \n and nonverbal cues [115] from each participant’s unique\n erawearer’spresentinteractionswithpeople. Peoplecom- \n viewandenablesembodiedapproachestosocialunderstand-\n municateusingspokenlanguage,makingthecaptureofcon- \n ing. Progressinegocentricsocialunderstandingcouldlead\n versationalcontentinbusinessmeetingsandsocialsettings \n tomorecapablevirtualassistantsandsocialrobots. Compu-\n aproblemofgreatscientificandpracticalinterest. While \n tationalmodelsofsocialinteractionscanalsoprovidenew\n diarizationhasbeenastandardprobleminthespeechrecog- \n toolsfordiagnosingandtreatingdisordersofsocialization\n nition community, Ego 4 D brings in two new aspects (1) \n andcommunicationsuchasautism[188],andcouldsupport\n simultaneouscaptureofvideoandaudio(2)theegocentric \n novelprosthetictechnologiesforthehearing-impaired.\n perspectiveofaparticipantintheconversation. \n Taskdefinition Whilethe Ego 4 Ddatasetcansupportsuch\n Task definition and annotations The Audio-Visual Di- \n along-termresearchagenda,ourinitial Socialbenchmark\n arization(AVD)benchmarkiscomposedoffourtasks(see \n focusesonmultimodalunderstandingofconversationalin-\n Figure 9): \n teractionsviaattentionandspeech. Specifically,wefocuson\n • Localizationandtrackingoftheparticipants(i.e.,candi- \n identifyingcommunicativeactsthataredirectedtowardsthe\n datespeakers)inthevisualfieldofview(Fo V).Abound- \n camera-wearer,asdistinguishedfromthosedirectedtoother\n ingboxisannotatedaroundeachparticipant‘sface. \n socialpartners: (1)Lookingatme(LAM):givenavideoin\n • Activespeakerdetectionwhereeachtrackedspeakerisas- \n whichthefacesofsocialpartnershavebeenlocalizedand\n signedananonymouslabel,includingthecamerawearer \n identified,classifywhethereachvisiblefaceislookingatthe\n whoneverappearsinthevisual Fo V. \n camerawearer;and(2)Talkingtome(TTM):givenavideo\n • Diarization of each speaker’s speech activity, where \n and audio segment with the same tracked faces, classify\n we provide the time segments corresponding to each \n whethereachvisiblefaceistalkingtothecamerawearer.\n speaker’svoiceactivityintheclip. \n • Transcriptionofeachspeaker’sspeechcontent(only En- Annotations Socialannotationsbuildonthosefrom AVdi-\n glishspeakersareconsideredforthisversion). arization(Sec.5.3). Given(1)faceboundingboxeslabeled\n withparticipant IDsandtrackedacrossframes,and(2)asso-\n Evaluationmetricsandbaselines Weusestandardizedob- ciatedactivespeakerannotationsthatidentifyineachframe\n ject tracking (MOT) metrics [18,19] to evaluate speaker whetherthesocialpartnerswhosefacesarevisiblearespeak-\n localizationandtrackinginthevisual Fo V.Speakerdetec- ing,annotatorsprovidethegroundtruthlabelsfor LAMand\n tionwithanonymouslabelsisevaluatedusingthespeaker TTMasabinarylabelforeachfaceineachframe.For LAM,\n error rate, which measures the proportion of wrongly as- annotatorslabelthetimesegment(startandendtime)ofa\n signedlabels. Weadoptthewellstudieddiarizationerror visiblepersonwhentheindividualislookingatthecamera\n 8 "
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n wearer. For TTM,weusethevocalactivityannotationfrom take \n AVD, then identify the time segment when the speech is doughin \n 0.8 s take \n directedatthecamerawearer. See Figure 9. doughin \n 0.8 s \n Evaluation metrics and baselines We use mean average Locomotion Movements Hands Movements Short-Term Anticipation\n precision(m AP)and Top-1 accuracytoquantifytheclassifi- prediction:kneaddough putdough packspice pourspice\n cationperformanceforbothtasks. Unlike AVD,wemeasure \n precisionateveryframe. Appendix Iprovidesdetailsand \n Input video Long-Term Anticipation \n presents Socialbaselinemodelsandresults. \n Relationtoexistingtasks Comparedto[67],Ego 4 Dcon- Figure 10.The Forecastingbenchmarkaimstopredictfutureloco-\n tainssubstantiallymoreparticipants,hoursofrecording,and motion,movementofhands,nextobjectinteractions,andsequences\n offutureactions. \n varietyofsensorsandsocialcontexts.The LAMtaskismost \n closelyrelatedtopriorworkoneyecontactdetectioninego- \n video[36,159],butaddressesmorediverseandchallenging \n Evaluationmetricsandbaselines Weevaluatefutureloco-\n scenarios. Mutualgazeestimation[54,150–152,172,176] \n motionmovementandhandmovementpredictionusing L 2\n andgazefollowing[37,65,111,186]arealsorelevant. The \n distance. Short-termobjectinteractionanticipationiseval-\n TTMtaskisrelatedtoaudio-visualspeakerdetection[7,193] \n uatedusinga Top-5 mean Average Precisionmetricwhich\n andmeetingunderstanding[21,132,154]. \n discountsthe Top-4 falsenegativepredictions.Long-termac-\n 5.5.Forecasting tionanticipationisevaluatedusingeditdistance. Appendix J\n detailsthetasks,annotations,baselinemodels,andresults.\n Motivation Havingaddressedthepastandpresentofthe \n Relation to existing tasks Predicting future events from\n camera wearer’s visual experience, our last benchmark \n egocentric vision has increasing interest [191]. Previous\n moves on to anticipating the future. Forecasting move- \n workconsidersfuturelocalization[113,120,174,230],ac-\n mentsandinteractionsrequirescomprehendingthecamera \n tionanticipation[76,77,86,118,127,219],nextactiveobject\n wearer’sintention. Ithasimmediateapplicationsin ARand \n prediction[20,74],futureeventprediction[149,167],andfu-\n human-robotinteraction,suchasanticipativelyturningon \n tureframeprediction[145,146,153,215,218,227]. Whereas\n appliancesormovingobjectsforthehuman’sconvenience. \n pastworkreliesondifferentbenchmarksandtaskdefinitions,\n Thescientificmotivationcanbeseenbyanalogywithlan- \n we propose a unified benchmark to assess progress in the\n guagemodelssuchas GPT-3[24],whichimplicitlycapture \n field. \n knowledgeneededbymanyothertasks. Ratherthanpredict \n thenextword,visualforecastingmodelsthedynamicsofan \n 6.Conclusion \n agentactinginthephysicalworld. \n Taskdefinition The Forecastingbenchmarkincludesfour Ego 4 Disafirst-of-its-kinddatasetandbenchmarksuite\n tasks (Fig. 10): (1) Locomotion prediction: predict a set aimed at advancing multimodal perception of egocentric\n of possiblefuture ground plane trajectoriesof the camera video. Comparedtoexistingwork,ourdatasetisordersof\n wearer. (2) Hand movement prediction: predict the hand magnitudelargerinscaleanddiversity. Thedatawillallow\n positionsofthecamerawearerinfutureframes. (3)Short- AItolearnfromdailylifeexperiencesaroundtheworld—\n termobjectinteractionanticipation: detectasetofpossible seeingwhatweseeandhearingwhatwehear—whileour\n futureinteractedobjectsinthemostrecentframeoftheclip. benchmark suite provides solid footing for innovations in\n Toeachobject,assignaverbindicatingthepossiblefuture videounderstandingthatarecriticalforaugmentedreality,\n interactionanda“timetocontact”estimateofwhentheinter- robotics,andmanyotherdomains. Welookforwardtothe\n actionisgoingtobegin. (4)Long-termactionanticipation: researchthatwillbuildon Ego 4 Dintheyearsahead.\n predictthecamerawearer’sfuturesequenceofactions. \n Contributionstatement \n Annotations Using the narrations, we identify the occur- \n renceofeachobjectinteraction,assigningaverbandatarget Projectledandinitiatedby Kristen Grauman. Program\n objectclass. Theverbandnountaxonomiesareseededfrom managementandoperationsledby Andrew Westbury. Scien-\n thenarrationsandthenhand-refined. Foreachaction, we tificadvisingby Jitendra Malik. Authorswithstars(∗)were\n identifyacontactframeandapre-conditionframeinwhich keydriversofimplementation,collection,and/orannotation\n we annotate bounding boxes around active objects. The developmentthroughouttheproject. Authorswithdaggers\n sameobjectsaswellashandsareannotatedinthreeframes (†)arefaculty PIsandworkinggroupleadsintheproject.\n precedingthepre-conditionframeby 0.5 s,1 sand 1.5 s. We Thebenchmarksbroughttogethermanyresearchersfromall\n obtain ground truth ego-trajectories of the camera wearer institutionsincludingcross-institutionbaselineevaluations.\n usingstructurefrommotion. Appendices Fthrough Jdetailthecontributionsofindividual\n 9 "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n authorsforthevariousbenchmarks. Thevideocollectedby \n Facebook Reality Labsused Vuzix Blade®Smart Glasses \n andwasdoneinaclosedenvironmentin Facebook’sbuild- \n ingsbypaidparticipantswhosignedconsentstosharetheir \n data. Allothervideocollectionandparticipantrecruitment \n wasmanagedbytheuniversitypartners. Appendix Apro- \n videsdetailsaboutthedatacollectiondonepersiteandac- \n knowledgestheprimarycontributors. Theannotationeffort \n wasledby Facebook AI. \n Acknowledgements \n Wegratefullyacknowledgethefollowingcolleaguesfor \n valuablediscussionsandsupportofourproject: Aaron Ad- \n cock, Andrew Allen, Behrouz Behmardi, Serge Belongie, \n Antoine Bordes, Mark Broyles, Xiao Chu, Samuel Clapp, \n Irene D’Ambra,Peter Dodds,Jacob Donley,Ruohan Gao, \n Tal Hassner,Ethan Henderson,Jiabo Hu,Guillaume Jean- \n neret,Sanjana Krishnan,Devansh Kukreja,Tsung-Yi Lin, \n Bobby Otillar, Manohar Paluri, Maja Pantic, Lucas Pinto, \n Vivek Roy,Jerome Pesenti,Joelle Pineau,Luca Sbordone, \n Rajan Subramanian,Helen Sun,Mary Williamson,and Bill \n Wu. Wealsoacknowledge Jacob Chalkforsettingupthe \n Ego 4 DAWSbackendand Prasanna Sridharfordeveloping \n the Ego 4 Dwebsite. Thankyoutothe Common Visual Data \n Foundation(CVDF)forhostingthe Ego 4 Ddataset. \n Theuniversitiesacknowledgetheusageofcommercial \n softwareforde-identificationofvideo. brighter.aiwasused \n forredactingvideosbysomeoftheuniversities. Personal \n datafromthe Universityof Bristolwasprotectedby Prim- \n loc’s Secure Redactsoftwaresuite. \n UNICT is supported by MIUR AIM - Attrazione e \n Mobilita Internazionale Linea 1 - AIM 1893589 - CUP \n E 64118002540007. Bristolissupportedby UKRIEngineer- \n ingand Physical Sciences Research Council(EPSRC)Doc- \n toral Training Program(DTP),EPSRCFellowship UMPIRE \n (EP/T 004991/1). KAUSTissupportedbythe KAUSTOf- \n ficeof Sponsored Researchthroughthe Visual Computing \n Center(VCC)funding. National Universityof Singaporeis \n supportedby Mike Shou’s Start-Up Grant. Georgia Techis \n supportedinpartby NSFaward 2033413 and NIHaward \n R 01 MH 114999. \n \n \n \n \n \n \n \n \n \n \n 10 \n \n \n "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n totheparticipantsindifferentpartsofthecountry. Videos\n weresharedbackeitherinexternalharddisksoroverthe\n Appendix \n cloudstorage. Eachvideowasmanuallyinspectedforany\n sensitivecontentbeforesharing. \n Primarycontributors:Raghava Modhugu-datacollection\n Table of Contents pipeline,designofthesetupandworkflow. Siddhant Bansal\n - IRB application, consent forms and de-identification. C.\n Appendices 11 \n V.Jawahar -leadcontributorfordata collection. Wealso\n A .Data Collection . . . . . . . . . . . . 11 acknowledgethecontributionsof Aradhana Vinod(coordi-\n B .De-identification Process . . . . . . . 16 nationandcommunication),Ram Sharma(localdataman-\n C .Demographics. . . . . . . . . . . . . 18 agementandverification),and Varun Bhargavan(systems\n D .Narrations . . . . . . . . . . . . . . . 20 andresources). \n E .Benchmark Data Splits . . . . . . . . 24 \n Universityof Tokyo,Japan: Werecruited 81 Japanesepar-\n F .Episodic Memory Benchmark . . . . 25 \n ticipants(41 male,40 female)livingaround Tokyo,Japan\n G .Handsand Objects Benchmark . . . . 44 \n throughatemporaryemploymentagency. Theparticipant’s\n H .Audio-Visual Diarization Benchmark. 51 \n genderandage(fromthe 20 sto 60 s)werebalancedtocollect\n I .Social Interaction Benchmark. . . . . 63 diversebehaviorpatterns. Wefocusedontwosingle-actor\n J .Forecasting Benchmark . . . . . . . . 67 activities: cooking(40 participants,90 hours)andhandcraft\n K .Societal Impact . . . . . . . . . . . . 82 (41 participants,51 hours). Inthecookingscenario,partici-\n pantswereaskedtorecordunscriptedvideosofcookingat\n theirhomes. Inthehandcraftscenario,participantsvisited\n A.Data Collection \n our laboratory and performed various handcraft activities\n (e.g.,origami,woodworking,plasticmodel,cutoutpicture).\n Thissectionoverviewsthecollectionproceduresandsce- \n Wecollecteddatausing Go Pro HERO 7 Blackcamerafor\n nariospersite. \n cookingand Weeview SID 3 Dstereocameraforhandcraft.\n International Institute of Information Technology Ourdatacollectionprotocolwasreviewedandapprovedby\n (IIIT), Hyderabad, India: At IIIT, Hyderabad, we fol- Universityof Tokyoethicalreviewboard.\n lowedaprotocolofdistributeddatacollectionwithacen- Primarycontributors: Yoichi Sato–leadcoordinatorfor\n tralizedteamdoingcoordinationandverification. Wefirst datacollection,Takuma Yagiand Takumi Nishiyasu–con-\n identifiedlocalcoordinatorsindifferentpartsofthecountry tributedtoparticipantrecruiting,protocoldesign,datacollec-\n andexplainedthedatacollectionplans,goalsandprocess. tionandinspection,and IRBsubmission,Yifei Huangand\n Theythenhelpedincollectingdataintheirownlocalregions Zhenqiang Li–contributedtodatainspectionandtransfer,\n fromnaturalsettingswithinformedparticipants.Participants Yusuke Sugano–contributedtoselectingvideorecording\n wererecruitedlocallyconsideringtherangeofactivities,and scenarios,protocoldesignand IRBsubmission.\n alsotheguidelinesandrestrictionsof COVID-19. Thecen- \n tralteamcouldnottraveltoalltheselocationsfortraining University of Bristol, UK: Participants were recruited\n thecoordinatorsorcollectingthedata. Weshippedmultiple throughadvertsonsocialmediaanduniversityinternalcom-\n camerastothelocalcoordinatorsandremotelyguidedthem munication channels. These participants then spread the\n ondatacollectionfollowingthe COVIDprotocols. Thecol- wordtotheiracquaintancesandsomeparticipantsjoinedthe\n lected data and consent forms were then shipped back to projectthroughword-of-mouthrecommendationsofprevi-\n theuniversity,wheremanualverification,de-identification ousparticipants. Datawascollectedbetween Janand Dec\n (whereverapplicable),andsharingwiththeconsortiumtook 2020,from 82 participants.Withthepandemictakingoverin\n place. March,theprojectshiftedtoonlineoperationwherecameras\n At IIITHyderabad,werecorded 660.5 hoursofdatawith wereposted,andtrainingtookplaceover Zoommeetings.\n the help of 138 subjects. The videos were collected in 5 Participantsfirstexpressedinterestbysendinganemailand\n differentstatesin India,geographicallywellapart. Wecover theywereprovidedwithaninformationsheet. Thiswasfol-\n 36 differentscenarios,suchasmakingbricksusinghands, lowedbyapreliminary Zoommeetingwitharesearcherto\n knitting, making egg cartons, and hairstyling. The age of briefparticipantsabouttheprocedure,answeranyquestions\n subjects ranged from 18-84 years with 10 distinct profes- andagreeonthescenariostoberecorded.\n sionalbackgrounds(teachers,students,farmers,blacksmiths, Wesetalimittothetotalnumberofminutesperscenario,\n homemakers,etc.). Outofallthesubjects,94 weremales, to increase diversity of recordings. For example, driving\n and 44 were females. We use Go Pro Hero 6 and Go Pro cannotbelongerthan 30 minuteswhilecookingcanbeup\n Hero 7 forrecordingthevideos. The Go Pro’swereshipped to 1.5 hours. Each participant was instructed to record a\n 11 "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n minimumof 2 hoursacross 4 scenarios. Importantly,partic- wascomprisedoffriendsorfamilymemberswhokneweach\n ipantswereencouragedtocollectactivitiestheynaturally otherpriortoparticipatinginthestudy. Participantswere\n do. For example if one regularly cycles or practices mu- requiredtobeaged 18-64,tonotbeconsideredhighriskfor\n sic,theywereaskedtorecordthesescenarios. Additionally, COVID-19,andtobeabletoplaysocialdeductiongamesin\n pairedscenarios(peoplecookingtogetherorplayinggames) English. Ourstudyprotocolwasreviewedandapprovedby\n wereencouragedandmultiple(2-3)cameraswerepostedfor the Georgia Tech Institutional Review Board(IRB).Intotal,\n participantssharingahousehold. Allparticipantssigneda approximately 43 hoursofegocentricvideowerecollected\n consentformbeforeacamerawaspostedtotheirresidence. from 19 participants(perparticipantdisclosure-10 male,\n Cameraswerepostedto 9 UKcitiesin England,Walesand 7 female,1 non-binary,1 notreported). Participantshada\n Scotlandincludingoneparticipantinthe Isleof North Uist. meanageof 31.6 yearswith 7 participantsaged 20-29 years,\n Uponreceiptofthecamera,asecond Zoommeetingwas 10 participants aged 30-39 years, and 2 participants aged\n scheduledtotraintheparticipantontheequipmentanddetail 40-49 years. \n how footage is reviewed and uploaded. Participants were Participantsworeanegocentrichead-worncameraand\n given 2 weekstorecord,withanadditionalweekofexten- on-earbinauralmicrophones. Someparticipantsworethe\n sionuponrequest. Oncerecordingiscompleted,footageis ORDROEP 6 camerawhileothersworethe Pupil Invisible\n uploadedbytheparticipantandreviewedforgoodlighting, cameras. Theaudiowasrecordedusinga Tascam DR-22 WL\n correctsettingandviewpoint. Participantswerereimbursed and Sound Professionals MS-EHB-2 Ear-hookbinauralmi-\n fortheirparticipationintheproject. crophones. A third-person video was also captured via a\n Scenariosrecordedinthe UKcovered: commuting(driv- Logitech C 930 e Webcam. Participantsworetheprovided\n ing,walking,cycling,takingthebus,hiking,jogging),en- recordingdeviceswhileeating,drinking,andplayingsocial\n tertainment(cardgames,boardgames,videogames,lego, deductiongamessuchas One Night Ultimate Werewolf and\n reading,practisingamusicalinstrument,listeningtomusic, The Resistance: Avalonintheirownhome. Thisat-home\n watching TV),jobs(labwork,carpentry),sports(football, game-nightsettingelicitedawiderangeofspontaneousand\n basketball,climbing,golf,yoga,workouts)andhome-based naturalistic social behaviors and interactions. In addition,\n daily activities (cooking, cleaning, laundry, painting, car- eatinganddrinkingbehaviorswerecapturedfromboththe\n ingforpets,tidying,wateringtheplants),DIY(fixing,gar- egocentricandthird-personcameras.\n dening,woodwork)andcrafts(colouring,crafting,crochet, Inadditiontoparticipatingintherecordedsession,partic-\n drawing, knitting, sewing). Footage was captured using ipantscompletedasurveythatcapturedtheirdemographic\n Go Pro Hero-7,Hero-8 and Vuzix. information. Alldatawasscreenedandcensoredbystudy\n Footagewasthenreviewedbyresearcherstoidentifyany personneltoremoveanyidentifyinginformationincluding\n PII.36%ofallvideosrequiredde-identification. Weused visiblepersonalinformationontheirphonescreensorthe\n Primloc’s Secure Redactsoftwaresuite,withintegratedtools exteriorofthehome. Participantsalsohadtheopportunity\n anduserinterfacesformanualtrackingandadjustingdetec- toreviewthevideosandrequestadditionalcensoring.\n tions. Redactedrecordingswerereviewedmanually, then Primarycontributors: Fiona Ryan-leadcoordinatorfor\n encodedanduploadedtothe AWSbucket. Duringencoding, datacollection,includingsynchronization,de-identification,\n IMUmetadatawasseparatelyextracted. Integratedaudio and ingestion; Audrey Southerland - lead coordinator for\n andvideousingnative 50 fpsrecordingsareavailable. IRBdevelopmentandrecruiting;Miao Liu-contributedto\n Intotal,262 hourswererecordedby 82 participants. On datacollectionandingestion;James M.Rehg-contributed\n average,eachparticipantrecorded 3.0 hours(σ =0.7 hours) toprotocoldesignanddatacollection.\n Thedataispublishedunder General Data Protection Regula- \n Indiana University,Bloomington,IN,USA: Participants\n tion(GDPR)compliance. \n in the Bloomington, Indiana, USA area were recruited\n Primary contributors: Michael Wray - data collection, \n throughadvertisementsonsocialmedia,onlineclassifieds\n consent forms and information sheets; Jonathan Munro - \n boards,andemaillists. Wealsousedsnowballsamplingby\n datacollectionandethicsapplication;Adriano Fragomeni- \n askingparticipantstoshareouradswiththeirfriends. Were-\n datacollectionandde-identificationoversight;Will Price- \n cruitedparticipantswhowerewillingtoperforminteractive\n dataingestion,encodingandmetadata;Dima Damen-sce- \n smallgroupactivitiessuchasplayingsports,playingboard\n narios,procedures,datacollectionoversightandparticipant \n orcardgames,playingmusicalinstruments,assemblingpuz-\n communication. Weacknowledgetheeffortsof Christianne \n zles, etc. The health of participants and study personnel\n Ferneeinmanuallyreviewingalldata. \n wassafeguardedbycollectingdataeitheroutdoors(where\n Georgia Tech, Atlanta, GA, USA: Participant groups peoplecanmoresafelyinteractwithoutwearingmasks),or\n fromthe Atlanta,Georgia,USAmetroareawererecruited indoorsinthehomesoftheparticipants. Ineithercase,we\n viaonlinepostsandadvertisementsonsitessuchas Face- initially required that all participants in a social group be\n book, Reddit, and Instagram. Each group of participants partofthesamehouseholdtominimizetheriskofspreading\n 12 "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n \n \n diseasebetweenhouseholds,butlaterweallowedgroupsof Primarycontributors: David Crandall-leadcoordinator\n peoplewhowerecomfortableinteractingwithoneanother fordatacollection;Yuchen Wang-contributedtoprotocol\n (e.g., becausetheyarevaccinatedfor COVID-19). Group design, participant recruiting, and data collection; Weslie\n sizesrangedfrom 1 to 6 people,withgroupsof 2 or 3 being Khoo - developed multi-camera synchronization and de-\n themostcommon. identificationpipelines. \n We collected data with four different devices: z Shade \n Universityof Minnesota,Twin Cities,MN,USA: Partic-\n 1080 p camera glasses, i Vue Rincon 1080 camera glasses, \n ipants in the Minneapolis and St. Paul, Minnesota, USA\n ORDRO EP-6, and Pupil Labs Invisible camera and gaze \n areawererecruitedthroughadvertisementsonsocialmedia\n trackingglasses. Weusedmultipledevicesbecauseeachhas \n anduniversitybulletinssuchas Facebook AD,Craiglist,and\n variousadvantagesanddisadvantages; z Shadehasalarge \n Redhat. A total of approximately 313 hours of data was\n horizontalfieldofview,forexample,whilei Vuehasanad- \n collected from 45 participants (22 males and 23 females).\n justableverticalfieldofview,ORDROsitsbytheearandis \n Agegroupsinclude 5 teenagers,20 peopleintheirtwenties,\n mountedonaheadbandwhichworkswellforpeoplewear- \n 11 people in their thirties, 8 people in their forties, and 1\n ingprescriptionglasses,and Invisibleoffersgazetracking \n personintheirfifties. Werecruitedparticipantsasmultiple\n but is very expensive. We asked as many participants as \n groupsandencouragedthemtoengageinunstructurednat-\n possibleinthegrouptowearcameras. Weprimarilyused \n uralsocialinteractions. Suchinteractionsincludedplaying\n ourtwo Pupil Labs Invisibleswheneverpossible,because \n cardgames, talkinginthekitchenwhilecooking, playing\n oftheireaseofuseandabilitytocollectgazedata,butwe \n basketball,andbuildingatentatacampsite. Inallcases,\n alsousedthe ORDROEP-6 whentherewerelargergroups \n werequiredthatallparticipantsinasocialgroupbepartof\n orwhenparticipantsworeprescriptionglasses. \n thesamehouseholdtominimizethe COVID-19 risk. Group\n Our protocol was reviewed and approved by the Indi- \n sizesrangedfrom 1 to 6 people,withgroupsof 2 or 3 being\n ana University Institutional Review Board(IRB).Wefirst \n themostcommon. \n conductedanonlinemeetingwithpotentialparticipantsto \n Wecollecteddatawiththez Shade 1080 pcameraglasses\n describethestudy,explaintheuseofthecameras,agreeon \n thathavealargefieldofview. Ourprotocolwasreviewed\n anactivityforthemtoperform,andanswertheirquestions. \n andapprovedbythe Universityof Minnesota Institutional\n We ask participants to try to limit capture of potentially \n Review Board (IRB). We first conducted an online meet-\n privacy-sensitivecontentbychoosingaplacewithintheir \n ingwithpotentialparticipantstodescribethestudy,explain\n homethatdidnothavepersonallyidentifiableinformation, \n the use of the cameras, agree on an activity for them to\n byavoidingrecordingpeopleotherthanthoseparticipating \n perform, and answer their questions. We then arranged a\n in the study, and by avoiding saying last names or other \n time for them to receive the cameras and provided them\n sensitiveaudio. \n withapostage-paidboxforcamerareturn. Afewdayslater,\n Wethenarrangeatimetomeetthem,typicallyoutside \n participants shipped the cameras to our designated return\n their home or in an outdoor public place. We set up the \n address. Wedownloadedthedataaftersanitizingcameras\n cameras,helptheparticipantsputthemon,givethemour \n and equipment. After the data capture was complete, we\n contact information in case they have any problems, and \n visuallyinspectedeverysecondofvideoinordertoexclude\n thenweleavewhiletheyperformtheactivity. Wethenre- \n anyprivacy-sensitiveinformation(e.g. licenseplates,smart\n turn after about one hour to pick up the cameras. Within \n phonescreens,andcreditcardnumbers),andtoassessthe\n a few days, we send each participant a copy of the video \n durationofnon-socialactivities. Forincidentalparticipants\n takenbytheircamera,andaskthemtoreviewthefootage \n (i.e. bystanders)appearingindatacollectedbythecamera\n andidentifyanyprivacy-sensitivecontent(videooraudio) \n wearerinpublicsettings(e.g.,shopping,concert,atapark,\n thattheywouldprefertobeblurredorremoved. Wemanu- \n etc.),datacollectionconsistsonlyofrecordingpubliclyob-\n allyeditoutanysuchcontent(using Adobe Premiere Pro). \n servablebehaviorwithnomanipulationordirectinteraction\n Wealsoreviewallvideoforfacesofnon-participantsand \n with the participants, and this university’s IRB allows an\n personally-identifyinginformationsuchashousenumbers \n assumedwaiverofconsentforthoseparticipants.\n or license plates, and blurred these accordingly. We use \n Primarycontributors: Hyun Soo Park-leadcoordinator\n Pupil Labssoftwaretosynchronizeeyegazewiththevideo \n fordatacollection;Jayant Sharma-contributedtoparticipant\n foreachparticipant,andthenused Adobe Premiere Proto \n recruiting, data collection, IRB submission, analysis, and\n temporallysynchronizevideoacrossdifferentparticipants \n dataingestion. \n usingaudiotrackcomparison. \n Intotal,approximately 103 hoursofvideowerecollected National University of Singapore, Singapore: Partici-\n from 66 participants(42 female,23 male,1 non-binary;for pantswererecruitedfrom Singaporethroughadvertisements\n age, 46 were 20-29 years old, 14 were 30-39 years old, 1 on social media, via flyers and surveys, as well as from\n was 40-49,2 were 50-59,1 was 60-69,and 2 were 70-79). sourcingbytheprojectcoordinator. Residentsof Singapore\n 13 "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n aged 21 to 70 whocouldwearacamerawhileparticipating 44 twenties, 3 thirties, 2 forties, 6 fifties, and 1 sixties).\n in social sessions were eligible for inclusion in our study. Ourdatacollectionfocusesmainlyonsimultaneousvideo\n Duringtherecordingsession,theparticipantswererequired recording in groups of camera wearers within a common\n toattendsocialeventssuchasfamilygatherings,exercising setting. Thus,thesedatacaptureasinglesceneandsocial\n with a trainer, hairdressing, getting manicure, attending a interactionsfromdifferentpointsofview. Weincludeboth\n sessionforteachingassistants,attendingagroupmeeting, outdoorandindoorscenariosin Colombia. Outdoorscenar-\n etc. Thedevicesusedfordatacollectionwere Go Pro Hero 8, iosinclude Bogota´ and Cartagena’shistoricalandcolonial\n Go Pro Hero 9,and ARglasses. Go Procamerashavebinau- centers,asurbansettings,anda Natural National Parkand\n ralmicrophoneswhilethe ARglassescanonlyrecordmono astream,asruralsettings. Indoorlocationsincludeprofes-\n audio. Intotal,51 hoursofvideoswerecollectedfrom 40 sionalactivitiessuchaslaboratoryworkersandhairstylers.\n participants(25 malesand 15 females). Agegroupsinclude Furthermore, we include sports events such as salsa and\n 31 twenties,5 thirties,3 fifties,and 1 sixties. urbandancerehearsalsandrockclimbing.\n Primarycontributors: Mike Zheng Shou-leadcoordina- Primarycontributors: Cristina Gonza´lezand Paola Ruiz\n torfordatacollection;Eric Zhongcong Xu-contributedto Puentes. \n datacollection;Ruijie Tao-contributedtodatacollection. \n Carnegie Mellon University, Pittsburgh, PA, USA and\n Facebook Reality Labs (FRL), Redmond, WA, USA: Kigali,Rwanda: Carnegie Mellon University(CMU)Pitts-\n Participants were recruited from the Seattle area through burghgatheredalargeportionofitsdatafromskilledwork-\n a FRL-hiredvendorcompany. Intotal,therewere 400 hours ers such as carpenters, construction workers, landscapers,\n collectedfrom 206 unique participantsin 6 scenes staged mechanics,arborists,painters,andartists. Thisportionof\n in FRL’sresearchlabsin 2019. Theethnicgroupsinclude thedatasetdoesnotincludeanygraduatestudentswiththe\n 50.8% Caucasian, 28.2% African, 11.9% Asian and 9% explicitgoalofcapturingadiverserangeofreal-worldoccu-\n Hispanic. The staged environments include four types of pationalactivities. Over 500 hoursofvideowerecaptured\n apartments, a clothing store, and a grocery store. During inthe Pittsburgharea. Thedatawasmostlyrecordedusing\n therecordingsessions,theparticipantswereaskedtowear a Go Pro camera and a small portion was collected using\n Vuzixglassestogothroughthefollowingeverydayscenarios Wee View,awearablestereocamera.\n asnaturallyaspossible: groceryshopping,buyingclothes, Carnegie Mellon University Africa gathered data from\n watching TV,playingvideogames,listeningtomusic,danc- hobbyistcraftspeopleanddailyworkersworkingin Kigali,\n ing, weightlifting, stretching, readingemail, payingbills, Rwanda. Aneffortwasmadetocollectdatamostrepresen-\n onlinegaming,cooking,talkingwithotherpeople,meetings, tativeofhowtasksarecarriedoutin Rwanda(suchasdoing\n whiteboarding,andvideocalling. Theemailsandbillswere laundrymanuallyasopposedtowithawashingmachine).\n always mock data, not personal emails or bills of the par- Over 150 hours of video were captured, and a portion of\n ticipants. Thevideocallstookplacebetweenparticipants thosehoursareavailableinthecurrentrelease. Allofthe\n only. datawascollectedusinga Go Procamera. \n Three out of four apartments have corresponding 3 D Primarycontributors: Kris Kitani-projectcoordinator\n scans. Weusethestate-of-the-artdensereconstructionsys- forboth CMUPittsburghand CMUAfricavideocollection.\n tem[209]toobtainthe 3 Dphoto-realisticreconstructionof Sean Crane-leadcoordinatorof CMUPittsburghdatacol-\n thoseapartments. Volumetricrepresentationsareobtained lection (over 500 hours), main lead of CMU IRB review.\n from a customized capture rig and dense 3 D meshes are Abrham Gebreselasie-leadcoordinatorof CMUAfricadata\n extracted by the Marching Cubes algorithm with textures. collection. Qichen Fuand Xindi Wu-developmentofvideo\n We further annotate the dense meshes by labeling object de-identification pipeline, manual video de-identification\n categoriesoverthemeshpolygons;35 objectcategoriesplus annotationof CMUPittsburghdata. Vivek Roy-mainarchi-\n abackgroundclasslabelareusedinannotation. tectureofthelicensesigningwebserver,coordinatingwith\n Primarycontributors: Mingfei Yan,Richard Newcombe, America Web Developers. \n Kiran Somasundaram,Chao Li. \n Universityof Catania,Italy: Morethan 359 hoursofvideo\n Universidad de los Andes, Colombia: We gather 302.5 have been recorded from 57 different subjects recruited\n hoursacross 20 scenariosfrom 77 uniqueparticipants. We through word of mouth, starting from family members,\n recordvideosusing Go Pro Hero 9 camerasbetween Julyand friendsandacquaintancesofstudentsandfacultymembers\n August 2021. Werecruitvolunteerparticipantsfromwithin oftheresearchgroup. Videosarerelatedto 25 scenarios. We\n the Uniandescommunityandtheirfamiliesandfriends. The chosetheparticipantstocoverawidevarietyofprofessional\n ethnicgroupsinclude 89.9%Hispanic,1.4%African,and backgrounds(24 backgroundsincludingcarpenters,bakers,\n 5.8%Caucasian.Thegenderdistributionfollows 41.6%male employees,housewives,artists,andstudents)andages(sub-\n and 58.4%femalewithagesrangingfrom 18 to 65(6 teens, jectswereagedfrom 20 to 77,withanaverageageof 36.42).\n 14 "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n \n \n \n Baker > 9.5 hrsof videos Carpenter > 7 hrsof videos Crafting> 12 hrsof videos Bike Mechanic> 5.5 hrsof videos Bike Mechanic> 17.5 hrsof videos Scooter Mechanic> 9.5 hrsof videos Car Mechanic> 3.5 hrsof videos\n \n \n \n \n \n \n Figure 11.Matterport 3 Dscans(top)relatedtosevendifferentlocationscoupledwithsomevideos(bottom).\n \n \n 21 oftheparticipantswerefemale,whiletheremaining 36 inour Facebookadvertisementsorpostersincampusrestau-\n weremale. Femaleparticipantscollectedabout 137 hours rants and supermarkets. Each candidate participant was\n ofvideo,whereasmalescollected 222 hoursofvideo. The requiredtoregisterthroughanonlineform,whichcontained\n averagenumberofhoursofvideosacquiredbyeachpartic- anintroductiontoandrequirementsoftherecordingtask,\n ipantis 6 h:18 m:23 s,withaminimumnumberofhoursof andcollectedhis/herbasicdemographicinformation. The\n 06 m:34 s,andamaximumnumberofhoursof 15 h:40 m:42 s. participants’ ages range from 22 to 53. They come from\n Toprepareparticipantstorecordvideos,wedemonstrated 20 different countries, and about half are females. Many\n tothemtheoperationsofthecameraandhowtowearit. We participantsweregraduatestudentsandresearchers,while\n providedexamplesofvalidrecordingandinvalidrecordings othershadvariouskindsofoccupationssuchaschefs,facil-\n beforetheystartedtheacquisitionsession. Therecording itymanagers,andteachers.\n procedurewasdescribedinadocumentlefttothepartici- Inordertopreparetheparticipantsfortherecordingpro-\n pantstohelpthemrememberthedeviceusageandhowto cess,theteamdescribedindocumentsanddemonstratedto\n performagoodacquisition. Acquisitionofvideoshasbeen themtheoperationsofthecamera. Theteamalsoprovided\n performedusingdifferentmodelsof Go Procameras(Go Pro examples of what constitute valid and invalid recordings\n 4, Go Pro 7, Go Pro 8, and Go Pro Hero Max), which were beforetheystarted. Eachparticipantwasprovideda Go Pro\n handedovertotheparticipantswhotypicallyacquiredtheir mountablecamerawith 2 batteriesanda 512/256 GBSD\n videosautonomouslyoveraperiodofafewdaysorweeks. card. Each participant needed to choose at least 2 differ-\n 3 Dscans for 7 locations usingthe Matterport 3 Dscanner entactivitiesfromourscenariolistandrecord 1-10 hours\n havebeenalsocollected(Figure 11). ofvideowithin 2 days. Theuniversityteamwentthrough\n Primarycontributors: Giovanni Maria Farinellaand An- therecordingsaftertheparticipantsreturnedthecamerato\n tonino Furnari-scenarios,procedures,datacollectionover- checktheirqualityaswellastomakesurethevideosmeet\n sight, data formatting, encoding, metadata and ingestion. theuniversity’s IRBrequirements.\n Irene D’Ambra-datacollection,consentformsandinforma- Primarycontributors: Chen Zhao,Merey Ramazanova,\n tionsheets,manualdatareview,de-identificationoversight. Mengmeng Xu,and Bernard Ghanem.\n King Abdullah University of Science and Technology \n (KAUST), Saudi Arabia: A total of 453 hours of videos \n havebeencollectedfrom 66 uniqueparticipantsin 80 differ- \n entscenarioswith Go Pro Hero 7. Alltheparticipantswere \n KAUSTcommunitymembers,whoarefromvariouscoun- \n triesandhavevariousoccupations.Allrecordingstookplace \n inthe KAUSTuniversitycompound,whichis 3600 hectares \n inareawithdiversifiedfacilities(e.g.,sportscourts,super- \n markets, a 9-hole golf course, and 2 beaches) and scenes \n (e.g.,buildings,gardens,theredsea,andthedesert). There- \n fore,theteamwasabletocollectvideosofvariousscenarios \n suchassnorkeling,golfing,cycling,anddriving. \n The participants were recruited from multiple sources, \n such as friends and families, individuals referred to us by \n earlierparticipants,aswellaspeoplewhowereinterested \n 15 \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n \n B.De-identification Process \n The dataset has two types of video. The first includes \n videos recorded indoors where informed consent for cap- \n turingidentitiesisexplicitlycollectedfromallparticipants \n inthescene,includingfacesandvoice. Onlyvideoofthis \n type is used in our Audio-Visual Diarization and Social \n Interaction benchmark studies. All 400 hours of data col- \n lectedby Facebook Reality Labsfallsinthatcategory. The \n second category, which forms the majority of our videos, \n requiresde-identificationasconsentforcapturingidentities \n isnotgiven—includingfootagecapturedoutdoorsinpublic \n spaces.4 Onlyvideocollectedbytheuniversitiesfallsinto \n thissecondcategory. See Appendix Afordetailsaboutthe \n per-sitecollectionapproaches. Figure 12. CMU’sde-identificationpipeline\n B.1 De-identificationoverview \n bemanuallyidentifiedandblurredper-frame. Forthispart\n Allvideosinthesecondcategoryweremanuallyscreened ofourde-identificationprocess,weusedbothcommercial\n to address any de-identification needs, and are further di- toolswithintheabove-mentionedcommercialsoftwareand\n videdintotwogroups. Group 1: videosthatdonotcontain opensourcesoftware,including Computer Vision Annota-\n anypersonallyidentifiableinformation(PII).5 Thisiswhen tion Tool(CVAT)8,Anonymal 9 and Siam Mask 10.\n thevideoisrecordedindoorswithonepersonwearingthe \n Timecosts. Therelativetimecostswithrespecttotheorig-\n camera performing tasks such as cleaning or knitting for \n inalvideolengthvariedsignificantlyforthedifferentscenar-\n example,andno PIIispresentinthevideo. Thesevideos \n ios. Videoscapturedoutdoorscouldtake 10 xthelengthof\n didnotrequirede-identification. Group 2: videoswhere PII \n thevideotocarefullyredact. \n is captured. These include indoor settings with multiple \n participants present, PII captured accidentally such as an \n addressonanenvelopeorareflectionofthewearer’sfaceon B.2 Samplepipeline \n amirrororasurface,aswellasvideosrecordedoutdoorsina \n publicspacewherebystandersorcarsappearinthefootage. Whilepartnersfollowedvaryingpipelines,weofferasam-\n Videosin Group 2 weremarkedforde-identification,deploy- plepipelinetoshowcasetheprocessfollowedby Carnegie\n ingadvancedvideoredactionsoftware,opensourcetools, Mellon University that uses brighter.ai as the commercial\n andhoursofhumanreviewstoredactvisible PIIs.University software. Thissamplepipelineshowcasesthecombination\n partnersundertookthisde-identificationeffortfortheirown ofautomatedprocessesandhumanlaborwithrelativespeeds\n data. Wesummarizetheapproachbelow. ofthesesteps. \n Videos marked for redaction were processed through This semi-automatic de-identification process was per-\n de-identificationsoftwarethatremovesspecificidentifiers formedinfoursequentialstages(Figure 12): (1)automatic\n at scale. We used two commercial softwares: brighter.ai 6 faceandlicenseplatedetection,(2)falsepositiveremoval,\n and Primloc’s Secure Redact 7 thatenableddetectingfaces (3)negativedetectionhandling,and(4)imageblurring.\n and number plates automatically. We carefully reviewed \n Sensitiveobjectdetection Giventhecollectedvideos(raw\n all outputs from automated blurring, identifying both in- \n data),areviewerscansthroughvideosandmarksthosecon-\n stancesoffalsepositives(blurringthatmistakenlyoccurred \n tainingsensitiveobjectssuchashumanfaces,licenseplates,\n on non-privacy related items) or false negatives (inaccu- \n creditcards,etc.Thende-identificationsoftware(brighter.ai)\n rate or insufficient automated blurring of faces and num- \n wasusedtoautomaticallydetectsensitiveinformation.\n ber plates). Additionally, other PII data such as written \n names/addresses,phonescreens/passwordsortattooshadto False positive removal To improve the quality of the de-\n tection,falsepositiveswereremoved. Reviewersmanually\n 4 Theexceptionisdatafrom Universityof Minnesota,whose IRBper- \n scanned through the bounding boxes detected by the de-\n mittedrecordingofincidentalparticipantsinpublicspaceshavingnoma- \n nipulationordirectinteractionwithstudypersonnel. identificationsoftware,andrejectedthoseboundingboxes\n 5 Weusetheabbreviation PIItocapturedataprotectedundervarious whichdidnotcontainsensitiveinformation.\n dataprotectionregimesincludingthe General Data Protection Regulation \n (GDPR)wheretheterm“personaldata”isused. 8 https://github.com/openvinotoolkit/cvat\n 6 http://brighter.ai 9 https://github.com/ezelikman/anonymal \n 7 http://secureredact.co.uk 10 https://github.com/foolwood/Siam Mask \n 16 "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n Falsenegativecorrection Additionally,reviewersstudied \n everyvideotosearchforfalsenegativesandmanuallyan- \n notatedthemusingaboundingbox. Tomaketheprocess \n more efficient, an online object tracking algorithm [222] \n wasusedtogenerateboundingboxproposalsacrossframes. \n Reviewers verified that all tracked bounding boxes were \n correct. \n Imageblurring Onceallofthedetectionsweremodified \n and corrected, a robust blurring process was used to de- \n identifyimageregionsdefinedbytheboundingboxes. \n Timecosts Therelativetimecostswithrespecttotheorig- \n inal video length for each step are shown in Figure 12. \n Though this number depends greatly on the scenario cap- \n turedinthevideo,roughlyspeakingtode-identify 500 hours \n ofvideodata,ittook 780 hoursofmanuallabor. Review 1 \n of 500 hoursofvideorequired 250 hoursofwork,removal \n of false positive over 115 hours of video took 115 hours \n ofwork, Review 2 of 115 videostook 115 hoursofwork, \n correctingfalsenegativesin 35 hoursofvideosrequired 50 \n hoursofwork,and Review 3 of 500 hoursofvideotook 250 \n hoursofwork(250+115+115+50+250=780 hrs). \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 17 \n \n \n "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n \n \n C.Demographics US 39 \n China 10 \n Wefurtherprovideself-declaredinformationonethnic \n India 10 \n groupsand/orcountryofbirthbytheparticipants. Wereport \n Bangladesh 2 \n theseseparatelyperstate/countryduetothedifferencesin \n Vietnam 2 \n granularityofethnicgroupings.Allparticipantsareresidents \n Georgia,USA,Residents 100%ofparticipantsthatreside\n in the country specified per paragraph. This data is not \n in Georgia,USA,self-reportedtheirethnicgroupmember-\n availableforparticipantsfrom Minnesota,US. \n shipasfollows: \n United Kingdom Residents Reportingdemographicswas White/Caucasian 16 \n optionalandthus 63%ofparticipants(52/82)thatresidein Black/African American 1 \n the United Kingdomself-reportedtheirethnicgroupmem- Asian/Indian&White/Caucasian 1\n bershipasfollows: Other/Taiwanese 1 \n White—English,Welsh,Scottish,Northern Irishor British 35 Japan Residents 100%ofparticipantsthatresidein Japan\n White—Anyother Whitebackground 12 \n self-reportedtheirethnicgroupmembershipasfollows:\n Mixed—Whiteand Asian 1 \n Asian(Japanese) 81 \n Mixed—Anyother Mixedor Multipleethnicbackground 2 \n Arab 1 Kingdomof Saudi Arabia Residents 100%ofparticipants\n Prefernottosay 1 that reside in KSA self-reported their country of birth as\n follows: \n Italy Residents 100% of participants that reside in Italy \n China 12 \n self-reportedtheircountryofbirthasfollows: \n Russia 9 \n Italy 53 Colombia 8 \n Germany 1 Mexico 5 \n Russia 1 Kazakhstan 4 \n Portugal 1 India 4 \n Poland 1 US 4 \n Saudi Arabia 3 \n India Residents 100%ofparticipantsthatresidein India \n Kyrgyzstan 2 \n self-reportedtheirethnicgroupmembershipasfollows: \n New Zealand 2 \n Eastern India 10 Greece 2 \n Northern India 15 Ukraine 2 \n Southern India 108 Italy 2 \n Western India 5 Lebanon 1 \n Jordan 1 \n Egypt 1 \n Pennsylvania,USA,Residents 100%ofparticipantsthat \n Kashmir 1 \n residein Pennsylvania,USA,self-reportedtheirethnicgroup \n Portugal 1 \n membershipasfollows: \n South African 1 \n White 42 \n Thailand 1 \n Asian 4 \n Mixed—Whiteand Black African 2 Singapore Residents 100% of participants that reside\n Black,African,Caribbean 1 in Singapore self-reported their nationalities as follows:\n Chinese 26 \n Washington,US,Residents 100%ofparticipantsthatre- Singaporean 12 \n sidein Washington,USA,self-reportedtheirethnicgroup Indian 1 \n membershipasfollows: Malayan 1 \n Caucasian 101 Colombia Residents 90% of participants that reside in\n Blackor African American 58 Colombia self-reported their ethnic group membership as\n American Indian(Native American) 24 follows: \n Hispanic 19 Hispanic/Latin 62 \n Indian(South Asian) 4 White/Caucasian 4 \n Black,Africanor Caribbean 1 \n Indiana,US,Residents 95%ofparticipantsthatresidein Mixed-Whitean African 1 \n Indiana,US,self-reportedtheircountryofbirthasfollows: Prefernottosay 1 \n 18 "
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n \n \n Rwanda Residents 100% of participants that reside in \n Rwandaself-reportedtheirethnicgroupmembershipasfol- \n lows: \n Black,Africanor Caribbean 14 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 19 \n \n \n "
  },
  {
    "page_num": 20,
    "text": " \n \n \n \n \n \n D.Narrations D.2 Narrationanalysis \n Thegoalofthenarrationsistoobtainadensetemporally- Wepresentsomestatisticsonthecollectednarrations. Al-\n aligned textual description of what happens in the video, together, we collected 3.85 M sentences across the 3,670\n particularlyintermsoftheactivitiesandobjectinteractions hours of video. Figure 15 (left) shows the distribution of\n bythecamerawearer. The Ego 4 Dnarrationdataisitselfa frequencyofnarrationsacrossallvideosinthedataset. De-\n newresourceforlearningaboutlanguagegroundedinvisual pendingontheactivitiesdepicted,videosareannotatedat\n perception. Inaddition,asdescribedinthemainpaper,we varyingfrequencies. Forexample,avideoofapersonwatch-\n leveragethenarrationsasaformof“pre-annotation”toindex ing television is sparsely annotated as very few activities\n thevideosbysemanticterms. Specifically,thenarrationsare occur (0.17 sentences/minute), while a video of a person\n usedtoconstructactionandobjecttaxonomiestosupport harvesting crops, performing repetitive actions is densely\n variousbenchmarks,toidentifyvideosthatarerelevantto annotated(63.6 sentences/minute). Onaverage,therearean\n eachbenchmark,andtoselectregionswithinthevideosthat 13.2 sentencesperminuteofvideo.\n requireannotation. Figure 15 (middle and right) show the distribution of\n Thissectionoverviewshowweinstructedannotatorsto lengthofthecollectednarrations. Theindividualtimepoint\n narratethevideos,andhowwetransformednarrationtext narrationsareshort, highlightasingleactionorobjectin-\n intotaxonomiesofobjectsandactions. teraction,andhaveanaverageof 7.4 words. Thoughshort,\n thesenarrationscoveravarietyofactivitiesrangingfromob-\n D.1 Narrationinstructionsandcontent jectinteractions,tooluse,camerawearermotions,activities\n ofotherpeopleetc. Incontrast,thesummarynarrationsare\n We divide the dataset into clips of (max) 5 minutes long \n longer(onaverage,16.8 words)anddescribeactivitiesata\n whenacquiringnarrations.Each 5-minuteclipisthenpassed \n higherlevel. Table 2 showsafewtextexamplesofeachtype\n totwodifferentannotators,tocollecttwoindependentsets \n ofnarrationinadditiontothevisualexamplesin Figure 14.\n of narrations for every video clip in the dataset for better \n Finally, we study the diversity of the video dataset by\n coverageandtoaccountfornarrationerrors.11 Narratorsare \n lookingatthefrequencyofoccurrenceofwordsinthenarra-\n instructedtowatchthe 5 minutevideoclipfirst, andthen \n tionscollectedforvideosofeachscenariotype. Figure 16\n askedtoprovideashort 1-3 sentence“summary”narration \n showswordcloudsdepictingobjectsthatprominentlyfea-\n fortheentireclipthatcorrespondstotheoverallactivityand \n tureinacrossvariousscenarios. Thewordcloudshighlight\n settingofthevideoclip(e.g.,“thepersondoeslaundryin \n characteristicobjectsperscenario(e.g.,bowl,spoon,plate\n thewashingmachine”). Thesesummariesaremarkedwith \n in “Cooking” videos; card, dice, pawn in “Playing board\n thetag“#summary”inthereleasednarrations. \n games”videos)whilealsohintingatcommonobjectsacross\n Following this first screening, which is critical for the \n allscenarios(e.g.,hands,paper,phones). Thediversityin\n overallunderstandingoftheclip, thedensenarrationsare \n narrationscollectedhighlightsthediversityofvideocontent\n collectedasfollows.Annotatorsre-watchtheclip,pauseand \n capturedinthedataset. \n markthetimepointwhensomethinghappensinthevideo, \n thenenterashortnaturallanguagedescriptionoftheongoing \n D.3 Actionandobjecttaxonomy \n actionorinteraction,beforeresumingwatchingthevideo. \n Narratorsareprovidedthefollowingprompt:“Pretendas \n Intotaltherawnarrationsdescribethe Ego 4 Dvideousing\n youwatchthisvideothatyouarealsotalkingtoafriendon \n 1,772 uniqueverbsand 4,336 uniquenouns. Thedistribution\n thephone,andyouneedtodescribetoyourfriendeverything \n of the most frequently occurring verbs and nouns can be\n thatishappeninginthevideo. Yourfriendcannotseethe \n seenin Figure 17. \n video.”Thispromptisintendedtoelicitdetaileddescriptions \n Following ideas from [44], we leverage the narrations\n thatprovideaplay-by-playoftheaction. See Figure 13 for \n datatoconstructataxonomyovertheactionsandobjects\n anillustrationofthenarrationtoolinterface. Eachnarration \n thatappearinthevideo,asfollows. Weuseapart-of-speech\n thuscorrespondstoasingle,atomicactionorobjectinter- \n (POS)taggeranddependencyparsertoidentifyverbsand\n actionthatthecamerawearerperforms(e.g.,“#Copensthe \n nouns from each narrated action. We use an ensemble of\n washing-machine”or“#Cpicksupthedetergent”,wherethe \n parsermodelsfromthe Spacy[98]toolkittodothis. Given\n tag#Cdenotesthecamerawearer). Importantly,ournarra- \n a natural language narration, we first identify verbs using\n tionsalsocaptureinteractionsbetweenthecamera-wearer \n their POStag. Thenusingthedependencytree,weidentify\n and others in the scene, denoted by other letter tags, e.g. \n all direct objects of the verb. To ensure verbs and nouns\n #X(e.g. “#Cchecksmobilewhile#Xdrivesthecar”,“#C \n areaccuratelyparsed, weadoptseveralheuristics: Parsed\n passesacardto#Y”).See Figure 14 fornarrationexamples. \n verbsaresplitintomultiplesenses(e.g.,“turn”issplitinto\n 11 Wesimplykeepbothindependentnarrations; theyarenotmerged “turn-on”,“turn-off”and“turn-over”);compoundnounsare\n becausetheydonotserveasgroundtruthforanybenchmark. decomposed into a root noun coupled with a modifier to\n 20 "
  },
  {
    "page_num": 21,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 13. Narrationtoolinterface. Narratorsmarkatimepointwheresomethinghappensinthevideo(bottombar),andenteratext\n descriptionoftheactivity(leftsidebar). \n \n Objectinteraction Contextobjects Multi-personactions Manipulationactions \n #ccflipsthepaper #cctapsahandonthefloor #oamanxmovesthelegs. #cccutsaleaffromtheplantwithhislefthand.\n #ccliftsthet-shirt #ccholdsthewheelwithhislefthand. #oamanysitsonachair #ccpullshishandoffthechesspiece\n #ccdropstheplate #ccputsthebrushinthecolours. #oawomanxstepsforward. #ccholdstheknittingneedlewiththeotherhand\n #ccholdsthepieceofcloth #ccplacesplasticmodelskitonthetable #oapersonxhitsthecricketball #ccopensthescrewdrivercontainerwithhishands\n #ccfixesonthemodelcraft #ccarrangesthedoughsonthetray #oamanythrowstheballtowardsmanx #cctouchesthepieceofwoodwiththehand\n Camerawearermotion Summarynarrations \n #ccraiseshands cwasinaroom,fixedawoodmodelkit.#summary \n #ccstands ctightenedthemotorontheheadofthehoeofthelawnmower.ccutgrassesonthefieldwiththelawnmower.#summary\n #ccstandsupfromthestairs cwasinakitchen,hecutsausagesintopieceswithaknife,mixedthesausagesandcookedthemwithapan.#summary\n #ccwalksaroundakitchen cwasinthehouseandshestudied#summary \n #ccsitsup cstudiedinaroom.cwentthroughamobilephoneandamobiletabletwhilereadingintheroom.#summary\n Table 2.Textexamplesofnarrations.Thecollectednarrationsdescribediverseaspectsofhumanactivity.Summarynarrationscapture\n highleveldescriptionsofactivitiesina 5 minuteclip.See Figure 14 forvisualexamples.\n ensurethenountaxonomyisunambiguous(e.g.,modifier D.4 Narrationsforannotationprioritization\n “egg”androotnoun“shell”in“eggshell”);collectivenouns \n aremappedtotheirmainentity(e.g,. “pieceofcheese” \n → \n “cheese”). Finally,wemanuallyclustertheverbsandnouns \n toavoidredundancyinthetaxonomy(e.g.,“cut”,“chop”, \n Allvideosin Ego 4 Darenarrated,andsubsetsofthemare\n “slice”areallmappedtotheverbcluster“cut”). \n manuallylabeledforeachbenchmark. Ratherthanrandomly\n labelinstancesforagivenbenchmark,weaimtotargetthose\n thataremostrelevanttothetask. Forexample,videoslikely\n tocontainmulti-personconversationaremostinterestingfor\n the AVDiarizationbenchmark,whereasvideoswithample\n Theresultingtaxonomyconsistsofasetof 115 verbs( ) hand-objectinteractionaremostinterestingfor Handsand\n V \n andasetof 478 nouns( ). Figure 39 showsthedistribution Objects. Tothatend,weusethenarrationsandsummaries\n N \n ofverbsandnounsinasetofvideodataannotatedwiththe asatooltoautomaticallyprioritizecertainvideostolabel\n taxonomy. See Section J.2 fordetailsonhowthetaxonomy perbenchmark. Thebenchmarkappendicesbelowprovide\n isusedinthecontextofthebenchmarktasks. details. \n 21 "
  },
  {
    "page_num": 22,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 14.Examplenarrationsatkeyframesofvideo.#Creferstothecamera-wearer.Thelastrowshowsnarrationsthatincludeother\n peoplethatparticipateinactivitieswiththecamera-wearer(denotedbyotherlettertags,e.g.,#O,#X).\n \n \n D.5 Contributionsstatement \n Tushar Nagarajandevelopedthetaxonomy,helpeddevelop \n narrationinstructions,andperformedthenarrationanalysis \n presentedinthepaper.Kristen Graumandevelopednarration \n instructions,helpedcoordinatepilotsandannotationwork, \n andcontributedtotaxonomyformation. Michael Wrayco- \n developedthetaxonomy. \n \n 22 \n \n \n "
  },
  {
    "page_num": 23,
    "text": " \n \n \n \n \n \n \n 1400 \n \n 1200 \n 1000 \n 800 \n 600 \n 400 \n 200 \n 0 \n 0 20 40 60 80 \n # narrations / minute \n tnuoc \n Narrations per minute \n 1.4 \n 1.2 \n 1.0 \n 0.8 \n 0.6 \n 0.4 \n 0.2 \n 0.0 \n 0 10 20 30 40 50 \n # words \n tnuoc \n 1 e 6 Narration length distribution \n 2000 \n 1750 \n 1500 \n 1250 \n 1000 \n 750 \n 500 \n 250 \n 0 \n 0 10 20 30 40 50 60 70 \n # words \n tnuoc \n Summary narration length distribution\n Figure 15.Collectednarrationstatistics.Left:Distributionoffrequencyofnarrationscollected.Middleandright:Thedistributionof\n lengthofthecollectednarrationsandsummaries. Summariesarenaturallylonger,anddescribeactivitiesatahigherlevelcomparedto\n individualactionnarrations.Seetextfordiscussion. \n Figure 16.Distributionofobjectsinnarrationsofvideosfromeightcommonscenarios.Thevarietyofobjectscoveredacrossscenarios\n showcasesthediversityofactivitiesinthevideocollected. \n kcip tup pord ecalp dloh evom evomer ekat tsujda hcuot nepo tuc llup tfil ruop worht epiw hsup ssap xif yrrac nrut naelc esolc hsaw yalp sserp tih egnarra dlof rits bur poocs tniap pid ekahs esnir tresni esu no_nrut nruter esiar daerps tcelloc etarepo revoc klaw hcterts kcehc barg ylppa llor evig kcap gnah nethgit pilf erusaem ekam xim refsnart pots hcatta daer pat gard gnirb ngila etator rewol tfihs dda nethgiarts yarps eit hctarcs tink tniop tae ezeeuqs eunitnoc nehtooms hctaw wes burcs etarapes tif rehtag raew tcennoc tcepsni eparcs leep dnah nesool egnahcxe tser dlofnu llird pohc tes kram nioj kool peews nam erats raet ward hctiws egnahc evird kcits kaerb eraperp ffo_nrut elknirps esaeler mirt wercs yal yrt enimaxe parw wohs knird tel tsud peek elffuhs edir llorcs kooc tnuoc wolb evael hcated od wercsnu noitisop ecils yrd hsurb no_hctiws gniws wardhtiw dneb trats hctef bmilc\n 105 \n 104 \n 103 \n secnatsni \n # \n dnah drac repap doow htolc elttob reniatnoc lwob hguod hsurb enohp noops etalp dil retaw eceip efink draob revoc puc koob gab daerht top doof kcits nap yart wercs enihcam xob nosrep cirbaf eriw tniap llab ehtolc tlob noino tekcub lewot epat tun elbac nolyn taem nac knalp emag tnalp epip ssalg elbategev nep latem rood rossics ruolf trid lian gel lios nit kcitspohc tfarc reppep ledom egnops elzzup epor regnif hcnerw aremac otatop nug evael ssarg loot eulg trihs nray eldeen relur rennaps lairetam kcirb eht reward llaw gge draobdrac rewom elbat reilp ebut nip remmah lio notrac elit llird yalc eohs daerb revirdwercs nottub tehcas kcap ecid licnep kcolb hsid evolg redloh revird teksab hcnarb rettuc meti rewolf tekcap enots paos erusaem alutaps guj eveis llor otamot pac rebmit dnas lewort torrac dor leehw raj pat rellor rab krof tam rac edalb roolf egap tnemec tiurf egabbac rebmucuc\n 104 \n Figure 17.Narrationverb/noundistribution.Distributionofautomaticallyextractedverbs(top)andnouns(bottom)fromnarrations.Top\n 150 mostfrequentlyoccurringofeachisshownforclarity. \n 23 "
  },
  {
    "page_num": 24,
    "text": " \n \n \n \n \n \n Numhours Numclips Avgcliplength \n EMVQ-2 D 432.9 5,831 6.1 min \n EMVQ-3 D 13 159 4.9 min \n EMMoments 328.7 2,522 7.9 min \n EMNLQ 227.1 1,659 8.2 min \n Hands+Obj. 196.2 88,585 8.0 sec \n Forecasting 110.5 1,498 4.4 min \n AVD 47.7 572 5 min \n Social 47.7 572 5 min \n Table 3.Amountofannotateddataforeachbenchmark.EMrefers \n to Episodic Memoryand AVDrefersto Audio-Visual Diarization. \n All 3,670 hoursofvideohavenarrationsandfeatures. \n E.Benchmark Data Splits \n Foreachbenchmarktask,certainportionsofthe Ego 4 D \n videorepositoryarelabeled. Table 3 showsthebreakdown \n of theamount ofdata annotated foreach. Notethat there \n are 764 totalhoursofvideorelevanttothe AVDand Social \n tasks(i.e.,haveaudio,conversation,andunblurredfaces), \n includingtheannotatedsetof 47.7 hoursabove. Forother \n benchmarks,therelevancehasasofterdependencyonthe \n specificvideocontent(e.g.,amemoryquerycanapplytoany \n ofthe 3,670 hours). Thefollowingappendiceswillexplain \n howwesampleddatatobeannotatedforeachbenchmark. \n Forthepublic Ego 4 Dbenchmarkchallenge,weensure \n thatthesplitsareconsistentwithinafamilyofrelatedtasks. \n Forinstance,allthe Forecastingand Hands+Objectstasks \n sharethesamesplitsandensuretrainingvideosinonedonot \n occurasvalidationvideosinanother. Similarly,the Episodic \n Memorytaskssharethesamesplits. However,itisharder \n toensurethisacrossverydifferenttasks, sincethevideos \n selectedforannotationsaredifferent. Forexample,the So- \n cialbenchmarkconsidersmulti-personinteractionswhich \n maynothavemanyhand-objectinteractions;hencetheset \n ofvideoslabeledfor Socialand Hands+Objectshavelittle \n overlapandthetrain/val/testsplitsarenaturallydifferent. \n Sinceweplantousethetestsetforthepublicchallenge, \n wearewithholdingallthetestannotationsandmakingthem \n accessible only through a submission server. We are also \n withholdingthenarrationsthatoverlapwithanyofthetest \n sets. \n \n \n \n \n \n \n \n \n \n \n 24 \n \n \n "
  },
  {
    "page_num": 25,
    "text": " \n \n \n \n \n \n F.Episodic Memory Benchmark andnotfactualqueries,i.e.,queriesthatrequireanexternal\n knowledgebasetoanswer. \n Thissectiondetailsthe Episodic Memorybenchmarktask \n NLQisachallengingmultimodaltaskrequiringvisual\n definitions,annotations,baselinemodels,andresults. \n and linguistic understanding and reasoning. Consider the\n query “What did I pick up before leaving the party?” In\n F.1 Formaltaskdefinitions ordertofulfillthisrequest,thesystemneedsto: (a)break\n downandunderstandthelanguagequeryasasearchforan\n As presented in the main paper, there are three kinds of \n object(what)withwhichtheuserinteracted(pickup)before\n Episodic Memory queries—visual, natural language, and \n anevent(leavingtheparty),(b)gothroughtheegocentric\n moments—eachofwhichrequireslocalizingtheresponsein \n videoandidentifythedesiredeventof“leavingtheparty”,\n thevideo. Theirformaldefinitionsareasfollows. \n (c)visuallysearchfortheobjectwithwhichtheuserinter-\n Visualqueries(VQ) Thistaskaimstoqueryanegocentric acted prior to this event. This example demonstrates the\n videobasedonastaticimagecropofanobject. Specifically, complexity of NLQfrom bothvisual(recognizingevents,\n it asks the question ‘Where was object X last seen in the objects,places,etc.)andlinguistic(breakingdownreason-\n video?’,where Xisasingle‘canonical’imagecropinwhich ing,understandingrelations,etc.)perspective. Inaddition,\n theobjectisclearlyvisibleandhuman-identifiable. Apo- thediversesetofquerieswithin NLQ,whilefacilitatinga\n tential use case for visual queries is where a user teaches flexiblesearchandretrievalthroughanintuitiveinterfaceof\n thesystemanewobjectbyshowingaphoto(“thesearemy language,alsoincreasesthecomplexityofthetask.\n keys”) and then later queries for it among past video. By Concretely, NLQ is formulated as follows: Given an\n enablingvisualqueries,asopposedtocategoricalqueries, egocentricvideo andanaturallanguagequery ,thegoal\n V Q \n thisisaformofopen-worldobjectlocalization. isagaintoidentifya‘responsetrack’r,suchthattheanswer\n Weformulatetheproblemasfollows.Givenanegocentric to can be deduced from r. The response track should\n Q \n video ,aqueryobjectospecifiedviaastaticvisualcrop beasetoftemporallycontiguousframeswithin . Given\n V V \n v, and a query frame q, the goal is to identify when the the episodic nature of our task, r should be sufficient to\n objectowaslastseeninthevideobeforethequeryframeq. answer ,withouttheadditionalneedfor oranyexternal\n Q V \n Theresponseisspecifiedasa‘responsetrack’rwhichisa knowledgebases. \n temporallycontiguoussetofboundingboxessurrounding \n Moments queries (MQ) This task aims to query an ego-\n theobjectoineachframe: \n centricvideobasedonacategoryofactions. Specifically,it\n posesthefollowingrequest‘Retrieveallthemomentsthat I\n r = r ,r , ,r ,r , (1) \n s s+1 e−1 e \n { ··· } do Xinthevideo.’,where‘X’comesfromapre-definedtax-\n onomyofactioncategories,suchas‘interactwithsomeone’\n wheresistheframewheretheobjecto(atleastpartially) \n or‘usephone’. Comparedtothenaturallanguagequeries,\n entersthecamera-wearer’sfieldofview,eistheframewhere \n themomentqueriesfocusondaily-lifeactionsoractivities.\n theobjectexitsthecamera-wearer’sfieldofview,andr isa \n i \n Onemomentquerycancorrespondtomultipleresponsein-\n boundingbox(x,y,w,h)inframei. Iftheobjectappears \n stances(temporalwindows)inthevideo. Thistaskprovides\n multipletimesinthevideo,theresponseonlyreferstothe \n theuserafastandconvenientwaytoretrievemultipleaction\n ‘mostrecentoccurrence’oftheobjectinthepast, i.e., the \n momentsatatime,wheretheuserdoesnotneedtocomeup\n responsetrackwhichminimizesq r withq >r . \n e e \n − withasentencetodescribewhathe/shewants,butinstead\n Whena 3 Dscanoftheenvironmentassociatedwiththe \n candirectlychooseamongthepre-definedcategories.\n videoisavailable,theresponseadditionallyincludesa 3 D \n Themomentqueriestaskisrelatedtothetaskoftemporal\n displacementvector∆d = (∆x,∆y,∆z)betweenthe 3 D \n actiondetection[141,229,237],whichaimstoidentifyand\n locationwherethequerywasmade(i.e.,atqueryframeq), \n localizeallinstancesofallactioncategoriesthattakeplace\n andthe 3 Dlocationintheenvironmentwheretheobjectwas \n inavideo. Bothtaskshavealistofactioncategoriespre-\n lastseen(i.e.,attheendoftheresponsetrackr ). \n e \n defined, andbothaimtopredictmultipleactioninstances\n Naturallanguagequeries(NLQ) Themotivationbehind with their temporal boundaries. The difference is that 1)\n the NLQtaskistoenablesearchingthroughanegocentric our moment queries task is a retrieval task where action\n videousinganaturallanguagequery. Thesystemresponds categoriesareprovidedasqueries,meaningitdoesnotneed\n toaquerybyprovidingatemporalwindowlocalizedinthe to produce instances of categories that are not among the\n video,fromwhichtheanswertothequerycanbededuced. queries; and 2)ourmomentstaxonomyisspecifictofirst-\n Thesequeriescanberelatedtoobjects,places,people,and personactivity. Weaimformomentsthatareactivitiesat\n activitiesthatappearedintheepisodicmemoryoftheuser. amediumlevelofgranularity—coarserthantheactionsin\n Note that we only consider episodic queries, i.e., queries Forecasting,andfinerthanthe“scenario”labelsshownin\n thatcanbeanswered/deducedfromtheegocentricvideos, Figure 3 ofthemainpaper. \n 25 "
  },
  {
    "page_num": 26,
    "text": " \n \n \n \n \n \n Navigationverbsforentropy-basedvideoselection Toselectvideosbasedontheseconsiderations, weuse\n atwo-stepprocess. First,wefilteroutvideosbasedonthe\n appear ascend bend bring carry catch \n climb close come descend dig dispose associated‘scenario‘labels(see Figure 3)thatprovidehigh-\n drag dribble drop enter fall fetch levelinformationaboutthecontentandactivitiesinvideos\n find fly gather get give grab (e.g.,cooking,cleaning,golfing,etc.). Wemanuallypreview\n hang jog jump kick lean leave \n randomly sampled videos from each scenario to identify\n lift lower move navigate open propel \n interesting scenarios such as cooking, indoor navigation,\n raise return ride rise run shut \n farmer,cleaning,andgroceryshopping. Wethensortvideos\n steer step turn vaccum walk \n withineachscenariobasedonascoringfunctionusingthe\n Table 4.Weprioritizevideostoannotateforvisualqueriesbased narrationsforthevideo. Specifically,weextractthelistof\n ontheentropyofthesenavigation-relatedverbsinthenarrations. verbsinthenarrations(alongwiththeirfrequencies). We\n then measure the entropy of the distribution of manually\n curated navigation verbs (See Tab. 4). The video is more\n The MQtaskisalsorelatedtotemporallanguageground- likelytoallowchallengingvisualqueriesifitsnavigation\n inginvideos[236],whichaimstoretrieveasegmentfrom entropy is higher. For videos with near-zero entropy, we\n a video, as queried by a natural language sentence. Both observethatthecamera-wearerisusuallystayingstaticin\n taskshaveaqueryandaimtopredictcorrespondingtempo- asinglelocationwithoutanymovement. Finally,alimited\n ral segments. The difference is that MQ uses pre-defined numberof 3 Dscanswereavailableforthe 3 Dlocalization\n querycategoriesratherthannaturallanguagesentences,and task. Videos associated with these scans were prioritized,\n onequerycancorrespondtomultipleinstancesratherthana regardlessoftheirnavigationentropy,insupportofthe 3 D\n uniqueone. responseversionofthe VQtask. \n Weformulatetheproblemasfollows. Givenanegocen- \n Naturallanguagequeries For NLQweapplysimilarsam-\n tric video , and a query action category c, the goal is to \n V pling criteria as above for VQ, but augment it to avoid\n retrievealltheinstancesofthisactioncategoryinthevideo, \n repetitiveactions(e.g.,sewingwhilesittingonthecouch).\n assuming that the query is made at the end of the video. \n First,wemanuallyselectamenablescenarios(see Figure 3).\n Theresponseisasetofactioninstancesofthecategoryc \n N Amongthose,weprioritizeclipswithhighentropycomputed\n Φ = φ =(t ,t ,s ) , where n is the number \n c { n n,s n,e n }n=1 overnavigationaltermsasabove. Finally,weprioritizenon-\n of instances for this category, t and t are start time \n n,s n,e \n repetitiveactionsbycomputingtheratioofthenumberof\n andendtimeofthenth instancerespectively,ands isits \n n \n unique verbs in a clip’s narration vs. the total number of\n predictionconfidence. \n verbsinthatsamenarration—higherisbetter. \n Momentsqueries Toselectclipsformomentsqueries,we\n F.2 Selectingclipsforannotation \n computetheoverlapofverbs/nounswiththemomentstax-\n Forallbenchmarkswesamplevideoclipstoannotatebased onomy. Wecalculateasimilarentropy-basedscoreandsort\n on criteria for geographic diversity and scenario diversity. videosaccordingtothisscore. Inaddition,werestrictvideos\n For Episodic Memoryweimposeadditionalsamplingcrite- toafixedsetofcategoriespresentinourtaxonomytoavoid\n riameanttohighlightdatamostinterestingforthetask,as labelingvideosthatdonotcontainrelevantactivities.\n follows. \n F.3 Annotation \n Visual queries Video clips to annotate for visual queries \n (VQ)areselectedbasedonthefrequencyofobjectoccur- Nextwedescribetheannotationproceduresandoutputsfor\n rences and amount of navigation in the video. To have Episodic Memory. \n interestingvisualqueriesinavideo,theremustbeseveral \n Visualqueries Forannotatingvisualqueries,wefirstsam-\n ‘interesting’ objects that can be queried about. An object \n plecontiguousclipsofvaryinglengths(5 mins,8 mins,and\n is‘interesting’inthecontextofvisualqueriesifthereisa \n 16 mins)fromthesetofinterestingvideos. Theannotators\n sufficientlyhighseparationinspaceandtimebetweenany \n are instructed to create and annotate 3 visual queries for\n twooccurrencesoftheobject. Thistypicallyhappenswhen \n each clip. A visual query consists of the query frame q,\n thecamera-wearervisitsthelocationneartheobjectbriefly, \n thevisualcropv ofthequeryobjecto, theresponsetrack\n and then navigates elsewhere before revisiting the object \n r = r ,r , ,r ,r , and a textual name for the\n s s+1 e−1 e \n again. Forexample,considerapersonwhofinishescleaning { ··· } \n object(eg. cup,hammer,broomstick,etc). Theannotators\n a living room, visits the kitchen for some period of time \n performedthefollowingstepstoannotateagivenclip:\n beforerevisitingthelivingroomagain. Mostobjectsinthe \n livingroomareinterestingtoqueryaboutwhentheperson 1. Identifythreeinterestingqueryobjectsintheclip. An\n isinthekitchen. objectisinterestingifitoccursinatleasttwodifferent\n 26 "
  },
  {
    "page_num": 27,
    "text": " \n \n \n \n \n \n partsofthevideo. In order to validate an annotation we collect two 3 D\n bounding boxes per query from two different annotators.\n 2. For a given object, enter a textual name. While our \n Leveragingthetwoboxeswecomputethefollowingvalida-\n currenttaskquerieswiththeimagecrop,notthename, \n tionmetrics: \n thisannotationwillallowfuturevariantsthatdoquery \n fortheobjectbyname. c c \n 1 2 2 \n d = (cid:107) − (cid:107) (3)\n norm \n m \n 3. Selectoneoftheobjectoccurrencesinthevideoand diag \n V \n mark a visual crop v = (x v ,y v ,w v ,h v ). The visual V = global , (4) \n norm \n cropmustbeagoodrepresentativeviewoftheobject, V union \n anditmusthavegoodlighting,large-enoughsize,and \n wherec andc arethecentroidsofthetwoboxes,m is\n mustnotbeblurred. 1 2 diag \n theaveragediagonallengthofthetwoboxes,V isthe\n global \n 4. Mark a different occurrence of the object as the re- volumeofthe 3 Dconvexhullofthetwoboxes,and V union\n sponse track r = r , ,r . The response track is the volume of the union of the two boxes. These met-\n s e \n { ··· } \n starts from the frame when the object is first visible ricsmeasuretheagreementlevelbetwenthetwoannotators.\n andendswhentheobjectleavesthefield-of-view. The Whenthetwoannotationsareperfectlyaligned,themetrics\n responsetrackmustalsobecontiguousintimeandthe areequaltod norm =0 and V norm =1.0. Theassumption\n boundingboxesmustaccuratelymarkthepositionand isthatifthetwoannotatorsagreeontheposition,scale,and\n sizeoftheobject. orientationoftheboundingboxthenitislikelytobecorrect.\n Ifthetwoannotationsarefarfromeachotherwewilldiscard\n 5. Thequeryframeq issampledsometimeafterthere- \n the query. There are a couple of reasons that can explain\n sponsetrackr. Theobjectomustnotappearanywhere \n suchcase: (1)oneannotatormislabeledthequery,(2)the\n between the response track r and the query frame q, \n queryishardtoannotate. Somequeriesrequireasignificant\n sothatthegroundtruthiswell-definedanduniquefor \n amount of hallucination to retrieve the object location in\n “whendid Ilastsee...?”. \n thescanwhichclearlyleadstosubjectiveannotations. We\n empiricallydefinedtwothresholdsof 1.5 over d and\n For each annotation, we apply automated and manual norm \n 15 over V to filter out poor annotations. Any query\n qualitycheckstoensurecorrectness. Incasethequalityfalls norm \n thathaseitheroneofthetwometricsabovethethresholdof\n belowacertainthreshold,theclipisreannotated. \n acceptanceisrejected. \n For visual queries associated with 3 D scans, we also \n collect 3 Dannotationsintheformof 3 Dboundingboxes \n Naturallanguagequeries Tocollect NLQannotations,we\n capturingwheretheobjectwaslastseen. Wethenusethose \n sample contiguous clips of length 8 minutes and 20 min-\n boundingboxestoestablishthegroundtruthdisplacement \n utes. Theannotatorsareinstructedtowatchtheseclipsand\n vectorfromthequeryframetotheobject,whichisthetarget \n generatenaturallanguagequeries,focusedonretrievingin-\n of the task. Each annotation a is collected in the scan \n q \n formationaboutobjects,places,andpeopleintheegocentric\n coordinatesystems: \n videoclips. Toreducethecognitiveoverloadontheanno-\n T =[R t ], (2) tators,andfocustheireffortsonmemory-relevantqueries,\n s s s \n | wealsoprovidealistof 13 querytemplates(see Table 5),\n whereq 1,..., , thetotalnumberofqueries,and correspondingtoqueriesausermightasktoaugmenttheir\n where T s \n ∈ {R 4 isth Q \n e \n } \n tra \n Q \n nsformationmatrixofthebounding memory. Note that these templates are provided only to\n ∈ \n box.R s andt s arethecorrespondingrotationandtranslation guidetheirchoiceofquery,anddoesnotlimitthelinguistic\n forannotationa q . variabilitysincetheannotatorsareinstructedtoparaphrase\n Theannotationprocedureisdefinedasfollows: Aquery thetemplatewithoutcopyingthemasis.\n consistsofavideoclip,avisualcrop,andaresponsetrack. To elaborate, the annotators performed the following\n Foreachquery,thegoalistoretrieveinthescanthelocation steps: \n of the object defined in the video. Once the location is \n found,wedrawa 3 Dboundingboxatthispositionwiththe 1. Watchtheentirevideoclip inordertounderstandthe\n V \n appropriatescaleandorientation. Itisimportanttonotethat high-levelcontext(optionallyin 2 fast-forward),\n × \n 3 Dscansandvideoshavebeenrecordedatdifferenttimes. \n Therefore,itislikelythatanobjectatacertainlocationin 2. Pickaquerytemplatefromtheavailablelistandpara-\n thevideowillnotbepresentatthatsamelocationinthe 3 D phrase/reword the query to obtain , e.g., template\n Q \n scan. Insuchcases, weasktheannotatortohallucinatea ‘Wherewasobject Xbefore/afterevent Y?’canbepara-\n 3 Dboundingboxinthe 3 Dscanatthepositionofthetarget phrasedas‘Wherewasthebluebucketpriortomydog\n objectdefinedinthevideo. exitingthelivingroom?’ \n 27 "
  },
  {
    "page_num": 28,
    "text": " \n \n \n \n \n \n Category Template resultofmomentarygazeshiftarestillconsideredtobe\n contiguous. \n Whereisobject Xbefore/afterevent Y? \n Whereisobject X? \n • Foragivenquery,iftherearemultiplenon-contiguous\n Whatdid Iputin X? \n temporalwindows(separatedbymorethan 3 seconds)\n Howmany X’s?(quantityquestion) \n asindependentlyvalidanswers,weinstructtheanno-\n Objects What Xdid IY? \n tatorstoeitherdiscardthequeryandcreateadifferent\n Inwhatlocationdid Iseeobject X? \n one,oraddmoredetailstothewordingtomakeitmore\n What Xis Y? \n specific. Similarly,queriesthatrequiremultipletem-\n Stateofanobject \n poralwindows(separatedbymorethan 3 seconds)to\n Whereismyobject X? \n deducetheanswerarealsodisallowed. Forexample,\n Place Wheredid Iput X? \n ‘Howmanyshirtsdid Ipackinmysuitcase?”isinvalid\n Whodid Iinteractwithwhen Ididactivity X? ifpackinghappensacrossmultipletemporalwindows,\n People Whodid Italktoinlocation X? separatedbymorethan 3 seconds(e.g.,theuserpauses\n Whendid Iinteractwithpersonwithrole X? tomakecoffee,andthenreturnstopacking).\n Table 5. The NLQtemplatescaptureadiversesetofqueriesthat • We encourage diversity by instructing that the query\n humanscanasktoaugmenttheirmemoryandrecollectobjects, responsesnotbeconcentratedatonepartofthevideo\n places,andpeopleintheireverydayexperience. clip,oraroundfewobjects/places/people. Inaddition,\n wealsodisallowthequeryresponsewindowtobemore\n than 50%ofthetotalcliplength. \n 3. Findthetemporalwindowwheretheresponsetothe \n natural language query can be deduced visually, and • Finally,queriesthatrequirereasoningandknowledge\n annotateitasr. on top of visual evidence are invalid. For instance,\n ‘Whatcountry‘sflagwashangingonthewall?”isin-\n Duringourdatacollection,wealsorequestedtheannota- validwhile‘Wherewastheflagthatwashangingon\n torstomarktheslotvaluesandcorrespondingverbs,forthe thewall?”isvalid.Similarly,queriesthatguessthemo-\n selectedlanguagequerytemplates. Whilewedonotusethis \n tivationorintentionsoftheuserorpeopleinthevideo\n informationforourtask, itmaybeusefulforotherfuture clip are also not allowed. As an example, ‘Why did\n research. thepersonatthedoorleaveapackageontheporch?’\n The desiderata for the collected queries are as follows. isdisallowedwhile‘Whatdidthepersonleaveonthe\n Theyshould: (a)reflecttheunderlyingmotivationofaug- porch?’ isaccepted. \n mentinghumanmemory,(b)berichanddiverseintermsof \n languageandtheobjects,places,people,andevents,and,(c) After the annotation process, we apply both automatic\n bechallengingenoughforanintelligentsystembutnottoo and manual quality checks, including the diversity of lan-\n complicatedorconvolutedtoreducethenaturalnessofthe guagequeriesandtemporalwindowlocations,toscorethe\n queries. Forinstance,thoughaquerylike‘Whatwasplaying annotations. Iftheoverallqualityscoreisbelowathreshold,\n onthetelevisionwhen Iwasfoldingmyseventh T-shirtafter theclipisre-annotated. \n my dog exited the room?’ is challenging from a learning \n Momentsqueries Toannotatemomentsqueries,wesam-\n perspective,itisnotnaturalfromanapplicationstandpoint. \n plecontiguousclipsof 8 minutesfromthesetofinteresting\n Inordertoensuretheabovequalitiesfor NLQ,weenforce \n moments videos. The annotators are instructed to mark\n thefollowingconstraints: \n instancesofactivitieswithatemporalwindowandtheactiv-\n • Allparaphrasedlanguagequeriesmustbeinpasttense, ity’snamefromafixedtaxonomyofactivities.Wehaveeach\n and must be posed as questions asked at the end of instancelabeledbythreeindependentannotators. Byassum-\n theentirevideoclip. Thisresemblesthereal-lifesce- ingeachannotatorisreliable,wetaketheunionofmoments\n narioofqueryingaboutepisodicmemory(past)ofthe acrossannotatorstoensurecompletenessofannotations.\n user,andresolvesambiguitywhentherearemultiple Thetaxonomywascreatedsemi-automaticallyfromthe\n occurrencesofanobjecttothethelastrelevantone. narrations. Specifically,weusethesummarynarrationscol-\n lectedforfive-minuteclipsegments,astheycapturehigher-\n • Toaccountformomentaryshiftsofviewfortheegocen- leveleventsandactivitiesthataresuitableforthemoments\n tricvideo,weallowsmallinterruptions(<3 seconds) retrievaltask. Thisisincontrasttotheverb-nountaxonomy\n betweenthetrulyrelevantframesforagivenquery. In thatissourcedfromindividualnarrationsforeachatomic\n otherwords,frameswheretheobject/person/placeof action, which are used in the Forecasting and Hands and\n interestgoesoutofviewforlessthan 3 secondsasa Objectsbenchmarks(see Appendices Gand J).\n 28 "
  },
  {
    "page_num": 29,
    "text": " \n \n \n \n \n \n Thetaxonomywascreatedasfollows. First,eachsum- Split Train Val Test \n mary narration was encoded into a feature vector using a \n #videohours 262(19) 87(5) 84(9) \n pre-trained BERT[51]languagemodel,andthenconcate- \n #clips 3.6 k(164) 1.2 k(44) 1.1 k(69) \n natedwiththewordembeddingsforthemainverbandnoun #queries 13.6 k(604) 4.5 k(164) 4.4 k(264)\n extractedfromthesummary. Thesesummarieswerethen \n clustered into groups, and then labels were manually as- \n Table 6. Visualqueriesdatasetstatistics. Thenumbersinthe\n signed to groups based on the coherent activities they de- \n paranthesescorrespondtothesubsetofdatausedfor 3 Dlocaliza-\n scribed. \n tion,wherewefocusonvideosforwhichwehave Matterport 3 D\n Notethatthisprocesswasdoneindependentlyforaset scans. \n ofscenariosthatweselectedbasedonhowfrequentlythey \n occurinthedataset,thediversityofactivitiestheyrepresent, \n Split Train Val Test \n andhowlikelytheycontainhigh-level,event-likeactivities. \n For example videos that primary involve a single activity #videohours 136 45 46\n like“driving”arenotinterestingcategoriesinthiscontext, #clips 1.0 k 0.3 k 0.3 k \n whereas“householdcleaning”containsseveraldifferentac- #queries 11.3 k 3.9 k 4.0 k\n tivitiesthataresharedacrossotherindoortasks,makingitan \n appropriatescenario. Intotal,weselectvideosfrom 5 sce- Table 7.NLQdatasetstatisticsacrossthetrain/val/testsplits.\n nariostocreateourmomentstaxonomy: Cooking,Cleaning, \n Shopping,Handyman,Farmer/Gardener. Eachannotationis \n throughouttheimage,withveryfewboundingboxesanno-\n intheformatof(starttime,endtime,label). \n tatedatthetop 10%oftheimage(see Figure 22,right). Our\n analyses indicate that there may be a potential bias in the\n F.4 Data Analysis \n firsttwomeasures,whiletheboundingboxespositionsare\n largelyunbiased. \n Wenowoverviewthestatisticsoftheannotationsperquery \n Forthe 3 Dlocalizationtask,weannotateasubsetof 1,043\n type. \n visualquerieswith 3 Dannotations. Thesecompriseof 13\n Visualqueries The VQannotationsconsistofsamplesfrom videohoursassociatedwith 4 scansfromthe Universityof\n a diverse set of scenarios and universities (see Figure 20 Catania(UNICT). \n and 21). In total, 433 hours of videos are annotated with \n Naturallanguagequeries Asoutlinedin Table 7,the NLQ\n 22,602 visualqueries. Thesevideosaresampledfrom 10 \n annotations are from 227 hours of video, with a total of\n universitiesandconsistof 54 scenarios. Thestatisticsover \n 19.2 K queries spanning the selected 13 query templates.\n thetrain/val/testsplitsareprovidedin Table 6. Weensured \n Theassociatedvideoclipscomefrom 10 differentuniversi-\n thatthesplitscontainadisjointsetofvideos. Tolookfor \n tieswithatotalof 34 scenarios(withatleast 1 hourofvideo\n possiblebiasesinthedata,weplotthedistributionoverthree \n annotated). Similartoothertaskswithintheepisodicmem-\n measures. \n ory,weensurethatthetrain/val/testsplits(60%,20%,20%)\n 1)Querytoresponseseparationisthetemporaldistance \n contain a disjoint set of video clips. We further analyze\n (in frames) between the query frame and the end of the \n the data through: (a) Distribution over template queries,\n responsetrack. Thismeasureshowfarbackintimeanalgo- \n shownin Figure 24. Thechallenging‘Whereisobject Xbe-\n rithmneedstosearchinordertofindthequeryobject. \n fore/afterevent Y?’isthemostpopulartemplatewitharound\n 2)Responsetracksizemeasuresthetemporallengthofthe \n 3 K queries,withareasonabledistributionoverothertem-\n responsetrack. \n plates. Overall, thequeriesin NLQhave 8.3 2.1 words\n 3)Responsebboxpositionisthespatialstartandend(x,y) ± \n inthem. (b)Distributionoftheresponsewindowlengthis\n coordinatesforeachboundingboxintheresponsetrack. We \n shownin Figure 25. Typically,thewindowsare 9.3 21.5\n normalize the coordinates by the image width and height ± \n secondslong. \n toaccountforvaryingimagesizesinthedata. Eachpixel \n Mostresponsewindowsarequiteshortcomparedtothe\n withintheboundingboxcontributestoanimageheatmap \n fullvideoclip,makingthetaskachallenging“needleinthe\n that shows the frequency of each pixel belonging to a re- \n haystack”searchproblem. (c)Distributionofquerywords\n sponsetrackboundingbox. \n is shown in Figure 19. The branching off evidences the\n The analyses are shown in Figure 22. The query to re- \n richnessanddiversityofthequeriesin NLQ. \n sponseseparationdistancesarefairlyspreadbetween 1 to \n 200 frames with a mode of 30 frames (see Figure 22, Moments queries For MQ, similar to other tasks in\n ∼ \n left). Theresponsetracksizesarewelldistributedbetween episodicmemory, wemaintainaratioof 6:2:2 amongthe\n 1 to 40 frameswithamodeof 8 frames(see Figure 22, train/val/test splits, which contains disjoint sets of video\n ∼ \n center). Theboundingboxesarenear-uniformlydistributed clips. Tomakesurethereareenoughsamplesineachcate-\n 29 "
  },
  {
    "page_num": 30,
    "text": " \n \n \n \n \n \n \n \n \n \n enoemos \n \n htiw tcaretni / esrevnoc \n enohp \n esu \n ...o \n smeti gnihtolc hguorht esworb \n ...ippohs \n / enizagam / koob a daer \n ...eti \n doof )tuo ekat ro( yawa tup \n rorrim \n eht ni sehtolc ta kool \n gnikooc \n elihw doof xim / rits \n ...elbategev \n a ecils / pohc / tuc\" \n sriats \n pu klaw / sriats nwod klaw \n ...ni \n hsart tup / hsart yawa worht \n ...o \n ro ecafrus rehto epiw / naelc \n hguod \n tuo-llor / epahs / daenk \n ...awekab \n / slisnetu / sehsid hsaw \n smeti \n rehto ezinagro / egnarra \n ...oof \n ro seirecorg hguorht esworb \n ...eoh \n a htiw lios eht llit ro gid \n egareveb \n knird \n ...nah \n no / tesolc ni sehtolc gnah \n meti \n rehto xif \n sdnah \n hsaw \n lian \n xif ot nug-lian / remmah esu \n trac \n gnippohs ni smeti ecalp \n dnuorg \n morf sdeew evomer \n pohs \n / tekramrepus a retne \n moorb \n htiw roolf peews / naelc \n retupmoc \n / potpal a esu \n emag \n drac ro emag draob yalp \n ...reniatnoc \n / elttob / top a llif \n noisivelet \n hctaw \n ...r \n no smeti rehto hguorht esworb \n .../ \n sehsid )tuo ekat ro( yawa tup \n ... \n htiw oediv drocer / otohp ekat \n steehs \n / sehtolc dlof \n etalp \n a otno doof evres \n sloot \n rehto htiw ssarg mirt / tuc \n ... \n roolf / doow / llaw otni llird \n ... \n .g.e( tnempiuqe ytefas no tup\" \n rellor \n / hsurb tniap gnisu tniap \n ...wob \n a ni stneidergni xim / rits \n sehcnarb \n ro segdeh mirt \n ...tni \n seirecorg / smeti doof kcap \n reddal \n a nwod / pu bmilc \n ... \n a ta enil / eueuq eht ni dnats \n elbategev \n ro tiurf a leep \n ... \n smeti gnihtolc raew / tuo-yrt\" \n sporc \n / stnalp / lios retaw \n loot \n gnisu meti rehto tuc \n renrub \n evots eht thgil / no-nrut \n ...o \n erit a ecalper / evomer / xif \n pohs \n / tekramrepus a tixe \n ...nehctik \n ro elbat a epiw / naelc \n retnuoc \n gnillib ta yap \n yrd \n ot sehtolc gnah \n ...a \n gnisu tneidergni / doof hgiew \n ...eidergni \n )tuo ekat ro( yawa tup \n sexob \n / sgab otni smeti rehto kcap \n ...enihcam \n gnihsaw a daolnu / daol \n meti \n doof / tiurf / elbategev hsaw \n lamina \n / tep htiw yalp ro tcaretni \n esicrexe \n emos od \n loot \n gnisu seceip doow tuc / pohc \n hguod \n tuc \n hcnarb \n eert tuc \n emag \n oediv a yalp \n rewomnwal \n a htiw ssarg mirt / tuc \n ecafrus \n / llaw retsalp \n kcans \n a tae \n ... \n srewolf / stnalp / sdees tnalp \n gniriw \n xif \n naelc \n ot renaelc muucav a esu \n ...rg \n no sevael yrd ekar / tcelloc \n ...r \n no seirossecca hguorht esworb \n ...gnisu \n .ge( lios / dnuorg level\" \n hguod \n yrf \n ...s \n htiw stnalp / sehcnarb pu eit \n ...s \n / repapdnas gnisu doow htooms \n gniyap \n erofeb yenom tnuoc \n ecnailppa \n nehctik epiw / naelc \n dnah \n yb lios eht llit ro gid \n ...arepo \n / mta morf yenom wardhtiw \n ... \n a ro dnuorg eht otni lios kcap\n rac \n fo enigne / tennob xif \n steehs \n ro sehtolc nori \n ....e( \n seirossecca raew / tuo-yrt\"\n ...c \n ni sehtolc ezinagro / egnarra\n ...c \n / stiurf / selbategev tsevrah\n ... \n epat gnisu meti nedoow erusaem\n smeti \n gnihtolc owt erapmoc \n ...oitcurtsnoc \n dnuora tfihs / evom \n sloot \n llams egnarra / tfihs / evom\n egdirf \n ni smeti ezinagro / egnarra\n gnibmulp \n / epip xif \n ... \n draobdrac / repap / daerht tuc\n ...rcnoc \n / tnemec ylppa ro eraperp\n ...ffoc \n a esu / aet ro eeffoc ekam\n ...swollip \n egnarra / deb eht ekam\"\n meti \n cillatem lio / epiw / naelc\n koob \n / repap a ni seton etirw\n ...tnempiuqe \n llams riaper / naelc\"\n ...s \n htiw .g.e( egakcap a nepo tuc\n elcihev \n a evird \n ...m \n / nep / licnep htiw meti kram\n riahc \n / hcuoc no swollip egnarra\n meti \n doof rehto yrf\n ...i \n meti doof rehto niard / esnir\n ekab \n ot nevo eht otni doof tup\n meti \n rehto eltnamsid\n nevo \n eht morf doof evomer\n epav \n / etteragic / ragic ekoms\n gnikooc \n elihw doof etsat\n 103 \n 102 \n secnatsni \n # \n Figure 18.Distributionofmomentslabels.Thefigureshowsthenumberofinstancespercategoryacross 5 scenariosand 300 hoursof\n data. All 110 categoriesareshown,sortedbyfrequency. Thedistributionislongtailed,withthesmallestclassescontainingatleast 50\n instances.Notethattheseareonlythe Momentsfor Episodic Memorywithtemporalwindowannotationsinthecurrentrelease;Ego 4 Dhas\n manyotherscenariosandactivitiesnotreflectedinthisdistribution. \n Split Train Val Test Total \n Videohours 194.9 68.5 62.9 326.4 \n #Videoclips 1,486 521 481 2,488 \n #Instances 13.6 k 4.3 k 4.3 k 22.2 k \n Table 8.MQdatasetstatisticsacrossthetrain/val/testsplits.\n clip. Theaveragedurationeachinstanceis 45.2 seconds. (b)\n Thedistributionofdifferentcategoriesisshownin Fig 18.\n Wenoticethatthisisalong-taileddistribution,somecate-\n gories(e.g.,‘usephone’,‘converse/interactwithsomeone’)\n withover 1000 instancesandsomecategorieswithlessthan\n 100 instances. Eachcategoryhas 205 instancesonaverage.\n (c)Thedistributionofinstancenumbersinavideoclipis\n shown in Fig 27. The majority of video clips have 1-20\n momentinstances,whereasveryfewcanhaveasmanyas\n over 80 instances. \n Figure 19.Distributionofquerywordsin NLQ. \n F.5 Evaluationmeasures \n Next we detail the evaluation metrics for all three query\n gory,weonlykeepcategoriesthathaveatleast 50 instances \n types. \n fromtheannotationsandhaveinstancesinalltrain/val/test \n splits. Visualqueries Wedefinethefollowinglocalizationmetrics\n Consequently,the MQdatasethas 110 categories,spans forthe 2 Dlocalizationtaskwithtop-1 retrieval.\n atotal 326.4 hoursofvideos,2,488 videoclipsand 22.2 kac- Temporal AP (t AP) measures how closely the temporal\n tioninstances. Wesummarizethestatisticsacrossthethree extent of the prediction matches with the ground-truth re-\n splits in Table 8. We further explore the data through the sponsetrack. Itiscalculatedastheaverage-precisionofthe\n followingaspects. (a)Thedistributionofactiondurationis predictedresponsetrack’stemporalextent,andisbasedon\n shownin Fig 26. Wecanseethatmostmomentshavevery the Activity Netm APmetric[61]. Weevaluatethet APat 4\n short duration. The majority of moments last less than 1 differentt Io Uthresholds 0.25,0.50,0.75,0.95 ,aswellas\n { } \n minute,and 22.4%actionshavedurationlessthan 3 seconds. theiraveragevalue. \n Notethatthereisalsoapeak(2.6%instances)atthelargest Spatio-temporal AP (st AP) measures how closely the\n durationbin,wheretheactionsalmostcoverthewholevideo spatio-temporalextentofthepredictionmatchestheground-\n 30 "
  },
  {
    "page_num": 31,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 20.Distributionoverscenariosforvisualqueries.Thedatasetcontainsalong-tailofscenarios.Theplottitleindicatesthenumber\n ofscenariosandthetotalvideohoursincludedinthedataset. \n \n rithmsearchingforthequeryobject. Itiscalculatedas\n \n n \n s Eff=1 (5) \n − N \n where n is the number of video frames previewed by an\n algorithmtopredicttheresponsetrack,and N isthetotal\n numberofframesinthevideobeforethequerywasmade\n (i.e., the search window). An algorithm that accesses ev-\n eryframeinthesearchwindowbeforelocalizingthequery\n objectgets 0.0 searchefficiency. This“timeliness”metric\n isdesignedtoencourageresearchonmethodsperforming\n intelligentcontextual-search. \n Weevaluateperformanceonthe 3 DVQlocalizationtask\n usingtherootmeansquareerror(RMSE)andtheangular\n Figure 21.Distributionoveruniversitiesforvisualqueries.The errormetrics: \n datasetcontainsannotationscorrespondingtovideosfrom 10 uni- \n versities.Theplottitleindicatesthenumberofuniversitiesandthe RMSE= t s tˆ s 2 (6)\n (cid:107) − (cid:107) \n totalvideohoursincludedinthedataset. v T \n vˆ \n Q Q \n angular error=acos( . ) (7) \n v vˆ \n Q 2 Q 2 \n (cid:107) (cid:107) (cid:107) (cid:107)\n where t and tˆ are the ground-truth and predicted object\n truthresponsetrack. Itiscalculatedastheaverage-precision s s \n positioninthescancoordinatesystem. v andvˆ arethe\n ofthepredictedspatial-tube,andisbasedonthevideo-AP Q Q \n ground-truthandpredicted 3 Ddisplacementvectorinthe\n metricfrom[88]. Weevaluatethest APat 4 differentst Io U \n queryframe Qcoordinatesystem. Wealsodefineasuccess\n thresholds 0.25,0.50,0.75,0.95 ,aswellastheiraverage \n { } metricleveragingthetwoannotationsperquery:\n value. \n Success (Succ) measures whether the prediction has any \n succ= c tˆ <6 ( c c +δ) (8) \n overlapwiththegroundtruthatall. Itiscalculatedasthe (cid:107) m − s (cid:107) 2 × (cid:107) 1 − 2 (cid:107) 2\n percentageofsampleswherethepredictedresponsetrack \n With c 1 and c 2 the centroids of the two bounding box\n hasatleast 0.05 spatio-temporal Io Uwiththegroundtruth. \n annotations, c the mid-centroid between c 1 and c 2 and\n Recovery% (rec%) measures how much of the ground- m \n δ =exp−mdiag,withm \n diag \n theaveragediagonallengthof\n truthresponsetrackisaccuratelyrecoveredbytheprediction. \n thetwoboxes. \n Itiscalculatedasthe%offramesintheresponsetrackwhere \n the predicted bounding box has at least 0.5 Io U with the Naturallanguagequeries Evaluationfor NLQissimilar\n groundtruth. Thisismotivatedbythetrackingrobustness toexistingvideo-languagegroundingproblems. Following\n metricfromthe VOTchallenge[121]. priorwork[236],weuserecall@k,Io U=m,whereweselect\n Searchefficiency(s Eff)measurestheefficiencyofthealgo- k = 1,5 and m = 0.3,0.5 . This metric computes\n { } { } \n 31 "
  },
  {
    "page_num": 32,
    "text": " \n \n \n \n \n \n Distribution of query to response Distribution of response track lengths Distribution of response bboxpositions\n separation distances \n \n \n \n \n \n \n \n \n Figure 22.Visualqueriesbiasanalysis.Weanalyzethefull VQdatasetforpotentialbiases.Left:Theplotshowsthedistributionofquery\n toresponseseparationdistancesinthe VQdataset.Whilethemodeofthedistributionis∼30 frames,wecanseethatseparationdistances\n arefairlyspreadbetween 1 to 200 frames.Center:Theplotshowsthedistributionofresponsetracksizesinthe VQdataset.Whilethe\n modeofthedistributionis∼8 frames,wecanseethattheresponsetracksizesarewelldistributedbetween 1 to 40 frames.Right:The\n heatmapshowsthenormalizedfrequencyofeachpixelbelongingtoaresponsetrackboundingbox.Theboundingboxesnear-uniformly\n distributedacrossmostoftheimage. \n \n \n \n \n \n \n gnikoo C yrdnual / gninael C snoc ot detaler sboj cinahcem ra C retnepra C reka B cinahcem retooc S w( noitagiva N roodn I dni gnippohs yrecor G ppohs rehto ,sehtol C teerts no gnikla W ylimaf htiw gnikla T eeffoc gnika M gnita E cinahcem eki B ni gnihtemos gnixi F c/pu-kcap/putes pma C step htiw gniyal P namydna H semag draob gniyal P emoh ta tuo gnikro W ep / god eht gnikla W htimskcal B s aetklim ni gnikro W es/gnittink/gnitfar C lcni( laicos roodtu O eki B aor ,gnitummoc -\n \n \n \n ra C \n lacisum a gnicitcar P ksed ta gnikro W siybboh( scinortcel E eugaelloc ot gnikla T ediv / semag gniyal P l/enohp( neercs a\n n O \n e - myg eht ot gnio G eneigyh ylia D gniggoj / gnilcy C su B ti gnikam( ba L reka M sdneirf htiw gnikla T krap eht ot gnio G cisum ot gninetsi L gninedra G\n 80 \n 60 \n 40 \n 20 \n 0 \n Scenarios \n sruoh \n oediv \n # \n Total: scenarios=43, hours=358.93 \n Figure 23. Distributionoverscenariosforthe NLQannotations,indicatingalongtailoverscenarios. Notethatthescenariolabelsare\n approximateandasinglevideocancontainmultiplescenariolabels.Forthisplot,weequallydividethetimeacrossallthelabelledscenarios.\n thepercentageoftimesatleastoneofthetopk predicted temporalactiondetectiondatasets,suchas Activity Net[61],\n candidateshaveanintersection-over-union(Io U)ofatleast themean AP(m AP)overallcategoriesiscomputedgivena\n m. Notethatweleantowardslowerthresholdvalues(m)as t Io Uthreshold. Multiplet Io Uthresholdsareadopted,and\n theaveragelengthofthewindow( 10 s)ismuchsmaller theaveragem APoveralltheset Io Uthresholdsiscomputed.\n ∼ \n thanthatofthevideoclip(500 s),about 2%ofthecliplength. Formomentqueries,weevaluatem APat 5 differentt Io U\n thresholds 0.1,0.2,0.3,0.4,0.5 ,aswellastheiraverage\n { } \n Moments queries Considering that the moment queries value. \n taskisrelatedtothetasksoftemporalactiondetection[61, Recall@kx, t Io U=m, is a metric adapted from the metric\n 141,229,237] and video grounding [236], we adapt their recall@k, t Io U=m, used for NLQ. The metric recall@k,\n respectivemetricstomomentqueries. t Io U=mmeasuresthepercentageofthequerysentencesthat\n Average Precision(AP)isacommonlyadoptedmetricin haveatleastonepredictionwithat Io Ulargerthanthethresh-\n temporalactiondetection. Itmeasureshowcloselythetem- oldminthetop-kresults. Inourmomentqueriescase,since\n poralextentofthepredictionsmatchestheground-truthac- wemighthavemorethanoneinstancecorrespondingtoa\n tioninstancesforeachactioncategory[61,141,229,237]in querymomentcategory,weneedtomeasurethepercentage\n termsofbothprecisionandrecall. Thetemporalintersection ofallthecorrectlypredictedinstancesthathaveatleastone\n overunion(t Io U)betweenapredictionandaground-truth predictionwithat Io Ulargerthanthethresholdminthetop-\n actioninstanceisusedtomeasuretheirdistance. Ifthet Io U k resultsofthisinstance. Consideringthatpredictionsare\n is higher than a threshold, the prediction is considered as usuallymadebasedonacategorynotaspecificinstance,we\n true positive; otherwise, false positive. In representative modifythemetrictobethefollowingrecall@kx,t Io U=m,\n 32 "
  },
  {
    "page_num": 33,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n X tcejbo si ereh W ?Y tneve retfa/erofeb ?X tup I did ereh W ?X tcejbo si ereh W ?X ni tup I did tah W ?s X ynam wo H )noitseuq ytitnauq( ?Y I did X tah W I did noitacol tahw\n \n \n n I \n ? X tcejbo ees ?Y si X tah W tcejbo na fo etat S tcaretni I did oh W did I nehw htiw ?X ytivitca ?X tcejbo ym si ereh W ni ot klat I did oh W ?X noitacol ro ot klat I did neh W nosrep htiw tcaretni ?X elor htiw\n 3000 \n 2500 \n 2000 \n 1500 \n 1000 \n 500 \n 0 \n Templates \n sruoh \n oediv \n # \n Figure 24.Distributionofqueriesoverthecorrespondingtemplates \n acrossobjects,place,andpeoplecategories(Tab.5). Seetextfor \n moredetails. \n 2000 \n 1500 \n 1000 \n 500 \n 0 \n 0.0 10.0 20.0 30.0 40.0 50.0 60.0+ \n window length (seconds) \n seireuq \n # \n Figure 25.Distributionofresponsewindowlengthfor NLQ.For \n thesakeofbrevity,weusethelastbintorepresentallwindows \n longerthanaminute.Seetextformoredetails. \n 6000 \n 5000 \n 4000 \n 3000 \n 2000 \n 1000 \n 0 \n 0 100 200 300 400 500 \n Action duration (seconds) \n secnatsni \n # \n 250 \n 200 \n 150 \n 100 \n 50 \n 0 \n 0 10 20 30 40 50 60 70 80 \n # instances in one video clip\n Figure 26.Distributionofmomentduration. \n wherexstandsforthenumberofinstancesforaquerycat- \n egory in one video. This metric measures the percentage \n of all the correctly predicted instances that have at least \n one prediction with a t Io U larger than the threshold m in \n the top-kx results of the action category. This metric has \n a similar idea to the multi-label metric proposed in [240] \n spilc \n oediv \n # \n Figure 27.Distributionofinstancenumbersinonevideoclip. whendealingwithmultipleinstancesforaquery. Weuse k = 1,2,3 andm = 0.3,0.5,0.7 inthemetric. Compared\n toaverageprecision,thismetriconlyevaluatestherecallfor\n the query categories, and does not penalize for false posi-\n tivepredictionsgivenacategorythathasnoinstancesinthe\n video. \n F.6 Baselines \n Wedevelopedbaselinemodelsforeachtask. Wedesigned\n thesemodelstoaddressourtasks,usingstate-of-the-artcom-\n ponentswhererelevant. Theyrepresentastartingpointupon\n whichfutureworkcanbuild. \n Visualqueries 2 Dlocalizationbaseline \n Wetreatvisualquerieswith 2 Dlocalization(VQ 2 D) asa\n detection + tracking problem (see Figure 28). At a high\n level,ourapproachconsistsofthreesteps. First,weperform\n frame-leveldetectionovertheinputvideowherewedetect\n the presence of the query object in each frame using an\n object detection model (Figure 28 top). For each frame,\n wegettheboundingboxthatismostsimilartothevisual\n cropandascoreindicatingitsvisualsimilarity. Second,we\n considerthesequenceofper-framesimilarityscoresoverthe\n entirevideoandidentifythemostrecentpeakinthesescores\n (Figure 28 bottom-left). Finally,weinitializeatrackeratthe\n video-framecorrespondingtothepeakdetection,andtrack\n thequeryobjectonbothforwardandbackwarddirections\n torecoverthecompleteresponsetrack(Figure 28 bottom-\n right). \n Step 1: Frame-leveldetection Wepropose Siam-RCNN,\n a Faster-RCNN [189] based approach to detect the query\n object in a given image. See Figure 28 top. Given\n a video frame at time t, a pre-trained Region Proposal\n Network (RPN) [189] with a Feature Pyramid Network\n (FPN) [142] backbone is used to generate bounding box\n proposals b , ,b . The Ro I-Align operation [94] is\n 1 N \n { ··· } \n thenusedtoextractvisualfeaturesforeachboundingbox\n (b ), , (b ) . Weusethesame FPNbackboneto\n 1 N \n {F ··· F } \n extractfeaturesforthevisualcropv. Todetectthepresence\n ofthequeryobjectinframet,eachproposalfeature (b )is\n i \n F \n 33 "
  },
  {
    "page_num": 34,
    "text": " \n \n \n \n \n \n Step 1: Frame-level detection with Siam-RCNN \n BBo Bx Bporxopporsoaplossals \n { b 1, {· b ·1· ,b · N ··} b N } BBoxfeatures \n RPN Ro IAlign (b 1), , (b N) \n {F ··· F } \n Backbone( ) \n F \n \n Backbone( ) (v) \n F F \n \n \n Inputvideo Visual crop (v) \n \n Step 2: Temporal detection \n \n \n \n )s( \n erocs \n \n ytiralimi S \n Prediction @ frame t \n Top -1 \n retrieval \n (bt,st) \n Per-frame predictions \n [(b ,s ), ,(b ,s )] \n 1 1 Q-1 Q-1 \n ··· \n Step 3: Tracking \n Signal peaks Initialize tracker with \n Tracker \n Query Forward tracking \n frame \n Response track prediction \n Nearest peak \n detection \n Initialize tracker with \n Tracker \n Q Backward tracking \n Video time (t) \n Figure 28.Visualqueries 2 Dlocalizationbaseline.Ourapproachconsistsofthreesteps.Step 1:Weperformframe-leveldetectionforthe\n entireinputvideotodetectthepresenceofthequeryobject(specifiedviathevisualcropv).Foreachframet,weextracttheregionproposals\n {b \n 1 \n ,··· ,b \n N \n }usingaregionproposalnetwork(RPN),andextractfeaturesforeachproposal{F(b\n 1 \n ),··· ,F(b \n N \n )}.Eachproposalfeature\n iscomparedwiththevisualcropfeature F(v)usinga Siamesehead S,andthemostsimilarproposalb tisretrievedalongwithitsscore\n s t. Thisprocessisrepeatedforallframes. Step 2: Wetreatthesimilarityscoress={s 1 ,··· ,s q−1 }asatemporalsignalandperform\n temporaldetectiontoobtainthe‘mostrecentoccurrence’ofthequeryobject.Wedetectthepeaks(localmaxima)inthesignalandrecover\n thepeakpnearesttothequeryframe.Step 3:Giventhedetectedpeakpanditscorrespondingproposalb p,weinitializetwotrackerswith\n b pandrunthemalongtheforwardandbackwarddirectionstorecoveracontiguoustrackoftheobject,i.e.,theresponsetrackprediction.\n comparedwiththevisualcropfeature (v)usinga Siamese b with the highest similarity score s for frame t can be\n t t \n F \n head thatpredictsa 0-1 similarityscore obtainedasfollows: \n S \n s i = S ( F (b i ), F (v)) (9) b t = argmax { s 1 , ··· ,s N } (12)\n b∈{b 1,···,b N} \n The Siamesenetworkprojectseachproposal/visual-crop \n s =max s , ,s (13) \n t 1 N \n feature to a 1024-D feature vector using a convolutional { ··· } \n projectionmodule , After repeating the above steps for all the video\n P \n frames, we can obtain the final per-frame predictions as\n p = ( (b )); p = ( (v)) (10) \n b i v \n P F P F [(b ,s ), ,(b ,s )]. \n 1 1 q−1 q−1 \n ··· \n andpredictsa 0-1 similarityscoreusingabilinearopera- \n Step 2: Temporaldetection Sofar,weused Siam-RCNN\n tion: \n togetthemostsimilarproposalsandtheirsimilarityscores\n s =σ(p TWp +b) (11) \n i b v foreveryframeinthevideo. Next,thegoalistotemporally\n where σ is a sigmoid non-linearity. After computing the detectthe‘mostrecentoccurrence‘oftheobjectinthevideo\n similarities to each bounding box proposal, the proposal (see Figure 28 bottom-left). Thisisachallengingproblem\n 34 "
  },
  {
    "page_num": 35,
    "text": " \n \n \n \n \n \n since our goal is not to identify the best detection of the Wenextdetailthetrainingprocedureforthe Siam Head\n object,butinsteadthemostrecentone,evenifthesimilarity ( ). Weuseasimilarityretrievalapproachwerethemodel\n S \n isnotashigh. Totacklethisproblem,wetreattheper-frame is trained to predict high visual similarity between the vi-\n similarityscoress = s , ,s asatemporalsignal, sual crop v and positives, and low visual similarity be-\n 1 q−1 \n { ··· } \n and use a signal peak detection approach to identify the tween v and negatives. The loss function for is a bi-\n S \n salientpeaks(a.k.a. localmaxima)ins. Toavoidspurious nary cross entropy loss defined over each (v,D ,D ) tu-\n p n \n peaks,wefirstsmoothsusingamedianfilterwithawindow \n ple (see Eqn. 16), where D = p \n |Dp| \n are positive \n sizeof 5. \n detections, D = n \n |Dn| \n a \n p \n re neg \n { \n ati \n i \n v \n } \n e \n i= \n d \n 1 \n etections, and \n n { j }j=1 \n s = ( (x), (v)): \n s¯=median filter(s) (14) x,v S F F \n p , ,p =find peaks(s¯) (15) \n 1 k \n ··· (cid:18) (cid:19) \n 1 (cid:88) (cid:88) \n = log(s )+ log(1 s ) \n Dependingonthevideo,thealgorithmmayreturnmultiple L S − D D p,v − n,v \n p n \n peaksspreadthroughoutthevideo(seesignalpeaksin Fig- | ∪ | p∈Dp n∈Dn \n (16) \n ure 28 bottom-right). Sinceourgoalistodetectthemost \n Bothpositivesandnegativesaredefinedbasedonpropos-\n recentoccurrenceoftheobject,weselectthepeakpthatis \n alsgeneratedbythe RPN.Givenavisualcropv,aproposal\n temporallynearesttothequeryframe. \n p fori (s,e)isapositiveifthe Io U(p ,r ) 0.5,where\n i i i \n Step 3: Tracking Aftertemporaldetection,wehaveidenti- r is the ∈ response track box in frame i. We r ≥ emove all r\n i i \n fiedapeak-framepinthevideowhichisestimatedtohave whichare toosmall, orhavesignificantly differentaspect\n the most recent occurrence of the object. For this frame ratiosfromthelargestboxinrsincethesetypicallycorre-\n p,wecanobtainthehighest-scoringboundingboxb p from spondtoobstructedviewsoftheobject. Aproposalp j isa\n theper-framedetectionsinstep 1. Notethatthisonlyrep- negativeifitsatisfiesanyofthefollowingtwoconditions:\n resentsoneframewheretheobjectmostrecentlyoccurred. \n However,thetaskobjectiveistoobtaintheresponsetrack, 1. j (s,e)and Io U(p j ,r j )<0.5\n ∈ \n i.e.,thecontiguoussetofallframes,startingfromwhenthe \n 2. p issampledfromanothervideo. \n j \n objectfirstenteredthefield-of-viewuntiltheobjectexitsthe \n field-of-view. See Figure 28 bottom-right. Tocomputethe We also found it beneficial to use hard-negative mining,\n restoftheresponsetrack,weuseb asastartingpoint,and whereweinitiallysamplealargenumberofnegativesand\n p \n runasingle-objecttrackerforwardandbackwarduntilthe thenselectthetop-Knegativeswiththehighestlossvalue.\n trackingfails(i.e.,theobjectexitsthefield-of-view). \n Forbothdirections,weinitializetheapperancemodelof We employ a few different augmentation strategies to\n thetrackerusingtheproposalb . Fortheforwardtracking, artificiallyexpandthedataset. First,weaugmenteachdata\n p \n werunthetrackerstartingfromframep+1 toq 1 andobtain samplebyreplacingthevisualcropvbyaboundingboxr\n i \n thetrackedregions: b =[¯b , ,¯b ]. For − thebackward fromtheresponsetrack. Thisworksbecausetheresponse\n f p+1 e \n ··· \n tracking, we run the tracking starting from frame p 1 track and the visual crop correspond to the same object.\n to 0 and obtain the tracked regions: b = [¯b , ,¯b − ]. Next, we augment the visual crop v by applying random\n b s p−1 \n ··· \n Wethenconcatenateb ,b ,andb toobtainthecomplete rotations between 120◦ to 120◦. This exploits the fact\n b p f \n − \n response track prediction. We use the KYS tracker [22], thatobjectscanhavesignificantviewpointvariationsinego-\n whichwasshowntoachievestate-of-the-artresultsforsingle- centricvideos(unlikeinternetphotos). Finally,weapplya\n objecttracking. randombrightnessaugmentationtothevideoframesandthe\n visualcroptosimulatedifferinglighting. \n VQ 2 Dbaselinetrainingsetup Wenowdiscussthetrain- \n ingprocedureforthe VQ 2 Dbaseline. Eachdatapointforthe Implementation details We train the Siam Head using\n S \n VQ 2 Dtask(definedon Ego 4 Dvideos)consistsofthefol- the Detectron 2 library[225]. Weusethedefaultconfigura-\n lowing: video V,visualcropimagev,queryframenumber tionfileandmakethefollowingchangesforourexperiments.\n q,andresponsetrackboxesr = r ,r , ,r ,where Foreachexperiment,weuse 8 GPUs,64 visualcropsper\n s s+1 e \n { ··· } \n sandearethestartandendframesofr,andr isabounding batch,andtrainfor 300,000 iterationswithaninitiallearn-\n i \n boxdefinedonframeiofvideo V. ing rate of 0.02 followed by a 0.1 decay after 200,000\n × \n As a high-level overview, we initialize and freeze the iterations. Weextractbackbonefeaturesfromthe“p 3”layer\n backbone and RPNusingweightsfroman MS-COCOpre- of FPN. Based on validation performance, we use 6 pos-\n F \n trained Mask-RCNNmodel. Weusethe VQ 2 Dannotations itives and 64 negatives for each visual crop. Specifically,\n to train the Siam Head ( ). We initialize and freeze the we sample 58 negatives per video frame which results in\n S \n KYS tracker using weights pre-trained on GOT-10 k [99], 58 64=3712 negativesperbatch. Foreachvisualcrop,\n × \n La SOT[62],and Tracking Net[162]datasets. wesamplethe 64 hardestnegativesoutof 3712.\n 35 "
  },
  {
    "page_num": 36,
    "text": " \n \n \n \n \n \n Inthe Siam Head architecture,theprojectionmodule itremainsrelativelystableforthesecondstrategywherewe\n S P \n consistsoffourresidualblocksfollowedbyaveragepooling, previewafractionofframesclosesttothequery. Forexam-\n anda 2-layermulti-layerperceptron(MLP)withahidden ple,wecanachieveasearchefficiencyof 48.0%withonlya\n sizeof 1024-Dand Re LUactivation. 6 16%relativedropinperformancewithk =50%inthe\n − \n For signal peak detection, we utilize the find peaks 2 ndstrategy. However,theperformancedropssignificantly\n functionfromthescipylibrary 12 withthefollowinghyper- ifwereducekfurther. Forexample,weobserveareduction\n parametersselectedthroughvalidation: distance=25,width of 38 60%fork =10%withthe 2 ndstrategy. Thissug-\n − \n =3,andprominence=0.2. geststhatmoreintelligentmethodsthatperformcontextual\n searchareneededtoimprovethesearchefficiencyfor VQ 2 D\n Experimentalresults Weevaluatetheperformanceofmul- \n whilemaintaininggoodperformance. \n tiplebaselinesonthe VQ 2 Dtaskin Tab.9. Thefirstcolumn \n inthetableshowsthedetectionandtrackingmethods,and \n thesecondcolumnshowsthe Siam Headprojectionarchitec- Visualqueries 3 Dlocalizationbaseline\n ture . Inadditiontothe KYStracker,wealsoexperiment \n P Nextwedescribethebaselineforthevisualquerywith 3 D\n withasimpleparticlefiltertracker(denoted‘PF’)toassess \n localizationtask. Recallthetaskdefinition: givenavideo,a\n theimpactofthetrackingquality. Asanablationof Siam R- \n queryframe,andavisualcropofatargetobject,thegoalis\n CNN, we replace the 4 residual blocks in the Siam Head \n tooutputa 3 Ddisplacementvectorfromthecameracenter\n projection module with a simple 3-layer CNN which has \n ofthequeryframetothecenterofthetargetobjectin 3 D.\n lowercapacitywithnoresidualconnections(indicatedby \n The 3 D position of the target object is defined at its most\n ‘Simple’). \n recentappearanceinthevideo. Figure 31 showsasampleof\n We make several observations. When we use a simple \n thetask. \n projection model with a particle filter tracker, we already \n Ourbaselinestrategyhasthreesteps. Wefirstestimate\n observe a good validation performance of 32.4% success, \n thecameraposesofthevideo. Thenweretrievethemost\n and 0.14 t AP . Thesecanbeattributedtousingastrong \n 25 \n recentinstanceofthetargetobjectinthevideo. Lastly,we\n proposal generator (RPN pre-trained on MS-COCO) and \n estimatethedepthofthedetectedobjectandretrieveits 3 D\n alearnedsiamesecomparisonmodel. Uponreplacingthe \n positionfromthequeryframe. \n particlefiltertrackerwitha So TAKYStracker[22],while \n thevalidationsuccessrateremainssimilarat 33.0%,weob- Cameraposeestimation Thecameraposesareestimated\n serve significant gains (absolute) in all other metrics: 2% usingakeypointmatchingstrategyalongwitha Perspective-\n t AP, 2% st AP 25 , and 14.3% recovery. This suggests that n-Point(Pn P)resolutionapproach. Atahighlevelourap-\n a good tracker is necessary to accurately capture the full proachconsistsofthefollowingfoursteps. Firstweestimate\n responsetrackafterlocalizingasingleframewithinit. Fi- thecameraintrinsicparametersusing Structure-from-Motion\n nally,uponreplacingthe‘Simple’siameseprojectionwith (Sf M).Secondly,weextractandmatchkeypointsfromeach\n 4 residualblocks,weobserveasignificantgainsof 6.8%in frameinthevideotokeypointsextractedfromthe Matter-\n success, 5% in t AP 25 , 4% in st AP 25 , and 5% in recovery port 3 Dpanoramas. Then,usingthematchedkeypointswe\n %. Thissuggeststhatusingahighercapacitymodelforthe setupandsolvea Pn Pproblemforeachframeinthevideo\n Siam Headishelpfulforimprovingtheper-framedetection toestimatethecorrespondingcamerapose. Lastly,werefine\n performanceforthe VQ 2 Dtask. Weobservesimilartrends theposesusingtemporalconstraints.\n onthetestset. Pleasesee Fig.29 forqualitativeexamplesof Step 1: Camera intrinsics estimation We start by ex-\n themodel’spredictions. tractingasetofcontiguousnon-blurryframesfromthevideo.\n Inallcasesfrom Tab.9,thesearchefficiencyis 0%since Inordertoselectnon-blurryframeswecomputethevariance\n thedetectorsareusedoneveryframeinthesearchwindow. ofthe Laplacianoneachimageandselecttheoneswitha\n In Fig.30 weexperimentwithtwosimpletechniquesforim- valuehigherthana 100 threshold. Wethenselectthelargest\n provingthesearchefficiency. Thefirstapproachuniformly contiguous set of non-blurry images. We cap the number\n subsamplesk%oftheframesinthesearchwindow(denoted ofselectedframesto 10 tolimitthecomputationaltimeof\n as‘SS’).Thesecondapproachsearchesoveronlyk%ofthe the Sf M module. Once we have selected the images we\n mostrecentframesinthesearchwindow,i.e.,framesthatare runtheautomaticreconstructionmoduleof COLMAP[196]\n nearesttothequery(denotedas‘N’).Weconsider 3 values to estimate the camera instrinsic parameters with a radial\n ofkinbothcases:10%,25%,and 50%.Considertheresults fisheyecameramodel. \n in Fig.30. Inbothstrategies,thesearchefficiencyimproves Step 2: Keypointextractionandmatching Weuse Su-\n aswereducek. Theperformancedropsdrasticallyforthe per Glue [195] to extract and match keypoints. We first\n 1 ststrategywherewesubsamplethesearchwindow,while extract keypoints from the scan panoramas k ,p\n {p,n} \n { ∈ \n 12 Peak detection: https://docs.scipy.org/doc/scipy/ P ,n ∈ N} where P is the number of panoramas and N\n reference/generated/scipy.signal.find_peaks.html is the number of keypoints. The scan panoramas are gen-\n 36 "
  },
  {
    "page_num": 37,
    "text": " \n \n \n \n \n \n validation set test set \n Detector+Tracker Succ t AP t AP 25 st AP st AP 25 rec% Succ t AP t AP 25 st AP st AP 25 rec%\n P \n Siam-RCNN+PF Simple 32.4 0.06 0.14 0.02 0.06 13.2 32.7 0.06 0.14 0.02 0.06 12.9\n Siam-RCNN+KYS Simple 33.0 0.08 0.15 0.03 0.08 27.2 33.4 0.09 0.16 0.03 0.08 26.9\n Siam-RCNN+KYS Residual 39.8 0.12 0.20 0.04 0.12 32.2 41.6 0.12 0.21 0.05 0.13 34.0\n Table 9.Visualqueries 2 Dlocalizationresults.Wecomparetheperformanceofvariousbaselinesonthe VQ 2 Dvalidationandtestdatasets.\n Column 1 indicatesthedetectorandtracker.Column 2 indicatestheprojectionarchitectureusedincaseofthe Siam-RCNNmodel.\n \n Predicted response track Query: When did I last see\n this object? \n . . . \n . . . \n \n \n Predicted response track Query: When did I last see\n this object? \n . . . \n . . . \n \n \n Figure 29. Qualitativeexamplesforvisualqueries 2 Dlocalization.Oneachrow,weshowthevisualcropofthequeryobjectontheright\n andthepredictedresponsetrackinthecenter(3 uniformlysamplesimages). Themodelwasabletocorrectlylocalizethemostrecent\n occurrenceoftheobjectandaccuratelytrackitthroughouttheoccurrence. \n \n \n (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) \n (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:49)(cid:12)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12)\n (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) \n (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) \n (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) \n \n \n (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)\n (cid:8)(cid:3)(cid:86)(cid:86)(cid:72)(cid:70)(cid:70)(cid:88)(cid:54) \n (cid:24)(cid:19) \n (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49)\n (cid:23)(cid:19) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12) \n (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:49)(cid:12)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:22)(cid:19)\n (cid:21)(cid:19) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12) \n (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12)\n (cid:20)(cid:19) \n (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12)\n (cid:19) \n (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19)\n (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)\n (cid:24)(cid:21)(cid:17)(cid:19)(cid:3)(cid:35)(cid:3)(cid:51)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:82)(cid:83)(cid:80)(cid:72)(cid:55)\n (cid:19)(cid:17)(cid:21)(cid:24) \n (cid:19)(cid:17)(cid:21)(cid:19) (cid:54)(cid:76)(cid:68)(cid:80)(cid:53)(cid:38)(cid:49)(cid:49) (cid:49)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12)\n (cid:49)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12) (cid:19)(cid:17)(cid:20)(cid:24) (cid:54)(cid:54)(cid:3)(cid:11)(cid:24)(cid:19)(cid:8)(cid:12)\n (cid:19)(cid:17)(cid:20)(cid:19) (cid:49)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12)\n (cid:54)(cid:54)(cid:3)(cid:11)(cid:21)(cid:24)(cid:8)(cid:12)\n (cid:19)(cid:17)(cid:19)(cid:24) \n (cid:54)(cid:54)(cid:3)(cid:11)(cid:20)(cid:19)(cid:8)(cid:12)\n (cid:19)(cid:17)(cid:19)(cid:19) \n (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19)\n (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:72)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)\n (cid:24)(cid:21)(cid:17)(cid:19)(cid:3)(cid:35)(cid:3)(cid:51)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:82)(cid:83)(cid:80)(cid:72)(cid:55)(cid:82)(cid:76)(cid:87)(cid:68)(cid:83)(cid:54)\n (cid:19)(cid:17)(cid:20)(cid:25) \n View from the response track\n (cid:19)(cid:17)(cid:20)(cid:21) \n (cid:19)(cid:17)(cid:19)(cid:27) \n (cid:19)(cid:17)(cid:19)(cid:23) \n (cid:19)(cid:17)(cid:19)(cid:19) \n (cid:19) (cid:21)(cid:19) (cid:23)(cid:19) (cid:25)(cid:19) (cid:27)(cid:19)\n Figure 30. Searchefficiencyforvisualqueries 2 Dlocalization. \n Weevaluatesimpletechniquesforimprovingthesearchefficiency, \n andplotthecorresponding VQ 2 Dperformance.Thebluedatapoint \n isthe Siam RCNNperformancewhenwepreviewtheentiresearch \n window.Thereddatapointsarethe Siam RCNNperformancewhen View from the query frame\n wesearchoverk%oftheframesuniformlysubsampled(SS)from \n thesearchwindow. Theyellowdatapointsarethe Siam RCNN Figure 31.Visualqueries 3 Dlocalizationtaskdemo.Thetopview\n performancewhenwesearchoverk%oftheframesnearest(N)to istheviewfromthelastframeoftheresponsetrackwiththetarget\n thequery(withoutanysubsampling).Thevalueofkisindicated objectannotatedwitha 2 Dredboundingbox. Thebottomview\n aboveeachdatapoint. istheviewfromthequeryframe. Thetargetobjectisannotated\n with a 3 D red bounding box at the top right of the figure. The\n figureshowstheground-truth(green)andthepredicted(red)3 D\n erated using the Matterport SDK.13 We render RGB and displacementvectors. \n depth images at each scan position and sweep over pitch \n 13 Matterport-SDK: https://matterport.github.io/ values \n ∈ \n [ \n − \n 30,30]withastepsizeof 5 deg. andyawval-\n showcase-sdk/sdk_intersection_inspector.html ues [ 180,180]withastepsizeof 15 deg. Wegenerate\n ∈ − \n 37 "
  },
  {
    "page_num": 38,
    "text": " \n \n \n \n \n \n on average 7 K images per scan. Note that while we are Video frame View from the scan Superposition\n not releasing the panoramas because of data anonymiza- \n tionconcerns,weareprovidingtheprecomputedkeypoints. \n Similarily, we extract keypoints from the video frames \n k ,i ,m where is the number of im- \n {i,m} \n { ∈ I ∈ M} I \n agesinthevideoand isthenumberofkeypoints. Once \n M \n the keypoints are extracted we loop through each frame \n i in the video and match the extracted frame key- \n ∈ I \n points k ,m to all the panoramas keypoints \n {i,m} \n { ∈ M} \n k ,p ,n . We use the pretained models \n {p,n} \n { ∈ P ∈ N} \n available 14 of Super Point[50]forkeypointsanddescriptors \n extractionand Super Glue[195]formatching. \n Step 3:Pn Presolution Wecomputethecameraposefor \n thevideoframeshavingatleast 20 matchedkeypoints. We \n empiricallyfindthatathresholdof 20 providesagoodtrade- Figure 32. Samplesofcameraposeestimation. Leftshowsthe\n off between the number of overall pose estimates and the framefromtheegocentricvideo,middlehastheviewrenderedfrom\n qualityoftheestimations. Thepositionsofthe 3 Dkeypoints theestimatedviewpointinthescanandrightisthesuperpositionof\n both.Weobservethatevenwithbigscenedifferencesbetweenthe\n arecomputedfromapinholecameramodelofthe Matterport \n videoandthescan(e.g.,thewheelinthesecondrow),thealgorithm\n camerausingtherenderedpanoramadepth,cameraintrin- \n isabletoaccuratelyretrievethecamerapose. \n sics,andcamerapose. Thepositionsofthe 2 Dkeypointsare \n directlyextractedfromthevideoframespixels. Wethenuse \n the Open CVlibrarytosolvethe Pn Psetupandestimatethe \n Theremainingunlocalizedframesareduetoabruptmo-\n cameraposefromthematchedpairsof 3 Dand 2 Dpointsand \n tion (lost track) and when the view is too close-up to the\n usingtheestimatedcameraintrinsicparameters. Usingthis \n scene(notenoughkeypointsmatched). \n methodwecanestimatethecameraposeofroughly 2%of \n Targetobjectretrieval Webuildoursolutionontopofthe\n thetotalnumberofframesinthevideo. Nextweincorporate \n visualqueries 2 Dlocalizationbaseline. The 2 Dlocalization\n temporalconstraintstoincreasethisnumber. \n Step 4: Temporal constraints and final pose estima- baselineoutputsaresponsetrackwith 2 Ddetectionsofthe\n tion Toincreasethenumberofestimateswerefinethepose target object. Our baseline combines these 2 D detections\n alongwithdepthestimationandcameraposeestimationto\n estimationpipelinebyincorporatingtemporalconstraintsin \n retrievethe 3 Dpositionoftheobject. \n aniterativeprocedure. Westartbyextractingandmatching \n 2 Dkeypointsfromlocalizedframestonon-localizedonesin Depth estimation We estimate the depth of the most re-\n thevideo. Thisstepissimilartotheabove Step 2;weuse centframeoftheresponsetrackforwhichwehaveapose\n thesame Super Glue[195]. Usingthematchedkeypointsand estimate. We use the DPT network [185] with pretrained\n current estimated poses we triangulate new 3 D keypoints weightson NYU v 2[202]. Figure 33 showsdepthestima-\n for the non-localized images. We then solve a new Pn P tionresultswhereleftistheframefromthevideo,middle\n setupwiththenewkeypoints. Weapplythisprocedureit- istheestimateddepth,andrightisthedepthfromthescan\n erativelyuntilconvergence. Afterrefinementweachievea rendered at the estimated viewpoint (not available to the\n performanceof 15%ofposeestimatesofthetotalnumber baselinemodel). Notethatduetoscenedifferencesbetween\n offramesaccrossallvideoclips. the video and the scan, the two depths frames will differ\n Camera pose estimation quality and sources of error in some region of the image. We then compute the depth\n valueofthetargetcentroidasthemedianofasquareregion\n Wequalitativelyevaluatethecameraposeestimationpipeline \n centeredatthe 2 Ddetection. \n byrenderingtheviewsinthe 3 Dscans. Recallthatthescans \n andvideoshavebeenrecordedatdifferenttimesandthus 3 D displacement vector reconstruction Given the esti-\n thescenescancontainlargedifferences. Figure 32 shows mated depth d of the object centroid c in frame f of the\n camera poses estimates where left is the frame from the response track and the estimated camera instrisics K, we\n video, middle is the view from the scan, and right is the constructthe 3 Dvectordisplacementvˆ inthecurrentframe\n f \n superposition. Weseethatevenwithlargescenedifferences f coordinatesystemusingapinholecameramodel:\n betweenthescanandvideo(e.g.,thewheelinthemiddle \n example)thealgorithmiscapableofproducinggoodpose \n     \n estimates. x u \n 14 Super Glue weights: https://github.com/magicleap/ \n vˆ \n f \n =y=d K−1 c=d K−1 v (17) \n z 1 \n Super Glue Pretrained Network \n 38 "
  },
  {
    "page_num": 39,
    "text": " \n \n \n \n \n \n Video frame Depth from DPT Depth from the scan RT depth L 2 angle Succ∗% Succ% Qw P%\n ground-truth random 7.93 1.99 0.00 0.00 1.83\n ground-truth scan 2.92 1.10 76.47 1.22 1.83\n ground-truth DPT 3.33 1.15 76.47 1.22 1.83\n Siam-RCNN+PF DPT 6.53 1.64 25.00 0.61 0.61\n Siam-RCNN+KYS(sim.) DPT 5.78 0.48 36.36 0.61 0.61\n Siam-RCNN+KYS(res.) DPT 5.98 1.60 30.77 1.22 1.83\n Table 10.Visualqueries 3 Dlocalizationresults.Wecomparethe\n performanceofvariousbaselinesonthevalsetofthe VQ 3 Dtask.\n Column 1 indicatesthe VQ 2 Dnetworkusedtopredicttheresponse\n track(RT).Thelastmetric Qw Pmeasuresthequeryratioforwhich\n wehaveposeestimationfortheresponsetrackandthequeryframe.\n The L 2 metricisexpressedinmetersandanglesareinradians.The\n firstthreerowsareablationstudiesusingtheground-truthresponse\n tracksandwithdepthestimatedrandomly,usingthescanandvia\n Figure 33. Samplesofdepthestimation. Leftshowstheframe the DPT[185]network. \n fromtheegocentricvideo,middlehastheestimateddepthfrom \n DPT[185]andrighthasthedepthfromthescanrenderedatthe \n estimatedviewpoint. using DPT(lines 2 and 3). Thissuggeststhatthereisalso\n roomforimprovementindesigningbetterdepthestimators.\n whereu,varethepixelindicesofthecentroidcinframef. \n Wethenestimatetheobjectcentroidpositiontˆ inthescan Naturallanguagequerybaselines\n s \n coordinatesystem: Sincethenaturallanguagequeriescanbeseenasalanguage-\n groundingprobleminavideo,weadopttwopriormethods\n tˆ =Psvˆ (18) \n s f f inordertoimplementthebaselinesforthistask.\n where Ps is the camera pose for the frame f. We further (a)2 DTemporal Adjacent Networks(2 D-TAN)[236]:\n f \n retrieve the displacement vector vˆ in the query frame Q We apply 2 D-TAN with a sliding window method to im-\n Q \n coordinatesystem: plementthenaturallanguagequerybaseline. Thegoalof\n 2 D-TANistoanswerwherethesemanticallycorresponding\n vˆ Q =P Q s−1 tˆ s (19) videomomentis,givenalanguagequeryinanuntrimmed\n video. Thelanguagequerystemsfromoneofthe 13 tem-\n where Ps isthecameraposeofthequeryframe. \n Q plate questions. The core idea of 2 D-TAN is to consider\n Experimentsandresults Wecomparetheperformanceof adjacent moment candidates as the temporal context on a\n multiplebaselinesalongwithablationstudies. Wepresent two-dimensionaltemporalmapandretrievethemostrele-\n theresultsin Table 10. Numbersarecomputedonthevalida- vantmomentfromthecandidates.Moreconcretely,2 D-TAN\n tionset(164 queries)ofthe VQ 3 Dtask. Wereportthequery takeseachmomentcandidateasoneelementinthe 2 Dtem-\n ratio Qw P,forwhichwehavecameraposeestimatesforthe poralmapsuchthattheadjacentmomentcandidatesonthe\n responsetrackandqueryframe. Additionally,wereportthe mapcanhavemuch-overlappedcontentorsharethesame\n success rate Succ∗ which is the success metric computed startorendtimeslot. Itappliesaconvolutionalneuralnet-\n onlyforquerieswithassociatedposeestimates. workonthe 2 Dmaptopredictthe Intersectionover Union\n Overall,wenoticealow Qw Pratioleadingtoalowsuc- of each moment candidate and the ground-truth moment.\n cessrate. Theselowmetricsareduetoasmallnumberof Pleasesee[236]formoredetails.\n cameraposeestimates(15%overall). Nonetheless,weob- Since the 2 D-TAN enumerates all the possible combi-\n servethatthebest VQ 2 Dbaselinemethodcombinedwiththe nationsofstart-endpairs,the O(N 2)spacecomplexityof\n pretrained DPT[185]depthestimatoryieldsthebestperfor- the 2 D map leads to a heavy model, especially when we\n mancesintermsof L 2 andsuccess. Thesenumberstellthat requireaprecisemomentboundary. Tomake 2 D-TANmore\n thereareopportunitiesforenhancementindesigningbetter appropriatetoourproblem,wefurtheruseaslidingwindow\n cameraposeestimators. Additionally,weperformablation methodontopof 2 D-TAN.Webreakdowntheclipintoa\n studiesusingtheground-truthresponsetracksanddifferent numberofoverlappingwindows,whereawindowpresentsa\n depthestimators(random,fromthescan,using DPT).For smallportionoftheclip. Thewindowsaretakenastheinput\n therandomexperimentweuniformlysampleadepthvalue ofthe 2 D-TANmodelinbothtrainingandtestingphases.\n between 0.1 and 10 meters. Fromtheablationexperiments During the training of the 2 D-TAN model, we use\n wenotethatrenderingthedepthfromthescanattheesti- Ego 4 D’sprovidedpre-extractedfeaturesforboththevideo\n mated viewpoint increases the performances compared to clip and language query. The clip feature is from a Slow-\n 39 "
  },
  {
    "page_num": 40,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 34.Baselinemodelarchitectures:momentqueries.Itstakesavideosequenceandgeneratesdetectedactionswithstart/endtime,\n theircategories,andconfidencescores. Ithastwocomponents: graphpyramidnetwork(GPN),andscoringandlocalization(So L).\n GPNiscomposedofmulti-levelencoderanddecoderpyramids.Theencoderaggregatesfeaturesindifferentlevelsviaastackofgraph\n networks(GN)(yellowtrapezoidarea;thedecoderrestoresthetemporalresolutionandgeneratesmulti-levelfeaturesfordetection.So L\n (bluedashedbox)containsfourmodules,thetoptwopredictingactionscoresandboundaries,thebottomtwoproducingsupplementary\n scoresandadjustingboundaries.Figureisadaptedfrom[237]. \n \n Fast[71]networkpretrainedon Kinetics 400 dataset, and Io U=0.3(%) Io U=0.5(%) \n Baseline \n the language feature is a based on the BERT model [52]. r@1 r@5 r@1 r@5 \n Thewindowdurationis 40 s,andstrideis 20 sinthesliding \n windowmethod. Notably,weonlyusewindowsthatcontain \n or are next to a ground-truth moment in training, but we \n useallthewindowsintesting. Wekeepalltheotherhyper- \n parametersin 2 D-TANthesameasitsdefaultexceptfort Io U \n thresholdandlearningrate. Wedecreasedthet Io Uthreshold \n from 0.5 to 0.3 toenablemorepositivesamplesduringtrain- \n ingandempiricallysetthelearningrateto 0.001. Wetrain \n themodelfor 100 epochsandreportthetestsetperformance \n onthebestcheckpointonthevalidationset. 2 D-TANgives \n top-1 and top-5 recalls of 5.80% and 13.90% at Io U=0.3, \n respectively. Inaddition,wealsoablatethemodeltoobtain \n performance by randomizing the video features ( visual) \n − andtextualfeatures( text)for NLQin Tab.11. \n − \n (b)Span-based Localization Network(VSLNet)[235]: \n Unliketraditionalapproachesinvideonaturallanguagelo- \n calizationworks,VSLNettreatstheinputuntrimmedvideo \n asatextpassage,andusesaspan-basedapproachtoidentify \n therelevantsectionssemanticallyrelatedtothegivennatural \n languagequery.Atitscore,VSLNetfirstencodesthenatural \n languagequeryandvideofeaturesusingacommon,shared \n Transformer[215]network. Next,itusestheencodedquery \n tothenattendtotherelevantpartsofthevideoclip(akinto \n atextparagraph). Theattendedsectionsarefurtherrefined \n usingaquery-guidedhighlighting(QGH)strategybyextend- \n ingtheselectionforegroundofthevideobyahyperparamter \n tocapturemorevisualcontext.Pleasereferto[235]formore \n la V (cid:8) 2 D-TAN[236] 5.04 12.89 2.02 5.88\n VSLNet[235] 5.45 10.74 3.12 6.63 \n tse T \n  \n 2 D-TAN[236] 5.80 13.90 2.34 5.96 \n  \n − \n v \n t \n i \n e \n s \n x \n u \n t \n al \n 3 \n 2 \n . \n . \n 4 \n 2 \n 6 \n 9 \n 1 \n 6 \n 0 \n . \n . \n 7 \n 1 \n 7 \n 3 1 \n 1 \n . \n . \n 7 \n 3 \n 8 \n 2 \n 4 \n 3 \n . \n . \n 3 \n 4 \n 8 \n 6 \n − \n VSLNet[235] 5.47 11.21 2.80 6.57 \n  \n − \n v \n t \n i \n e \n s \n x \n u \n t \n al \n 3 \n 1 \n . \n . \n 0 \n 8 \n 5 \n 0 \n 7 \n 5 \n . \n . \n 3 \n 4 \n 9 \n 4 \n 1 \n 0 \n . \n . \n 4 \n 9 \n 5 \n 0 \n 4 \n 2 \n . \n . \n 1 \n 4 \n 2 \n 5 \n − \n Table 11.Performanceofthe NLQbaselinesonvalandtestsplits.\n detailsonthemotivationandarchitecture. \n Forourexperiments,wemaintainconsistencywiththe\n other NLQbaselinesandusepre-extractedfeaturesforboth\n thevideoclip(Slow Fastnetwork[70])andnaturallanguage\n query(BERT[52]). Weusetheimplementationprovided\n by the authors 15 with the following changes: (a) Set the\n videofeaturessizeto 2304 dimensionstoaccommodatethe\n featuresextractedfromthe Slow Fastnetwork,(b)Replace\n thetextencodertoafrozen,pretrained BERT[52]model,\n (c)Settheinternaldimensionofthemultimodalnetworkto\n 128,andprojectthepre-trained BERTfeaturesfrom 768 to\n 128. Wetrainthemodelfor 200 epochsandpickthemodel\n withthebestperformanceonvalsplit. Thecorresponding\n 15 https://github.com/Isaac Changhau/VSLNet\n 40 "
  },
  {
    "page_num": 41,
    "text": " \n \n \n \n \n \n testperformanceofthis VSLNetmodelisreportedin Tab. Table 12. Momentqueriesresultsonthevalidationsetandthe\n 11,alongwithvisualandtextualablations. test set,measuredbym AP(%)atdifferentt Io Uthresholds.\n t Io Uthreshold 0.1 0.2 0.3 0.4 0.5 Average\n Momentsqueriesbaseline \n validation set 9.10 7.16 5.76 4.62 3.41 6.03\n test set 8.61 6.52 5.43 4.30 3.57 5.68 \n Weformulateamomentqueriesbaselineasatemporalac- \n tion detection method [141,229,237], plus simple post- \n processing. \n 5 levelsinthegraphpyramidnetwork,eachwithtemporal\n The MQtaskonlyexpectspredictionsforthequerycat- \n length 232,116,58,29,and 14 respectively. Wepre-define\n egories,whereasthetemporalactiondetectiontaskreturns \n twobaseanchorsofsizes 4 and 12 for Level 1 andincrease\n the predictions for all categories. Therefore, we can first \n thesizesby 2 foreachdeeperlayer. Wetrainfor 30 epochs\n use a temporal action detection method to predict for all \n withabatchsize 32 andlearningrate 0.0001. Ininference,\n categories,andonlyoutputtheresultscorrespondingtothe \n weonlyapplyper-category NMSwithaconfidencethreshold\n querycategories. \n 0.0005. \n To predict all categories, we adopt a recent method \n VSGN[237],whichwasdesignedfortemporalactiondetec- \n Experiments and results We show our baseline perfor-\n tioninthird-personvideos. Weuse VSGNwithoutthe VSS \n manceintermsofm APin Table 12 andrecall@kx,t Io U=m\n component. Figure 34 illustratesthearchitecture. Ittakes \n in Table 13. \n avideo asinput,extractsfeaturesforeachsnippetinthe \n V Weprovidefurtheranalysisontheaverageprecisionre-\n videousinganetworksuchas Slow Fast[70],andfeedsthese \n sultsusing DETAD[9]. In Fig 35,weillustratethepropor-\n featuresintoagraphpyramidnetwork. Thegraphpyramid \n tionofeacherrortypeforthefalsepositivepredictions. It\n network contains a encoder and a decoder, where the en- \n shows that both localization and classification are respon-\n coderiscomprisedofmultiplelevelsofgraphconvolutional \n sible for the false positive, improving either can increase\n networks,andthedecoderiscomprisedofmultiplelevels \n theoverallperformancebyanontrivialamount. In Fig 36,\n ofde-convolutionalnetworks. Itisananchor-basedmethod \n wedemonstratetheperformanceofdifferentgroupsofmo-\n thatpre-definestemporalsegmentsforeachfeaturelevelas \n mentinstances basedonmoment durationandnumber of\n predictionreference. Itpredictsthescoresandrefinesthe \n instancesbelongingtothesamecategorypervideoclip. We\n locationsoftheanchorsintwostages. Inthefirststage,it \n notice that short moments tend to have low performance\n usesaregionproposalnetwork(RPN)fromthedecoderto \n eventhoughtheyarelargeinnumber. Whenthereare 2-3\n predictclasslabelsandregressboundariesforeachanchor; \n instancesinonevideo,theyareeasiesttodetect.\n inthesecondstage,itappliesaboundaryadjustmentmodule \n torefinetheboundaryoffsetsbasedontheupdatedanchors \n fromthefirststage. Italsohasstartness/endnesspredictions \n toprovideauxiliarysupervisionandsupplementscoresfor \n each predicted segment. Its output predictions are formu- \n M \n lated as Φ = φ =(t ,t ,c ,s ) , where m { m m,s m,e m m }m=1 is the number of predictions, t and t are start time m,s m,e\n andendtimeofthemth predictionrespectively,c isthe m \n predictedcategory,ands istheconfidencescore. Formore m \n details,pleasereferto[237]. \n Given a query category c, the retrieval results for the \n momentqueriestaskareobtainedasfollows \n Φ = φ =(t ,t ,c ,s ) c =c,1 m M) . \n c m m,s m,e m m m \n { | ≤ ≤ } \n (20) \n Implementation details For feature extraction, we use \n Ego 4 D’s provided pre-extracted features using a Slow- \n Fast[70]networkpre-trainedon Kinects 400[108]at 1.87 \n featurespersecond. Thefeaturedimensionis 2304. \n Consideringthatthemaximumcliplengthis 8 minutes, \n which has 897 features, we make the input length of our \n network 928 framestocoverthelongestvideoclip. Wehave \n G 1 G 2 G 3 G 4 G 5 G 6 G 7 G 8 G 9 G 01 \n 100 \n 90 80 \n 70 60 \n 50 \n 40 \n 30 \n 20 \n 10 \n 0 \n Top Predictions \n )%(nwodkaer Brorr E \n Background Err Localization Err Double Detection Err\n Confusion Err Wrong Label Err True Positive\n False Positive Profile 2.00 \n 1.75 1.50 \n 1.25 \n 1.00 \n 0.75 \n 0.50 \n 0.25 \n 0.00 \n Error Type \n NPAm-egarev A )%(tnemvorpm I\n Removing Error Impact\n 1.2 \n 0.8 0.8 0.8 \n 0.1 \n Figure 35.Momentqueriesresults:falsepositiveanalysis.The\n errortypesaredeterminedbythet Io Ubetweenground-truthand\n predicted moments, as well as the correctness of the predicted\n labels,accordingto[9]. Backgrounderror: t Io U<1 e−5;confu-\n sionerror: 1 e−5 < t Io U < α,labeliswrong;wronglabelerror:\n t Io U>=α,labeliswrong;localizationerror:1 e−5 <t Io U<α,\n labeliscorrect,whereαreferstothet Io Uthresholds{0.1,0.2,0.3,\n 0.4,0.5}.‘G’referstothenumberofground-truthinstances.\n 41 "
  },
  {
    "page_num": 42,
    "text": " \n \n \n \n \n \n Table 13.Momentqueriesresultsonthevalidationsetandthetestset,measuredbyrecall(R)@kx,t Io U=m(%).\n m 0.3 0.5 0.7 \n k 1 3 5 1 3 5 1 3 5 \n Validation Set 33.45 51.26 58.43 25.16 39.46 46.18 15.36 22.67 25.81\n Test Set 33.56 52.23 59.79 24.25 39.22 46.22 14.83 23.15 26.28 \n \n \n 80 \n 70 \n 60 50 \n 40 \n 30 \n 20 10 \n 0 XS S M L XL XS S M L XL \n htur T \n dnuor G \n fo \n % \n Length #Instances \n 63.4 \n 52.2 \n 31.0 \n 20.5 \n 12.5 7.5 3.5 5.7 3.1 0.5 \n 20.0 \n 17.5 \n 15.0 \n 12.5 \n 10.0 \n 7.5 \n 5.0 \n 2.5 \n 0.0 XS S M L XL XS S M L XL \n )%( \n NPAm-egarev A \n video clips, moving a step closer to augmenting a user’s\n episodicmemory. \n Momentqueriesinegocentricvideosisachallengingtask\n duetothelong-taileddistributionofcategoriesandthelarge\n variationinmomentduration. Ourbaselineachievesarea-\n sonableresultaccordingtothemetricrecall@kx,t Io U=m,\n whichevaluatestheperformanceofeachquerycategoryand\n doesnotrequirecorrectclassificationofallcategories. In\n contrast, itsaveragem APscoreof 5.96%islowwhenall\n 13.9 \n 11.4 10.5 11.2 categoriesareevaluated.Accordingtothefalsepositiveanal-\n 8.0 ysisin Fig 36,errorscausedbywronglabelsaresignificant.\n 6.9 6.3 6.36 \n 5.1 Amoresophisticatedclassifierforallcandidatemoments\n 3.1 \n can be explored in future work. In addition, as shown in\n 0.1 \n Fig 36, theperformanceofshortmoments, whichoccupy\n a large proportion in the dataset, is not as good as that of\n Figure 36. Momentqueriesresults: sensitivityanalysis. Top: \n long moments. Therefore, improving short moments will\n Distribution of instance per action characteristic: length; # in- \n significantlyimprovetheoverallperformance.\n stances. Bottom: averagem APN (%)[9]ineachcharacteristic \n bucket.The‘length’characteristicdividesallmomentinstances 5 \n bucketsbasedonthemomentsdurationinseconds:XS(0,10],S Contributionsstatement \n (10,60],M(60,180],L(180,300],and XL(300,inf].The‘#in- \n stances’characteristicdividesallmomentinstancesinto 5 buckets Kristen Graumanledthe Episodic Memorybenchmarkand\n basedonthenumberofinstancesbelongingtothesamecategory paper writing, wrote annotation instructions, contributed\n inonevideoclip:XS(0,1],S(1,3],M(3,10],L(10,20],and XL todataselectionandtaxonomyformation,andco-advised\n (20,inf]. the VQbaselinedevelopment. Bernard Ghanemco-ledthe\n Episodic Memorybenchmark,managedbaselinedevelop-\n Discussion mentandevaluationforthe MQand NLQtasks, andcon-\n tributed to the annotation instructions, data selection, and\n Visual queries presents a novel and challenging task for taxonomyformationforthe MQand NLQdatasets. Jackson\n objectlocalizationinegocentricvideos. Whileourproposed Hamburger contributed to the development of the annota-\n baselineachievesareasonablesuccessrateof 42.9%,itonly tioninstructionsandtaxonomiesofthe NLQ,VQ,and MQ\n achieves a localization performance of 0.13 t AP and 0.06 datasetsalongwiththedesignoftheearly VQbaselines.\n st AP.Furthermore, thebestperformanceisachievedwith Santhosh Kumar Ramakrishnan led VQ data selection,\n 0%searchefficiency,andna¨ıvetechniquestoimprovethe annotation,analysisandauditing,contributedtotheformu-\n searchefficiencyleadtodrasticperformancereductions. We lationandannotationinstructionsof VQ,dataselection,and\n hopethatthistaskwillspurfutureresearchintoaccurateand implementedthe VQbaseline. Vince Cartilliercontributed\n efficienttechniquesforobjectsearch. tothe VQ-3 Dformulationandannotationinstructions,led\n Natural language queries is a challenging multimodal VQ-3 D data selection, annotation, analysis and auditing,\n taskthathaswideapplicationsinhelpinguserssearchand and implemented the VQ-3 D baseline. Dhruv Batra co-\n retrieve relevant pieces of their episodic memory, thanks mentored Vince Cartillierondevelopingbaselinesandpro-\n to the flexibility of the queries. The performance of the vided guidance on 3 D scans using Matterport. Hyun Soo\n existingstate-of-the-artvideolocalizationmodelshighlights Parkcontributedto 3 Dreconstructionofegocentricvideos\n theneedle-in-a-haystacknatureofthetask, duetoshorter with respect to 3 D Matterport scans. Tien Do developed\n response windows of about 10 s in a large video clip of 8 algorithmstoreconstruct 3 Degocentriccameraposeswith\n minutes. Wehopethatthe NLQdatasetopensthedoorto respectto 3 DMatterportscans. \n futureresearchthatspecializesinidentifyingandretrieving James Hillisprovidedbackgroundknowledgeonhuman\n a large diversity of language queries in longer egocentric episodicmemoryfunctionandcontributedtoearlydiscus-\n 42 "
  },
  {
    "page_num": 43,
    "text": " \n \n \n \n \n \n sionsonbenchmarkdefinitionandannotation.Satwik Kottur \n ledthedesignof NLQdataselectionandannotationinstruc- \n tions,contributedtothe NLQtaskformulation,coordinated \n NLQ data annotation and data analysis, implemented the \n VSLNet NLQbaseline,wrotepartofthe NLQsections.Men- \n meng Xudesignedandimplementedtheexperimentpipeline \n forthe NLQtask,implementedseveral NLQmethods,did \n NLQresultanalysisandvisualization,andwrotepartofthe \n NLQsections. Michael Wraycontributedtoearlyformula- \n tionofthebenchmarktasks,definitionsof NLQqueriesand \n annotationinstructions,providedinputfordatasetconstruc- \n tionandevaluationmetrics,andhelpedinthecreationofthe \n MQtaxonomy. \n Chen Zhaodesignedandimplementedthe MQbaseline, \n proposedandimplementedthenewmetricfor MQ,wrote \n the MQsections,did MQresultanalysisandvisualization, \n contributed to the formulation, data selection and annota- \n tioninstructionsof MQ.Tushar Nagarajancontributedtothe \n MQformulationandannotationinstructions,developedthe \n MQlabeltaxonomy,andledthedataselectionandannota- \n tionofthe MQdataset. Merey Ramazanovamanagedthe \n datasetsfortheexperimentsof MQand NLQbaselines,and \n assistedwiththetaxonomyformationforthe MQbaseline. \n Antonino Furnariprovidedkeypointfeatureextractionfrom \n the Matterport 3 Dpanoramasforthe VQ 3 Dbaseline. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 43 \n \n \n "
  },
  {
    "page_num": 44,
    "text": " \n \n \n \n \n \n G.Handsand Objects Benchmark \n Thissectiondetailsthe Handsand Objectsbenchmarkin- (a) \n cludingdefinitions,annotations,baselinemodelsandresults. \n \n G.1 Motivation \n \n Inavideoofahumanoperatingandmanipulatinganobject (b) \n with their hands, there may exist an object state change, \n i.e.,thepointwherethestateoftheobjectsbeingoperated \n changes, either temporarily or permanently in a way that \n cannot be easily reversed. Examples of temporary state \n (c) \n change include turning on a machine, while examples of \n permanentstatechangesincludephysicalchangessuchas \n choppingatomatointopiecesandchemicalchangessuchas \n mixingwaterandcementpowdertogethertocreateanew Figure 37. Examples of object state change. (a) State change\n composition of cement. Some examples are illustrated in through construction: attaching to two metal plates results in a\n Figure 37. newobject. (b)Statechangethroughphysicalchange: cuttinga\n piece of wood results in two smaller pieces of wood. (c) State\n Theconceptofanobjectstatechangehasbeenexplored \n changethroughchemicalreaction:combiningtwoobjects,water\n onlyinalimitedmannerinthevideoliterature[8,45,69] \n andcementpowder,resultsinanewobject,cement.\n andthecharacterizationofstatechangeshasdependedon \n manybrittlevision-basedcomponenttechnologies,makingit \n difficulttoanalyzestatechangesatscale. Fortunately,inthe \n tionsthatrequirerichhumandemonstrations,suchasrobotic\n lastdecadewehaveseentremendousadvancesincomputer \n manipulation. \n visionalgorithmsforunderstandingbothobjectsandhands. \n Defining Object State Changes: This benchmark fo-\n Asaresult,webelievethatnowitistimetoinvestigatethe \n cusesonidentifyingandlocalizingthestatechangeofan\n ideaofcharacterizingstatechangesatscaleandindepth. \n object in an egocentric video. Specifically, a object state\n Whyisrecognizingtheimpactofagentsonobjectsand \n changecanberepresentedbythethreeaspectsinthevideo:\n environments so critical? We believe that understanding, \n temporal,spatial,andsemantic. \n recognizing,andreplicatingobjectstatechangesareanes- \n sentialaspectofcreatingartificialintelligence(AI)systems. \n Whilecurrent AIsystemshavetheabilitytoreplicatecertain Temporal: An object state change can be represented by\n typesofhumanactionssuchasassemblingfurniture[116]or three distinct temporal points in the video. (1) Point-of-\n cuttingtomatoes[200],mostsystemsdonotpossessagen- no-return: Thepoint-of-no-return(PNR)istheframe I pnr\n eralunderstandingofhowtheenvironmentandtheobjects in a video that identifies the beginning of an object state\n canbetransformedasaresultofinteraction. Understanding change that cannot be easily reversed. (2) Pre-condition:\n theimpactofinteractionsonobjectsandtheenvironmentis Thepre-conditionisdefinedassomeframe I pre thatmarksa\n animportantaspectofreasoningandcanhelp AIsystems momentpriortothestate-changeinwhichtherelatedobjects\n performmoreadvancedtasks. Forexample,understanding werevisiblewithinthefieldofviewofthecamera. (3)Post-\n theimpactofinteractionsontheenvironmentcanhelp AI condition: Thepost-conditionissomeframe I post atwhich\n systems relate multiple ways to achieve the same change, thecompletionofthestatechangeisvisibleafterthepoint-\n discoverefficientmethodsforachievinggoalstates,recog- of-no-return. Thesethreeframesmarkthedistincttemporal\n nizethecompletion/incompletionofgoals[58,97],recover stagesoftheobjectstatechange:beforeandafterthechange,\n fromfailure,andlearnfrommistakes. respectively. Thisproposalmatchesthe Rubicon Boundaries\n Inegocentricvideosspecifically,theobjectstatechanges proposedin[160]. \n offerrichandimportantinformationthatarerelatedtomany \n otherproblems. Forexample, theobjectundergoingstate Spatial: Anobjectstatechangecanberepresentedbythe\n changeinanegocentricvideocanimplyhuman-centricin- boundingboxoftheobjectatthe PNR,pre-conditionand\n formationsuchashumanactivityandintention. Moreover, post-condition,alongwithanytoolsinvolvedinperforming\n the state change of an object shown provides cues about the state change. Tools offer extended capabilities of the\n human-specificaffordanceandactionableinformationofan actor’shand,suchasusinganelectricsawtocutapieceof\n objectortool,whichcannotbeeasilyinferredfromstatic woodinhalf. Theseboundingboxesrepresentthespatial\n images. Additionally,ajointunderstandingofhumanhands dimensionsofhands,toolsandtheobjectsundergoingthe\n andtheobjectsundergoingstatechangecanbenefitapplica- statechange. \n 44 "
  },
  {
    "page_num": 45,
    "text": " \n \n \n \n \n \n Semantic: Werepresentanobjectstatechangethroughthe gettaskisactivityrecognition[180]. Inthe UT-Egocentric\n humanaction(verb),theobjectidentity(noun)andthetype dataset(UT-Ego),subjectswearahead-mountedcameraand\n ofstatechangeapplied. Thesamestatechangecanbeper- performlongunscriptedactivitiesinsideandoutsideofthe\n formedondifferentobjectsusingdifferenttools. Forexam- home,withatotalof 17 hoursfrom 4 subjects(4-5 hoursof\n ple,cuttingapieceofwoodwithelectricsawandcuttinga continuouscaptureforeachperson);thetargettaskisvideo\n pieceofpaperwithscissorsaredifferentinteractionswith summarization[130]. The UTEgocentric Engagement(UT\n differentobjectsanddifferenttoolsbuttheybothresultin EE)datasetconsistsof 14 hoursofhead-mountedcamera\n thesameobjectstatechangeofbeingcut. videocapturedinpublicspaceslikemuseums, malls, and\n grocerystores,andisannotatedformomentsofengagement\n bythecamerawearerwiththeenvironment. Inthe EGTEA+\n G.2 Related Work \n dataset,32 subjectswearinghead-mountedcamerasinasin-\n Object State Changes: Existingapproachesformodeling glekitchenenvironmentcapture 28 hoursofvideo;thetask\n object states and/or their changes can be categorized into is to recognize 44 meal preparation activities [136]. The\n two research lines. The first deals with collections of im- EPIC-KITCHENSdatasetconsistsof 100 hoursofkitchen\n ages. Arepresentativedatasetforthispurposeisthe MIT activitiesrecordedin 45 uniqueenvironments,withatotalof\n States dataset [103]. By considering object states as ob- 89,977 differentobjectinteractionsacross 97 verband 330\n jectattributes(e.g. burnt,sliced),thislineofworkstudies nounclasses;thetaskistorecognizeobjectsandactivities\n attribute-object composition, e.g. composition with con- andanticipateinteractionsinthenextmomentofvideo[43].\n text [158], modeling attributes as operators [164], and an The Charades-Ego dataset consists of 34 hours of video\n architectureforcompositionalreasoning[182]. from 71 participants,withbothfirst-andthird-personpaired\n Thesecondresearchlinedealswithvideoandviewsan instanceslabeledfor 156 actions[201].\n actionasastatetransformationovertime. Onedirectionis \n thediscoveryofobjectstatesand/ormanipulatingactions, G.3 Benchmark Definitions\n e.g.inegocentric[45,69]andinstructionalvideos[8]. Fathi \n et al. [69] explore object state detection in video using a Wenowdefinethethreetasksthatcomprisethe Handsand\n weaklysupervisedapproach. Anotherdirectionisthemod- Objectsbenchmark. Thethreetaskscorrespondtothethree\n elingofstatetransitions. Zhouetal.[244]studytemporal aspectsofobjectstatechangesdescribedabove,namely,the\n transformationsofasingleobjectstateintime-lapsevideos. temporal,spatialandsemanticaspectsofastatechange.\n Wangetal.[223]proposetomodelstatetransformationsin (1) PNR Temporal Localization. The goal of Point-of-\n ahigh-levelfeaturespacewith Siamesenetworks. Doughty no-return (PNR) Temporal Localization is to predict I .\n pnr \n et al. [55] leverage natural language and treat adverbs as Onepossibleformulationistoviewthisproblemasaper-\n modifiersforstatetransformations. Intermsofapplications, frame classification problem, predicting the Point-of-no-\n Changetal.[30]showstatetransformationscanbeutilized return frame within a short video clip. The performance\n forprocedureplanning. is evaluated only on the videos that contain object state\n change,andismeasuredbytheabsolutetemporalerrorof\n Human Hand Action Datasets: Several video datasets \n I predictioninseconds. \n havebeenproposedforhumanhandactionrecognition. The pnr \n The PNRwasfirstdiscussedby P.Gollwitzerinhiswell-\n Yalehumangraspingdataset[25]focusesonhumangrasping \n cited handbook of behavior [89]. Specifically, the book\n behaviorandconsistsof 27.7 hoursofannotatedvideos. The \n proposes the Rubicon Model of Action Phases, focusing\n Something-Somethingdataset[90]consistsof 220,847 short \n onhand-objectinteraction. Actionphasesaredelimitedby\n videosannotatedwith 174 categoriesofgeneralhand-object \n threetransitionpoints: initiationofpriormotion,PNR,and\n interactions. The Jesterdataset[214]provides 148,092 short \n goalachievement. Thiswaslaterexperimentallyassessedby\n videos in 27 hand gesture types. Wang et al. [220] con- \n ourpreviouswork[160],where PNRannotationswereac-\n structasyntheticvideodatasetofhuman-objectinteraction \n quiredforthreeegocentricdatasets,demonstratingincreased\n throughrenderinghandandobject CADmodels. Therecent \n accuracyofannotations(see Fig.10 in[160])andimproved\n Human Handsdataset[198]annotates 100 Ksingleframes \n robustnessintrainingmodels(see Sec.5 in[160]). Below,\n fromweb-basedvideos,focusingonhandinteractionsand \n wefind PNRcloselyalignswiththenarrationtimestamps\n theoffsetbetweenthehandandtheinteractingobjectduring \n thatweindependentlycollected,suggesting PNRisanatural\n interaction. \n timepointforhumanunderstanding(andthusnarration)of\n Several egocentric video datasets capture daily living \n theinteraction. \n activitiesbypeople[43,130,136,180,201,210]. Inthe Ac- \n tivitiesof Daily Living Dataset(ADL),subjectswearchest- (2) State Change Object Detection. We define a State\n mountedcamerasandperformunscriptedactivitiesathome, Change Objectastheobjectthatismanipulatedbyaperson\n withatotalof 10 hoursofvideofrom 20 participants;thetar- andundergoesachangeinitsstate. Thegoalofthistaskis\n 45 "
  },
  {
    "page_num": 46,
    "text": " \n \n \n \n \n \n topredictthe 2 Dboundingboxesofthe State Change Object toselectthreecriticalframesintime: PNR,PRE,and POST.\n in Point-of-no-return frame I given three frames: Pre- Weasktheannotatorstostartwiththe PNRframethatiden-\n pnr \n condition I , Point-of-no-return I , and Post-condition tifiesthebeginningofthestatechange. Thisframeisless\n pre pnr \n I . We expect that a good solution to this task would ambiguousandhelpsprovidethecontextfortheinteraction.\n post \n incorporate the visual information before and after state Wethenasktheannotatorstolabelaframepriortothestate\n change to detect the State Change Object. The detection change(PRE)andaframeafterthecompletionofthestate\n performanceisevaluatedontheboundingboxesestimatedin change(POST).Notethatthe PREand POSTframesarenot\n the Point-of-no-returnframe I andmeasuredby Average uniquelydefined. Welettheannotatorspickany,aslongas\n pnr \n Precision(AP). therelevantobjectsarefullyvisiblewithinthefieldofview\n ofthecamera. \n (3)Object State Change Classification. Thetaskof Ob- \n ject State Change Classificationclassifiesashortvideoclip Preperiod. Next,welabelboundingboxesforthehands,\n to a state change type. With N object state change types tools, and objects, as well as the category names for the\n defined,objectstatechangeclassificationisessentiallyan toolsandobjects. Wedothisintwosteps. Firstwelabel\n (N+1)-wayclassificationproblem,wheretheoneadditional the frames in the pre period, starting at PNR and going\n category is “without state change.” Object State Change backwardtothepreframe. Thevideoframesarereversed\n Classificationisevaluatedbyclassificationaccuracy. andtheannotatorscanplaythevideo.Wefindthatitiseasier\n tostartfromthe PNRframesincethehandsandobjectsare\n clearlyvisible. Tospeeduphandboxlabeling,weinitialize\n G.4 Data Selection \n thehandboxeswithapre-trainedobjectdetector[198]and\n Nextwedescribeourdataselectionprocedureandannotation asktheannotatorstocorrectthese.\n pipeline, and we present the analysis of the data for the \n Postperiod. Finally,weasktheannotatorstolabelspatial\n objectstatechangebenchmark. Webeginbydescribingour \n annotationsandcategoriesforthepostframe. Asbefore,we\n procedureforselectingthesubsetofdatatoannotateforthis \n firstpresenttheannotatorswiththe PNRframe. Notethat\n benchmark. \n inthiscasethe PNRframeisalreadylabeledwhichhelps\n Westartwithalargepoolofvideosannotatedwithhigh- \n identifythehandsandobjectstolabelinthepostframe.\n levelscenariolabels(e.g.,gardening,cooking,landscaping, \n etc.)andnarrations. Weassesseachscenarioonthescale \n G.6 Data Analysis \n of 0 to 3 based on how likely it is to contain hand-object \n interactions (e.g., 0 for “watching tv”, 3 for “carpentery”, Finally,wepresenttheanalysisofourannotations.\n etc.).Wethensampledatatoannotatefollowingtheresulting \n Critical frames. In Figure 40 we show the temporal dis-\n scenariodistribution. Givenascenarioandatargetnumber \n tributionofcriticalframeswithinthe 8 secondhand-object\n ofhours,wesampleclipsrandomlyinahierarchicalfashion: \n interactionsnippets. First,weobservethatthe PNRframe\n wefirstsampleaparticipant, thenavideo, andfinallya 5 \n distributioniscenteredaroundthemiddleofthe 8 second\n minuteclipfromthevideo. Ifthevideoisshorterthan 5 min \n snippet. Interestingly,thiscloselyalignswiththenarration\n wetakethewholevideo. Foreachscenario,webalancethe \n point(4 smark). Next,weseethatmostofthepreandpost\n data across universities to maximize geographic diversity. \n framescomeshortlybeforeandafterthe PNRframe,respec-\n Theresultingscenarioanduniversitydistributionsareshown \n tively,highlightingthequicknatureofthesestatechanges,\n in Figure 38. Intotal,ourdatasethas 120 hoursrepresenting \n andthusthechallengeinthisbenchmark.Wealsonoticetwo\n 53 scenarios,7 universities,and 406 participants. \n additionalmodesforpreandpostframesthatcomeatthe\n startandtheendofthe 8 sinterval,respectively. Thesecorre-\n G.5 Data Annotation \n spondtolongrepetitiveactionsthatstartbeforeorcontinue\n pastthevideosnippet(e.g.,knitting). \n Weannotatehand-objectinteractionscorrespondingtoeach \n narration within the selected 5 minute clips. We use the Handsandobjects. Ourbenchmarkcontainsalargenum-\n taxonomy from Section D.3 for semantic verb and noun berofhandsandobjectsannotatedwithboundingboxes. In\n labeling.Theannotationpipelineconsistsofthreesequential total, we have 825 K bounding boxes, including 245 K\n ∼ ∼ \n stages: criticalframelabeling,pre-periodlabeling,andpost- forlefthand, 260 Kforrighthand, 280 Kforobjects,and\n ∼ ∼ \n periodlabeling. 40 Kfortools. In Figure 41 and Figure 42,weshowthe\n ∼ \n distributions of box sizes and locations, respectively. We\n Criticalframes. Givenanarration,wecreatean 8 second \n observethatourdatacontainshandsandobjectsatavariety\n videosnippetcenteredatthenarrationtimepointandpresent \n ofsizesandlocations. \n ittotheannotators. Weasktheannotatorstofirstreadthe \n narration and select a corresponding verb from the taxon- Actions. Oneofthefeaturesofourbenchmarkisthediver-\n omy. Theannotatorscanthenplaythevideobackandforth sityofinteractions. Wefocusonlow-levelatomicactions\n 46 "
  },
  {
    "page_num": 47,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n (a) Scenarios (b) Universities \n \n Figure 38.Numberofhours.Weshowthedistributionofthenumberofhoursacrossscenarios(left)anduniversities(right)\n \n \n \n \n \n \n \n \n \n \n Figure 39.Labeledactions.Distributionofverbs(left)andnouns(right)inannotatedactioninstances.Top 45 verbsandnounsareshown\n forclarity.See Section D.3 formoredetails. \n \n \n \n \n \n \n Figure 42.Handandobjectlocations.Distributionofbounding\n boxcenters.Showninnormalizedimagecoordinates.\n Figure 40. Criticalframes. Distributionofcriticalframetimes. \n Shownrelativetothe 8 shand-objectinteractionsnippet. \n Wenotethatourobjectsarecommondailyobjectsthatare\n nottypicallypresentinobjectdetectiondatasets(e.g.,442\n outofour 478 objectcategoriescovercategoriesbeyondthe\n 80 COCO[143]categories). \n G.7 Baselines: Object State Change Classificationand\n PNRTemporal Localization \n Figure 41. Handandobjectsizes. Distributionofboundingbox \n sizes.Shownintermsofthesquarerootoftheboxareas. Wepresenttheimplementationofseveralbaselinemethods\n for the Object State Change Classification and PNR Tem-\n poral Localizationtasks. Amongtheimplementedbaseline\n ratherthanhigh-levelactions. Weshowthedistributionof models,ingeneralthereareoneortwotypesofoutputnet-\n verbs(Figure 39,left)andnouns(Figure 39,right). Wesee workheads: aclassificationheadforthevideoclipusedfor\n thatwehavealargenumberofverbscorrespondingtocom- statechangeclassification,and/oraper-frameclassification\n monmanipulationactions(e.g.,put,take)andanaturallong headfortemporallocalization. Onecanchoosetotraintwo\n tail. Theobjectdistributionfollowsthesamegeneraltrend. modelsseparately,orusethesamebackbonemodelbuttwo\n 47 \n \n \n "
  },
  {
    "page_num": 48,
    "text": " \n \n \n \n \n \n networkoutputheadsandtrainthejointmodelwithamulti- Table 14. Numberofpositiveandnegativevideoclipsofobject\n tasklossfunction. Thefollowingbaselinemethodsincludes statechangeintrain,validationandtestsplits.\n bothtypesofmodeldesigns: \n I 3 DRes Net-50. Weuse I 3 D[29]with Res Net-50[95]as Split Positive Negative Total\n Train 20,041 21,044 41,085 \n backbonearchitectureofthemodelforboththe Object State \n Val 13,628 14,720 28,348 \n Change Classificationandthe PNRTemporal Localization \n Test 13,561 14,870 28,431 \n tasks. The Res Net backbone is followed by two network \n outputheads: astatechangeclassificationheadanda PNR \n temporallocalizationhead. Thestatechangeclassification \n Table 15.Resultsof State Change Classificationaccuracy(%).\n head is produced by global average pooling on the entire \n spatiotemporal feature tensor followed by a classification \n Baseline Val Test \n layer. The PNRtemporallocalizationheadisproducedby \n Always Positive 48.1 47.7 \n per-frameaveragepoolingfollowedbyaclassificationlayer. \n Bi-directional LSTM[91] 65.3 63.8 \n Theoveralltraininglossofthemodelisthecombinationof \n I 3 DRes Net-50[29] 68.7 67.6 \n thelossoftwoheadswhicharebothcross-entropylossfor \n classification. \n Boundary Matching Network (BMN). We use BMN \n [140]asabaselineforthe PNRTemporal Localizationtask. negativeclipsarebalancedinnumber.\n BMNisatemporalsegmentdetectionmethodbasedoncon- \n Besides the above learnable baselines, for object state\n fidencepredictionofdensetemporalsegmentproposals. We \n changeclassification,wealsopresenttheresultofthenaive\n viewthestartofthevideoasthestartofthetemporalseg- \n baseline of always predicting the positive category as the\n mentand Point-of-no-return I astheendofthetemporal \n pnr prediction. Forthe PNRtemporallocalizationtask,wead-\n segment,sowecanconverttheproblemoflocalizing Point- \n ditionallypresenttheresultofthenaivebaselineofalways\n of-no-return I totheproblemofdetectingthetemporal \n pnr selectingthecenterframeofthetrimmedvideoasthe PNR\n segment. Inourimplementation,BMNuses Res Netasthe \n frame,giventhepossiblecentrebiasofthedata.\n backbonemodel. Furthermore, BMNisonlyusedforthe \n PNRtemporallocalizationtask. Theresultsforobjectstatechangeclassificationtaskare\n Slow Fast+Perceiver. Weimplementabaselinemodel illustratedin Table 15. Thenaivebaselineofalwayspositive\n whosearchitectureconsistsof Slow Fast[70]and Perceiver predictionyieldsstatechangeclassificationaccuracyofclose\n [105] for both object state change classification and PNR to 50%. All the learnable baselines outperform the naive\n temporallocalization. Slow Fastactsasthevideodeepfea- baselineandachieveaccuracyofmorethan 60%while Bi-\n tureextractor. Thefeaturesarethenpassedtoa Perceiver directional LSTMbaselineachievesthebestperformance.\n model. Similartotheprevious BMNmodel,the Slow Fast Thisshowsthatthelearnablebaselinescanlearnmeaningful\n +Percievermodelisonlytrainedfortemporallocalization informationaboutobjectstatechange,thoughthereisclearly\n task. Thetraininglossofthemodelisthecross-entropyloss stillspaceforimprovement. Onechallengeinthistaskis\n forper-frameclassification. thatthereisverylargevarianceintermofthetypesofobject\n statechangesandobjectscontainedinthevideos.\n Bi-directional LSTM. Weimplementa Bi-directional \n LSTMmodel[91]forboththeobjectstatechangeclassifica- The results for the PNR temporal localization task are\n tionand PNRtemporallocalization. Wefirstpassindivid- illustratedin Table 16. Thenaivebaselineofalwayspredict-\n ualframestoa Res Netmodel[95]toextractdeepfeatures. ingthecenterframeyieldsatemporallocalizationerrorof\n The sequence of per-frame features is then passed to the around 1.1 seconds. Otherlearnablebaselinescanachieve\n Bi-directional LSTMasinput,withtheoutputsenttoboth bettertemporallocalizationerrorofaround 0.85 secondsor\n the per-frame classification head and the whole-sequence lesswhichshowsthebaselinemodelscanlearnmeaningful\n classificationhead. Theoveralltraininglossofthemodel informationfortemporallocalizationofobjectstatechange.\n isthecombinationofthelossoftwoheadswhichareboth Notethatthe Slow Fast+Perceivermodelachievesthebest\n cross-entropylossforclassification. temporallocalizationperformanceof 0.425 secondsonvali-\n Fortheobjectstatechangeclassificationtasks,inthecur- dationsetand 0.489 secondsontestset,whichhighlightsthe\n rentversionwefocusonthetwo-wayclassificationproblem necessityofusingattention-basedmechanismtomodelthe\n ofwhetherthereisaobjectstatechangeintheegocentric changeofobjectstate. Onechallengeforthistaskisthatin\n video.In Table 14,weillustratethenumberofpositivevideo someactions,e.g.,cuttingapieceofpaperwithscissors,the\n clipsthatcontainsanobjectstatechangeandthenumberof statechangeofanobjectdoesnotnecessarilycausesignifi-\n negativevideoclipsthatdonotcontainobjectstatechange cantchangeofvisualappearanceandthereforeitisdifficult\n inthetrain/val/testsplits. Inallthreesplits,thepositiveand tolocalizethe PNR. \n 48 "
  },
  {
    "page_num": 49,
    "text": " \n \n \n \n \n \n Table 16.Resultsof Point-of-no-returntemporallocalizationerror Table 17. Number of State Change Object and hand bounding\n (seconds). boxesintrain,validationandtestsplits. \n Baseline Val Test Split State Change Object Hand \n Always Center Frame 1.032 1.056 Train 19,347 33,254 \n BMN[140] 0.780 0.805 Val 12,912 22,098 \n I 3 DRes Net-50[29] 0.739 0.755 Test 13,118 22,576 \n Bi-directional LSTM[91] 0.790 0.759 \n Slow Fast[70]+Perceiver[105] 0.804 0.828 Table 18.Resultsofsingle-frame State Change Object Detection.\n Theperformanceismeasuredin Average Precision(AP).\n G.8 Baselines: State Change Object Detection \n Baseline Backbone AP AP 50 AP 75 \n Faster-RCNN[190] Res Net-101[95] 13.4 25.6 12.5\n Whileweexpectthatnewmethodsdevelopedforthetasks \n DETR[27] Res Net-50[95] 15.5 32.8 13.0\n ofstatechangeobjectdetectionwillutilizeallthreeinput \n Center Net[241] DLA-34[233] 6.4 11.7 6.1 \n frames (pre, PNR, post), in this initial stage of the bench- \n 100 DOHModel[199] Res Net-101[95] 10.7 20.6 10.1\n mark, we only evaluate single-frame detection baselines, \n whereonlythe PNRframe I isusedasinput. Welimited \n pnr \n ourinputasmanymethodsforobjectdetectionareprimarily the 100 DOHmodelpre-trainedon 100 DOHdataset[199]\n designedtoworkwithasingleimage. to first detect hand bounding boxes and then predict state\n Wepresenttheimplementationofseveralbaselinemeth- changeobjectboundingboxesgiventhehands.\n ods for the state change object detection task. In general, We show the number of state change objects and hand\n the baseline models for the task can be categorized into boundingboxescontainedinourdatasetin Table 17. The\n two types: (1) directly detecting the bounding box of the resultsofsingle-frame State Change Object Detectionare\n statechangeobjectincluding Faster-RCNN[190],Center- illustratedin Table 18. Allbaselinesstruggleindetecting\n Net[241],and DETR[27],and(2)detectinghandbounding the State Change Objectswithonlyoneframeasinputas\n boxesfirstthenpredictstatechangeobjectboundingboxes an APof 8-14%. Thereareseveralchallengesinthistask.\n giventhehandssuchasthe 100 DOHmodel[199]. Specifi- First,theboundingboxsizesofstatechangeobjectshave\n cally,thebaselinemethodsarethefollowing: largevariance. Forexample,thesizeofstatechangeobjects\n Faster-RCNN[190] isatwo-stageanchor-based 2 Dob- canbeaslargeashalfofimageintheactionof“paintingthe\n ject detector on a single RGB image. In its classification wall”andassmallasafewpixelsintheactionof“igniting\n head,thestatechangeobjectistheonlypositivecategory. the match.” Second, when only using one frame as input,\n Wetrain Faster-RCNNonourbenchmarkanduseittodi- thedetectionmodelsdidnotconsiderthechangeofobject\n rectlydetecttheboundingboxesofstatechangeobjectsin appearance across different frames. As future work, we\n PNRframes. hopetheresearcherswillinvestigateusingmodelsthattake\n Center Net[241] isanotherobjectdetectionmethodon multipleframesasinputandperhapsdevelopframeworks\n asingle RGBimage. Itestimatesobjectkeypointstofind thatincorporatetrackingorassociation.\n objectcenterpointsandregressesallotherobjectproperties, \n suchassize,3 Dlocation,andorientation. Wetrain Center- \n G.9 Discussion \n Net to directly detect the bounding boxes of state change \n objects. Thisnovelbenchmarkexploresthreeaspectsofobjectsun-\n DETR[27]isanobjectdetectionmodelonasingle RGB dergoingstatechangesasaresultofhandmanipulation: the\n imagebasedon Transformer[216]. Itviewsobjectdetection when(i.e. temporallocalizationofstatechange),where(i.e.,\n as a direct set prediction problem and uses a transformer spatial localization of objects that undergo change) and\n encoder-decoderarchitecturetoproduceasetofobjectpre- what (i.e., semantic notion of action and object transfor-\n dictionsincludingboundingboxinformationaswellasother mation). As a first step, we have explored these indepen-\n information such as category. We train DETR to directly dentlyusingreadilyavailablelocalizationandclassification\n detecttheboundingboxesofstatechangeobjects. methods. However,approachesthataimtotacklethischal-\n 100 DOHModel[199] firstdetectstheboundingboxes lenge should focus on jointly understanding the manipu-\n of the human hand and objects as well as the relational lation with its spatio-temporal impact on objects as these\n vectors that links from each hand bounding box center to aretransformed. Forexample,knowinganobjectisbeing\n anobjectboundingboxcenter. Thefinalpredictionofthe splitshouldofferastrongpriortothe PNRlocalisationand\n objectsaredecidedastheobjectpredictionsthatsatisfiesthe detect two or more bounding boxes after the point-of-no-\n boththepredictionsofhandandrelationalvectors. Weused return. Suchmethodsthattacklethedependenciesbetween\n 49 "
  },
  {
    "page_num": 50,
    "text": " \n \n \n \n \n \n thetasksareyettobedeveloped. Wehopethisbenchmark \n willspurinnovativeapproachesthatbridgethegapbetween \n actionperceptionandtheimpactofactionsonobjectsand \n environments. \n G.10 Contributionsstatement \n \n Kris Kitani helped formulate and write the object state \n changebenchmark,designedtheannotationsandtasksfor \n the HObenchmark. Dima Damenhelpedwiththeformu- \n lation and writing of the object state change benchmark, \n designedtheannotationsforthe Handsand Objects(HO), \n and Forecastingbenchmarks. Ilija Radosavoviccoordinated \n HO data annotation, annotation analysis, and contributed \n to the definition and writing of the HO benchmarks. Ro- \n hit Girdharhelpedcoordinatethe HOdataannotationand \n annotationanalysis. Abrham Gebreselasieadaptedthe Slow- \n Fast+Perceivermodelfor PNRtemporallocalization.Qichen \n Fuimplementedallofthestatechangeobjectdetectionbase- \n lines. Raghava Modhuguimplementedthe BMNbaseline \n for PNRtemporallocalization.Kristen Graumancontributed \n totheformulationandwritingofobjectstatechangebench- \n mark. Siddhant Bansalhelpedwiththeprocessingof HO \n data,developmentof HOdataloaderfor PNRtemporallo- \n calization and implemented the I 3 D Res Net-50 baselines. \n Xingyu Liuwastheleadcoordinatorandmentorofthe HO \n benchmarkbaselineimplementations,andalsocontributed \n to the definition and writing of HO benchmarks. Xuhua \n Huangdevelopedoftheinitial Slow Fast+Perceivermodel. \n Yifei Huangimplementedthe Bi-directional LSTMbaseline \n forthe PNRtemporallocalizationandstatechangeclassifi- \n cation. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 50 \n \n \n "
  },
  {
    "page_num": 51,
    "text": " \n \n \n \n \n \n H.Audio-Visual Diarization Benchmark thevisualstream—somesuchnoiseisstructuredandrele-\n vantforunderstandingthecontextandsemanticcontentin\n Thissectiondetailsthe Audio-Visual Diarization(AVD) \n thescene. \n benchmark task definitions, annotations, baseline models, \n andresults. Asnotedin Appendix B,the AVDbenchmark \n usesonlyvideowhereinformedconsentforcapturingiden- H.2 Related Audio Visual Learning Work\n titiesisexplicitlycollectedfromallparticipantsinthescene, \n includingfacesandvoice. Thereisarecentresurgenceofworkonaudio-visualanalysis\n withinandbeyondthecomputervisioncommunity. These\n workstacklevariousaspectsofaudio-visualunderstanding,\n H.1 Motivation \n includingsourcelocalization,cross-modalfeaturelearning,\n Egocentrichumanperceptionisdrivenbyinferringuseful audio spatialization, and audio source separation, as we\n informationfromalltheprimarysenses. Whilevisualscap- brieflyreviewnext. \n tured by the eyes are one of the main information chan- On audio-visual detection and tracking, recent works\n nels, sounds as captured by the ears are equally relevant. on multimodal learning explore ways to localize sounds\n In particular, for understanding humans’ interaction with inagivenvideoframe[14,197,212]andinferspatialized\n the environment from the first-person perspective, detect- soundfromvideo[80,161]. Capturingandprocessingmulti-\n ing, localizing, tracking (both in 3 D space and time) and channelaudioisbeingstudiedinaudioandmicrophonearray\n understandingsoundsbycombiningthenecessaryacoustic signal processing communities, specifically from a user’s\n informationwithvisualsignalsbecomesevenmorecritical. perspectivetounderstandagivenscene[101,169]. Building\n Severalpsychophysicalstudieshaveproventhathumansare upon these, it is reasonable to expect that human-centric\n remarkablygoodatlocatingwhereasoundcamefromin 3 D audiohasinformationcontentthatcandirectlyimprovevi-\n spacewithrespecttotheirheadposition[156]. Sensitivityof sualobjectcategorizationandrecognition. Indeed, thisis\n humanstomovingsoundsinhorizontalandverticalplanes observedinsomerecentworkwhereaudiodisambiguates\n isalsowelldocumented[117,178]. certainvisuallyambiguousactions[110,226]. Foractions\n For a long time, the computer vision community has andactivity, audioeventscanalsobedirectlyusedtoper-\n studyiedtheproblemofpreciselocalizationofobjectsand formsummarization[13]. Inparticular,capturingego-driven\n people, robustly tracking and segmenting them using im- actionsandactivityandseparatingthemfromgeneralback-\n ages. Inthiseffort,weaimtobringaudio(humanspeech groundactionsandactivityinthesceneiscritical.\n inparticular)intothemix. Trulyaudio-visualsystemsnot Alternatively,visualinformationhasbeenusedtodisam-\n onlyenablerichercaptureandanalysisoftheenvironment biguatecertainaudiotaskslikespeechtranscription. Specifi-\n (and a user’s interaction with it), but they also help build cally,audio-visualspeechrecognitionhasreceivedalotof\n technologiesforvisuallyoracousticallyimpairedusers(e.g., attention in the last decade with multiple studies suggest-\n hearingaids,augmentedreality). ingthatautomaticspeechrecognition(ASR)couldbenefit\n Thegoalofthisbenchmarkistohelpadvancethestate from visuals of the scene, or other non-acoustic informa-\n oftheartinaudio-visualunderstandingfromtheegocentric tion[5,104]. Asshowninhere, itisreasonabletoexpect\n viewpoint. Specifically,fromaconversationalperspective, thatlipreadingfromafirstpersonpointofviewwouldalso\n thebenchmarkaimstounderstandwhoistalkingwhen,and benefit ASRsystems. \n aboutwhat.Fromavisualperspective,wearealsointerested Inaddition,audio-visualcross-modallearningmaypro-\n inwherethespeakerislocated. Givenanegocentricvideo, videinsightandsolutionstooneoftheoldestproblemsin\n theproposedtasksrequireextractingthespatiallocationof egocentric human communication ecology, referred to as\n the speakers, their voice activity across the length of the cocktailpartyproblem(CPP).Theessenceof CPPis“How\n video,andthecontentoftheirspeech. dowerecognizewhatonepersonissayingwhenothersare\n Egocentricdatapresentsseveraluniqueattributestothis speakingatthesametime?” Humanlistenersmustperceptu-\n problem. Firstly, soundsourcesmaybevisiblewithinall, allyintegratethesimultaneoussoundsoriginatingfromone\n some,ornoneofthevisualframes,dependingontheirmove- person’svoice(e.g., harmonicsandspeech formants)and\n ment within the scene and the movement of the camera segregatethesefromtheconcurrentsoundsofothertalkers.\n wearer. Secondly,althoughthecamerawearerisnevervisi- Insuchsituations,humansleveragevisualinformationsuch\n ble(duetheheadmountedcameradevice)theyareclearly asfromlipmovementstobetterunderstand,whiletheirau-\n audible and in fact often amplified compared to the other ditory system helps with focusing on a particular speaker\n conversationparticipantsduetotheclosenesstothemicro- characteristic while ignoring other speech/noise. Recent\n phonethatcapturesthevideo. Third,naturaldynamicsin workonaudio-visualdiarization[83]andmultimodalsource\n thescene(camerawearerwalking,running,rapidchanges separationfromvideoshowthat CPPanditsvariationscan\n inheadmovementetc.) addsignificantbluranddistortionto benefitfromvisualsignals[6,57,79,81,82,171,238].\n 51 "
  },
  {
    "page_num": 52,
    "text": " \n \n \n \n \n \n Furthermore,humansareprettygoodinunderstanding annotated audio-based speech activity collection of AVA\n thecontextofaconversationevenwhenwordsareincom- 1.0 third-personvideos,andexplicitlylabels 3 background\n prehensible. Theyareabletofillinthemissingdetailsusing noiseconditions,resultinginapproximately 46,000 labeled\n their context knowledge. This can be extended to sound segments spanning 45 hours of data. AVA active speaker\n sourcesthatarenon-humansaswell. Foramoredetailed associatesspeakingactivitywithavisibleface,resultingin\n accountof CPPpleasereferto[17]. Fullyaddressing CPP 3.65 million frames labeled across approximately 39,000\n requires not only identifying and separating the different facetracks. \n soundsourcesinthescene,butalsounderstandingtheaudi- AVDIAR:[84]Theclosestegocentricdatasetforaudio-\n toryattentionofthecamerawearer—inotherwords,which visual diarization is AVDIAR. It consists of 23 staged se-\n soundsourceistheuserattendingtoatthemoment,orwhich quences,witheachsequencedurationrangingfromtensec-\n onemaytheuserwanttoattendtointhenearfuture. ondstothreeminutes(atotalof 27 minutesofvideo). Each\n sequencecomprisesof 1-4 speakerssomestandingandsome\n walkingaroundinthevisual FOVandhavingaconversation.\n H.3 Related Datasetsand Benchmarks \n Thecaptureisdoneviaaheadmountedcaptureonadummy\n EPIC-Kitchens:[42,44]EPIC-Kitchensisamongthemost head. \n widely known ego-centric dataset with first-person view \n EASYCOM:[53]EASYCOMisarecentdatasetopen \n events and annotations. The dataset comprises of multi- sourcedforthepurposeofboostingegocentricaudio-visual\n faceted,audio-visual,non-scriptedrecordingsinnativeen- learning research with a focus on multi-channel data and\n vironments, i.e. the wearers’ homes, capturing all daily CPP.Thedatasetcorrespondsto 5 hoursofconversational\n activitiesinthekitchenovermultipledays. Thedatasetis content with 3 5 participants in a closed room setting.\n − \n 100 hours,20 Mframes,90 Kactionsin 700 variable-length The content involves playing games, ordering food from\n videos, capturinglong-termunscriptedactivitiesin 45 en- a menu, and a general discussion on a prespecified list of\n vironmentsusinghead-mountedcameras. Annotationsare topics. Duringtherecordingoftheconversations,restaurant-\n collected using a Pause-and-Talk narration interface. The likenoisewasplayedonloudspeakersintheroomtomimic\n dataset is widely used in action recognition, action detec- arealrestaurantscene. The EASYCOMcapturedeviceuse\n tion, action anticipation, cross-modal retrieval, as well as glasseswith 6 micsattachedtotheframe. Althoughrichin\n unsuperviseddomainadaptationforactionrecognition. termsofmulti-channelegocentricacousticcontent,thesetup\n Vox Celeb:[40,165]Vox Celeb 1 and 2 compriserecord- isconstrainedintermsofrealism,thedataisnotinthewild,\n andmostimportantlythedatasetissmall. \n ings of more than 6 K speakers spanning a wide range of \n differentethnicities,accents,professions,andages. Thedata \n Existingaudio-visualdatasetsvs. Ego 4 D:Oftheseex-\n is non-egocentric and is annotated for active speaker face istingdatasets,EPIC-Kitchens,AVDIARand EASYCOM\n bounding boxes, face tracks, and anonymous person IDs. areegocentric. However,EPIC-Kitchensfocusesonsolitary\n Vox Celeb 2 inparticularisdefinedforboostingresearchin activity by the camera wearer, and neither the video nor\n speakerrecognition,anditcontainsoveramillionutterances. annotationsaccommodateaudio-visualconversationtasks\n Videos included in the dataset are shot in a large number requiringmultiplepeople. Although EASYCOMcontains\n of challenging visual and auditory environments. These audio-visual conversation, it is a small dataset containing\n includeinterviewsfromredcarpets,outdoorstadiumsand partlyscriptedconversationsthatarenotin-the-wild. The\n quietindoorstudios,speechesgiventolargeaudiences,ex- participantsinthesessionsalsodonotmovearound. AV-\n cerptsfromprofessionallyshotmultimedia,andevencrude DIARdoesincludesomeparticipantswhomovearound,but\n videosshotonhand-helddevices.Audiosegmentspresentin thecamerawearerisadummyheadand,similarto EASY-\n thedatasetaredegradedwithbackgroundchatter,laughter, COM,thedataisnotin-the-wild(sessionsallaredonein\n overlappingspeechandvaryingroomacoustics. thesameenvironment/scene). Ego 4 Daccountsforallthese\n Vox Converse:[39]Vox Converseisarelatedaudio-visual aspects. Lastly,incontrastto Vox Celeb,Vox Converseand\n AVA,Ego 4 Doffersfirst-personvideoanditsconversation\n diarization dataset consisting of over 50 hours of multi- \n videostakeplaceincasualdaily-lifeenvironmentswithmul-\n speaker clips of human speech, extracted from You Tube \n tiplespeakers. \n videos. Similarto Vox Celeb,thisdataisalsonon-egocentric. \n Thisdatasetwasproposedtoboostresearchinspeakerdi- \n arizationforaudio-visualinputs.Abulkofthedatainstances \n H.4 Tasks: Definitionand Annotations \n arefrompoliticaldebatesandnewsanchorssoastocapture \n conversationalscenarioswithoverlappingandinterrupting Herewedetailthetaskdefinitions,thecorrespondingannota-\n speech. tions,andtheevaluationmetrics. Weproposeasuiteoftasks\n AVA:[31,192]The AVAspokenactivitydatasetsare AVA forthe Audio-Visual Diarization(AVD)benchmark. These\n speech and AVA active speaker. AVA speech is a densely tasks are abbreviated as: Localization & Tracking, Active\n 52 "
  },
  {
    "page_num": 53,
    "text": " \n \n \n \n \n \n Speaker Detection, Diarization and Transcription. These people in the scene are speaking at a given time [192]. It\n tasks jointly capture who is talking when, to whom, and buildsontopofthepreviouslocalizationandtrackingtaskto\n aboutwhatinagivenegocentricconversationalscene. Ob- recognizeeachofthespeakerswhosefaceboundingboxes\n servethatthesetasksareimplicitlytiedtoeachother;each are detected. Hence, this task does not take into account\n subsequenttaskisdriveninsomeformbyaprevioustask speakers who are not visible in the camera’s FOV. Note\n (asfurtherclarifiedinthetaskdescriptionsbelow).16 thatactivespeakerdetectionisalsoanimportantaspectof\n speakerdiarization(whichisthenexttaskinthebenchmark).\n Task 1: Localization & Tracking: Where is the person \n Annotations: We provide an anonymous speaker label\n inthevisualfieldofview? Thisfirsttaskin AVDcaptures \n (e.g.,speaker 1,2 etc.) foreachspeakervisibleintheclip.\n thespatialpositionofalltheprobablespeakersinthescene, \n The camera wearer is assigned the label C. This is done\n fromthepointofviewofthecamerawearer. Thegoalofthe \n byutilizingthefaceboundingboxtracksannotationsand\n taskistocomputeboundingboxesforthem.Unlikeclassical \n labeling each track one at a time. Hence, each face track\n face detection benchmarks, this task is challenging in the \n getsassignedoneuniquelabel,andmultipletrackswithina\n sensethatthedynamicsofthecamerawearer’shead(coming \n singleclipmaysharethesamelabel(correspondingtothe\n fromnaturalconversations)leadstosignificantmovementin \n samespeaker). However,thelabelsareclip-specific,i.e.,a\n aspeaker’sapparentspatiallocation. \n speakerwhomaybepresentacrossmultipleclipsdoesnot\n Annotations: Foreachspeakerpresentinthe 5 minclip \n getassignedashareduniquelabelacrosstheclips. Again,\n a bounding box is provided. Each frame of the video is \n speakerswhoareneverinthevisual Fo Varenotassigneda\n annotatedforthetask. Wefirstutilizedafacedetectionand \n label. \n trackingmodeltoestimatetheseboundingboxes,andthen \n Evaluation: Weusetheobjectdetectionm APtoquantify\n ateamofhumanannotatorsvalidatedandcorrectedthese \n the active speaker detection result. This is a frame-wise\n machine-generatedboxestoimproveannotationquality. A \n metric. Inavideoframe,iftheintersectionoverunion(Io U)\n boundingboxisconsideredavalidhumanannotationifit \n betweenadetectedfaceboundingboxandthegroundtruth\n captures 80% of the speaker’s face; we peform a quality \n faceboundingboxexceedsapredefinedthreshold,i.e. 0.5,\n checksteuptoensurethis. Sidewayslookingfacesarealso \n we have a positive face detection. Each detection has an\n annotated. Note that speakers who are very far from the \n associated class to indicate whether it corresponds to an\n camerawearer(oftentimesseveralmetersawayinthescene) \n active speaker. Active speaker detection methods give a\n andwhodonotcomeintoconversationalcontactwiththe \n confidencescoreoftheactivespeakerclassforeachdetected\n wearerarenotannotated. \n faceboundingbox[211]. \n Evaluation: Recallthatthegoalofthetaskistolocalize \n Camera Wearer’s Voice Activity Detection: Notethatthe\n aswellastrackthespeakersinthescene. Hencetheevalua- \n camerawearer’sfaceisnevervisibleinthecamera’sfield\n tionmetricsproposedaccountfortheaccuracyoftrajectory \n ofview,andsotheydonothaveanyfacetracksassociated\n ofdetectedboundingboxes.Wefollowthestandardmultiple \n withthem. However,inmanycases,theyarethedominant\n objecttracking(MOT)metricstoquantifythespeakertrack- \n speakers. Thisismainlybecausetheyaredrivingtheinter-\n ingresults. Therearemanydifferent MOTmetrics,inwhich \n actionsinmanycases,andsincetheirmouthsaretheclosest\n we are most interested in the MOTA in the CLEARMOT \n tothemicrophones,theirvoiceisingeneralamplifiedinthe\n metrics[19],and IDF 1,IDP,IDRinthe Identitymetrics[18]. \n audio stream compared to other speakers. We propose to\n MOTA,themultipleobtecttrackingaccuracy,isacombined \n alsoconsiderthemasactivespeakersanddetecttheirvoice.\n metricoffalsealarms,falsepositivesandidentityswitches. \n Weusetheobjectclassificationm APtoquantifytheresult\n MOTAisbased onmatching thetracking resultswith the \n ofthecamerawearer’svoiceactivitydetection.\n ground truth at frame level, while the IDP (ID precision), \n IDR(IDRecall)and IDF 1(IDF 1 score)arebasedonthe Task 3: Diarization: Who spoke when? This next task\n tracking result to ground truth matching at the trajectory further expands on the temporal aspect of active speaker\n level. IDmetricsgiveatracker’sperformanceonmaintain- detection(fromtheprevioustask). Giventhesetofspeakers\n ingcorrectidentificationforeachtarget. andtheirspatiallocalizationinthevisualfieldofview,this\n Task 2: Active Speaker Detection: Who is speaking? task aims to capture the voice activity of the speakers. It\n is identical to speaker diarization, a well studied research\n Thenexttaskin AVDistodetecttheactivespeakerinthe \n problem in the speech and audio domains [10,177] and\n scene. This task is in principle similar to active speaker \n answers the question, “who spoke when”. While speech\n detection—wherethegoalistodetectwhichofthevisible \n from speakers that overlap with each other is one of the\n 16 Notethatalthoughspeechtranscriptionandsourcelocalizationare biggestissuestosolveinthistask,theegocentricperspective\n distinctfromaudio-onlyspeakerdiarization—allofwhicharewelldefined \n addsmorecomplexityintermsofheadmotionsandother\n researchparadigmsinmainstreamaudio,speechandvisioncommunity— \n dynamicsassociatedwithnaturalconversations. Notethat\n wecumulativelyrefertoallthesetogetherundertheumbrellaofaudio- \n visualdiarizationfor Ego 4 D. theoutputsofactivespeakerdetection(theearliertaskinthe\n 53 "
  },
  {
    "page_num": 54,
    "text": " \n \n \n \n \n \n benchmark)alsodrivethistask. arenotthesameastheonesusedindiarizationbecausewe\n Annotations: Foreveryactivespeakerlabel(wherethe separatelyannotatedtheoverlappingregionsheretoreduce\n annotationsarefromtheprevious Active Speaker Detection transcriptionerrorsandaccountforspeakerstalkinginlow\n task), a human annotator marks the start and end time of volume. This allows us to also distinguish voice activity\n thatpersonspeaking. Weaccountforoverlappingspeech fromspeechactivity. Inaddition,theuseoftime-segmented\n segmentswheremultiplespeakerstalkovereachother,but transcriptionsisalsoslightlydifferentfromstandard ASR\n we ignore speech not relevant to the conversation such as datasetsinspeechcommunitywhichmainlyhavetextand\n backgroundspeechfroma TVorspeechfurtherawayfrom notimestamps. \n the camera wearer. Note that speech segments from the Evaluation: We utilize the Word Error Rate (WER), a\n camerawearerarealsoannotated. Theannotatorsrelyboth standard ASRmetric,forevaluatingthistask[114].First,the\n ontheaudioandthevisualstreamforcreatingtheselabels. minimumeditor Levenshteindistanceiscomputedbetween\n Evaluation: Diarizationerrorrate(DER)isthedefacto the reference and hypothesized transcription. WER then\n evaluationmetricforspeakerdiarization[11],anditiswell measurestheratioofthenumberofwordsubstitutions(S),\n studiedintheaudioandspeechprocessingcommunity. DER deletions(D)andinsertions(I),i.e.thetotalnumberofedits\n measuresthefractionoftotaltime(inagivenclip)thatis necessarytoconvertthehypothesizedtranscriptionintothe\n notattributedcorrectlytoaspeakerortonon-speech. Itis referencerelativetothetotalnumberofwords(N )inthe\n w \n definedasfollows: reference: \n DER(%)=(E +E +E ) 100, (21) S+D+I \n miss fa spk \n × WER(%)= 100. (22) \n N × \n where E denotesthefractionoftimethathasbeenpre- w \n miss \n dictedtobenon-speechwhilethatsegmentisattributedto \n H.5 Data Statistics \n aspeakerinthereference. E denotesthefractionoftime \n fa \n thathasbeenpredictedtobeassociatedwithaspeaker,but \n From across the 3,670 hours of video in Ego 4 D, approxi-\n isactuallylabelledasnon-speechinthereference,and E \n spk mately 764 hours of data contains conversational content,\n denotesthefractionoftimewherespeechisassociatedwith \n andaredirectlyrelevantforthe AVDand Socialbenchmarks.\n thewrongspeaker. Allerrorsarecomputedasafractionof \n Pleasereferto Section I.5 foracompletedescriptionofthe\n thetotalamountofspeech. \n experimental design and scenarios used in these sessions.\n Task 4: Transcription: Whatdidthespeakersay? The Fromthisset,arandomlychosensubsetof 572 clips(each 5\n finaltaskof AVDistotranscribethespeechofeachspeaker, minuteslong)areannotatedforthisfirstversionrelease. Of\n i.e.,performing ASR.Similartothediarizationtask,someof these 572 clips,389 clipsaremarkedfortraining,50 clips\n thechallengesassociatedwiththetranscriptiontaskinclude forvalidation,andtheremainderisthetestingset.\n overlapping speech and environmental noise. In addition, Table 19 and Figure 43 summarize statistics about the\n thecamerawearer’sheadmovementresultsinasignificant speakercontentfromacrosstheseclips. Observethelong\n change of the audio volume of the speech recorded from tails of mean and maximum number of speakers in the\n others. dataset. Wenotethatinthefirstversionofthedatarelease,\n Annotations: Sincetheclipscontainmultiplespeakers due to the fact that the total number of clips is relatively\n with overlapping speech segments and with different vol- small, the test and/or validation batches may be biased in\n umes,thefinaltranscriptionsareobtainedinmultiplepasses. termsofchangesinspeakers’accents,changesinvocabulary\n Inthefirstpass,initialhumanannotationsbasedonvoice usage(sincetheparticipantsarefromdifferentculturalback-\n segmentsaremergedwithautomaticannotationsforregions groundsfromacrosstheworld),andingeneralchangesin\n withlowvolume. Inasubsequentpass,humanannotators natureofinteractivenessbetweenspeakersinascene. There\n hadtocorrectandassignsegmentsoftranscriptionstothe ismarginaldistributionalshiftamongthetraining,testing\n corresponding voice activity segments per speaker while andvalidationsplits. Thisismainlybecauseofthesmaller\n also annotating overlapping speech. Note that annotators numberofannotationsinthisversionof AVDfor Ego 4 D.\n hadboththeaudioandvideoavailableforannotationand, Weexpectthesedistributionalshiftstobelesssignificantin\n besidesspokenwords,theoccurrenceofotherartifactssuch futurereleasesandasmoredatawillbeannotated.\n asunintelligiblespeechorincompletewordshavealsobeen \n annotated. Thefinaltranscriptionannotationsforaclipcon- \n H.6 Baseline Modeling Framework \n sistofasequenceofsegmentslabeledwithbegintime,end \n time, transcriptandspeaker IDwithintheclip. Inevalua- Recallthatthe 4-parttasksinthisbenchmarkaretiedtoeach\n tions,weapplied ASRtothesesegmentsindividuallyand other,inthesensethatrepresentationslearnedfromonetask\n computedtheperformanceoverallofthesesegments.Please mayberelevantfortheothers. Tothatend, weproposea\n notethatthetimesegmentsassociatedwiththetranscripts baselinelearningframeworkthataddresseseachtaskina\n 54 "
  },
  {
    "page_num": 55,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 43. AVDiarizationdatastatistics.Meanandmaximumnumberspeakersin FOV,andnumberspeakersperclip.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 44. AVDiarizationbenchmarkannotationssummary.Thefourtasksareannotatedinasequentialfashion,startingwithlocalization\n andtrackingofspeakers,activespeakerdetectionlabels,diarizationtimestamps,andfinallytranscriptions. Thefigureshowstheface\n detections(highlightedbyboundingboxes),speakerdetection(shownbytheanonymousperson IDs 1,2,etc.),activespeaker(highlightedin\n green)andvoiceactivity(shownbelowingreenhighlightedtimesegments).Speakersinthevisual FOVwhoarenottalkingarehighlighted\n indottedredboxes.Theclipsusedfor AVD(and Social Interaction)haveconsentfromparticipantstoleavetheirfacesunblurred.\n \n Statistic(Avg.) Value sequentialfashion. Theframeworkincludesthefollowing\n Speakersperclip 4.71 steps: \n Speakersperframe 0.74 \n • Wefirstdetectpeople’sheadsanddoshorttermtrack-\n Speakingtimeinclip 219.81 sec \n inginthevideo. Theshorttermtrackerfollowseach\n Speakingtimeperpersoninclip 43.29 sec \n detectedheadbyexpandingasetoftrajectoriesbased\n Camerawearerspeakingtime 77.64 sec \n ontheirpositions,sizesandtheappearanceoftheper-\n son. Thetrajectoriesmayendwhenocclusionhappens\n Table 19. AVDData Statistics. \n orwhenthetrackedpersongoesoutofthefieldofview.\n Newtrajectoriescanalsobeaddedtothetrajectoryset.\n • Theshorttermtracker’strajectoryforeachpersonis\n 55 "
  },
  {
    "page_num": 56,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 45. Exampleannotationsshowingthefacedetections(highlightedbyboundingboxes),speakerdetection(shownbytheanonymous\n person IDs 1,2,etc.),activespeaker(highlightedinred)andvoiceactivity(shownbelowinbluehighlightedtimesegments).Asillustrated\n here,thedatafor AVDincludespeoplewalkingaroundandtalking,sittingandplayinggamesetc.Theclipsusedfor AVDhaveconsent\n fromparticipantstoleavetheirfacesunblurred. \n \n oftenfragmentedintomultipleparts. Hence,wethen across all segments. Evaluating the system by using\n optimizethegroupingofthetrackletsinsteponesothat anothersegmentationmethodischallengingespecially\n thetrajectoriesofeachpersoncanbelinkedtogether. in the case of overlapping speech segments. Jointly\n We formulate the problem as a constrained combina- modeling time segments and transcriptions will be a\n torialoptimizationproblem. Integerprogrammingcan challengingproblem(aswediscussin Section H.7).\n beusedtosolvetheproblemdirectlybutithasexpo- \n Wedescribefurtherdetailsabouteachofthesestepsbe-\n nentialcomplexity. Forefficiency,wedevelopagreedy \n low,and Tables 20–29 summarizetheresultingperformance\n approach which is much faster and still gives strong \n metricsforthetasks. \n results. \n • Wethenclassifyeachperson/headineachvideoframe Audio Only Models for Speaker Diarization The prob-\n as an active speaker or not. Based on the classifica- lemofspeakerdiarizationfromaudiohasbeenstudiedtoa\n tion result and the corresponding detected long-term considerableextentinthefieldofspeechprocessing[10,177].\n trajectories, we further associate the audio/speech to For the audio-only baseline system, the VBx diarization\n eachpersoninthevideo. Weusethispreliminarylistof approach has been utilized [128] for having shown supe-\n audiofeatureembeddingstofurtherextractandmatch rior results on different types of datasets such as CALL-\n un-associatedaudiosegmentstospeakerlabels. HOME[3](telephoneconversations),AMI[28](meetings)\n • Wethenusetwomethodstodetectthecamerawearer’s and DIHARDII[59](myriadofdomainsrangingfromau-\n voiceactivity. Thefirstmethoduseshighenergyaudio diobooksto You Tubevideos). Thismethodrequiresspeech\n segment in the clip (under the assumption that their activityregionsandthesewereobtainedusingthe ASp IRE\n voicehasnaturalamplificationcomparedtotheremain- modelbasedonatimedelayneuralnetwork(TDNN)with\n ingspeakers). Thesecondmethodisadeepclassifier statisticspooling,availablewiththe Kaldispeechrecogni-\n thatpredictswhetherthewearerisspeaking. tiontoolkit[181]. Werefertothisask VAD(the Kaldi VAD\n • Lastly,weapplied ASRtothespeechregionsbasedon model). Althoughthisk VADhasbeentrainedonslightly\n thegroundtruthsegmentationandevaluatedthe WER differentdata(telephoneconversations),andthusdoesnot\n 56 "
  },
  {
    "page_num": 57,
    "text": " \n \n \n \n \n \n providethebestpossibleresults,ithasbeenchosenforthe 0,itisremovedfromthetrajectoryset.\n baselinesystembecauseofitsgeneralavailability. Thekeycomponentoftheshort-termtrackerismatching\n Thespeechactivityregionsareuniformlysegmentedto trajectories to the candidate head bounding boxes in each\n obtainshortersegmentsandspeakerembeddings(so-called frame. Thiscanbeformulatedasthefollowingoptimization\n x-vectors[206])areextractedonepersubsegment. Thex- problem: \n vectorsareobtainedwitha Res Net 101 extractor[96]trained \n toproducespeaker-discriminativeembeddings. Theinput (cid:88) \n min c x (23) \n tothenetworkarelog Mel-filterbankfeaturesevery 10 ms, i,j i,j \n and given a segment of speech, it computes a single 256 (i,j) \n dimensionalvectorthatrepresentsthewholesegment. The s.t. x i,j formsamax-matching,\n informationofthewholesegmentisaggregatedwithasta- x =0, if(i,j) E, \n i,j \n tisticalpoolinglayerwhichcomputesthemeanandstandard ∈ \n x =0,1, \n i,j \n deviation of activations over the time domain. A linear \n transformationisthenusedtoreducethedimensionalityto wherex is 1 iftrajectoryimatchescandidateheadboxj\n i,j \n 256. Thetrainingdataconsistedof Vox Celeb 1[165],Vox- and 0 otherwise. E isasetinwhichthepairsoftrajectory\n Celeb 2[40]and CN-CELEB[64]together,totalling 2877 andcandidatecannotmatcheachother, examplesinclude\n hoursofspeechfrom 8178 speakers. casessuchasthecandidateistoofaraway,thesizeistoo\n Thex-vectorsareinitiallyclusteredtoafewdozensof differentortheappearancedoesnotmatch. c isthecostof\n i,j \n classes using agglomerative hierarchical clustering. This matchingtrajectoryiandcandidateheaddetectionj. This\n initial clustering is fed as initialization to a Bayesian hid- costofmatching,c ,iscomputedasalinearcombination\n i,j \n den Markovmodelwhichestimatesaltogetherthenumber ofthenormalizedboundingboxdistancesandthedifference\n of speakers in the recording as well as the assignment of oftheappearancefeatures. Thenormalizedboundingbox\n x-vectorstothestates. Eachstateinthemodelcorresponds distance is defined as the ratio of the Euclidean distance\n toonespeakerandtheprobabilityofobservingaparticular between the two corners of the last bounding box in the\n x-vectorinaparticularstatecanbeinterpretedasthecor- trajectoryandthedetectedheadboundingboxintheimage\n respondingspeakerproducingthecorrespondingsegment tothesizeofthedetectedboundingbox.Eachtrajectoryalso\n ofspeech. Themostrelevanthyperparametersofthemodel maintains a feature vector to characterize the most recent\n werefine-tunedtoobtainthebest DERperformanceonthe appearance of the tracked person. This feature vector is\n Ego 4 Dvalidationset. The VBximplementationpublished obtained from a feature embedding network trained on a\n by Brno Universityof Technologyispubliclyavailableas largepersonheaddataset. \n wellasthetrainingrecipepublishedby Phonexia Research. Thisoptimizationproblemcanbesolvedefficientlyus-\n ingthe Hungarianalgorithmortheprimaldualalgorithm.\n Short-term People Tracking Thegoalhereistotrackpeo- \n Due to the imperfect features, the optimization may have\n ple’sfaces. However,ourmethodcanalsobeusedtotrack \n an identity switching problem if two targets cross paths.\n thewholebodyofeachperson. Theshort-termtrackermain- \n Tosolvetheproblem,weenforcethelongertrajectoriesto\n tains a set of trajectories. The trajectories include the at- \n havehigherprioritytomatch. Weuseatwo-stepmatching\n tributes such as the person-ID, the frames tracked, a life \n scheme. Wefirstmatchallthetrajectoriesthatarelonger\n counter, the appearance features and the positions of the \n thanaspecificthresholdchosenempirically. Oncedone,we\n tracked bounding boxes. Throughout, we use the term \n thenmatchtheshortertrajectories. Thisschemenaturally\n “person-ID” to refer to an anonmyous tag for a person in \n giveshigherprioritytolongertrajectories,therebyreducing\n thevideo(person 1,person 2,etc.);noactualidentitiesare \n mismatchesamongthem. Thisismorerobustthanasingle\n available in the data, and the benchmark does not aim to \n stagematchingwherealltrajectoriesarehandledtogether.\n performanypersonidentification. Therearetwokindsof \n Inourimplementation,thepersondetectorisa Yolo-V 3\n trajectories. Ifatrajectory’strackedframesarelessthana \n detector[187]whichdetectstheheadandpersonbounding\n threshold,e.g.5,itisinprobationandisnotcountedasareal \n boxsimultaneously. Thedetectoristrainedonimagesfrom\n trajectoryeventhoughwemaintainalltheinformationfor \n the Google Open Imagedataset[123]andafisheyeimage\n them.Whenatrajectory’strackedframesaregreaterthanthe \n dataset[73]. Weusethedetectedheadboundingboxesfor\n threshold,itbecomesarealtrajectory. Eachtrajectoryalso \n people tracking. The person head appearance’s feature is\n hasalifespan.Thelifeofanewtrajectorystartsfromafixed \n extracted using the person embedding network, which is\n value. Thelifeofatrajectoryisrestoredtoafixedmaximum \n trainedonthe Vox Celeb 2 datasetusingthetripletloss. The\n value,suchas 10,ifthetrajectoryismatchedtoacandidate \n networkhasthestructureofa Res Net-18. \n personheadboundingboxes. Otherwise,thetrajectorygoes \n into a maintenance mode and its life decreases by 1 each Long-term Trackingby Trajectory Matching Theshort\n timeitfailstofindamatch. Ifthelifeofatrajectorygoesto termtrackergeneratesfragmentedpersontrajectories. Ifa\n 57 "
  },
  {
    "page_num": 58,
    "text": " \n \n \n \n \n \n personisoccludedorgoesoutofthefieldofviewandreap- Metric Valid Test \n pears,itwillreceiveanew ID.Thefragmentedtrajectories MOTA 74.52 71.94 \n arereferredtoastracklets. Weneedtogroupthetracklets MOTP 79.07 79.17 \n throughoutthewholevideotogeneratethefinaltrajectories IDF 1 84.92 80.07 \n foreachperson. Thegroupingproblemcanbeformulated IDR 80.40 73.52 \n asfollows: IDP 89.97 87.90 \n (cid:88) Table 20. Localizationandtrackingbaselinemetricsonthevalida-\n min D y (24) \n m,n m,n tionandthetestsetsrespectively. \n m,n \n s.t. y =y , m,n, \n m,n n,m \n ∀ \n would probably never occur and the greedy result would\n y +y 1+y , m,n, \n m,k k,n m,n \n ≤ ∀ approachthegloballyoptimalsolution. \n y =0,ifmandnoverlapintimeor D >g, \n m,n m,n Table 20 summarizesthetrackingmetrics MOTA,MOTP,\n y m,n isbinary, IDF 1,IDR,and IDPonthevalidationandtestsets.\n wherey =1 iftrackletmandncanbegroupedtogether Active Speaker Detection: Weusetwoapproachesforac-\n m,n \n andotherwisey = 0. D istheappearancedistance tive speaker detection. One approach is based on mouth\n m,n m,n \n betweenthetrackeletmandnandg isathreshold. Here region classification, and the second method is a trans-\n D =min f f 2,where T isthesetof former based audio-visual method for active speaker de-\n m,n {i∈Tm,j∈Tn} \n || \n i \n − \n j \n || \n i \n personheadboxesintrackletiandf isthecorresponding tection[211]. \n i \n featureembedding. Theconstraintsrequirethegroupingto Region Cls: Ourfirstapproachisbasedontheclassification\n bereflective: iftrackletmandncanbegroupedtogether ofmouthregions. Itfirstcomputesthe 3 Dheadorientation\n socannandm, transitive: ifmandk canbegroupedto- using a regression network. In our implementation, the z\n gether and so can k and n, then m and n can be grouped direction is into the image; if the head 3 D orientation z\n together. Twotrackletscannotbegroupedtogetherifthey coordinateontheunitsphereisgreaterthan 0.3,weassume\n havetimeoverlaportheirdistanceisgreaterthanathreshold the face is away from the camera. If the face is facing\n g. Theoptimizationcanbesolvedusingintegerprogram- awayfromthecamera,weignoretheimageandtheactive\n ming. However, this method has exponential complexity. speakerdetectionresultissettonull. Forfaceslookingat\n Weproposeafastgreedyalgorithmtosolvetheproblem. thecamera,ourmethodfirstregressesthefacialkeypoints\n Thegreedyalgorithmstartsbytreatingeachinitialtrack- usingtheimagewithintheperson’sheadboundingbox. We\n let as a trajectory and progressively groups two trajecto- usethemouthkeypointstocropoutthemouthimage. The\n rieswiththeclosest Duntilnotrajectoriescanbegrouped croppedmouthimageisthensenttoaclassificationnetwork\n together. Since the distance between two trajectories can toclassifywhetherthespeakeristalkingornot.\n becomputedbyfindingtheminimumofallthe“element” Note that we also explored using multiple images,\n trackletpairdistances,themergingprocedureisefficientif wherein we stack a short sequence of cropped mouth im-\n wepre-computeandcachetheelementpairdistance. This agesinatimeintervalforactivespeakerclassification. Our\n greedyapproachgivesstrongresultswhilemaintaininglow experimentsshowthemultiplemouthimagesinputdonot\n complexity. significantlyimprovetheresult. Thisisprobablyduetothe\n The algorithm reduces to the minimum spanning tree fastmovementofthecameraandsometimesdifficultangles\n methodifthereisconflictbetweeneachpairoftrajectories. oftheface. Thiscausesinaccuratecroppedmouthregions.\n However,iftherearetime-conflictingtracklets,thereisno Talk Net:[211]Talk Netisanend-to-endpipelinethattakes\n guaranteethegreedyalgorithmgivesthegloballyoptimal the cropped face video and corresponding audio as input,\n solution. Weillustratethemethodthroughasimpleexample: anddecidesifthepersonisspeakingineachvideoframe. It\n Assumetherearetrackelets T ,T ,T ,T ,T and T have consistsofafeaturerepresentationfrontendandaspeaker\n 1 2 3 4 1 2 \n { } \n timeconflict, and T and T havetimeconflict. D(T ,T ) detectionbackendclassifier,asillustratedin Figure 46. The\n 3 4 1 3 \n = 10, D(T ,T ) = 1, D(T ,T ) = 3 and D(T ,T ) = 4. We frontend contains an audio temporal encoder and a video\n 2 4 1 4 2 3 \n assume g = 20. Using the proposed greedy method, the temporalencoder. Theyencodetheframe-basedinputaudio\n solution P is T ,T , T ,T whose overall cost is 11. andvideosignalsintothetimesequenceofaudioandvideo\n 2 4 1 3 \n {{ }{ }} \n However,theoptimalsolutionis T ,T , T ,T whose embeddings, representingtemporalcontext. Thebackend\n 1 4 2 3 \n {{ }{ }} \n overall cost is 7. Even though the greedy method does classifierconsistsofaninter-modalitycross-attentionmech-\n not guarantee the global optimal solution, empirically we anismtodynamicallyalignaudioandvisualcontent,anda\n observe that the proposed method give strong results. In self-attentionmechanismtoobservespeakingactivitiesfrom\n fact,ifthepersonembeddingisaccurate,thesecornercases thetemporalcontextattheutterancelevel.\n 58 "
  },
  {
    "page_num": 59,
    "text": " \n \n \n \n \n \n Algorithm 1 Greedy Tracklet Grouping \n Initializesets P= S ,S ,...,S ,where S = T ,T isthetrackletiand N isthenumberoftracklets.\n 1 2 N i i i \n { } { } \n for(m,n),m=1..Nandn=1..Ndo \n compute D(m,n) \n endfor \n while Truedo \n for (S ,S ),S P and S P,and(S ,S )donothavetimeconflictdo \n m n m n m n \n ∈ ∈ \n compute F(S ,S )=min D(a,b) \n m n Ta∈Sn,Tb∈Sm \n endfor \n (m∗,n∗)=argmin(F(S ,S )) \n m n \n if(m∗,n∗)isemptyor F(S m∗,S n∗)>gthenbreak \n endif \n S m∗ =S m∗ S n∗ and P.pop(S n∗) \n ∪ \n endwhile \n Pincludesthegroundedtrajectories \n \n Figure 46. Talk Net:Anaudio-visualtemporalnetworkfordetectingandtrackingtheactivespeakerinavideo[211].Figureisfrom[211].\n \n Tables 21, 22, 23 and 24 summarize the resulting per- Model m AP@0.5 \n formance. Foreachofthetwoproposedbaselinemodels, Reg Clsw/osmoothing 29.65 \n wereportperformancesummarieswithpretrainingbasedon Reg Cls+max-filtering 32.77\n AVA andalsomodelstrainedusingonlyvideosfromthe Reg Cls+max-filtering+s VAD 34.35\n Ego 4 Dtrainingdataset. Notethatthevideo-onlyapproach Talk Net 50.90 \n canbecombinedwithanyvoiceactivitydetectiontoremove Talk Net+s VAD 49.66 \n falsealarms. Hereweusesuchanalgorithmfrom[203],and \n werefertothisass VADThiscangreatlyimprovetheactive Table 22. Activespeakerdetectionbaselinemetricsonthetestset\n speakerdetectionresults. Themax-filteringhasawindow usingtrainingvideosinthe Ego 4 Ddataset.\n sizeof 11. Talk Netalsohasabuilt-insmoothnessfiltering \n topost-processtherawclassificationresult. \n Model m AP@0.5 \n Reg Clsw/osmoothing 22.09 \n Model m AP@0.5 Reg Cls+max-filtering 22.88 \n Reg Clsw/osmoothing 29.68 Reg Cls+max-filtering+s VAD 25.53 \n Reg Cls+max-filtering 31.95 Talk Net 34.36 \n Reg Cls+max-filtering+s VAD 33.72 Talk Net+s VAD 34.65 \n Talk Net 34.75 Always Speak 20.94 \n Talk Net+s VAD 34.56 \n Always Speak 24.46 Table 23. Activespeakerdetectionbaselinemetricsonthevalida-\n tionsetwithmodelstrainedon AVAdataset.In Always Speak,all\n thedetectedfacesareclassifiedasactivespeakers.\n Table 21. Activespeakerdetectionbaselinemetricsonthetest \n setwithpre-trainingusing AVA.In Always Speak,allthedetected \n facesareclassifiedasactivespeakers. \n Matching Speakers Outside Fo V: Based on the tracked\n headsandtheactivespeakerdetectionresults,wecanasso-\n 59 \n "
  },
  {
    "page_num": 60,
    "text": " \n \n \n \n \n \n Model m AP@0.5 wearer’s voice often has higher energy than other partici-\n Reg Clsw/osmoothing 20.33 pant’svoices. Weusethisheuristictoextractcandidatesof\n Reg Cls+max-filtering 21.93 thewearer’svoicebychoosingportionsofaudiowithenergy\n Reg Cls+max-filtering+s VAD 24.60 higher than certain threshold. Since different recordings\n Talk Net 51.04 have different levels of loudness, we normalize the audio\n Talk Net+s VAD 50.58 using the maximum energy and then choose the possible\n wearer’s voice using a fixed percentage of the maximum\n Table 24. Activespeakerdetectionbaselinemetricsonthevalida- energy. This threshold percentage is set to be as high as\n tionsetusingtrainingvideosinthe Ego 4 Ddataset. possibletoavoidfalsealarms. Oncethecandidateaudiois\n selected,weusethesameaudiomatchingmethoddescribed\n intheprevioussectiontofindalltheaudiothatbelongsto\n ciatetheaudiotothevisiblepeopleinthescene. However, thecamerawearer. Thissimplemethodworksreasonably\n thisisstillnotcompletebecausetherearecasesinwhichthe wellassummarizedin Table 25. Theapproachfailswhen\n speakerisoutsideofthevisualfieldofview. Tosolvethis thewearernevertalksortalksinaverylowvoice,andin\n problem,wefirstcreateanaudio-signatureforeachvisible generalthebaselineworksbetterfornearrangemicrophones\n personinthevideo. thanlongrangemicrophones. \n Weextractonesecondofaudiocenteredateachvideo Method II: In the second method, we directly classify\n frametimeinstant. Iftheaudiocorrespondstoaspeaking the audio at each time instant to two categories: wearer’s\n head in the image, we compute the audio embedding of voiceornotwearer’svoice. Thelogarithmmagnitudeofthe\n theonesecondaudioandinsertthefeatureintotheaudio spectrogramat 40 mswindowistheinput. Thenetworkisa\n signaturelibraryoftheperson.Theaudioembeddingscanbe modified Res Net. Thenetworkistrainedonthe Ego 4 d AV\n obtainedfromanyspeechrepresentationlearningmethods. trainingdatasetusingastandardcross-entropyloss.\n Weexploredseveralmodelsincludingamodified Res Net 18 Weuseclassificationm APtoquantifytheweareraudio\n whichtakesaudiospectrogramlogarithmmagnitudeinone- activity detection result. We report the average m AP on\n secondwindowsastheinputandtrainedonthe Vox Celeb 2 boththetestvideosandvalidationvideosin Table 25.\n datasetusingtripletloss,andaversionofwav 2 vec 2.0[15]— \n aself-supervisedapproachtospeechrepresentationlearning. Model Valid Test \n We parse the video and find instants when a particular Method I 43.95 50.61 \n person is not in the video frame and match the audio em- Method II 72.00 74.29\n bedding to the person’s audio signature library. We find Always Speak 21.30 26.09\n the minimum distance of this audio embedding to all the \n signatureaudioembeddingsinthelibrary. Ifthedistanceis \n Table 25. Cameraweareractivitydetectionbaselinemetrics(m AP)\n lessthanapredefinedthreshold,weclassifythepersonas onthevalidationandtestsetsrespectively.Always Speakassigns\n speakingandotherwisenot. Notethattheaudioembedding thatthewearerspeakingineachvideoframe.\n isusedonlywithinthesame 5 minutevideoclipandnever \n acrossvideoclips. Person IDsarealwaysanonymoustags \n (person 1,2,etc.). Speaker Diarization Tables 26,27 and 28 summarizethe\n Weusethismethodtodetectallthebackgroundaudioof speaker diarization DER metrics for the baseline models\n thepeopleofinterestwhentheyarenotvisible. Thismethod proposedintheearliersections. Wereporttheresultswith\n assumesthattheactivespeakerisperfect. Inreality,active trainingonlyon Ego 4 ddataaswellasonwithtrainingon\n speakergivesnoisyresults. Thiswouldcauseotherpeople’s existingdiarizationdatasets. Notethattheaudio-only DER\n voicefeaturetobeincludedinaperson’ssignaturelibrary isaggregatedwhiletheaudio-visual DERisaveraged. Also\n andaffectthefinalaudioclassificationresult. notetheimpactofthe VADonthediarizationperformance\n withtheaudio-onlybaseline. Itshouldbenotedthatamodel\n Tracking Camera Wearer’s Audio: Thecameraweareris \n more tailored to Ego 4 D-like data could be used to obtain\n a special participant because their face is invisible in the \n better performance. Nevertheless, this aspect still poses\n egocentricvideos. Theactivespeakerdetectionmethodthus \n challengesonthe AVDbenchmark. \n cannotbeusedtoassociatethewearerwiththeirvoice. We \n usetwomethodstodetectthecamerawearer’svoice. Transcription To obtain baseline transcriptions, we used\n Method I:Thefirstmethodusesenergyfilteringfollowed thepre-trained Gigaspeechmodelprovidedinthe ESPNet\n byaudiomatching. Thismethoddoesnotneedgroundtruth model zoo [1]. This model is trained on the Gigaspeech\n labelingofthecamerawearer’svoiceactivities. Sincethe dataset[34]whichcontains 10000 hoursofspeech. Input\n microphoneofthecameraisusuallyclosertothewearer’s featurestothemodelarelogmelfeaturesaugmentedusing\n mouththanothersubjectsinthescene,theamplitudeofthe the Spec Augmentmethod[173]andnormalizedbyglobal\n 60 "
  },
  {
    "page_num": 61,
    "text": " \n \n \n \n \n \n Model trained s VAD DER[%] languagemodelisused. Fordecoding,weused CTCweight\n on Ego 4 D of 0.3 andbeamsize 20 whichwedidnotfine-tuneonthe\n Region Cls no no 84.79 Ego 4 D dataset. The pre-trained model obtained from [1]\n Region Cls no yes 83.88 cannotsupport 5-minvideos,hence,weusedoraclesegment\n Talk Net no no 86.68 informationfromthetranscriptionannotationstosegment\n Talk Net no yes 85.85 thedataandwedecodedeachsegmentseparately. Thefinal\n Region Cls yes,only no 80.52 WERisobtainedbycountingthetotalnumberoferrorsover\n Region Cls yes,only yes 80.17 thewholevalidationortestset. \n Talk Net yes,only no 73.14 In Table 29,wesummarizethe WERresultsdepending\n Talk Net yes,only yes 73.32 onthe VADsegmentationmethodonbothvalidationandtest\n Always Speak - - >100 sets. Tocomputethefinal WER,we 1)removedpunctuation\n Never Talk - - 100 fromboththereferenceandthe ASRhypothesis,2)allowed\n soft-matchoncontractionssuchas(Iwillvs. I’ll)usingthe\n Table 26. Diarization Baseline Metricsshowing DERonthetestset. Englishglobalmappingfilefrom Kaldirepository[2],and 3)\n In Always Speak,allthedetectedpeoplearelabeledas”speaking” usedthe NISTsclitetool[72]. Aswecanseefrom Table 29,\n ineachvideoframe. In Never Talk, allthedetectedpeopleare onboththetestandvalidationsets,the WERsarequitehigh.\n labeledas”notspeaking”ineachvideoframe. Thisshowsthatthedatasetischallengingforanoff-the-shelf\n ASRmodelbecauseofoverlappingspeech,noise,different\n volumelevelsfordifferentspeakers,occasionalforeignword\n Model trained s VAD DER[%] \n usage,etc. \n on Ego 4 D \n Region Cls no no 98.82 \n Region Cls no yes 90.98 Speech Segments Valid Test \n Talk Net no no 99.73 \n Ground Truth 64.8 59.2 \n Talk Net no yes 92.14 \n Region Cls yes,only no 81.66 \n Table 29. ASRtranscription WERs(%)onthevalidationandtest\n Region Cls yes,only yes 79.97 \n datausingthereferencespeechsegmentation. \n Talk Net yes,only no 80.58 \n Talk Net yes,only yes 79.30 \n Always Speak - - >100 \n H.7 Discussion \n Never Talk - - 100 \n Although AVdiarizationpresentsatasksuitecomposedof\n Table 27. Diarizationbaselinemetricsshowing DERontheval- reasonably well understood tasks from the vision, speech\n idationset. In Always Speak,allthedetectedpeoplearelabeled \n andaudiocommunities,ourbaselineresultsclearlysuggest\n as”speaking”ineachvideoframe.In Never Talk,allthedetected \n thatefficientspeakerlocalization,tracking,diarizationand\n peoplearelabeledas”notspeaking”ineachvideoframe. \n transcriptionisarathercomplexproblemintheegocentric\n perspectiveandwithin-the-wilddata. Thisisspecifically\n Typeof VAD Valid Test evidentfromtheperformanceofthejointaudioandvideo\n drivendiarizationandtranscriptionbaselines(with DERof\n k VAD 67.24 65.28 \n > 80%and WERof> 60%). Overlappingspeechmakes\n Ref. Activity 36.56 39.99 \n both these tasks particularly difficult to annotate as well\n asevaluateanyproposedmodels. Performingsomeaudio-\n Table 28. Diarizationperformancewithaudio-onlymodelsfor \n visualsourceseparationpriortothesetasksmayimprovethe\n validationandtestsetsusingk VADandreference(groundtruth) \n voiceactivityannotations. efficacy,neverthelesssensitivitytochangesanddifference\n inspeechamplitudesofoverlappingspeakerswouldstillbe\n challengingtoaddress. \n mean-variancenormalization. Theencoderoftheacoustic Novelcross-modallearningapproachesthatjointlymodel\n model is based on macaron-style conformer [93] with 12 audio and visual modalities while accounting for such at-\n blocksand 8 attentionheadsandthedecoderisbasedona tributes (overlapping speakers, interruptions, noise in the\n 6-layertransformer[217]with 8 attentionheads. Inboththe wildetc.) areneededtofurtherimprovetheseperformances.\n encoderanddecoder,linearlayershave 2048 unitsandthe Thebaselineframeworkweutilizedherealsodoesnotac-\n encoderoutputis 512 dimensional. Thedecoderoutputhas countforefficientinformationsharingacrossthefourtasks\n 5000 sentencepiece[122]units. Themodelistrainedusing inthebenchmark. Specifically,therelationshipbetweenro-\n ajoint CTCandattentionobjective[112]. Fordecoding,no bustlocalizationandtrackingwithmulti-speakerdiarization\n 61 "
  },
  {
    "page_num": 62,
    "text": " \n \n \n \n \n \n is not studied, and this is also not well understood in the thebenchmarkformulationandwriting.\n literature. Weexpectthistobeachallengingproblem. \n Wealsoobservedthatsubjectiveattributesinconversa- \n tions, like speaker accents, changes in vocabulary usage \n basedonculturaldifferencesetc.,influenceboththecontent \n ofthespeechandtheclaritywithwhichitcanbecaptured \n in human annotations. The camera wearer’s head motion \n addssignificantblurtospeakers’faces. Toaccountforsuch \n aspectsweperformedqualitychecksonhumanannotations, \n andweexpectnovelunsupervisedandself-supervisedlearn- \n ingwillhelpfurtheraddresssuchsubjectiveattributes. \n In future versions, we expect to increase the scope of \n the task suite (i.e., proposing new tasks and annotations), \n therebyopeningnewavenuesforbothcoremachinelearning \n infirstpersonperspective,andalsoforrobustmulti-modal \n representationlearning. Wecouldalsoinvestigateresearch \n directionsfocusedonspatialaudiobycreating 3 Denviron- \n ments coupled with Sound Spaces [32]. This enables new \n research and tasks in audio-visual sound source localiza- \n tion,audio-visualdirection-of-arrivalestimationandrelated \n immersive reality applications. We note that a small frac- \n tionofourdatasetdoescompriseofbinauralaudiocaptured \n using in-ear microphones and an audio recorder (Tascam, \n Appendix A). \n H.8 Contributionsstatement \n Vamsi Krishna Ithapu co-led the audio-visual diarization \n benchmarkworkstream,thecorrespondingtasksdefinition, \n data selection methodology, data annotation tooling and \n guidelinesandwriting. Christian Fuegenco-leadtheaudio- \n visualbenchmarkworkstream,thediarizationandtranscrip- \n tiontasksdefinition,thecorrespondingannotationguidelines \n and paper writing. Hao Jiang worked on data annotation \n tooling,tasksdefinitionforlocalizationandtracking,active \n speakerdetectionanddiarization;alsoworkedonbuilding \n the baseline models for these tasks and writing. Federico \n Landiniand Jachym Kolarworkedonbaselinemodelsfor \n audio-onlyvoiceactivitydetectionanddiarization,andwrit- \n ing. Leda Sariworkedontranscriptiontaskdefinition,corre- \n spondingannotationguidelinesandbaselinemodeling. Eric \n Zhongcong Xuworkedondataselectionmethodologyand \n the baseline modeling of active speaker detection. Ruijie \n Taoand Mike Zheng Shouworkedonthemodelingofactive \n speakerdetection. Hanbyul Jooworkedondataannotation \n toolinganddataselectionmethodology. Christoph Feicht- \n enhoferworkedonthetaskdefinitionandmetrics. Anurag \n Kumarworkedonactivespeakerdetectionanddiarization \n tasksdefinition,andonaudioembeddingsmodelingforthese \n tasks. Morrie Doulatyworkedonbaselinemodelsforvoice \n activitydetectionanddiarizationanddataanalysisofanno- \n tations. Lorenzo Torresaniworkedonthetasksdefinition \n andannotationguidelines. Kristen Graumancontributedto \n 62 \n \n "
  },
  {
    "page_num": 63,
    "text": " \n \n \n \n \n \n I.Social Interaction Benchmark \n Thissectiondetailsthe Social Interactionbenchmarktask \n definitions, annotations,baselinemodels, andresults. We \n also provide details on the video data collection process \n formulti-personcapturewithparticipantswhoconsentedto \n havetheirfacesunblurredandconversationrecorded(Ap- \n pendix I.5). Asnotedin Appendix B,thesocialbenchmark \n videoswerescreenedtoremoveanyinformation(e.g. last \n namesorsocialmediaaccounts)thatcoulddirectlyidentify \n participants. However, participants’ faces and voices are \n presentasperourinformedconsent. \n \n I.1 Formal Task Definition \n LAM and TTM are defined as follows: (1) LAM: y = \n f(I,B); (2) TTM: y = f(I,A,B) where I = I T 2 , \n { t }−T 1 (a) Annotationtool \n A = A T 2 , and B = B T 2 aretime-synchronized \n { t }−T 1 { t }−T 1 \n pastsequencesofvideo,audio,andboundingboxes,respec- \n tively,where T and T arethelengthofthepastandfuture \n 1 2 \n time horizon, respectively, and t = 0 is the center frame. \n Theboundingboxindicatesthetargetpersontoclassify. y \n isabinaryclassificationlabeldefinedby: \n (cid:26) \n 1 if targetlooks/talksatcamerawearer \n y = \n 0 otherwise. \n (25) \n (b) Visualizationofannotations. \n The LAM and TTM tasks are defined as a frame-level \n predictiony,whichstandsincontrasttoaudioanalysistasks \n Figure 47. (Top)The GUIofthe annotationtool; (Bottom)Vi-\n wherelabelsareoftenassignedatthelevelofaudioframes \n sualizationofexampleannotations. Notethat LAM(denotedby\n orsegments. Adesiredmodelmustbeabletomakeacon- magentatext)and TTM(denotedbycyantext)maynotnecessarily\n solidateddecisionbasedonthevideoandaudiocuesover occurtogetherasshownintheseexamples.\n thetimecourseofanutterance. Forexample,ifthespeaker \n turnstheirheadtothesidemomentarilywhilespeakingto \n thecamera-wearer,thenaframewherethespeakerislooking short-duration LAMor TTMbehaviors,lasting 1 or 2 sec-\n awaywouldhavey = 0 whiley = 1. Figure 47 onds. Thedatawasorganizedasfollowsforbaselinemodel\n LAM TTM \n givessomeframelevelvisualizationofannotationsthatil- trainingin Section I.3: 389 clipswereheldoutfortraining,\n lustratethetaskdefinitions. comprising 32.4 hoursintotal. Anadditional 50 clips(4.2\n hours)and 133 clips(11.1 hours)wereheldouttoformthe\n validationandtestingsets,respectively. \n I.2 Annotation Statistics \n Thesocialtaskannotations, LAMand TTM,buildonthe \n I.3 Social Baseline Modelsand Results \n same video clips used in the AV diarization tasks and de- \n scribedin Appendix H.5. Fig 48 summarizesthestatisticsof LAM Ourbaselinemodelfor LAMisavideo-basedmodel\n LAMand TTMannotationsacrosstheseclips. Wecompute using Res Net-18 and Bidirectional LSTM.Ourmodeluses\n thepercentageoftheframeswith LAMor TTMannotations thecroppedfaceregionsinvideoasinputinordertofocus\n ineachclipandshowthehistogramsin Fig 48(a)and(b), oncuesabouttheheadposeandsocialattentionvisiblein\n respectively. Inmanyclips,theseeventshappenrarely(10 theface. Thearchitectureofourbaselineissimilartothe\n %orlower),andtheframeswith LAMannotationsareless Gaze 360[111]. Asillustratedin Fig 49(a),weinput 7 con-\n frequentthan TTMcases. Wealsolistthedurationofeach secutiveframes(T =3 and T =3)fromonefacetracklet,\n 1 2 \n LAM or TTM annotation (the duration between start and andeachimageisresizedto 224 224. Eachframeisthen\n × \n end time) in Fig 48 (c) and (d), in order to illustrate the processedbythe Res Net-18 backboneindependentlytogen-\n significantvariationsinlength. Themostfrequentcaseis erate 256 dimensionalfacefeatures. Thefeaturesequenceis\n 63 "
  },
  {
    "page_num": 64,
    "text": " \n \n \n \n \n \n \n 120 \n 100 \n 80 \n 60 \n 40 \n 20 \n 0 0 20 40 60 80 Percentage of frames w/ LAM annotations (%) \n spil C \n fo \n rebmu N \n 100 \n 80 \n 60 \n 40 \n 20 \n 0 0 20 40 60 80 Percentage of frames w/ TTM annotations (%)\n (a) %of LAMperclips \n spil C \n fo \n rebmu N \n (b) %of TTMperclips \n 2000 \n 1500 \n 1000 \n 500 \n 0 0 2 4 6 8 \n Duration of LAMM annotations (sec) \n ycneuqer F \n 2000 \n 1500 \n 1000 \n 500 \n 0 0 2 4 6 8 \n Duration of TTM annotations (sec) \n (c) Durationof LAM \n ycneuqer F \n val test \n Acc m AP Acc m AP \n Random Guess 8.57 51.19 7.98 50.96 \n Baseline(Gaze 360) 91.78 79.90 87.97 78.07\n Baseline(Random) 86.45 72.11 75.38 66.07\n Table 30. Resultsof LAM.Thebaselinemodelwasinitialized\n from Gaze 360[111](2 ndrow)andatrandom(3 rdrow).\n thesameas LAM.However,sometimesthespeakersleave\n thefieldofvieworbecomeinvisibleduetotherapidmotion.\n Inthiscase,wepadthefacesequenceswithblankimages.\n The MFCCfeatureisextractedevery 10 mswitha 25 mswin-\n dowlength. Thefeatureisthenfedintotheaudiobackbone,\n a Res Net-18 designedforaudiotasks[38]. Followingthe\n encoders,weconcatenatetheaudioandvisualembeddings\n (d) Durationof TTM andpassthemtothefinalclassificationheadtogetthe TTM\n resultforthevisiblefacesassociatedwiththesegment. To\n Figure 48. Socialtaskannotationstatistics.(a)Histogramshowing trainthemodelinparallel,wefirstsorttheshortsegments\n thenumberofclipsvs.thepercentageofframeswithlook-at-me basedonthelengthandgroupthesegmentsintoabatchif\n annotations;(b)Histogramshowingthenumberofclipsvs. the theyhavethesameduration. Thebatchsizeisrestrictedby\n percentageofframeswithtalk-to-meannotationsineachclip;(c) the GPUmemory;weuseabatchsizeof 400. Themodelis\n Histogram showing the duration of look-at-me annotations; (d) alsooptimizedusing Adamwithalearningrateof 5 10−4.\n Histogramshowingthedurationoftalk-to-meannotations. × \n Table 31 summarizesthe TTMresults. TTMismorechal-\n lengingthan LAM.Wecanseethatourbaselinemodelonly\n increasesthem APby 9.77%onthetestsplitincomparison\n encodedbya Bidirectional LSTM,whichhastworecurrent \n totherandomguessmodel. \n layers with dimensionality 256. The output is fed into a \n classificationheadtopredictthebinary LAMresultforthe \n center frame at the t-th timestamp. The LAM task has a I.4 Discussion \n classimbalanceissue,andweuseweightedcross-entropy \n Whilethebenchmarktasksofdetectingwhenattentionand\n loss. Sincethearchitectureissimilarto Gaze 360,wehave \n speakingbehaviorsaredirectedtowardsthefirst-personare\n twooptionsfortheinitialization: first,initializingtheback- \n closely related to existing analysis tasks, it is clear from\n bonefromapretrained Gaze 360 model;second,initializing \n thebaselineperformancethatthereissubstantialroomfor\n the model randomly and training from scratch on Ego 4 D. \n improvement,withm APof 78.07 for LAMand 55.06 for\n Duringtraining,wesamplecenterframeswithastrideof 3. \n TTM. \n Thenetworkisoptimizedby Adamwithalearningrateof \n 5 10−4. The TTMtaskisparticularlychallengingbecauseitre-\n × quiresanalysisoftheaudiocontenttounderstandthetarget\n Theresultsareshownin Table 30. Ourbaselinemodel \n audienceofanutterance,aswellasthefusionofaudioand\n achievesanm APof 66.07%onthetestsplitwheninitialized \n videocues. Themostcompletesolutiontothisproblemwill\n randomly, andtheperformanceishigherat 78.07%when \n requireanunderstandingofthesemanticsoftheutterancein\n initializedfrom Gaze 360. Thesefindingshighlighttheclose \n thecontextofanevolvingconversationalinteraction. Future\n relationshipbetweenthe LAMtaskandgazeestimation.The \n workonthistaskmightinvolvemoresophisticatedlanguage\n randomguessmodelachievesabout 8%accuracybecause \n modelingandpossiblyhierarchicalanalysisapproachesthat\n thenegativesamplesaccountfor 92%ofthetestsplitand \n allowtheintegrationofcuesatmultiplelevels,e.g. atthe\n themodelalwayspredictslookingatme. \n dialogleveltounderstandwhoisparticipatinginaconversa-\n TTM Thebaselinemodelfor TTMdigestsmulti-modalin- tionalexchange,attheutteranceleveltoaccesssemantics,\n puts: eachaudiosegmentispairedwithanassociatedface andattheaudioleveltoexploitprosodicandothercues.\n crop.Sincetheaudiosegmentsvarysubstantiallyinduration, The LAMtaskpresentsadditionalchallengessuchasthe\n webreakthelongutterancesintoshortsegmentswhosemax- need to deal with motion blur and fast head movements,\n imumdurationislimitedto 1.5 s. Ifthesegmentisshorter andmayalsobenefitfromamoreexplicitmodelingofhead\n than 0.15 s,weskipitinthetrainingstage. Theassociated movement and the patterns of gaze behavior that arise in\n facesarealsoresizedto 224 224,andthevideoencoderis conversationalinteraction. \n × \n 64 "
  },
  {
    "page_num": 65,
    "text": " \n \n \n \n \n \n datacollectiontookplaceduringthe COVID-19 pandemic,\n andtheresultingstudyprotocolsweredesignedtosafeguard\n participantsagainstadditionalrisk. \n Thesocialdataconsistsofdatacollectedatfivesites: At-\n lanta,Bloomington,Redmond,Twin Cities,and Singapore.\n In total, 764 hours of video and audio were collected for\n (a) LAM \n thesocialbenchmarktask. Adetailedsummaryofthedata\n collectionpracticesateachsitecanbefoundin Appendix A.\n I.6 Derived Tasksfor Future Social Benchmarks\n Thecoretasksof LAMand TTMdefineastartingpointfor\n analyzingmulti-modalegocentricdataandinferringsocial\n interactions. Wenowdescribetwogroupsofpotentialfuture\n tasks,attentiontasksandspeakingtasks,thatcouldbesup-\n portedviatheexistingannotationsin Ego 4 DSocialandthe\n (b) TTM gazedatacollectedfromeyetrackers. \n Egocentric Attention Prediction(EAP) Priorwork[135,\n Figure 49. Baselinemodelarchitectures. (a)LAMmodeluses \n 137] has demonstrated the feasibility of predicting where\n a Res Net-18 asabackbonetoextractthefeatureofeachframe. \n A Bidirectional-LSTM then takes the sequence and encode the the camera-wearer is looking (i.e. their egocentric atten-\n featuresintooneembedding.Wepasstheembeddingto FClayer tion)usingonlyegocentricvideocapturedfromahead-worn\n thatpredictsthe LAMresult. (b)TTMmodelhastwoencoders. camera. Thisworkleveragedthecontextofhand-eyecoor-\n Thevideoencoderisthesameas LAM.Theaudioencoderextracts dination tasks, which require gaze to be coordinated with\n the MFCCfrequencymapoftheaudiosegmentandthefeatureis handmovementsandobjects. Asubsetofthe Ego 4 DSocial\n fedintoa Res Net-18 network.Thevisualandaudioembeddings dataincludesgazemeasurementsproducedbywearableeye\n areconcatenatedandpassedthroughthe FClayertopredictthe \n trackersby Indiana Universityand Georgia Techparticipants\n targetofthisutterance. \n (e.g.,Pupil Invisible),whichwillgreatlyexpandthesizeof\n dataforhand-eyecoordinationinthewild. \n val test \n Acc m AP Acc m AP Social Gaze Prediction (SGP) The LAM task addresses\n Random Guess 32.44 53.82 47.41 50.16 thespecialcaseofsocialgaze: apersonlooksatthecamera-\n Baseline 64.31 56.50 49.75 55.06 wearer. Itispossibletogeneralizethetaskbypredictingthe\n socialgazetargetforeachofthevisiblefacesinanegocen-\n Table 31. Resultsof TTM.Thebaselinemodelisinitializedran- tric video, i.e., yp 0,1,...,M , where M is the total\n domly. ∈ { } \n number of participants in a group social interaction, and\n p 0,1,...,M . pistheindexforsocialmembers. The\n ∈{ } \n caseyp = qmeansthattargetpwaslookingatparticipant\n I.5 Social Dataset Collection \n q. The case yp = 0 captures alternative gaze targets, in-\n The Ego 4 DSocialdatacollectionprocesswasdesignedto cludingnon-socialgazetargets(e.g. lookingatanobject),\n achieve: 1)naturalisticinteractions,2)multi-modalcapture, lookingatpeoplewhoarenotwearinganegocentriccamera\n and 3)diverseparticipantsandenvironments. Participants (withtheresultthatgroundtruthannotationsarenotavail-\n consistedoffriendsandfamilygroupsanddatawascaptured able),andlookingatunknowntargetsnotcapturedinany\n inresidencesandlocalneighborhoods,ensuringnaturalistic of the egocentric videos. Let yˆq,p denote the LAM label\n interactions. Capture hardware varied across sites but in- fortargetpersonpvisibleinframeofegocentricvideo I\n q \n cludedwearablecameras,wearableeyetrackers(at Georgia capturedbyparticipantq. Thenthe SGPlabelisgivenby\n Techand Indiana University), binauralrecordingsystems, yp =argmax yˆq,p . The Ego 4 DSocialdataincludessyn-\n q{ } \n and smart watches (at Georgia Tech). Protocols included chronizedvideosfrommultiplesocialmembers,whichwill\n highly-structuredsettings,whereparticipantswereaskedto allowustoexpandtheannotationbymatchingtheperson ID\n playgamesoveraperiodofafewhoursinaresidence,and withthecamera-wearers. Notethatsincethevideorecorders\n unstructuredsettingswhereparticipantscapturedsocialin- arenotgenlocked,theidentificationofcorrespondingframes\n teractionsindailylifeoveraperiodaweekormore. Sample willonlybeapproximate. However, sincegazebehaviors\n socialinteractioncontextsincludedplayingboardandcard persistovermultipleframeswedonotbelievethiswillbe\n games,preparingmeals,andgoingonwalks.Thebulkofthe anissue. \n 65 "
  },
  {
    "page_num": 66,
    "text": " \n \n \n \n \n \n Akeyissueindefiningthetaskisthedeterminationofthe tonsiteandcontributedtothesocialbenchmarkformulation\n participantset. Fora 2 Dversionof SGP,termed SCG-2 D, andpaperwriting. Vamsi Ithapucontributedtothesocial\n theparticipantsetisdefinedbyparticipantswhoarevisible benchmarkformulationanddataannotation. Hyun Soo Park\n inframet. Thisisasocialversionofthevideo-basedgaze leddatacollectionatthe Twin Citiessiteandcontributedto\n followtask[37],wherethegoalistopredictwhethereach thesocialbenchmarkformulationandpaperwriting.\n targetparticipantislookingatanyoftheotherparticipants Hao Jiang contributed to model development and data\n whoarevisibleintheframe. Amorechallenging 3 Dversion annotation. Yunyi Zhucontributedtomodelimplementation\n of the task, SCG-3 D, uses all of the participants who are and experiments. Eric Zhongcong Xu contributed to the\n presentinthesocialsceneatthetimeofframet. Thistask socialbenchmarkdatapreparationandthemodelimplemen-\n requires the ability to predict which participant the target tationandexperiments,andcontributedtoalldatacollection\n personpislookingatinthecasewherethatparticipantis relatedtasksforthe Singaporesite. Ruijie Taocontributed\n notvisibleinframet. Thiscaninprinciplebeaccomplished todatacollectionforthe Singaporesite. Fiona Ryanledthe\n by maintaining a birds-eye view layout map of the social datacollectioneffortforthe Atlantasite,includingprotocol\n scene, that captures the approximate spatial relationships design,multimodalsensordeploymentandsynchronization,\n betweentheparticipants. Suchalayoutmapcouldbeused andde-identification. Miao Liucontributedtodatacollec-\n inconjunctionwithanapproachlike Gaze 360[111]tosolve tionandanalysisforthe Atlantasite. Audrey Southerland\n the SCG-3 Dtask.Notethatthistaskcouldpotentiallybenefit contributedtotheprotocoldesign,IRBauthoringandsub-\n fromtakingrecordedbinauralaudioasanadditionalinput,as mission, participant recruiting, and data ingestion for the\n theabilitytolocalizesoundsourcescouldprovideadditional Atlanta site. Jayant Sharma contributed to participant re-\n cuesfordeterminingthelocationsofgazetargetswhichare cruiting,datacollection,IRBsubmission,analysis,anddata\n notvisibleinthevideo. ingestionforthe Twin Citiessite. Yuchen Wangcontributed\n totheprotocoldesign,participantrecruiting,anddatacollec-\n Utterance Target Prediction(UTP) The TTMtaskcanbe \n tionforthe Bloomingtonsite. Weslie Khoodevelopedthe\n generalized to the full set of participants in the same way \n multi-camerasynchronizationandde-identificationpipelines\n that LAMcanbeextendedto SGP.Theinputspaceisthe \n atthe Bloomingtonsite. \n sameas TTMandtheoutputspaceissimilarto SGP,where \n yp = q means that participant p is talking to participant Acknowledgements The social benchmark team would\n q, and yp = 0 denotes the cases where the participant is liketoacknowledgethefollowingadditionalcontributions\n nottalkingtoanyone,oristalkingtosomeonewhoisnot fromindividualsateachsite: Atlanta: Jeffrey Valdez(re-\n wearinganegocentriccamera(andthereforegroundtruth cruitment and data collection), Gabriella Stripling, Ruth\n cannotbedetermined). Incontrastto SGP,UTPrequiresthe Stolovitz,and Andrea Sucre-Pardo(recruitmentanddataset\n identificationofallofthetargetrecipientsofanutterance. de-identification). Twin Cities: Reese Kneeland, Angad\n Infact,our TTMannotationalreadysupportsthistask,as Cheema, Silong Tan, Anjali Oleksy, Zhiteng Cao, Di-\n itdifferentiatesthecasewheretheutteranceisdirectedto ana Begelman(datacollectionandannotation)Facebook:\n multiple participants including the camera wearer. This Samuel Clapp and Peter Dodds (binaural audio recording\n additionallabelisignoredinthedesignofthesimpler TTM andmultimodalsynchronization). Bloomington: Zunaeed\n task. Salahuddin,Zehua Zhang,Ziwei Zhao. \n Transcript-based Variants For all of the previously- \n definedsocialtasksitispossibletodefineavariantwhich \n utilizesatranscriptoftheaudiofileasanadditionalinput \n modality. For example, the TTM-T task is the variant of \n TTM with the prediction defined as yp = f(I,A,T,B), \n where T the transcript (time-stamped sequence of words) \n obtainedfrom A. Thiscanpotentiallysimplifytheuseof \n dialogcuestoidentifytheintendedtargetsforutterancesand \n socialgaze. \n I.7 Contributionsstatement \n James M.Rehgco-ledthesocialbenchmarkeffortandpaper \n writing. Hanbyul Jooco-ledthesocialbenchmarkeffortand \n dataannotation. Mike Zheng Shouco-ledthesocialbench- \n markeffortandproblemformulationandmodelingexperi- \n ments. David Crandallleddatacollectionatthe Blooming- \n 66 "
  },
  {
    "page_num": 67,
    "text": " \n \n \n \n \n \n J.Forecasting Benchmark Short-Term Object Interaction Anticipation \n Thissectiondetailsthe Forecastingbenchmarktaskdefi- Thistaskaimstopredictthenexthuman-objectinteraction\n nitions,annotations,baselinemodels,andresults. happeningafteragiventimestamp. Givenaninputvideo,\n thegoalistoanticipate: \n \n J.1 Formaltasksdefinitions • Thespatialpositionsoftheactiveobjects,amongthose\n whichareinthescene(e.g.,boundingboxesaroundthe\n Asnotedinthemainpaper,therearefourforecastingtasks: objects). Weconsiderthenextactiveobjecttobethe\n future locomotion movement prediction, future hand pre- nextobjectwhichwillbetouchedbytheuser(either\n diction,short-termobjectinteractionanticipation,andlong- withtheirhandsorwithatool)toinitiateaninteraction;\n termactionanticipation. \n • Thecategoryofeachofthedetectednextactiveobjects\n (e.g.,“knife”,“tomato”); \n Future Locomotion Movements Prediction \n • Howeachactiveobjectwillbeused,i.e.,whataction\n will be performed on the active objects (e.g., “take”,\n This task aims to predict the future locomotion of a user \n “cut”); \n givenasequenceofpastimages. Weformulatetheproblem \n as: • Whentheinteractionwitheachobjectwillbegin(e.g.,\n “in 1 second”, “in 0.25 seconds”). This is the time\n (cid:2) (cid:3)T \n = x t+1 x t+F =f(x t−T , ,x t−1 ; ), tothefirstframeinwhichtheusertouchestheactive\n X ··· ··· I \n (26) object(timetocontact).Thispredictioncanbeusefulin\n scenarioswhichinvolvehuman-machinecollaboration.\n where isthefuturetrajectory,T and F arethepastand For instance, an assistive system could give an alert\n X \n future time horizons, respectively, x t is the point on the if a short time to action is predicted for a potentially\n trajectoryattimet,and istheegocentricimageattimet. dangerousobjecttotouch. \n I \n Withanassumptionthatthepersonwalksoveramajorplane \n Inthistask,modelsarerequiredtomakepredictionsat\n (e.g., ground plane), we represent the trajectory in a 2 D \n plane,i.e.,x R 2. a specific timestamp, rather than densely throughout the\n t \n ∈ video. Figure 51 illustratestheset-up. Themodelisallowed\n Theessenceofthelocomotiontaskistodesignafunc- \n to process the video up to frame t, at which point it must\n tion f to predict a set of plausible K future trajectories \n anticipate the next active objects, and how they will take\n k giventhecurrentimage. Sincethereexistsanumber \n k \n {X } partinaninteractioninδseconds,whereδisunknown. The\n ofplausiblefuturetrajectorieswithdifferenttopology,e.g., \n modelcanmakezeroormorepredictions. Eachprediction\n trajectories that bifurcate at an Y-junction, we predict K \n indicates the next active object in terms of noun class (nˆ)\n futuretrajectories. \n andboundingbox(ˆb),averbindicatingthefutureaction(vˆ),\n aswellasthetimetocontact(δˆ),whichestimateshowmany\n Future Hand Prediction seconds in the future the interaction with the object will\n begin. Eachpredictionalsocomprisesaconfidencescore(sˆ)\n In addition to future locomotion movements prediction, usedforevaluation. \n we consider another challenging task of predicting future Specifically,let V beanuntrimmedvideo.Wewilldenote\n hand positions of key-frames (see visual illustration in with V theframeof V occurringattime-steptandwith V\n t :t \n Fig. 50). Specifically, we denote the contact frame 17 as thevideosegmentstartingatthebeginningof V (timestamp\n x \n c \n , pre-conditionframe 18 asx \n p \n , andthethreeframespre- 0)andendingattimestampt. Givenatimestampt,denoted\n ceding the pre-condition frame by 0.5 s, 1 s and 1.5 s as as“stoppingtime”,theshort-termobjectinteractionanticipa-\n x p 1 , x p 2 , x p 3 , respectively. Formally, given an input ego- tiontaskrequiresthatamodelisabletoexploittheobserved\n centric video 1.5 s before the pre-condition time step (de- video V topredict N tuples(where N isarbitrary):\n :t \n noted as x = x ,...,x , with t referred as \n { p 3−to−1 p 3−1 } o (ˆb ,nˆ ,vˆ,δˆ,sˆ) N (27) \n observation time), this task seeks to predict the positions { i i i i i }i=1 \n of both hands (hl,hr) in the future key frames, where \n i i where: \n i c,p,p ,p ,p . \n ∈{ 1 2 3 } • ˆb R 4 isaboundingboxindicatingthepositionof\n i \n ∈ \n 17 Thecontactframeisdefinedasthefirstframeinwhichtheusertouches thepredictednextactiveobject;\n theobject,hencethemomentinwhichtheobjectbecomesactive. \n 18 Asdefinedin Section G,thepre-conditionframemarksamomentprior • nˆ i \n ∈N \n isanounindicatingtheclassofthenextactive\n tothestate-changeofanobject. object,where isthesetofpossiblenouns. \n N \n 67 "
  },
  {
    "page_num": 68,
    "text": " \n \n \n \n \n \n { ℎ𝑙,ℎ𝑟} { ℎ𝑙,ℎ𝑟} \n 𝑖 𝑖 𝑗 𝑗 \n \n … … \n \n \n \n Input Video Future Key Frames \n Figure 50.Exampleoffuturehandprediction. \n \n \n \n Last observedframe (𝑽 ) Unobserved future frame (𝑽 ) \n 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝟏 𝒕 𝒕+𝜹 \n 𝑏෠ = 450,90,510,140 \n 1 \n 𝑛ො =𝑑𝑜𝑢𝑔ℎ \n 1 \n 𝑣ො =𝑡𝑎𝑘𝑒 \n 1 \n 𝛿መ =0.75𝑠 \n 1 \n 𝑠Ƹ =0.8 \n 1 \n 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝟐 \n frame of \n 𝑏෠ = 500,100,550,150 \n 2 contact \n 𝑛ො =𝑑𝑜𝑢𝑔ℎ \n 2 \n 𝑣ො =𝑡𝑎𝑘𝑒 \n 2 \n 𝛿መ =0.75𝑠 \n 2 \n 𝑠Ƹ =0.75 \n 2 \n 𝛿 \n 𝑡 𝑡+𝛿 \n Input video: 𝑉 \n :𝑡 \n Figure 51.Exampleofshort-termobjectinteractionanticipation. \n • vˆ is a verb indicating the action which will be Figure 51 illustratestheproposedtask. Givenavideo V ,\n i :t \n ∈ V \n performed,where isthesetofpossibleverbs; a method should be able to detect the next active objects\n V \n (e.g., two instances of “dough”), predict the action which\n • δˆ i R+isthetimetocontact,apositivenumberwhich willbeperformedwiththatobject(e.g.,“take”),andthetime\n ∈ \n estimateshowmanysecondintothefuturetheinterac- tocontact(e.g.,0.75 s). \n tionwiththeobjectwillbegin; \n • sˆ [0,1]isaconfidencescoreassociatedtothepredic- Long-Term Action Anticipation\n i \n ∈ \n tion. Objectswithalargeconfidencevaluearedeemed \n tobelikelynext-active. Long-term action anticipation aims to predict further into\n the future. Rather than predict the next action at a given\n Themodelisallowedtoperform N predictionsforeach timestamp,modelswillberequiredtopredictthesequence\n observed example (with N arbitrary) both to account for of Z future actions which the camera-wearer is likely to\n thepresenceofmultiplenext-active-objectsandtohandle perform. Thisisimportantforlong-horizonplanningwhere\n the multi-modality of future predictions. Each of the N asequenceofactionsisrequiredtobeperformedinaspecific\n predictionsisintendedasaplausiblefutureobjectinteraction. ordertoachieveagoal. Critically,theseactionsoccurover\n 68 "
  },
  {
    "page_num": 69,
    "text": " \n \n \n \n \n \n 𝑛ො ,𝑣ො , 𝑛ො ,𝑣ො , 𝑛ො ,𝑣ො , 𝑛ො ,𝑣ො \n 1,𝑖 1,𝑖 2,𝑖 2,𝑖 3,𝑖 3,𝑖 4,𝑖 4,𝑖 \n 𝑖𝑡ℎprediction: kneaddough → putdough → packspice → pourspice\n \n \n \n \n Input video 𝑡 \n \n Figure 52.Exampleoflong-termactionanticipation.Afterobservingavideouptoaparticulartimestept,amethodshouldbeableto\n predictthesequenceofactionsthatwilllikelyoccur,inthecorrectorder(e.g.,first“takedough”,next“putdough”etc.)\n \n long time horizons, may be of variable length and do not focusonthehandmanipulation. Weconsidervideosfrom\n occuruniformlyacrosstime(e.g.,anactionevery 5 s). Thus, glass-mountedcamerasofwhichfieldofviewapproximately\n the task is defined at a more abstract level — models are alignswiththefirstperson.(4)3 Dreconstructionandground\n required to predict sequences of action classes (verb and planeneedtobeaccurate. Afterrunningstructurefrommo-\n noun), rather than time to action or to next active objects tion,weensure 3 Dreconstructionfromthevideosachieves\n boundingboxesinthecurrentframe. reasonablequalitybychecking 2 Dreprojectionofthepoint\n Moreformally,givenanuntrimmedvideo V andastop- cloudandgroundplane. Givenasetofthesevideoclips,we\n pingtimetasdescribedabove,thelong-termactionanticipa- chooseframesfortraining/testingdataforeverysecond.\n tionmodelmustobserve V andpredict N setsofsequences \n :t \n of Z plausiblefutureactions: \n Remaining Tasks \n (nˆ ,vˆ ) Z N (28) \n {{ z,i z,i }z=1}i=1 For the remaining tasks we first manually ranked the sce-\n nariosbasedontheirapplicabilitytotheforecastingtasks.\n where: \n Forinstance,scenarioslikecarpenterywerehighpriorityfor\n • nˆ is the predicted noun and vˆ is the forecasting whereas walking in the park was low-priority.\n z,i z,i \n ∈ N ∈ V \n predictedverbofthez-thfutureaction. Wescoredallscenariosfrom 1-3 basedonthispriority. We\n imposeconstraintsontheminimumnumberof hoursand\n • { (nˆ z,i ,vˆ z,i ) } Z z=1 representsthesequenceoffutureac- participantstosub-selectscenariosthathavesufficientdata\n tionssortedbythepredictedorderinwhichtheywill fortraining(eachparticipantshouldhavecontributedatleast\n appearinthevideo. 15 minutes;andthereshouldbeatleast 20 minutesofvideos\n forthatscenario). Next,wechunkourvideosinto 5 minute\n Liketheshort-termobjectinteractionanticipationtask, \n clips,andusethefollowingalgorithmtoselectclipstobe\n the model is allowed to generate N sets of predictions to \n labeled. Toensuregeographicaldiversity,wedistributethe\n accountforthemulti-modalnatureoffutureprediction. Fig- \n totalhoursoveruniversitiesandrandomlyselectclipsfrom\n ure 52 illustratestheproposedtask. \n each to fill the hours allocated to that university. If there\n are universities that contributed less, then their hours are\n J.2 Data Selection \n distributedacrosstheotheruniversities. Toselecttheclips\n Future Locomotion Movements Prediction givenauniversityandthehoursallocated; wewouldfirst\n sampleaparticipant,thensampleavideoforthatparticipant,\n Egocentricvideosforlocomotionandhand-objectinterac- \n andsample 1 clipfromthatvideo. Forcertainrepetitivesce-\n tionarenearlymutuallyexclusive. Amongthesevideos,we \n narios(likebrickmaking),werejectthisclipifwealready\n skim through each video to manually identify video clips \n haveselectedatleast 2 clipsfromthesamevideo. Werepeat\n (beginningandendframes)thatsatisfythefollowingselec- \n theprocessuntiltherequirednumberofhoursareselected.\n tioncriteria. (1)Locomotion,bydefinition,involvesdiverse \n activitiesassociatedwithwalking. Theclipshouldinclude \n J.3 Data Annotation \n substantialtranslationalmovement. (2)Eachvideoclipmust \n belongerthan 10 secondsforpasttrajectoryobservationand \n Future Locomotion Movements Prediction \n future prediction. (3) The videos must observe surround- \n ing scenes. This differs from the videos for hand-object We generate the ground truth of future trajectories using\n interactionwherethecameraisdeliberatelytilteddownto 3 D reconstruction of the camera trajectories. Given a se-\n 69 "
  },
  {
    "page_num": 70,
    "text": " \n \n \n \n \n \n \n \n  \n \n o \n x \n n Offset t \n \n Ground plane \n (a) Geometry (b) Futuretrajectoryprediction \n \n Figure 53.(a)Werepresentthefuturetrajectoryofapersonusingthegroundplane.Giventhe 3 Dreconstructionofthecameratrajectory,\n weprojectitintotheestimatedgroundplanetoformthefuturetrajectory.(b)Thegroundtruthfuturetrajectory(blue)andthepredicted\n trajectories(redandwhite)areshownintheegocentricimagewiththegroundplanecoordinate(magentagrid).Wepredicttop 5 trajectories\n wherethetoppredictionismarkedinred. \n \n Data Outdoor Indoor Mixed Total 1 sand 1.5 sbeforethepre-conditiontimestep. Therefore,\n Train 34.1 k 0.41 k 16.7 k 51.3 k \n foreachinteractiontherewillbe 5 keyframeslabeledwith\n Val 7.5 k 0.23 k 6 k 13.9 k \n boundingboxesofhands,includingthecontactframe. We\n Test 7.4 k 0.18 k 3 k 10.6 k \n use the bouding box center as the ground truth of hands\n Table 32.Wesplittheimagedataforlocomotionpredictionbased positions. \n onscenesthatincludingoutdoor,indoor,andmixed. \n Short-Term Object Interaction Anticipation \n quence of egocentric images, we reconstruct the 3 D ego- Eachvideo V ofthedatasetislabeledwithasetofshortterm\n motionandscenegeometryusingastandardstructurefrom object interaction anticipation annotations = S (j)\n S V { V } j \n motionpipelinewithafewmodificationtohandlealarge indicatingtheoccurrenceofobjectinteractionsinthevideo.\n numberofimages. Withthe 3 Dscenepointcloud,wees- Eachannotation \n timate the ground plane using RANSAC with the ground \n planenormalprior. The 3 Dreconstructedcameratrajectory S V (j) =(t( s j), { n ( h j) } h ,v(j), { A ( h j) } h , { B h (j) } h ) (29)\n is projected onto the ground plane to form the 2 D future \n trajectoryasshownin Figure 53. includes: \n Our image dataset includes locomotion in outdoor, (j) \n • t s :thetimestampindicatingthebeginningoftheinter-\n indoor, and mixed scenes. We split the image data \n actionwiththeactiveobjects. Thisisthefirstframein\n into training/validation/testing sets with approximately \n whichtheusertouchesatleastoneoftheactiveobjects;\n 70%/15%/15%,respectively. Theratioacrossscenesdoes \n notexactlymatchbecausethesplitisperformedbasedon (j) \n • n : the set of categories of the h interacted ob-\n the(anonymous)participant ID.Thesummaryofthedata { h } h \n jects; \n splitcanbefoundin Table 32. \n • v(j): theclassoftheactioninvolvingtheactiveobjects;\n Future Hands Movements Prediction (j) \n • A : theboundingboxannotationsfortheactive\n { h } h \n (j) \n Forthethefuturehandpositionandtrajectoryprediction,the objects. The cardinality of { A h } h is equal to the\n annotationwillbeperformedbylabelingboundingboxes cardinalityof n (j) , i.e., A (j) = n (j) . The\n { h } |{ h } h | |{ h }| \n around hands in the frame in which the user touches the hthset A (j) containsboundingboxannotationsfor\n active objects as well as in frames preceding each object { h } h (j) \n interactions. Hands bounding boxes will be associated to theactiveobjectsofcategoryn h attimestampt s ;\n a label useful to distinguish among left and right hands. (j) \n • B : the bounding box annotations for the next\n Therefore,givenanobjectinteraction,wewillannotatekey { h } h \n (j) \n activeobjects. Thecardinality B isequaltothe\n framesprecedingthebeginningoftheinteraction. Specif- { h } h \n (j) (j) (j) \n ically,t c andt p denotethetimestepofcontactframeand cardinality of { A h } h , i.e., |{ B h } h | = |{ A h } h | .\n pre-conditionframe,andt ,t ,t ,denotetimesteps 0.5 s, Thejthset B (j) containstheboundingboxannotations\n p 1 p 2 p 3 h \n 70 "
  },
  {
    "page_num": 71,
    "text": " \n \n \n \n \n \n future action short-term annotations S (i) are available (see Section J.3)\n V \n andavaluefor Zhasbeenchosen,thelong-termannotations\n (j) \n L canbeeasilyobtainedbysamplingthefirst Z actions\n V \n 𝑡 𝑠 (𝑗)−4𝛼 𝑡 𝑠 (𝑗)−3𝛼 𝑡 𝑠 (𝑗)−2𝛼 𝑡 𝑠 (𝑗)−𝛼 𝑡 𝑠 (𝑗) annotatedinvideo V beginningaftertimestampt(j). More\n (j) \n formally,thefutureactionlabelsfor L areobtainedas:\n Figure 54. Anexampleofhowframesaresampledtobelabeled V \n withnextactiveobjectannotations.Foragivenactioni,wesample \n mframesatregularintervalsα.Ifwesetm=4 andα=0.5,we {(n(iz),v(iz))|(t(iz),{n(iz)} ,v(iz),{A(iz)} ,{B(iz)} )∈S ∧\n labeltheframeofcontactaswellas 4 framesalongasegmentof 0 s h h h h h h V \n 2 sprecedingthebeginningoftheactionataframerateof 2 fps. t( \n s \n iz) ≥t(j)∧ \n t(i 1) ≤...≤t(i Z)∧ \n s s \n ofnextactiveobjectsofclassn h . Inparticular, B h (j) (cid:64)S V (j) ∈S V |t(j) ∈/ {i 1 ,...,i Z },t≤t s (j) <t( s i Z)}Z z=1\n containsannotationsforthesameobjectinstancesanno- \n (j) (j) wheren \n (iz) \n referstotheprimaryinteractedobjectfromthe\n t \n i \n a \n c \n t \n a \n e \n l \n d \n ly, \n in \n B \n A \n h (j \n h \n ) = \n ,tr \n { \n a \n B \n ck \n l ( \n e \n , j h \n d \n ) | l \n in \n = \n fr \n 1 \n am \n ,.. \n e \n . \n s \n ,m \n pr \n } \n e \n , \n ce \n w \n d \n h \n in \n e \n g \n re \n t s \n B l ( , \n . \n j h ) \n S \n i \n p \n s \n e \n t \n c \n h \n if \n e \n - \n s e e x t am of p i l n e t \n 0 \n e o r f ac h t o e w do lo b n je g c - t t s er { m n\n ( \n h a \n iz \n n \n ) \n n } o h t . at F io ig n u s r a e re 56 ob il t l a u i s n t e ra d te f s or a m n\n setofboundingboxannotationsofnextactiveobjectof \n short-termannotations. \n classn annotatedattimestampt lα. Heremindi- \n h s \n − \n catesthenumberofframesprecedingthebeginningof \n Annotationanalysis \n theinteractioninwhichobjectsareannotated,whereas \n αisthetemporaldistancebetweenthesampledframes. \n Datasetstatistics Asdiscussedearlier,oneofourprimary\n For instance, setting α = 0.5 s and m = 4, we will \n objectives when selecting the data to annotate was to\n labeltheframeinwhichtheobjectisinteractedaswell \n maximizethediversityintermsofactivitiesandgeographic\n as 4 framesina 2 ssegmentprecedingtheinteraction. \n locations. Ourdatasetincludesscenariosspanningawide\n Figure 54 showsanexampleofhowframesaresampled \n range of everyday activities (e.g., gardening, cleaning,\n withtheconsideredscheme. \n fishing,etc.). Inadditiontodiversityacrossscenarios,there\n Figure 55 reportsasampleclipwiththediscussedannota- isalsogeographicdiversitywithinscenarios. Forexample,\n tions. Thetimestampt isselectedasthefirstoneinwhich cookingmaylookverydifferentin Italy,India,Saudi Arabia,\n s \n theusertouchestheactiveobjects.Theframesfollowingthis or Japan. In Figure 38,weshowtheresultingscenarioand\n timestamparenotlabeled. Activeobjectboundingboxesare universitydistributions. Overall,ourbenchmarkconsistsof\n labeledattimestampt ,whereasnextactiveobjectbounding 120 hoursofannotatedvideocomingfrom 53 scenarios,7\n s \n boxesarelabeledinframesprecedingt . universities,and 406 participants. \n s \n Temporal structure of activities Human activity is goal-\n Long-Term Action Anticipation \n driven and structured over time, with certain action se-\n Eachvideo V islabeledwithasetoflong-termactionan- quencesbeingfavoredoverothers.Wemeasurethistemporal\n notations L (j) , corresponding to a stopping time until structureusing Normalized Pointwise Mutual Information\n { V } j \n whichthevideocanbeobserved,andasequenceof Z future (NPMI)[41]overpairsofactionsfollowingpriorwork[92].\n actionlabelsdefinedasfollows: NPMIisameasureofhowlikelyactionsfolloweachother.\n Inourdataset,typicalpatternsinclude“pullgrass throw\n L ( V j) =(t(j), { (n( z j),v z (j)) } Z z=1 ) (30) grass(0.87)”,“holdspinach → cutspinach(0.83)”, → “turn-on\n faucet turn-offfaucet(0.68)”,“takecloth foldcloth\n where: → → \n (0.49)”etc. Severalactionsalsooccurinsequencewithhigh\n • t(j): the timestamp until which the video can be ob- NPMIscoresduetotherepetitivenatureoftheactivity. For\n served(i.e.,V )beforemakingpredictionsoffuture example, “flippage flippage(0.83)”whilereading, or\n :t(j) → \n actions; “cutcarrot cutcarrot(0.82)”whilecooking. Finally,we\n → \n see common action sequences involving multiple objects\n (j) \n • n z :thenouncategoryoftheprimaryinteractedobject like“filltire closevalve(0.89)”,or“liftvacuum-cleaner\n inthez-thfutureaction; cleanstair → case(0.87)”. Thisstructureisvaluableandcan\n → \n (j) informlong-termactionanticipationmodels. \n • v z : theverbdescribinghowtheobjectswillbeinter- \n Dataset split To facilitate future research and compar-\n actedwithinthez-thfutureaction. \n isons, we construct training, validation, and test splits\n Foreachvideo,t(j)areselectedfromthelasttimestampof containing 40%, 30%, and 30% of the data, respectively.\n eachannotatedobjectinteraction.Itisworthnotingthatonce Wenote,however,thatwedonotreleasethegroundtruth\n 71 "
  },
  {
    "page_num": 72,
    "text": " \n \n \n \n \n \n 𝐵 1 ( , 𝑖 ℎ )={[0.4,0.6,0.6,1.0]} 𝐵 1 ( , 𝑖 ℎ )={[0.5,0.8,0.7,1.0]} 𝐵 1 ( , 𝑖 ℎ )={[0.4,0.5,0.6,0.9], 𝐵 1 ( , 𝑖 ℎ )={[0.3,0.2,0.6,0.7], 𝐴( ℎ 𝑖)={[0.1,0.2,0.5,0.8], First frame of contact\n [0.4,0.9,0.6,1.0]} [0.3,0.7,0.6,1.0]} [0.3,0.6,0.6,1.0]} UNLABELED FRAMES\n \n \n \n 𝑛(𝑖)=𝑏𝑢𝑐𝑘𝑒𝑡 \n ℎ \n 𝑡(𝑖)−3𝛼 𝑡(𝑖)−2𝛼 𝑡(𝑖)−𝛼 𝑡(𝑖) 𝑣(𝑖)=𝑡𝑎𝑘𝑒 \n 𝑠 𝑠 𝑠 𝑠 \n Figure 55.Exampleofannotationsfortheshort-termobjectinteractionanticipationtask.\n Video 𝑉 \n 𝑆(1) 𝑆(2) 𝑆(3) 𝑆(4) 𝑆(5) 𝑆(6) 𝑆(7) 𝑆(8) 𝑆(9) 𝑆(10) 𝑆(11) \n 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 𝑉 \n 𝑡(𝑗) \n Z=3 \n 𝐿(𝑗)= 𝑡𝑗, 𝑛𝑗,𝑣𝑗 , 𝑛𝑗,𝑣𝑗 , 𝑛𝑗,𝑣𝑗 \n 𝑉 1 1 2 2 3 3 \n Figure 56. An exampleof a long-termannotation L(V:t) for anuntrimmed video V at timestamp t can beobtained fromshort-term\n annotations S(i).Intheexample,Z =3,hencethelongtermannotationisobtainedbyconsideringthefirstthreeactionsbeginningafter\n V \n timestampt. \n annotations for the test set. Following common practice, is smaller than the error tolerance, it is considered as a\n evaluation on the test set will be supported through the correct trajectory prediction. PCT(cid:15) measures how many\n publicevaluationserverandleaderboard. Weassigndatato trajectoriesamong K retrievedtrajectoriesareclosetothe\n splitsrandomlyatthelevelof 5 minuteclips. Thisensures groundtruthtrajectory. \n that all interactions within a 5 minute clip were labeled \n byanannotatorandprovidesenoughtemporalcontextfor Future Hand Movement Asforthefuturehandsmovements\n long-termvideotasks,likelong-termactionanticipation. prediction,weonlyconsiderthekeyframeprediction,and\n thereforeadopt Mean Key Frame Displacement Error Con-\n tact(M.Disp.) Key Frame Displacement Errorasevaluation\n J.4 Evaluationmeasures metrics(C.Disp.): \n Future Locomotionand Hands Movements Prediction \n • Mean Key Frame Displacement Error(M.Disp.):\n Future Locomotion Wemeasuretheaccuracyofthepredic- \n tion using two metrics. (1) K best mean trajectory error \n (K-MTE):wemeasure Kbesttrajectoryerror: D m = n 1 (cid:88) (cid:107)h i −hˆ i (cid:107) (33)\n i∈Ht \n 1 (cid:88) \n K MTE= argmin (cid:80) v t x t x (cid:98)t , (31) \n − v (cid:107) − (cid:107) \n {Xk}K k=1 t t t H t refers to the set of visible hand positions of key\n frames, andnisthelengthofset H . h denotesthe\n t i \n x R 2 isthepredictedlocationattimet,x istheground \n t (cid:98)t predictedhandpositionintheimagecoordinate,while\n ∈ \n truthlocation,andv t isthevisibility. Thevisibilityindicates hˆidenotesthegroundtruthhandpositions.\n the availability of the ground truth trajectory, i.e., due to \n severeegocentricvideos,thegroundtruthtrajectoriesmay \n includemissingdata. v =0 indicatesmissingdataattime • Contact Key Frame Displacement Error(C.Disp.):\n t \n t. (2)Probabilityofcorrecttrajectory(PCT):wemeasure \n thesuccessrateofthecorrecttrajectoryretrieval: D c =(cid:107)h c −hˆ c (cid:107) (34)\n (cid:32) (cid:33) \n 1 1 (cid:88) \n PCT(cid:15)= δ (cid:80) v t x t x (cid:98)t <(cid:15) , (32) h c referstothehandpositionsat Contactframe.\n K v (cid:107) − (cid:107) \n t t t \n whereδ()isoneifthestatementistrueandzerootherwise. Note that all reports are reported on downsampled video\n · \n (cid:15)isthetrajectoryerrortolerance,i.e.,ifthetrajectoryerror frameswithheightof 256 andoriginalaspectratio.\n 72 "
  },
  {
    "page_num": 73,
    "text": " \n \n \n \n \n \n Short-Term Object Interaction Anticipation • Noun+Verb Top-Km AP:predictioniandannotation\n j areapossiblematchifthefollowingconditionsare\n Methodswillbeevaluatedatthetimestampsinwhichnext- \n satisfied: \n activeobjectshavebeenannotated,i.e., \n * IOU(ˆb ,b )>0.5; \n (cid:110) i j \n tt = t l α \n s * nˆ =n ; \n | − · i j \n ∀ t s ∈{ t( s j) |∃ h:B h (j) (cid:54) = ∅} j * vˆ i =v j . \n (cid:111) \n l 1,...,m (35) • Noun+TTCTop-Km AP:predictioniandannotation\n ∀ ∈{ } \n j areapossiblematchifthefollowingconditionsare\n (j) (j) satisfied: \n where t s h:B = j isthesetofalltimestampsin- \n { |∃ h (cid:54) ∅} \n dicatingthebeginningofaninteraction,forwhichatleast * IOU(ˆb ,b )>0.5; \n i j \n onenextactiveobjecthasbeenannotated, andαandmare \n * nˆ =n ; \n definedin Appendix J.3. i j \n Sincedetectingnextactiveobjectsisamajorpartofthe * δˆ δ <T . \n i j δ \n | − | \n task,webaseourevaluationmeasuresonmean Average Pre- \n • Overall Top-Km AP:predictioniandannotationjarea\n cision(m AP),asdefinedinthe Pascal VOCchallenge[60]. \n possiblematchifthefollowingconditionsaresatisfied:\n As in standard m AP, we first match each of the detected \n nextactiveobjectstogroundtruthannotations. Apredicted * IOU(ˆb ,b )>0.5; \n i j \n andagroundtruthboundingboxesareapossiblematchif \n their Intersection Over Union(IOU)valueexceeds 0.5 and * nˆ i =n j ; \n if some matching criteria are met. We will define match- * vˆ =v ; \n i j \n ing criteria later. Predictions are matched to ground truth \n * δˆ δ <T . \n annotationsbelongingtothesameevaluatedexampleina | i − j | δ \n greedy fashion, prioritizing predictions with higher confi- \n Where T is a tolerance threshold, parameter of the\n δ \n dencescoresandchoosingmatchescorrespondingtolarger \n evaluationmeasure. \n IOUvalues. Agroundtruthannotationcanbematchedat \n mostwithonepredictedbox. Allmatchedpredictionsare The goal of the different measures is to assess the ability\n countedastruepositives,whereasallunmatchedpredictions ofthemodeltopredictnextobjectinteractionsatdifferent\n arecountedasfalsepositives. Performanceonthewholetest levelsofgranularity. Weuse K =5 and T =0.25.\n δ \n setissummarizedusingthemeanofthe Average Precision \n valuesobtainedforeachclass. Long-Term Action Anticipation \n Toaccountforthemulti-modalnatureoffuturepredic- \n tions(i.e.,morethanonenextactiveobjectcanbelikely), Methods will be evaluated at the set of timestamps spec-\n we “discount” the number of false positives obtained in a ified by the end of each annotated object interaction in a\n givenexamplebythenumberofavailablegroundtruthan- video V. Let L ( V j) = { (n ( z j) ,v z (j) ) } Z z=1 bethegroundtruth\n notationsinthatexamplemultipliedby K 1,where K is annotation related to video V at time-stamp t(j) and let\n − \n a parameter of the evaluation measure. Specifically, if an (nˆ (j) ,vˆ (j) ) Z K be the K predicted sequences of\n {{ z,k z,k }z=1}k=1 \n example contains two ground truth annotation, we ignore Z actions. We will consider single noun/verb/action pre-\n the(K 1) 2 falsepositiveswiththehighestscores. This dictionscorrectfollowingthedefinitionsdiscussedin Sec-\n − ∗ \n effectivelyimplementsa“Top-Kmean Average Precision” tion J.4. The K predictedsequenceswillhencebeevaluated\n criterion which does not penalize methods for predicting usingtheeditdistancemetricasfollows.\n upto K 1 possiblylikelynextactiveobjectswhichare For a given k, this is obtained by evaluating the edit\n not anno − tated. Given a generic prediction (ˆb i ,nˆ i ,vˆ i ,δˆ i sˆ i ) distancebetweenapredictedsequenceandthegroundtruth\n andagenericgroundtruthannotation(b j ,n j ,v j ,δ j ),wede- sequenceoffutureactions. Theeditdistance\n finethefollowingvariantsofthis Top-Kevaluationmeasure \n consideringdifferentmatchingcriteria: ∆ ( (nˆ (j) ,vˆ (j) ) Z , (n(j),v(j)) Z )\n E { z,k z,k }z=1 { z z }z=1 \n • Noun Top-Km AP:predictioniandannotationj area iscomputedasthe Damerau-Levenshteindistance[47,133]\n possiblematchifthefollowingconditionsaresatisfied: oversequencesofpredictionsofverbs,nounsandactions.\n Thegoalofthismeasureistoassessperformanceinaway\n * IOU(ˆb ,b )>0.5; \n i j whichisrobusttosomeerrorinthepredictedorderoffuture\n * nˆ =n ; actions. A predicted verb/noun is considered “correct” if\n i j \n 73 "
  },
  {
    "page_num": 74,
    "text": " \n \n \n \n \n \n it matches the ground truth verb label at a specific time- Verb time to contact\n (softmax) (softplus)\n step. Theallowedoperationstocomputetheeditdistance \n areinsertions,deletions,substitutionsandtranspositionsof f f e e a a t t s s 𝛿 𝛿\n any two predicted actions. Following the “best of many” Video Slow Fast feats 𝛿\n feats 𝛿 \n criterion, the K predictions are evaluated considering the feature pooling \n Pre-Trained Detector \n smallesteditdistancebetweenthegroundtruthandanyof Detected \n the K predictions: Last Frame R F - a C s N te N r Next Active Objects attachlabels\n output \n ∆ ( (nˆ (j) ,vˆ (j) ) Z K , (n (j) ,v (j) ) Z )= \n E {{ z,k z,k }z=1}k=1 { z z }z=1 \n min ∆ ( (nˆ (j) ,vˆ (j) ) Z , (n (j) ,v (j) ) Z ) Figure 57.Short-Termobjectinteractionanticipationbaseline.\n k=1..K E { z,k z,k }z=1 { z z }z=1 \n Note that we consider edit distance over simple accu- Future Hands Movements Prediction\n racybasedmeasures. Treatingpredictionsforeachfuture \n Baseline Description Theproposedfuturehandmovement\n time-stepindependentlyandcalculatingaccuracydoesnot \n prediction task can be factorized as a regression problem.\n accountforthesequentialnatureofthepredictiontaskwhere \n To address this task, we adopt a baseline that utilizes the\n theorderofpredictionsisimportant. Weevaluateeachmet- \n I 3 Dnetworkasthebackbonetoextractthespatial-temporal\n ric independently for verbs, nouns and actions (verb and \n videorepresentationsoftheinputvideosequence,andthen\n nountogether). Wereporteditdistanceat Z =20(ED@20) \n usealinearmappingfunctionastheregressortopredictthe\n anduse K = 5 inourexperiments. Weselect Z = 20 as \n futurekeyframehandpositions. Weadoptthesmootherl 1\n baselinesbegintopredictactionsatrandomforhighervalues \n lossastheobjectivefunction: \n of Z. \n (cid:40) \n 0.5 w (h hˆ)2/β, if h hˆ <β \n J.5 Baselinedefinitionsandimplementationdetails L h = w ∗ (h ∗ hˆ − 0.5 β), oth | er − wise | (38)\n ∗ | − |− ∗ \n Future Locomotion Movements Prediction \n whereh R 20 isavectorthatrepresentsthex,ycoordinates\n Wemakeuseofthemethodby Parketal.[175]forabaseline ofbothle ∈ ftandrighthandsintheaforementionedfivefuture\n algorithm. The method models the trajectory prediction key frames. If the hand is not observed in the keyframe,\n function in Equation (26) using KNN classification with we pad 0 into the hˆ, and adopt a binary mask w to pre-\n CNNimageencoding,i.e., ventthegradientspropagationoftheseunobservedinstances.\n =KNN( φ( ) ,φ( )) (36) \n i Training Details Weadoptthe I 3 Dmodelasthebackbone\n {X} { I } I \n network and a regression header, composed of two linear\n where KNN(A,B)findsthe Knearestneighborof Bgiven \n operations, to predict the hand positions in the future key\n the set A, and φ( ) Rn is a function that extracts the \n I ∈ frames. For our experiments, we set observation time T o\n image feature of . We use the Alex Net image feature \n I as 2 s. For training, we applied several data augmentation\n extractorforφ. \n techniques, including random flipping, rotation, cropping\n Notably, the baseline algorithm leverages a polar co- \n andcolorjitteringtoavoidoverfitting. Ourbaselinemodel\n ordinate system to represent the trajectory, i.e., X 2 D = \n j was trained with a batch size of 64 for 25 epochs using\n (cid:2) (cid:3)T \n r j θ j isa 2 Dtrajectoryonthegroundplanewherer i a cosine learning rate decay with a initial learning rate of\n andθ i arethepolarcoordinatesofthetrajectoryrepresented 0.0375. Wesetβ to 5 intheweightedsmoothed L 1 lossas\n intheegocentriccoordinatesystem,i.e.,distance(radial)and introducedin Eq.38. \n direction(angle)withrespecttotheperson’sfeetlocationas \n shownin Figure 53: Short-Term Object Interaction Anticipation \n X 2 D =cart 2 polar(r TX ,r TX ) (37) Dataandannotationsusedfortheexperiments Weper-\n j 1 j 2 j \n formedourexperimentsonasubsetofthedataandannota-\n wherer andr arethetwospanningvectorsoftheground tionstoobtainverbandnountaxonomiesconsistentwiththe\n 1 2 \n plane that are aligned with the rotation matrix R . r is Short-Term Object-Interaction Anticipationtask. Westarted\n t 1 \n the facing direction and r is lateral direction. Both are by considering all annotated actions for which a contact\n 2 \n perpendicular to the ground plane normal n as shown in framehasbeenspecifiedbytheannotators. Notethatthese\n Figure 53. cart 2 polar is a coordinate transform from constituteabout 30%ofthewholesetofannotatedactions\n cartesiantopolarcoordinates. andthatthenotionofacontactframeisfundamentaltoour\n 74 "
  },
  {
    "page_num": 75,
    "text": " \n \n \n \n \n \n task. Wethengatheredallannotatedframesandreferenced on all frames with annotated next active objects. We use\n themtotheirrespectivecontactframes,computingthetime the Faster RCNNdetectorbasedon Res Net 50 usingthe“3 x”\n toactiontargets. Wediscardedallthoseannotationswhich training schedule provided with the Detectron 2 library 19.\n comprisedaverboranounclassmarkedbytheannotator Afterthisstage,theweightsofthe Faster R-CNNcomponent\n as“null”. Wefurtherdiscardedannotationsrelatedtonouns arenotupdatedanymore. Wehencetraina Slow Fastmodel\n whichhadbeenlabeledinconsistentlyandnon-objectclasses basedon Res Net 50. Wefollowtheconfigurationprovided\n such as “wall” or “wallpaper”. We similarly removed all inthe Py Slow Fastlibrary 20 totacklethe AVAdetectiontask\n annotationsrelatedtotheverb“talk”whichdonotinvolve (“SLOWFAST 32 x 2 R 50 SHORT.yaml”). The Slow Fast\n interactionswithobjects. modeltakesasinputvideoclipsof 32 framessampledwith\n Toavoidhavinganover-specificnountaxonomy,weclus- a temporal stride of 1 frame. During training, we match\n teredselectednounclassesintohomogeneousgroups. For eachdetectedobjecttothegroundtruthinstancewithlargest\n instancethenouns“okra”,“apple”,“celery”and“avocado” Intersection Over Union(IOU),providedthatitislargerthan\n haveallbeengroupedunderthe“vegetable fruit”class. We 0.5. Wehenceattachtheverbandtimetocontactlabelsof\n alsogroupedverbswhichhavesimilarsemanticwhenantici- thegroundtruthboxestothematchedones. Wethentrain\n pated. Forinstance,theverbs“take”,“carry”,“lift”,“pull” themodelapplyingthefollowinglossonlytoboxeswhich\n and “remove” have all been grouped in the “take” cluster. havebeenmatchedtogroundtruthinstances:\n Notethatwhiletheseactionsmaybevisuallydifferent,they \n = +λ (39) \n v ttc \n all have similar effects on objects, which makes them in- L L L \n distinguishablewhenanticipated. Wefurtherremovedall where isthecrossentropylossforverbprediction, is\n v ttc \n L L \n annotations related to nouns appearing less than 50 times thesmooth L 1 loss[87]appliedtotimetocontactprediction,\n inthetestset(wefollowthecommonsplitdefinedforthis andwesetλ = 10 tocontrolthecontributionsofthetwo\n benchmark). Wechoosetoretainonlynounsappearingat losses. Toregulatethenumberofframesprocessedbythe\n least 50 timesinthetestsettoallowforareliableevaluation slowbranch,wesetα=8.Wetrainthemodelon 4 NVIDIA\n throughthem APmeasure. V 100 GPUswithabatchsizeof 64 for 50 epochsusinga\n Thefinalsetofdataincludes 64,798 annotatedexamples cosinelearningratepolicywithabaselearningrateof 0.001.\n in total with 87 nouns and 74 verbs. Our taxonomy is Wevalidatethemodelattheendofeachepochandconsider\n adapted from the one presented in Figure 39. Figure 58 theweightswhichachievedthebestoveralltop-5 m AP on\n and Figure 59 report the distributions of verb and noun thevalidation. \n annotations in the selected data. Among the 64,798 \n annotations, 27,801 are in the training set, 17,217 are in Long-Term Action Anticipation\n thevalidationset,and 19,780 areinthetestset. \n Baseline Description Thegoalofthebaselinemodelisto\n takeasinputatrimmedvideoofarbitrarylength,andpre-\n Baseline Description Figure 57 illustratestheproposed \n dict N differentplausiblesequencesoffutureactions. The\n baselineforshort-termobjectinteractionanticipation. The \n baselinemodelsthusconsistofthreecomponents: (1)the\n baselineincludestwomaincomponents. AFaster R-CNN \n encoderbackboneforobtainingcliplevelfeatures,(2)the\n objectdetector[87]isusedtodetectnextactiveobjectsin \n aggregation module for combining the obtained features\n thelastframeoftheinputvideoclipprocessedatfullreso- \n from different clips, and (3) the decoder network for de-\n lution. ASlow Fast 3 DCNN[71]ishenceusedtopredict \n coding the plausible sequences of future actions. For en-\n averblabelandatimetoactionforeachpredictedobject. \n coderbackbones,weconsiderstateoftheartvideorecog-\n This is done by obtained a fixed-length representation of \n nition networks from both convolutional model, namely,\n each object through ROI pooling [87]. Two linear layers \n Slow Fast [71] and the newly proposed video transformer\n are hence used to predict a probability distribution over \n models,namely,MVi T[63]. Foraggregationmodule,we\n verbsandapositivequantityfortimetocontactprediction \n experiment with simple concatenation operators that con-\n respectively. Verb probability distributions are obtained \n catenatestheobtainedclipfeaturesfrommultipleinputclips\n using a softmax layer, whereas a softplus activation is \n aswellastransformerbasedself-attentionmodules. Forthe\n used for time to contact prediction to make sure that the \n decodernetworksweconsiderthefollowingoptions:\n prediction is a positive number. The final output of the \n modelisobtainedbyattachingthepredictedverbandtime • No Change: Asimplerecognitionbaselinethatassumes\n to contact to each detected next active object. The noun nofuturechangeinthecurrentactionandsimplypredicts\n labelandconfidencescoresarecopiedfromtheoutputof thecurrentlyobservedactionasaduplicatedstaticfuture\n the Faster R-CNNcomponent. sequencefor Z steps. \n 19 https://github.com/facebookresearch/detectron 2\n Training Details Wefirsttrainthe Faster R-CNNcomponent 20 https://github.com/facebookresearch/Slow Fast\n 75 "
  },
  {
    "page_num": 76,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n ekat tup dloh naelc evom tuc tsujda nepo hcuot esolc nrut ward_tniap hcatta hsup_sserp pid ylppa etarepo tih dnas edivid ffo_nrut llor egnarra no_nrut poocs nethgit nori elffuhs worht riaper kcap tresni htooms eparcs kram hcated daenk llird ruop leep dlof erusaem nesool wes dlom tcepsni wercs eit dlew ezeeuqs wercsnu etirw ekahs elif kcits gnah gid evres tcepsni_hcraes llif evig evird etarg yarps yalp pmup emusnoc llorcs kcik retaw tnalp bmilc kcol gniws\n 25000 \n 20000 \n 15000 \n 10000 \n 5000 \n 0 \n Figure 58.Verbdistributioninthe Short-Term Object-Interaction Anticipationdata.\n \n \n \n \n \n \n doow tiurf_elbategev tnemudni hguod tnalp reniatnoc tecuaf repap gab hsurb wercs eriw dil leehw doof koob tnemec dlom elttob etalp latem kcirb enohp lwob hcnerw revirdwercs efink puc egnops rood gnirts nikpan dor noops reward yart tekcub top guj nap tniap elbat llird roolf rac elkcis egdirf tun tenibac llaw nep lewot telbat rewolf tam remmah eohs erusaem_epat ksam enigne llab rettuc rewom eveis elcycib sdrac_gniyalp krowtra ssalg dnab_rebbur srossics eldeen epor enots ladep hsart llebbmud eulg moorb rekooc was pom retupmoc pmup repilac elit flehs alutaps\n 5000 \n 4000 \n 3000 \n 2000 \n 1000 \n 0 \n Figure 59.Noundistributioninthe Short-Term Object-Interaction Anticipationdata.\n • Multi Head: This model trains Z independent heads in \n parallel,oneforeachfuturetimestep. Thefinalsequence \n issimplytheconjoinedpredictedactionsofeachhead. \n Finally, to generate N plausible future sequences for \n constructing multimodal baselines, we simply sample the \n predictedfutureactiondistribution N times. Theframework \n for a particular instantiation of the Multi Head baseline is \n illustratedin Figure 60. \n Figure 60.Long-Term Action Anticipationbaseline.Abaseline\n Training Details Foreachvideo,wesamplemultipleinput \n modelwitha Slow Fastbackbone,and Z =3 isshownhere.Blue\n clipstoprocesswithourbackbonenetwork. Asingleclip box:clipencodernetwork.Yellowbox:multipleclassifierheads,\n length for both the backbones, Slow Fast and MVi T, com- oneforeachfutureaction.See Sec.J.5 formoredetails.\n prises of 16 frames sampled 4 frames apart. Each clip is \n processedindependentlybythesameencoderweightsand \n combinedwiththeaggregationmodule. Theaggregatedfea- \n tureisdecodedwiththedecodermodulewheretheoutput (cid:88) Z \n = ((pn,pv),(n ,v )) (40) \n behaviorchangesduringtrainingandtesting. Intraining,the L lta L v z z z z \n decoderpredictsthenextactionprobabilitydistributionsfor z=1 \n each future step. We calculate the sum of losses for each where is cross entropy loss, p∗ refers to the predicted\n L v z \n predictionasourtotalloss: probabilitydistributionoververbsandnouns,and(n ,v )\n z z \n 76 "
  },
  {
    "page_num": 77,
    "text": " \n \n \n \n \n \n Set Metric Mean Median Left Hand Right Hand \n Set Method \n Val 5-MTE 5.11 m 2.53 m M.Disp.↓ C.Disp.↓ M.Disp.↓ C.Disp.↓\n Val 3-MTE 6.19 m 2.99 m Val I 3 D+Reg 54.11 57.29 54.73 57.94 \n Val 1-MTE 8.81 m 4.63 m Test I 3 D+Reg 52.98 56.37 53.68 56.17 \n Test 5-MTE 4.84 m 2.69 m \n Test 3-MTE 5.54 m 3.24 m Table 35.Resultsoffuturehandmovementpredictiontask.Note\n Test 1-MTE 7.66 m 4.73 m thattheleftandrighthandsmovementsareevaluatedseparately.↓\n indicateslowerisbetter \n Table 33. Resultsofthelocomotionpredictiontask. Wereport \n mean/medianfor 7-15 secondpredictions.Weuse K =1,3,5. \n Set Method Noun Noun+Verb Noun+TTC Overall\n Set (cid:15)=1 m (cid:15)=2 m (cid:15)=3 m (cid:15)=4 m (cid:15)=5 m (cid:15)=6 m Val FRCNN+Rnd. 17.55 1.56 3.21 0.34\n Val 0.14 0.29 0.39 0.46 0.51 0.54 Val FRCNN+SF 17.55 5.19 5.37 2.07 \n Test 0.16 0.31 0.40 0.47 0.53 0.58 Test FRCNN+Rnd. 20.45 2.22 3.86 0.44 \n Test FRCNN+SF 20.45 6.78 6.17 2.45 \n Table 34. Resultsofthelocomotionpredictiontask. Wereport \n the probability of correct trajectory (PCT) as varying the error Table 36.Resultsoftheshort-termobjectinteractionanticipation\n threshold(cid:15). task.Seetextfordiscussion. \n refertothegroundtruthfutureactionlabels. \n placementerror(C.Disp.) onbothvalidationandtestsets\n During testing, we sample action class labels (nˆ ,vˆ ) \n z z in Table 35. Our baseline model achieves M.Disp. of\n fromthepredicteddistributionindependentlyforeachfuture \n (52.98/53.68) and C.Disp. of (56.37/56.17) for left/right\n step. We repeat this sampling procedure N times to gen- \n handpositionpredictiononthetestset. Itisworthnoting\n erate multiple cancidate sets of predictions for evaluation \n thatpredictinghandpositionsoncontactframeismorechal-\n describedin Section J.4. \n lengingthanonotherkeyframes. Thisisbecause,bythe\n We use the taxonomy presented in Figure 39 for our \n definitionofcontactframeandpre-conditionframe,thean-\n experiments. Wefinetunea Kinetics-400[109]pretrained \n ticipationtemporalfootprintofcontactframeislargerthan\n encoder backbones on Ego 4 D action recognition and use \n otherkeyframes. Wefurtherprovidequalitativeresultsof\n thismodelforallbaselinestoextractthecliplevelfeatures. \n our baseline method in Fig. 61. Notably, the model can\n Theaggregationmoduleanddecodernetworksaretrained \n makereasonablepredictionsonfuturehandpositions. How-\n fromrandominitializationdirectlyontheforecastingtask. \n ever,themodelismorelikelytofailwhenthereisdrastic\n Theencoderweightsarekeptunchangedduringthedecoder \n embodiedmotions. \n network training. We set Z = 20 for long horizon future \n evaluation and K = 5 as the number of plausible future \n sequences predicted by the model. For all baselines, we \n sample 2 inputclipstocapturepastcontextunlessotherwise \n Short-Term Object Interaction Anticipation \n specified. We train the model on 8 NVIDIA V 100 GPUs \n withabatchsizeof 64 for 30 epochsandabaselearningrate \n Table 36 reportstheresultsfortheshort-termobjectinterac-\n of 0.0001. \n tionanticipationtaskonboththevalidationandtestsets. We\n comparetheproposedbaselinebasedon Faster RCNNand\n J.6 Results Slow Fast(“FRCNN+SF”inthetable)withasimplerbase-\n linewhichuses Faster RCNNtodetectobjectandpredict\n Future Locomotion Movements Prediction \n theirclasses,butdrawsverband TTCpredictionsrandomly\n Weevaluatethe KNNbasedbaselinealgorithmbymeasuring from the training set distribution (“FRCNN+Rnd.” in the\n meantrajectoryerror(K-MTE)andprobabilityofcorrect table). Resultsarereportedin Top-5 m AP%accordingtothe\n trajectory (PCT) given an error tolerance. The trajectory differentmatchingcriteriadiscussedin Appendix J.4.Ascan\n length ranges from 7 to 15 seconds (70-150 points in a benoted,theproposedbaselineoutperformsrandompredic-\n trajectorygiven 10 FPS).Ourbaselineachievesmeanerror tionbybigmarginswhenverbsand TTCsarepredictedon\n 8.81 mfor 1 MTEand 0.39 for PCT . Theresultis boththevalidationandtestsets. Thissuggeststhat,despite\n (cid:15)=3 m \n − \n summarizedin Table 33 and 34. beingsimple,thebaselinecanleveragetheobservedvideo\n to anticipate future object interactions. Figure 62 reports\n some qualitative examples of the baseline. The model is\n Future Hands Movements Prediction \n sometimesabletodetectthenextactiveobjectsandpredict\n For future hands movements prediction task, we report suitableverbsand TTCs,butperformancetendstobelimited\n meandisplacementerror(M.Disp.) andcontactframedis- especiallyincomplexscenarios.\n 77 "
  },
  {
    "page_num": 78,
    "text": " \n \n \n \n \n \n 1.5 sec before PRE 1.0 sec before PRE 0.5 sec before PRE PRE Frame Contact Frame\n \n \n \n \n \n \n \n \n \n \n \n : Ground Truth : Prediction \n \n Figure 61.Qualitativeexamplesoffuturehandsmovementspredictionusingtheproposedbaseline.Thegroundtruthhandspositionsare\n plottedasgreencrosses,whilethepredictedhandspositionsareplottedasredcrosses. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 62.Qualitativeexamplesofshort-termobjectinteractionanticipationusingtheproposedbaseline.Thenumbersinbracketsrepresent\n theconfidencescoresassociatedtothepredictions. Thegroundtruthnext-activeobjectishighlightedusingadashedredline,whereas\n modelpredictionsarereportedinbluesolidlines. \n \n Long-Term Action Anticipation architecturefrom Slow Fastto MVi Tgreatlyimprovesverb\n forecasting prediction performance, but deteriorates noun\n forecastingperformance,highlightingthetrade-offbetween\n Table 37 showsourresultsonboththevalidationandtestsets. \n thetwodespitesimilaractionclassificationperformanceon\n The No Changebaselinesimplypredictsthecurrentactionas \n Kinetics. Finally,includinglargervideocontextinformation\n thenext Z actions,andperformspoorlyatpredictingfuture \n intheformofmultipleinputclipsbyusingthetransformer\n actions. Explicitlytrainingmultipleheadsimprovesperfor- \n basedaggregatormoduleresultsinthebestperformance.\n manceonverbs,nounsandactions. Changingthebackbone \n 78 "
  },
  {
    "page_num": 79,
    "text": " \n \n \n \n \n \n Valset ED@(Z=20) Future Locomotion Movements Prediction \n Backbone Aggregator Decoder Verb Noun Action \n Slow Fast Concat No Change 0.766 0.830 0.960 Thebaselinequantitativeresultsonthelocomotionpredic-\n Slow Fast Concat Multi Head 0.747 0.808 0.952 \n tiontaskimplythatthevisualcues,e.g.,sidewalk,obstacles,\n MVi T Concat Multi Head 0.707 0.901 0.972 \n Slow Fast Transformer Multi Head 0.745 0.779 0.941 androad,inegocentricimagesarehighlyindicativeoffu-\n turemovement. However,thebaselinemethodthatencodes\n test set ED@(Z=20) the visual semantics of an image with a global feature is\n Backbone Aggregator Decoder Verb Noun Action \n notdetailedenoughtomodelcomplexwalkingmovement,\n Slow Fast Concat No Change 0.761 0.810 0.959 \n e.g., avoiding pedestrians. This opens an opportunity for\n Slow Fast Concat Multi Head 0.743 0.791 0.948 \n MVi T Concat Multi Head 0.697 0.904 0.969 challengeparticipantstoincorporateafine-grainedvisual\n Slow Fast Transformer Multi Head 0.739 0.780 0.943 representation. \n Table 37.Resultsofthelong-termactionanticipationtask.Lower \n isbetter.Seetextfordiscussion. Future Hands Movements Prediction \n Ourbaselinemodelforfuturehandsmovementsprediction\n suffersfromthedrasticheadmovementsinegocentricvideo\n Figure 63 showssomequalitativeresultsofourmethod. andthestochasticnatureoffutureforecasting. Wespeculate\n Ineachrow,thegroundtruthfutureactionsareshownalong thatexplicitlymodelingtheheadmovementsandnext-active\n withthepredictionsfromourmodel(for 5 time-steps). Cor- objectsmaycomplementthevideorepresentationsforpre-\n rectpredictionsarehighlightedingreen,whilevalidactions dictingfuturehandsmovements.\n that areincorrectly ordered(or paritallycorrect) arehigh- \n lightedinblue. Notethatthoughnotperfectlyaligned,in- \n Short-Term Object Interaction Anticipation \n correctlyorderedsequencesaregivenpartialcreditviathe \n edit-distancemetric. Theshort-termobjectinteractionanticipationresultshigh-\n lightthattheproposedtaskischallenging,withthebaseline\n achievinganoverall Top-5 m AP of 2.07%onthevalidation\n setand 2.45%onthetestset. Thekeychallengesarelikely\n duetotheuncertainnatureoffuturepredictionsaswellas\n J.7 Discussion \n totheinabilityoftheobjectdetectortocorrectlydetectnext\n activeobjectsandignoretheothers. Nevertheless,thepro-\n Data Annotation \n posed baseline, even if simple, allows to greatly improve\n overacombinationofanobjectdetectorandarandompre-\n Annotatingthevideosforforecastingtasksposedanumber dictionofverbsandtimetocontactquantities. Thissuggests\n ofinterestingchallenges. First,wefoundthediversityofthe thatmethodscanlearntoanalyzetheinputvideoinorderto\n dataledtoalargeanddiversetaxonomy,whichsomeanno- makereasonablepredictionsaboutthefuture.\n tatorsfoundhardtonavigate. Hence,wefoundanumberof \n annotatorsusedthe”OTHER”option,whichweeventually \n Long-Term Action Anticipation \n manuallymappedtothetaxonomywherepossible. Infuture \n annotations, we plan to ask annotators to always pick the Wediscussseveralimportantaspectsofthelong-termaction\n closesttaxonomyitemevenifwritinginafree-form OTHER forecastingproblemthroughourexperimentsandablation\n label,toencouragethemtosticktothetaxonomyasmuch studies. All ablations are run with Slow Fast backbone\n aspossible. Second,wenoticedannotatorsstruggledwith networks,andmodelsaretrainedfor 30 epochs.\n definingboundingboxesover“stuff”categories. Forexam- \n ple,whenlabeling“cuttinggrass”,itwasoftenchallenging Howimportantis Ego 4 Dactionrecognitionpre-training?\n to draw a box that covers the full extent of the object of Table 38 shows the performance of our models when\n change(i.e.“grass”). Finally,itwassometimeschallenging pretrained only on Kinetics-400 action recognition (as\n todefinewhattheobjectofchangewas,whenusinglarge opposed to further fine-tuning on Ego 4 D action recogni-\n tools. For example, if using a lawn mower to clear grass, tion). All models benefit greatly from training on Ego 4 D\n doesoneconsiderthemowerasthetoolandhencethegrass data in two ways. First, there is a large domain gap\n astheobjectofchange,ortheleversandbuttonsinsidethe between Kinetics and Ego 4 D both in terms of visuals\n mower as the object of change. We chose to rely on the (third-person vs. egocentric viewpoint) and the diver-\n narratorstodefinewhichinteractiontolabel(i.e.pushing sity of activities they contain, which pre-training helps\n thelever/buttonvscuttinggrass),andaskedtheannotators account for. Second, action recognition models benefit\n tolabeltoolsandobjectsaccordingly. frombiasesinthelabelstructureoffutureactionsasseen\n 79 "
  },
  {
    "page_num": 80,
    "text": " \n \n \n \n \n \n ED@5: 0.60 \n take → hold → cut → put → take \n GT: \n sickle spinach spinach sickle rubber band \n take → cut → hold → cut → throw \n PRED: \n sickle spinach spinach spinach sickle \n ED@5: 0.80 smooth → remove → smooth → sand → remove \n GT: \n wood sander wood wood sander \n hold → sand → hold → sand → sand \n PRED: \n sander wood sander wood wood \n Figure 63.Longtermactionanticipation-qualitativeresults.Actionsingreenrepresentcorrectpredictions(correctaction,atthecorrect\n position).Actionsinbluerepresentincorrectorderingofvalidactions.Ouredit-distancemetricaccountsforbothcases.\n Val Set ED@(Z=20) \n Init Backbone Aggregator Verb Noun Action 0.800 \n K 400 Slow Fast Concat 0.752 0.820 0.958 0.775 \n +Ego 4 D Slow Fast Concat 0.747 0.808 0.952 \n 0.750 \n K 400 Slow Fast Transformer 0.746 0.809 0.953 \n +Ego 4 D Slow Fast Transformer 0.745 0.779 0.941 0.725 \n 0.700 \n Table 38.Longtermanticipation-varyingpretrainingdata.Mul- \n 0.675 \n ti Head decoder used for all models. Ego 4 D action recognition \n 0.650 pretraininggreatlyimprovesdownstreamforecastingperformance.\n 0.625 \n Val Set ED@(Z=20) 1 2 3 4 5 6 7 8 91011121314151617181920\n Z \n #clips Backbone Aggregator Verb Noun Action \n 2 Slow Fast Transformer 0.743 0.790 0.946 \n 4 Slow Fast Transformer 0.744 0.796 0.947 \n 8 Slow Fast Transformer 0.745 0.779 0.941 \n Table 39.Longtermanticipation-varyingnumberofinputclips. \n Multi Headdecoderusedforallmodels. Performanceincreases \n withmoreinputcontext. \n fromtheperformanceofthe No Changebaselinein Table 37. \n How important is past context for transformer based \n models? Our transformer aggregation modules aggregate \n informationacrossalargertemporalhistorycontrolledby \n thenumberofinputclipstothemodel. Table 39 showsthe \n sensitivity of these models to the amount of past context \n videothatithasaccessto. Overall,performanceincreasesas \n morecontextinformationisprovidedtothemodel,however \n thisincreasecomesatthecostofmemoryconsumption— \n 8 is the maximum number of clips that can be fit in GPU \n memory. \n Howfarintothefuturecanmodelspredict? Asmentioned \n in Section J.4 wereportresultsforpredictionsat Z = 20 \n as baselines begin to predict actions at random for higher \n values of Z. Figure 64 shows the plot of edit distance vs. \n Z@DE \n verbs \n nouns \n Figure 64.Performancevs.numberoffutureactions Z.Predicting\n furtherintothefutureisnaturallymoredifficult.Modelsbeginto\n predictclosetorandomactionsforveryhighvaluesof Z.\n Z forourbaselinemodels. Asexpected,itisfareasierto\n anticipateactionsthatoccurimmediatelynext,whichgets\n moredifficultas Z increases,andsteadilyplateaus.\n Howtogeneratemultiplecandidatepredictions? Asmen-\n tioned in Section J.4 we evaluate the best of K = 5 pre-\n dictions to arrive at our final results. To generate the K\n predictions,wesampleeachclassifierheadindependently,\n howeverthereareseveralmethodstoimprovethisinclud-\n ingheuristicsearchalgorithms(likebeamsearch). Ideally,\n the multi-modal nature of future prediction should be ac-\n countedforinthemodeldesignitself. Moreover,decoder\n modelsthattakeintoaccountthesequentialnatureduring\n inferenceshouldbeconsidered. Theseincludetransformer\n baseddecodersthatarepopularinrecentlanguagemodels\n (e.g.,BERT,GPT)Thisisanimportantfuturedirectionof\n research. \n 80 "
  },
  {
    "page_num": 81,
    "text": " \n \n \n \n \n \n J.8 Contributionsstatement \n Giovanni Maria Farinellaledthe Forecasting Benchmark \n workingonthedefinitionoftheproposedtasks,onthecol- \n lection,andwritingthepaper. \n Rohit Girdharco-ledthe Forecasting Benchmarkworking \n onthedefinitionoftheproposedtasks,onthecollection,and \n writingthepaper. \n Antonino Furnari contributed to the definition of the pro- \n posedbenchmarktasksandinparticulartothe Short-Term \n Object Interaction Anticipation task and has been key \n driver of implementation, collection, annotation develop- \n mentthroughouttheproject,andwritingthepaper. \n Ilija Radosavovicworkedonthedefinitionoftasksandhas \n been key driver of implementation, collection, annotation \n developmentthroughouttheproject,andwritingthepaper. \n Tushar Nagarajancontributedtothedefinitionofthepro- \n posedbenchmarktasksandinparticulartothe Long-Term \n Action Anticipationtaskandhasbeenkeydriverofimple- \n mentation,collection,annotationdevelopmentthroughout \n theproject,andwritingthepaper. \n Tullie Murrell worked on baseline implementation of the \n Long-Term Action Anticipationtask. \n Karttikeya Mangalamworkedonbaselineimplementation, \n experimentsandwritingthe Long-Term Action Anticipation \n task. \n Christoph Feichtenhofer oversaw the development of the \n task,baselinesandimplementationofthe Long-Term Action \n Anticipationtask. \n Miao Liuworkedonthedefinitionof Future Hands Move- \n ment Predictiontaskandhasbeenkeydriverofimplemen- \n tation, collection, annotation development throughout the \n project,andwritingthepaper. \n Wenqi Jiaworkedonbaselineimplementationofthe Future \n Hands Movement Predictiontask. \n Zachary Chavisworkedonthe Locomotion Forecastingtask \n andhasbeenkeydriverofimplementation,collection,and \n annotationdevelopmentthroughouttheproject. \n Hyun Soo Park worked on the definition of Locomotion \n Forecasting tasks, collection, annotation, and writing the \n paper. \n \n \n \n \n \n \n \n \n \n \n 81 \n \n \n "
  },
  {
    "page_num": 82,
    "text": " \n \n \n \n \n \n K.Societal Impact \n Ourcontributioncanpositivelyimpactvideounderstand- \n ing. Itofferstheresearchcommunityalarge-scaleresource \n capturedwithrigorousprivacyandethicsstandards(detailed \n in Appendix Aand B)togetherwithadiversityofsubjects, \n andthebenchmarkswillpromotereproducibletechnicalad- \n vances. Morebroadly,egocentricperceptionhasthepoten- \n tialtopositivelyimpactsocietyinmanyapplicationdomains, \n includingassistivetechnology,education,fitness,entertain- \n mentandgaming,eldercare,robotics,andaugmentedreality. \n Nonetheless, future research in this area must guard \n againstthepotentialnegativesocietalimpactiftechnology \n foregocentricvisionweremisused. \n First, thereareriskssurroundingprivacy. Aswebegin \n toseeaproliferationofwearablecamerasinpublicspaces, \n producersofthesewearabledeviceswillneedtodevelopand \n implement protocols for notice and consent regarding the \n collectionofdatainpublicspaces,aswellasusercontrols \n for how such data may be used, stored, and shared with \n any third parties. Similarly, models that may be used to \n transcribespeechorperformothertasksrelatedtofootage \n should include robust user controls such as the ability to \n removeorobscurepersonaldataorsensitivecontent. \n Notethatforallouraudio-visualandsocialbenchmarking \n work, thedatausedhasfullconsentfromtheparticipants \n inthevideo,i.e.,tousetheirunblurredfacesandaudioof \n their conversation. To date, the research community has \n lacked any large-scale data resource with which to study \n thesekindsofproblems;Ego 4 Dwillhelpthecommunityto \n considernewsolutionswhileleveragingreal-world,diverse \n datathatrespectstheprivacyprotocolsofdifferentcountries. \n Furthermore,the Ego 4 Ddataisavailableonlyforuserswho \n signalicensethatenumeratestheallowableusesofthedata, \n whichisintendedtohinderpotentialnegativeapplications. \n Second,thereisariskthatourlarge-scalecollectioncould \n inspirefuturecollectioneffortswithoutthesamelevelofcare \n orattentiontotheprivacyandethicalconcernsasweretaken \n in Ego 4 D.Tomitigatethisrisk,wehaveaimedtobecompre- \n hensiveinourdescriptionsofallpartsofourprocedures,and \n wewillincludeourbestpracticesrecommendationswhen \n publiclydisseminatingtheresultsoftheproject. \n Finally,despiteourbesteffortsasdiscussedinthemain \n paper,therearestillsomeimbalancesinthedataset. Forex- \n ample,thedatafrom Rwandaisrelativelysmall,andthough \n 74 citiesrepresentsaleapincoverage,theydonotcapture \n allpossibledemographics. Weacknowledgethatnomatter \n howfaronegoes,fullglobalcoverageofdailylifeactivity \n iselusive. Still,wecanmitigatethisriskbycontinuingto \n growglobalcollaborationswithresearchersandparticipants \n inunderrepresentedareas. \n 82 \n \n \n "
  },
  {
    "page_num": 83,
    "text": " \n \n \n \n \n \n References \n [16] Sven Bambach,Stefan Lee,David J.Crandall,and Chen Yu.\n Lendingahand:Detectinghandsandrecognizingactivities\n [1] Github repository of the ESPNet model zoo. https: \n incomplexegocentricinteractions. In The IEEEInterna-\n //github.com/espnet/espnet_model_zoo. We \n tional Conferenceon Computer Vision(ICCV),December\n used the Shinji Watanabe/gigaspeech_asr_ \n 2015. 3 \n train_asr_raw_en_bpe 5000_valid.acc.ave \n [17] Mark ABeeand Christophe Micheyl. Thecocktailparty\n model. 60,61 \n problem:whatisit?howcanitbesolved?andwhyshould\n [2] Kaldi English GLM file. https://github.com/ \n animalbehavioristsstudyit? Journalofcomparativepsy-\n kaldi-asr/kaldi/blob/master/egs/ami/s 5/ \n chology,122(3):235,2008. 52 \n local/english.glm. 61 \n [18] Keni Bernardin,Alexander Elbs,and Rainer Stiefelhagen.\n [3] NISTSRE 2000 Evaluation Plan. https://www.nist. \n Multipleobjecttrackingperformancemetricsandevaluation\n gov/sites/default/files/documents/2017/ \n inasmartroomenvironment. In Sixth IEEEInternational\n 09/26/spk-2000-plan-v 1.0.htm_.pdf. 56 \n Workshopon Visual Surveillance,inconjunctionwith ECCV,\n [4] Yazan Abu Farha, Alexander Richard, and Juergen Gall. volume 90.Citeseer,2006. 8,53\n Whenwillyoudowhat?-anticipatingtemporaloccurrences \n [19] Keni Bernardinand Rainer Stiefelhagen. Evaluatingmul-\n ofactivities. In Computer Visionand Pattern Recognition, \n tiple object tracking performance: the clear mot metrics.\n pages 5343–5352,2018. 3 \n EURASIPJournalon Imageand Video Processing,2008:1–\n [5] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, 10,2008. 8,53 \n Oriol Vinyals,and Andrew Zisserman. Deepaudio-visual \n [20] Gedas Bertasius,Hyun Soo Park,Stella X.Yu,and Jianbo\n speechrecognition. IEEEtransactionsonpatternanalysis \n Shi. First-personaction-objectdetectionwithegonet. In\n andmachineintelligence,2018. 8,51 \n Proceedingsof Robotics:Scienceand Systems,July 2017. 9\n [6] Triantafyllos Afouras,Joon Son Chung,and Andrew Zisser- \n [21] Cigdem Beyan,Francesca Capozzi,Cristina Becchio,and\n man. Theconversation:Deepaudio-visualspeechenhance- \n Vittorio Murino. Predictionoftheleadershipstyleofan\n ment. In Interspeech,2018. 51 \n emergentleaderusingaudioandvisualnonverbalfeatures.\n [7] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, IEEETransactionson Multimedia,20(2):441–456,2018. 9\n and Andrew Zisserman. Self-supervised Learningof Audio- \n [22] Goutam Bhat,Martin Danelljan,Luc Van Gool,and Radu\n Visual Objectsfrom Video. In Proceedingsofthe European \n Timofte. Know Your Surroundings:Exploiting Scene Infor-\n Conferenceon Computer Vision(ECCV 20),volume 12363 \n mationfor Object Tracking. ar Xiv:2003.11014[cs],May\n LNCS,pages 208–224,2020. 9 \n 2020. 35,36 \n [8] Jean-Baptiste Alayrac,Josef Sivic,Ivan Laptev,and Simon \n [23] Eric Brachmann, Alexander Krull, Frank Michel, Stefan\n Lacoste-Julien. Jointdiscoveryofobjectstatesandmanipu- \n Gumhold, Jamie Shotton, and Carsten Rother. Learning\n lationactions. ICCV,2017. 7,44,45 \n 6 dobjectposeestimationusing 3 dobjectcoordinates. In\n [9] Humam Alwassel,Fabian Caba Heilbron,Victor Escorcia, Europeanconferenceoncomputervision,pages 536–551.\n and Bernard Ghanem. Diagnosingerrorintemporalaction Springer,2014. 7 \n detectors. In Proceedingsofthe European Conferenceon \n [24] Tom B.Brown,Benjamin Mann,Nick Ryder,Melanie Sub-\n Computer Vision(ECCV),2018. 41,42 \n biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\n [10] Xavier Anguera,Simon Bozonnet,Nicholas Evans,Corinne tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\n Fredouille,Gerald Friedland,and Oriol Vinyals. Speaker hini Agarwal,Ariel Herbert-Voss,Gretchen Krueger,Tom\n diarization:Areviewofrecentresearch. IEEETransactions Henighan,Rewon Child,Aditya Ramesh,Daniel M.Ziegler,\n onaudio,speech,andlanguageprocessing,20(2):356–370, Jeffrey Wu,Clemens Winter,Christopher Hesse,Mark Chen,\n 2012. 8,53,56 Eric Sigler,Mateusz Litwin,Scott Gray,Benjamin Chess,\n [11] Xavier Anguera Miro´. Robustspeakerdiarizationformeet- Jack Clark,Christopher Berner,Sam Mc Candlish,Alec Rad-\n ings. Universitat Polite`cnicade Catalunya,2006. 8,54 ford,Ilya Sutskever,and Dario Amodei. Languagemodels\n [12] Stanislaw Antol,Aishwarya Agrawal,Jiasen Lu,Margaret arefew-shotlearners,2020. 9\n Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi [25] Ian MBullock, Thomas Feix, and Aaron MDollar. The\n Parikh. VQA:Visual Question Answering. In International yalehumangraspingdataset:Grasp,object,andtaskdatain\n Conferenceon Computer Vision(ICCV),2015. 7 householdandmachineshopenvironments. IJRR,2015. 45\n [13] Mehmet Ali Arabacı,Fatih O¨zkan,Elif Surer,Peter Jancˇovicˇ, [26] Minjie Cai,Kris MKitani,and Yoichi Sato. Understand-\n and Alptekin Temizel. Multi-modal egocentric activity inghand-objectmanipulationwithgrasptypesandobject\n recognition using audio-visual features. ar Xiv preprint attributes. In RSS,2016. 3\n ar Xiv:1807.00612,2018. 51 [27] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas\n [14] Relja Arandjelovic´ and Andrew Zisserman. Objectsthat Usunier,Alexander Kirillov,and Sergey Zagoruyko. End-\n sound. In ECCV,2018. 8,51 to-end object detection with transformers. In European\n [15] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Conferenceon Computer Vision,pages 213–229.Springer,\n and Michael Auli. wav 2 vec 2.0: A framework for self- 2020. 49 \n supervisedlearningofspeechrepresentations.ar Xivpreprint [28] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike\n ar Xiv:2006.11477,2020. 60 Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec,\n 83 "
  },
  {
    "page_num": 84,
    "text": " \n \n \n \n \n \n Vasilis Karaiskos,Wessel Kraaij,Melissa Kronenthal,etal. [41] Kenneth Churchand Patrick Hanks.Wordassociationnorms,\n The AMI meeting corpus: A pre-announcement. In In- mutualinformation,andlexicography. Computationallin-\n ternationalworkshoponmachinelearningformultimodal guistics,16(1):22–29,1990. 71\n interaction,pages 28–39.Springer,2006. 56 [42] Dima Damen, Hazel Doughty, Giovanni Farinella, Sanja\n [29] Joao Carreiraand Andrew Zisserman. Quovadis, action Fidler,Antonino Furnari,Evangelos Kazakos,Davide Molti-\n recognition?anewmodelandthekineticsdataset. Inpro- santi,Jonathan Munro,Toby Perrett,Will Price,etal. The\n ceedingsofthe IEEEConferenceon Computer Visionand epic-kitchensdataset:Collection,challengesandbaselines.\n Pattern Recognition,pages 6299–6308,2017. 48,49 IEEETransactionson Pattern Analysis&Machine Intelli-\n gence,(01):1–1,2020. 52 \n [30] Chien-Yi Chang,De-An Huang,Danfei Xu,Ehsan Adeli, \n Li Fei-Fei,and Juan Carlos Niebles. Procedureplanning [43] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\n ininstructionalvideos. ar Xivpreprintar Xiv:1907.01172, ,Antonino Furnari,Jian Ma,Evangelos Kazakos,Davide\n 2019. 45 Moltisanti,Jonathan Munro,Toby Perrett,Will Price,and\n Michael Wray. Rescalingegocentricvision. IJCV,2021. 2,\n [31] Sourish Chaudhuri,Joseph Roth,Daniel PWEllis,Andrew \n 3,45 \n Gallagher,Liat Kaver,Radhika Marvin,Caroline Pantofaru, \n Nathan Reale,Loretta Guarino Reid,Kevin Wilson,etal. [44] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\n Ava-speech:Adenselylabeleddatasetofspeechactivityin Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-\n movies. ar Xivpreprintar Xiv:1808.00606,2018. 8,52 vide Moltisanti,Jonathan Munro,Toby Perrett,Will Price,\n and Michael Wray. Scaling egocentric vision: The epic-\n [32] C. Chen, U. Jain, C. Schissler, S. V. Amengual Gari, \n kitchens dataset. In European Conference on Computer\n Z. Al-Halah, V. Ithapu, P. Robinson, and K. Grauman. \n Vision(ECCV),2018. 2,3,5,20,52 \n Soundspaces:Audio-visualnavigationin 3 denvironments. \n [45] Dima Damen,Teesid Leelasawassuk,Osian Haines,Andrew\n In ECCV,2020. 62 \n Calway,and Walterio Mayol-Cuevas.You-Do,I-Learn:Dis-\n [33] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vi- \n coveringtaskrelevantobjectsandtheirmodesofinteraction\n cenc Amengual Gari,Ziad Al-Halah,Vamsi Krishna Ithapu, \n frommulti-useregocentricvideo. In BMVC,2014. 44,45\n Philip Robinson,and Kristen Grauman. Audio-visualem- \n [46] Dima Damen,Teesid Leelasawassuk,and Walterio Mayol-\n bodiednavigation. environment,97:103,2019. 8 \n Cuevas. You-do,i-learn:Egocentricunsuperviseddiscovery\n [34] Guoguo Chen,Shuzhou Chai,Guanbo Wang,Jiayu Du,Wei- \n of objects and their modes of interaction towards video-\n Qiang Zhang,Chao Weng,Dan Su,Daniel Povey,Jan Trmal, \n basedguidance. CVIU,2016. 3 \n Junbo Zhang,etal. Gigaspeech:Anevolving,multi-domain \n [47] Fred JDamerau. Atechniqueforcomputerdetectionand\n asrcorpuswith 10,000 hoursoftranscribedaudio. ar Xiv \n correctionofspellingerrors. Communicationsofthe ACM,\n preprintar Xiv:2106.06909,2021. 60 \n 1964. 73 \n [35] Xinlei Chen,Hao Fang,Tsung-Yi Lin,Ramakrishna Vedan- \n [48] Ana Garcia Del Molino,Cheston Tan,Joo-Hwee Lim,and\n tam,Saurabh Gupta,Piotr Dolla´r,and CLawrence Zitnick. \n Ah-Hwee Tan. Summarizationofegocentricvideos:Acom-\n Microsoft coco captions: Data collection and evaluation \n prehensivesurvey. IEEETransactionson Human-Machine\n server. ar Xivpreprintar Xiv:1504.00325,2015. 7 \n Systems,47(1),2016. 3 \n [36] Eunji Chong, Elysha Clark-Whitney, Audrey Souther- \n [49] Jia Deng,Wei Dong,Richard Socher,Li-Jia Li,Kai Li,and\n land, Elizabeth Stubbs, Chanel Miller, Eliana L Ajodan, \n Li Fei-Fei. Image Net: A large-scale hierarchical image\n Melanie RSilverman, Catherine Lord, Agata Rozga, Re- \n database. In CVPR,2009. 1,3 \n becca MJones,and James MRehg.Detectionofeyecontact \n [50] Daniel De Tone, Tomasz Malisiewicz, and Andrew Rabi-\n withdeepneuralnetworksisasaccurateashumanexperts. \n novich. Superpoint:Self-supervisedinterestpointdetection\n Nature Communications,11(1):6386,dec 2020. 9 \n anddescription. In CVPRWorkshop,2018. 38\n [37] Eunji Chong,Yongxin Wang,Nataniel Ruiz,and James M. \n [51] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina\n Rehg. Detecting Attended Visual Targetsin Video. In Pro- \n Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans-\n ceedingsofthe IEEEConferenceon Computer Visionand \n formers for language understanding. ar Xiv:1810.04805,\n Pattern Recognition(CVPR 20),pages 5395–5405,Seattle, \n 2018. 29 \n WA,2020. 9,66 \n [52] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina\n [38] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae \n Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans-\n Lee,Hee Soo Heo,Soyeon Choe,Chiheon Ham,Sunghwan \n formersforlanguageunderstanding. In Proceedingsofthe\n Jung,Bong-Jin Lee,and Icksang Han. Indefenceofmetric \n 2019 Conferenceofthe North American Chapterofthe As-\n learningforspeakerrecognition. In Interspeech,2020. 64 \n sociationfor Computational Linguistics:Human Language\n [39] Joon Son Chung,Jaesung Huh,Arsha Nagrani,Triantafyl- Technologies, Volume 1 (Long and Short Papers), pages\n los Afouras, and Andrew Zisserman. Spot the conver- 4171–4186,Minneapolis,Minnesota,June 2019.Associa-\n sation: speaker diarisation in the wild. ar Xiv preprint tionfor Computational Linguistics. 40\n ar Xiv:2007.01216,2020. 8,52 [53] Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark\n [40] J.S.Chung, A.Nagrani, and A.Zisserman. Vox Celeb 2: Broyles,Hao Jiang,Jie Shen,Maja Pantic,Vamsi Krishna\n Deep Speaker Recognition. In INTERSPEECH,2018. 52, Ithapu,and Ravish Mehra. Easycom:Anaugmentedreality\n 57 dataset to support algorithms for easy communication in\n 84 "
  },
  {
    "page_num": 85,
    "text": " \n \n \n \n \n \n noisyenvironments.ar Xivpreprintar Xiv:2107.04174,2021. [66] Alireza Fathi,Jessica K.Hodgins,and James M.Rehg. So-\n 8,52 cialinteractions:Afirst-personperspective. In CVPR,2012.\n [54] Bardia Doosti, Ching-Hui Chen, Raviteja Vemulapalli, 3 \n Xuhui Jia,Yukun Zhu,and Bradley Green. Boostingimage- [67] A.Fathi,J.K.Hodgins,and J.M.Rehg. Socialinteractions:\n basedmutualgazedetectionusingpseudo 3 dgaze.In Thirty- Afirst-personperspective. In Proceedingsofthe IEEECon-\n Fifth AAAIConferenceon Artificial Intelligence,pages 1273– ferenceon Computer Visionand Pattern Recognition(CVPR\n 1281,2021. 9 12),pages 1226–1233.IEEE,jun 2012. 9 \n [68] A. Fathi and J. Rehg. Modeling actions through state\n [55] Hazel Doughty,Ivan Laptev,Walterio Mayol-Cuevas,and \n changes. In CVPR,2013. 7 \n Dima Damen. Actionmodifiers: Learningfromadverbs \n [69] Alireza Fathiand James MRehg. Modelingactionsthrough\n ininstructionalvideos. ar Xivpreprintar Xiv:1912.06617, \n statechanges. In CVPR,2013. 44,45 \n 2019. 45 \n [70] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\n [56] Matteo Dunnhofer, Antonino Furnari, Giovanni Maria \n Kaiming He. Slowfastnetworksforvideorecognition. In\n Farinella, and Christian Micheloni. Isfirstpersonvision \n ICCV,2019. 3,5,40,41,48,49 \n challenging for object tracking? In IEEE/CVF Interna- \n [71] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\n tional Conferenceon Computer Vision Workshops(ICCVW) \n Kaiming He. Slowfastnetworksforvideorecognition. In\n -Visual Object Tracking Challenge,2021. 3 \n Proceedingsofthe IEEE/CVFinternationalconferenceon\n [57] Ariel Ephrat,Inbar Mosseri,Oran Lang,Tali Dekel,Kevin \n computervision,pages 6202–6211,2019. 40,75\n Wilson, Avinatan Hassidim, William T Freeman, and [72] Jonathan Fiscus. NIST sclite sscoring toolkit. https:\n Michael Rubinstein. Lookingtolistenatthecocktailparty: //github.com/usnistgov/SCTK. 61\n Aspeaker-independentaudio-visualmodelforspeechsepa- \n [73] Jianglin Fu,Ivan VBajic´,and Rodney GVaughan. Datasets\n ration. In SIGGRAPH,2018. 8,51 \n forfaceandobjectdetectioninfisheyeimages.Datainbrief,\n [58] Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops! 27:104752,2019. 57 \n predictingunintentionalactioninvideo. In Arxiv,2019. 44 [74] Antonino Furnari, Sebastiano Battiato, Kristen Grauman,\n [59] N.Ryantet.al.The Second DIHARDDiarization Challenge: and Giovanni Maria Farinella. Next-active-objectprediction\n Dataset,task,andbaselines. In Proceedingsof Interspeech, fromegocentricvideos. Journalof Visual Communication\n 2019. 56 and Image Representation,49:401–411,2017. 9\n [60] Mark Everingham,Luc Van Gool,Christopher KIWilliams, [75] Antonino Furnariand Giovanni Farinella. Rolling-unrolling\n John Winn,and Andrew Zisserman.Thepascalvisualobject lstmsforactionanticipationfromfirst-personvideo. IEEE\n classes(voc)challenge. Internationaljournalofcomputer Transactionson Pattern Analysisand Machine Intelligence,\n vision,88(2):303–338,2010. 1,73 2020. 3 \n [76] Antonino Furnariand Giovanni Maria Farinella.Whatwould\n [61] Bernard Ghanem Fabian Caba Heilbron,Victor Escorciaand \n you expect? anticipating egocentric actions with rolling-\n Juan Carlos Niebles.Activitynet:Alarge-scalevideobench- \n unrolling lstms and modality attention. In International\n mark for human activity understanding. In Proceedings \n Conferenceon Computer Vision,2019. 9 \n ofthe IEEEConferenceon Computer Visionand Pattern \n [77] Jiyang Gao,Zhenheng Yang,and Ram Nevatia. Red: Re-\n Recognition,pages 961–970,2015. 1,3,30,32 \n inforcedencoder-decodernetworksforactionanticipation.\n [62] Heng Fan,Haibin Ling,Liting Lin,Fan Yang,Peng Chu, \n BMVC,2017. 9 \n Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, and Chunyuan \n [78] R.Gao, R.Feris, and K.Grauman. Learningtoseparate\n Liao. La SOT:AHigh-Quality Benchmarkfor Large-Scale \n objectsoundsbywatchingunlabeledvideo. In ECCV,2018.\n Single Object Tracking. In 2019 IEEE/CVFConferenceon \n 8 \n Computer Visionand Pattern Recognition(CVPR),pages \n [79] Ruohan Gao,Rogerio Feris,and Kristen Grauman.Learning\n 5369–5378,Long Beach,CA,USA,June 2019.IEEE. 35 \n toseparateobjectsoundsbywatchingunlabeledvideo. In\n [63] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao \n ECCV,2018. 51 \n Li, Zhicheng Yan, Jitendra Malik, and Christoph Feicht- \n [80] Ruohan Gaoand Kristen Grauman. 2.5 dvisualsound. In\n enhofer. Multiscale vision transformers. ar Xiv preprint \n CVPR,2019. 8,51 \n ar Xiv:2104.11227,2021. 75 \n [81] Ruohan Gaoand Kristen Grauman. Co-separatingsounds\n [64] Yue Fan,JWKang,LTLi,KCLi,HLChen,STCheng,PY ofvisualobjects. In ICCV,2019. 51\n Zhang,ZYZhou,YQCai,and Dong Wang. CN-CELEB:a [82] R.Gaoand K.Grauman. Visual Voice:Audio-visualspeech\n challenging Chinesespeakerrecognitiondataset.In ICASSP separationwithcross-modalconsistency. In CVPR,2021. 8,\n 2020-2020 IEEE International Conference on Acoustics, 51 \n Speechand Signal Processing(ICASSP),pages 7604–7608. [83] I.Gebru,S.Ba,X.Li,and R.Horaud. Audio-visualspeaker\n IEEE,2020. 57 diarizationbasedonspatiotemporalbayesianfusion. PAMI,\n [65] Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, 2018. 8,51 \n Li Song,and Guangtao Zhai. Dual Attention Guided Gaze [84] Israel D.Gebru,Sile`ye Ba,Xiaofei Li,and Radu Horaud.\n Target Detectioninthe Wild. In Proceedingsofthe IEEE Audio-visualspeakerdiarizationbasedonspatiotemporal\n Conferenceon Computer Visionand Pattern Recognition bayesianfusion.IEEETransactionson Pattern Analysisand\n (CVPR 21),2021. 9 Machine Intelligence,39,2017. 52 \n 85 "
  },
  {
    "page_num": 86,
    "text": " \n \n \n \n \n \n [85] Georgios Georgakis,Md Alimoor Reza,Arsalan Mousavian, [99] Lianghua Huang,Xin Zhao,and Kaiqi Huang. GOT-10 k:A\n Phi-Hung Le,and Jana Kosˇecka´. Multiviewrgb-ddataset Large High-Diversity Benchmarkfor Generic Object Track-\n forobjectinstancedetection. In 2016 Fourth International inginthe Wild. IEEETransactionson Pattern Analysisand\n Conferenceon 3 DVision(3 DV),pages 426–434.IEEE,2016. Machine Intelligence,43(5):1562–1577,May 2021. 35\n 7 [100] Noureldien Hussein, Efstratios Gavves, and Arnold WM\n [86] Rohit Girdhar and Kristen Grauman. Anticipative video Smeulders. Timeceptionforcomplexactionrecognition. In\n transformer. In ICCV,2021. 3,9 CVPR,2019. 7 \n [101] Go Irie,Mirela Ostrek,Haochen Wang,Hirokazu Kameoka,\n [87] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE \n Akisato Kimura,Takahito Kawanishi,and Kunio Kashino.\n internationalconferenceoncomputervision,pages 1440– \n Seeingthroughsounds:Predictingvisualsemanticsegmen-\n 1448,2015. 75 \n tationresultsfrommultichannelaudiosignals. In ICASSP\n [88] Georgia Gkioxariand Jitendra Malik. Findingactiontubes. \n 2019-2019 IEEE International Conference on Acoustics,\n In 2015 IEEEConferenceon Computer Visionand Pattern \n Speechand Signal Processing(ICASSP),pages 3961–3964.\n Recognition(CVPR),pages 759–768,Boston,MA,USA, \n IEEE,2019. 51 \n June 2015.IEEE. 31 \n [102] Phillip Isola,Joseph J.Lim,and Edward H.Adelson. Dis-\n [89] P.Gollwitzer. Actionphasesandmind-sets,Handbookof \n coveringstatesandtransformationsinimagecollections. In\n motivationandcognition:Foundationsofsocialbehavior. \n CVPR,2015. 7 \n 1990. 45 \n [103] Phillip Isola,Joseph JLim,and Edward HAdelson. Dis-\n [90] Raghav Goyal,Samira Ebrahimi Kahou,Vincent Michal- coveringstatesandtransformationsinimagecollections. In\n ski,Joanna Materzynska,Susanne Westphal,Heuna Kim, CVPR,2015. 45 \n Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz [104] Koji Iwano, Tomoaki Yoshinaga, Satoshi Tamura, and\n Mueller-Freitag,etal. The”somethingsomething”video Sadaoki Furui. Audio-visualspeechrecognitionusinglip\n databaseforlearningandevaluatingvisualcommonsense. information extracted from side-face images. EURASIP\n In ICCV,2017. 45 Journalon Audio,Speech,and Music Processing,2007:1–9,\n [91] Alex Graves,Santiago Ferna´ndez,and Ju¨rgen Schmidhuber. 2007. 8,51 \n Bidirectionallstmnetworksforimprovedphonemeclassi- [105] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew\n fication and recognition. In International conference on Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver:\n artificialneuralnetworks,pages 799–804.Springer,2005. Generalperceptionwithiterativeattention. ar Xivpreprint\n 48,49 ar Xiv:2103.03206,2021. 48,49 \n [92] Chunhui Gu,Chen Sun,David ARoss,Carl Vondrick,Car- [106] Baoxiong Jia,Yixin Chen,Siyuan Huang,Yixin Zhu,and\n oline Pantofaru,Yeqing Li,Sudheendra Vijayanarasimhan, Song-Chun Zhu. Amulti-viewdatasetforlearningmulti-\n George Toderici,Susanna Ricco,Rahul Sukthankar,etal. agentmulti-taskactivities. In ECCV,2020. 3\n Ava:Avideodatasetofspatio-temporallylocalizedatomic [107] Hao Jiangand Kristen Grauman. Seeinginvisibleposes:\n visualactions. In Proceedingsofthe IEEEConferenceon Estimating 3 dbodyposefromegocentricvideo. In CVPR,\n Computer Visionand Pattern Recognition,pages 6047–6056, 2017. 3 \n 2018. 1,3,71 [108] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\n [93] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par- Chloe Hillier,Sudheendra Vijayanarasimhan,Fabio Viola,\n mar,Yu Zhang,Jiahui Yu,Wei Han,Shibo Wang,Zheng- Tim Green,Trevor Back,Paul Natsev,etal. Thekineticshu-\n dong Zhang,Yonghui Wu,etal. Conformer:Convolution- manactionvideodataset. ar Xivpreprintar Xiv:1705.06950,\n augmented transformer for speech recognition. ar Xiv 2017. 1,3,4,41 \n preprintar Xiv:2005.08100,2020. 61 [109] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\n Chloe Hillier,Sudheendra Vijayanarasimhan,Fabio Viola,\n [94] Kaiming He,Georgia Gkioxari,Piotr Dolla´r,and Ross Gir- \n Tim Green,Trevor Back,Paul Natsev,etal. Thekineticshu-\n shick. Mask R-CNN. ar Xiv:1703.06870[cs],Jan.2018. \n manactionvideodataset. ar Xivpreprintar Xiv:1705.06950,\n 33 \n 2017. 77 \n [95] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. \n [110] Evangelos Kazakos,Arsha Nagrani,Andrew Zisserman,and\n Deep residual learning for image recognition. In CVPR, \n Dima Damen. Epic-fusion:Audio-visualtemporalbinding\n 2016. 48,49 \n for egocentric action recognition. In Proceedings of the\n [96] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. \n IEEEInternational Conferenceon Computer Vision,pages\n Deep residual learning for image recognition. In CVPR, \n 5492–5501,2019. 3,7,8,51 \n 2016. 57 \n [111] Petr Kellnhofer,Simon Stent,Wojciech Matusik,and An-\n [97] Farnoosh Heidarivincheh, Majid Mirmehdi, and Dima tonio Torralba. Gaze 360: Physically Unconstrained Gaze\n Damen. Detectingthemomentofcompletion: Temporal Estimationinthe Wild. In Proceedingsofthe IEEEInterna-\n modelsforlocalisingactioncompletion. In BMVC,2018. tional Conferenceon Computer Vision(ICCV 19),2019. 9,\n 44 63,64,66 \n [98] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, [112] Suyoun Kim,Takaaki Hori,and Shinji Watanabe. Jointctc-\n and Adriane Boyd. spa Cy:Industrial-strength Natural Lan- attentionbasedend-to-endspeechrecognitionusingmulti-\n guage Processingin Python,2020. 20 tasklearning. In 2017 IEEEinternationalconferenceon\n 86 "
  },
  {
    "page_num": 87,
    "text": " \n \n \n \n \n \n acoustics,speechandsignalprocessing(ICASSP),pages [126] Kevin Lai,Liefeng Bo,and Dieter Fox. Unsupervisedfea-\n 4835–4839.IEEE,2017. 61 ture learning for 3 d scene labeling. In 2014 IEEE Inter-\n [113] Kris M.Kitani,Brian Ziebart,James D.Bagnell,and Martial national Conferenceon Roboticsand Automation(ICRA),\n Hebert. Activityforecasting. In ECCV,2012. 9 pages 3050–3057.IEEE,2014. 7 \n [114] Dietrich Klakowand Jochen Peters. Testingthecorrelation [127] Tian Lan,Tsung-Chuan Chen,and Silvio Savarese.Ahierar-\n ofworderrorrateandperplexity. Speech Communication, chicalrepresentationforfutureactionprediction. In ECCV,\n 38(1-2):19–28,2002. 8,54 2014. 9 \n [115] Mark L. Knapp, Judith A. Hall, and Terrence G. Hor- [128] Federico Landini,Ja´n Profant,Mireia Diez,and Luka´sˇBur-\n gan. Nonverbal Communication in Human Interaction. get.Bayesianhmmclusteringofx-vectorsequences(vbx)in\n Wadsworth Cengage Learning,8 thedition,2014. 8 speakerdiarization:theory,implementationandanalysison\n [116] Ross A Knepper, Todd Layton, John Romanishin, and standardtasks. Computer Speech&Language,71:101254,\n Daniela Rus. Ikeabot: Anautonomousmulti-robotcoor- 2022. 56 \n dinatedfurnitureassemblysystem. In 2013 IEEEInterna- [129] Y.J.Lee,J.Ghosh,and K.Grauman.Discoveringimportant\n tionalconferenceonroboticsandautomation,pages 855– peopleandobjectsforegocentricvideosummarization. In\n 862.IEEE,2013. 44 CVPR,2012. 2,3 \n [117] Andrew JKolarik,Brian CJMoore,Pavel Zahorik,Silvia [130] Y.J.Lee,J.Ghosh,and K.Grauman.Discoveringimportant\n Cirstea, and Shahina Pardhan. Auditorydistancepercep- peopleandobjectsforegocentricvideosummarization. In\n tioninhumans: areviewofcues,development,neuronal Proceedingsofthe IEEEConferenceon Computer Vision\n bases,andeffectsofsensoryloss. Attention,Perception,& and Pattern Recognition(CVPR),2012. 45\n Psychophysics,78(2):373–395,2016. 51 \n [131] Yong Jae Leeand Kristen Grauman. Predictingimportant\n [118] Hema S.Koppulaand Ashutosh Saxena.Anticipatinghuman objectsforegocentricvideosummarization. IJCV,2015. 3\n activitiesusingobjectaffordancesforreactiveroboticre- \n [132] Bruno Lepri,Ramanathan Subramanian,Kyriaki Kalimeri,\n sponse.Pattern Analysisand Machine Intelligence,38(1):14– \n Jacopo Staiano, Fabio Pianesi, and Nicu Sebe. Connect-\n 29,2016. 9 \n ingmeetingbehaviorwithextraversion-asystematicstudy.\n [119] Ranjay Krishna,Kenji Hata,Frederic Ren,Li Fei-Fei,and \n IEEETransactionson Affective Computing,3(4):443–455,\n Juan Carlos Niebles. Dense-captioningeventsinvideos. In \n 2012. 9 \n International Conferenceon Computer Vision(ICCV),2017. \n [133] Vladimir ILevenshteinetal. Binarycodescapableofcor-\n 7 \n rectingdeletions,insertions,andreversals. In Sovietphysics\n [120] Alexei A.Efros Krishna Kumar Singh,Kayvon Fatahalian. \n doklady,1966. 73 \n Krishnacam:Usingalongitudinal,single-person,egocentric \n [134] Cheng Li and Kris Kitani. Model recommendation with\n datasetforsceneunderstandingtasks. In IEEEWinter Con- \n virtualprobesforego-centrichanddetection.In ICCV,2013.\n ferenceon Applicationsof Computer Vision(WACV),2016. \n 3 \n 9 \n [135] Yin Li, Alireza Fathi, and James M. Rehg. Learning to\n [121] Matej Kristan, Ales Leonardis, Jiri Matas, Michael \n predict gaze in egocentric video. In Proceedings of the\n Felsberg, Roman Pflugfelder, Joni-Kristian Kamarainen, \n Luka Cˇehovin Zajc,Martin Danelljan,Alan Lukezic,On- IEEEInternational Conferenceon Computer Vision,pages\n 3216–3223,2013. 65 \n drej Drbohlav,Linbo He,Yushan Zhang,Song Yan,Jinyu \n [136] Y.Li,M.Liu,and J.Rehg. Intheeyeofbeholder: Joint\n Yang,Gustavo Fernandez,andetal.Theeighthvisualobject \n learningofgazeandactionsinfirstpersonvideo. In ECCV,\n tracking VOT 2020 challengeresults,2020. 31 \n 2018. 45 \n [122] Taku Kudo and John Richardson. Sentencepiece: A \n [137] Yin Li,Miao Liu,and Jame Rehg.Inthe Eyeofthe Beholder:\n simpleandlanguageindependentsubwordtokenizerand \n detokenizer for neural text processing. ar Xiv preprint Gazeand Actionsin First Person Video. IEEETransactions\n ar Xiv:1808.06226,2018. 61 on Pattern Analysisand Machine Intelligence,2021. 65\n [123] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui- [138] Yin Li,Miao Liu,and James MRehg.Intheeyeofbeholder:\n jlings,Ivan Krasin,Jordi Pont-Tuset,Shahab Kamali,Stefan Jointlearningofgazeandactionsinfirstpersonvideo. In\n Popov,Matteo Malloci,Alexander Kolesnikov,etal. The Proceedingsofthe European Conferenceon Computer Vi-\n openimagesdatasetv 4. International Journalof Computer sion(ECCV),pages 619–635,2018. 2,3\n Vision,128(7):1956–1981,2020. 57 [139] Yanghao Li,Tushar Nagarajan,Bo Xiong,and Kristen Grau-\n [124] F.Dela Torre,J.Hodgins,J.Montano,S.Valcarcel,R.For- man. Ego-exo: Transferring visual representations from\n cada,and J.Macey. Guidetothecarnegiemellonuniversity third-persontofirst-personvideos. In CVPR,2021. 3,7\n multimodalactivity(cmu-mmac)database. In Tech.report [140] Tianwei Lin,Xiao Liu,Xin Li,Errui Ding,and Shilei Wen.\n CMU-RI-TR-08-22,Robotics Institute,Carnegie Mellon Uni- Bmn:Boundary-matchingnetworkfortemporalactionpro-\n versity,2009. 3 posalgeneration. In Proceedingsofthe IEEE/CVFInterna-\n [125] Loic Lacheze,Yan Guo,Ryad Benosman,Bruno Gas,and tional Conferenceon Computer Vision,pages 3889–3898,\n Charlie Couverture. Audio/videofusionforobjectsrecog- 2019. 48,49 \n nition. In 2009 IEEE/RSJInternational Conferenceon In- [141] Tianwei Lin,Xu Zhao,Haisheng Su,Chongjing Wang,and\n telligent Robotsand Systems,pages 652–657.IEEE,2009. Ming Yang. Bsn:Boundarysensitivenetworkfortemporal\n 8 actionproposalgeneration. In Proceedingsofthe European\n 87 "
  },
  {
    "page_num": 88,
    "text": " \n \n \n \n \n \n Conferenceon Computer Vision(ECCV),pages 3–19,2018. Conferenceon Applicationsof Computer Vision(WACV),\n 7,25,32,41 pages 1507–1516,January 2021. 7 \n [142] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He, [156] Christophe Micheyl,Christian Kaernbach,and Laurent De-\n Bharath Hariharan,and Serge Belongie. Feature Pyramid many. Anevaluationofpsychophysicalmodelsofauditory\n Networksfor Object Detection.ar Xiv:1612.03144[cs],Apr. changeperception.Psychologicalreview,115(4):1069,2008.\n 2017. 33 51 \n [143] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, [157] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\n Pietro Perona,Deva Ramanan,Piotr Dolla´r,and CLawrence Makarand Tapaswi, Ivan Laptev, and Josef Sivic.\n Zitnick. Microsoft COCO:Commonobjectsincontext. In How To 100 M:Learninga Text-Video Embeddingby Watch-\n ECCV,2014. 1,3,47 ing Hundred Million Narrated Video Clips. In ICCV,2019.\n [144] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Fore- 3,6 \n castinghuman-objectinteraction:jointpredictionofmotor [158] Ishan Misra,Abhinav Gupta,and Martial Hebert. Fromred\n attentionandactionsinfirstpersonvideo. In ECCV,2020. winetoredtomato: Compositionwithcontext. In CVPR,\n 3 2017. 45 \n [145] Wen Liu,Weixin Luo,Dongze Lian,and Shenghua Gao.Fu- \n [159] Yu Mitsuzumi, Atsushi Nakazawa, and Toyoaki Nishida.\n tureframepredictionforanomalydetection–anewbaseline. \n Deepeyecontactdetector:Robusteyecontactbiddetection\n In Proceedingsofthe IEEEConferenceon Computer Vision \n usingconvolutionalneuralnetwork. In BMVC,2017. 9\n and Pattern Recognition,pages 6536–6545,2018. 9 \n [160] Davide Moltisanti,Michael Wray,Walterio Mayol-Cuevas,\n [146] William Lotter, Gabriel Kreiman, and David Cox. Deep \n and Dima Damen. Trespassingtheboundaries: Labelling\n predictivecodingnetworksforvideopredictionandunsu- \n temporalboundsforobjectinteractionsinegocentricvideo.\n pervisedlearning. ar Xivpreprintar Xiv:1605.08104,2016. \n In ICCV,2017. 44,45 \n 9 \n [161] Pedro Morgado,Nono Vasconcelos,Timothy Langlois,and\n [147] Cewu Lu,Renjie Liao,and Jiaya Jia. Personalobjectdis- \n Oliver Wang.Self-supervisedgenerationofspatialaudiofor\n coveryinfirst-personvideos. TIP,2015. 3 360◦video. In Neur IPS,2018. 8,51 \n [148] Zheng Luand Kristen Grauman. Story-drivensummariza- \n [162] Matthias Mu¨ller,Adel Bibi,Silvio Giancola,Salman Alsub-\n tionforegocentricvideo. In CVPR,2013. 3 \n aihi, and Bernard Ghanem. Tracking Net: ALarge-Scale\n [149] Tahmida Mahmud, Mahmudul Hasan, and Amit K Roy- \n Datasetand Benchmarkfor Object Trackinginthe Wild. In\n Chowdhury. Jointpredictionofactivitylabelsandstarting \n Vittorio Ferrari,Martial Hebert,Cristian Sminchisescu,and\n times in untrimmed videos. In Proceedings of the IEEE \n Yair Weiss,editors,Computer Vision–ECCV 2018,volume\n International Conferenceon Computer Vision,pages 5773– \n 11205,pages 310–327.Springer International Publishing,\n 5782,2017. 9 \n Cham,2018. 35 \n [150] Manuel JMarin-Jimenez,Vicky Kalogeiton,Pablo Medina- \n [163] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen\n Suarez,and Andrew Zisserman. Laeo-net:revisitingpeople \n Grauman.Groundedhuman-objectinteractionhotspotsfrom\n lookingateachotherinvideos. In Proceedingsofthe IEEE \n video. ICCV,2019. 3 \n Conferenceon Computer Visionand Pattern Recognition, \n [164] Tushar Nagarajanand Kristen Grauman. Attributesasop-\n pages 3477–3485,2019. 9 \n erators: factorizing unseen attribute-object compositions.\n [151] Manuel Jesu´s Mar´ın-Jime´nez,Andrew Zisserman,Marcin \n In Proceedingsofthe European Conferenceon Computer\n Eichner,and Vittorio Ferrari. Detectingpeoplelookingat \n Vision(ECCV),pages 169–185,2018. 7,45 \n eachotherinvideos. International Journalof Computer \n [165] A. Nagrani, J. S. Chung, and A. Zisserman. Vox Celeb:\n Vision,106(3):282–296,2014. 9 \n a large-scale speaker identification dataset. In INTER-\n [152] Manuel JMar´ın-Jime´nez,Andrew Zisserman,and Vittorio \n SPEECH,2017. 52,57 \n Ferrari.Here’slookingatyou,kid.Detectingpeoplelooking \n ateachotherinvideos.In BMVC,5,2011. 9 [166] Katsuyuki Nakamura,Serena Yeung,Alexandre Alahi,and\n Li Fei-Fei. Jointlylearningenergyexpendituresandactiv-\n [153] Michael Mathieu,Camille Couprie,and Yann Le Cun. Deep \n itiesusingegocentricmultimodalsignals. In CVPR,2017.\n multi-scale video prediction beyond mean square error. \n 3 \n ar Xivpreprintar Xiv:1511.05440,2015. 9 \n [154] Iain Mc Cowan,Jean Carletta,Wessel Kraaij,Simone Ashby, [167] Lukas Neumann,Andrew Zisserman,and Andrea Vedaldi.\n Sebastien Bourban,Mike Flynn,Mael Guillemot,Thomas Futureeventprediction:Ifandwhen. In Proceedingsofthe\n Hain,Jaroslav Kadlec,Vasilis Karaiskos,Melissa Kronen- IEEEConferenceon Computer Visionand Pattern Recogni-\n thal,Guillaume Lathoud,Mike Lincoln,Agnes Lisowska, tion Workshops,pages 0–0,2019. 9\n Wilfried Post, Dennis Reidsma, and Pierre Wellner. The [168] Evonne Ng,Donglai Xiang,Hanbyul Joo,and Kristen Grau-\n AMImeetingcorpus. In Proceedingsof Measuring Behav- man. You 2 me:Inferringbodyposeinegocentricvideovia\n ior 2005,the 5 th International Conferenceon Methodsand firstandsecondpersoninteractions. In CVPR,2020. 3\n Techniquesin Behavioral Research,pages 137–140,2005. 9 [169] Joonas Nikunenand Tuomas Virtanen. Directionofarrival\n [155] Jean-Philippe Mercier,Mathieu Garon,Philippe Giguere, basedspatialcovariancemodelforblindsoundsourcesep-\n and Jean-Francois Lalonde. Deep template-based object aration. IEEE/ACMTransactionson Audio, Speech, and\n instancedetection. In Proceedingsofthe IEEE/CVFWinter Language Processing,22(3):727–739,2014. 51\n 88 "
  },
  {
    "page_num": 89,
    "text": " \n \n \n \n \n \n [170] C.Northcutt,S.Zha,S.Lovegrove,and R.Newcombe.Ego- inanindustrial-likedomain. In IEEEWinter Conferenceon\n com: Amulti-personmulti-modalegocentriccommunica- Applicationof Computer Vision(WACV),2021. 3\n tionsdataset. PAMI,2020. 3 [185] Rene´Ranftl,Alexey Bochkovskiy,and Vladlen Koltun. Vi-\n [171] Andrew Owensand Alexei AEfros.Audio-visualsceneanal- siontransformersfordenseprediction.In Proceedingsofthe\n ysiswithself-supervisedmultisensoryfeatures. In ECCV, IEEE/CVFInternational Conferenceon Computer Vision,\n 2018. 51 pages 12179–12188,2021. 38,39 \n [172] Cristina Palmero,Elsbeth Avan Dam,Sergio Escalera,Mike [186] Adria Recasens,Aditya Khosla,Carl Vondrick,and Antonio\n Kelia,Guido FLichtert,Lucas PJJNoldus,Andrew JSpink, Torralba. Wherearetheylooking? In Advancesin Neural\n and Astridvan Wieringen. Automaticmutualgazedetec- Information Processing Systems,pages 199–207,2015. 9\n tioninface-to-facedyadicinteractionvideos. Measuring [187] Joseph Redmonand Ali Farhadi. Yolov 3:Anincremental\n Behavior 2018,2018. 9 improvement. ar Xivpreprintar Xiv:1804.02767,2018. 57\n [173] Daniel SPark,William Chan,Yu Zhang,Chung-Cheng Chiu, [188] James M. Rehg, Agata Rozga, Gregory D. Abowd, and\n Barret Zoph,Ekin DCubuk,and Quoc VLe. Specaugment: Matthew S. Goodwin. Behavioral Imaging and Autism.\n Asimpledataaugmentationmethodforautomaticspeech IEEEPervasive Computing,13(2):84–87,2014. 8\n recognition. ar Xivpreprintar Xiv:1904.08779,2019. 60 [189] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun.\n [174] H.S.Park,J.-J.Hwang,Y.Niu,and J.Shi.Egocentricfuture Fasterr-cnn:Towardsreal-timeobjectdetectionwithregion\n localization. In CVPR,2016. 9 proposalnetworks. In Neur IPS,2015. 33 \n [175] H.S.Park,J.-J.Hwang,Y.Niu,and J.Shi.Egocentricfuture [190] Shaoqing Ren,Kaiming He,Ross Girshick,and Jian Sun.\n localization. In Conferenceon Computer Visionand Pattern Faster r-cnn: Towards real-time object detection with re-\n Recognition(CVPR),2016. 74 gionproposalnetworks. Advancesinneuralinformation\n [176] Hyun Soo Park,Eakta Jain,and Yaser Sheikh. 3 Dsocial processingsystems,28:91–99,2015. 49\n saliencyfromhead-mountedcameras.In Advancesin Neural [191] Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, and\n Information Processing Systems,volume 1,pages 422–430, Giovanni Maria Farinella. Predictingthefuturefromfirst\n 2012. 9 person(egocentric)vision:Asurvey. Computer Visionand\n [177] Tae Jin Park,Naoyuki Kanda,Dimitrios Dimitriadis,Kyu J Image Understanding,2021. 9\n Han,Shinji Watanabe,and Shrikanth Narayanan. Areview [192] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad-\n ofspeakerdiarization:Recentadvanceswithdeeplearning. hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh\n ar Xivpreprintar Xiv:2101.09624,2021. 53,56 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid,\n [178] David R Perrott and Kourosh Saberi. Minimum audible Zhonghua Xi, et al. Ava-activespeaker: An audio-visual\n anglethresholdsforsourcesvaryinginbothelevationand dataset for active speaker detection. ar Xiv preprint\n azimuth. The Journalofthe Acoustical Societyof America, ar Xiv:1901.01342,2019. 8,52,53\n 87(4):1728–1731,1990. 51 [193] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad-\n [179] Hamed Pirsiavash and Deva Ramanan. Detecting activi- hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh\n tiesofdailylivinginfirst-personcameraviews. In 2012 Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid,\n IEEEconferenceoncomputervisionandpatternrecogni- Zhonghua Xi,and Caroline Pantofaru. Ava Active Speaker:\n tion,pages 2847–2854.IEEE,2012. 2,3 An Audio-Visual Dataset for Active Speaker Detection.\n [180] H.Pirsiavashand D.Ramanan. Detectingactivitiesofdaily In ICASSP, IEEE International Conference on Acoustics,\n livinginfirst-personcameraviews. In Computer Visionand Speechand Signal Processing-Proceedings,volume 2020-\n Pattern Recognition(CVPR),2012. 45 May,pages 4492–4496,2020. 9 \n [181] Daniel Povey,Arnab Ghoshal,Gilles Boulianne,Lukas Bur- [194] M.S.Ryooand L.Matthies. First-personactivityrecogni-\n get,Ondrej Glembek,Nagendra Goel,Mirko Hannemann, tion: Whataretheydoingtome? In IEEEConferenceon\n Petr Motlicek,Yanmin Qian,Petr Schwarz,Jan Silovsky, Computer Visionand Pattern Recognition(CVPR),2013. 3\n Georg Stemmer,and Karel Vesely. The Kaldispeechrecog- [195] Paul-Edouard Sarlin,Daniel De Tone,Tomasz Malisiewicz,\n nitiontoolkit. In IEEE 2011 Workshopon Automatic Speech and Andrew Rabinovich.Superglue:Learningfeaturematch-\n Recognitionand Understanding,2011. 56 ing with graph neural networks. In Proceedings of the\n [182] Senthil Purushwalkam,Maximilian Nickel,Abhinav Gupta, IEEE/CVFconferenceoncomputervisionandpatternrecog-\n and Marc’Aurelio Ranzato. Task-drivenmodularnetworks nition,pages 4938–4947,2020. 36,38\n forzero-shotcompositionallearning. In Proceedingsofthe [196] Johannes LSchonbergerand Jan-Michael Frahm. Structure-\n IEEEInternational Conferenceon Computer Vision,pages from-motion revisited. In Proceedings of the IEEE con-\n 3593–3602,2019. 45 ferenceoncomputervisionandpatternrecognition,pages\n [183] F.Ragusa,A.Furnari,S.Battiato,G.Signorello,and G.M. 4104–4113,2016. 36 \n Farinella. Egocentricvisitorslocalizationinculturalsites. [197] A.Senocak,T.-H.Oh,J.Kim,M.Yang,and I.S.Kweon.\n Journal on Computing and Cultural Heritage (JOCCH), Learningtolocalizesoundsourcesinvisualscenes:Analysis\n 2019. 3 andapplications. TPAMI,2019. 8,51 \n [184] Francesco Ragusa, Antonino Furnari, Salvatore Livatino, [198] Dandan Shan,Jiaqi Geng,Michelle Shu,and David Fouhey.\n and Giovanni Maria Farinella. Themeccanodataset:Under- Understandinghumanhandsincontactatinternetscale. In\n standinghuman-objectinteractionsfromegocentricvideos CVPR,2020. 45,46 \n 89 "
  },
  {
    "page_num": 90,
    "text": " \n \n \n \n \n \n [199] Dandan Shan,Jiaqi Geng,Michelle Shu,and David Fouhey. [214] Twenty BN. The 20 BN-jester Dataset V 1. https://\n Understandinghumanhandsincontactatinternetscale. In 20 bn.com/datasets/jester. 45\n CVPR,2020. 49 [215] Joost Van Amersfoort,Anitha Kannan,Marc’Aurelio Ran-\n [200] Mohit Sharma,Kevin Zhang,and Oliver Kroemer. Learning zato, Arthur Szlam, Du Tran, and Soumith Chintala.\n semanticembeddingspacesforslicingvegetables. ar Xiv Transformation-basedmodelsofvideosequences. ar Xiv\n preprintar Xiv:1904.00303,2019. 44 preprintar Xiv:1701.08435,2017. 9,40 \n [201] Gunnar ASigurdsson,Abhinav Gupta,Cordelia Schmid,Ali [216] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko-\n Farhadi,and Karteek Alahari. Charades-ego:Alarge-scale reit,Llion Jones,Aidan NGomez,Łukasz Kaiser,and Illia\n datasetofpairedthirdandfirstpersonvideos.ar Xivpreprint Polosukhin.Attentionisallyouneed.In Advancesinneural\n ar Xiv:1804.09626,2018. 2,3,45 inprocessingsystems,pages 5998–6008,2017. 49\n [202] Nathan Silberman,Derek Hoiem,Pushmeet Kohli,and Rob \n [217] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko-\n Fergus. Indoorsegmentationandsupportinferencefrom \n reit,Llion Jones,Aidan NGomez,Łukasz Kaiser,and Illia\n rgbdimages. In Europeanconferenceoncomputervision, \n Polosukhin.Attentionisallyouneed.In Advancesinneural\n pages 746–760.Springer,2012. 38 \n informationprocessingsystems,pages 5998–6008,2017. 61\n [203] Silero Team. Silero vad: Pre-trained enterprise-grade \n [218] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu\n voice activity detector (VAD), number detector and lan- \n Lin, and Honglak Lee. Decomposing motion and con-\n guageclassifier. https://github.com/snakers 4/ \n tentfornaturalvideosequenceprediction. ar Xivpreprint\n silero-vad,2021. 59 \n ar Xiv:1706.08033,2017. 9 \n [204] Michel Silva,Washington Ramos,Joa˜o Ferreira,Felipe Cha- \n [219] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\n mone, Mario Campos, and Erickson R. Nascimento. A \n Anticipatingvisualrepresentationsfromunlabeledvideo. In\n weightedsparsesamplingandsmoothingframetransition \n CVPR,2016. 9 \n approachforsemanticfast-forwardfirst-personvideos. In \n 2018 IEEE/CVFConferenceon Computer Visionand Pat- [220] He Wang,So¨ren Pirk,Ersin Yumer,Vladimir GKim,Ozan\n tern Recognition(CVPR),2018. 3 Sener,Srinath Sridhar,and Leonidas JGuibas. Learninga\n [205] Krishna Kumar Singh, Kayvon Fatahalian, and Alexei A generativemodelformulti-stephuman-objectinteractions\n Efros. Krishnacam: Using a longitudinal, single-person, fromvideos. In Eurographics,2019. 45\n egocentricdatasetforsceneunderstandingtasks. In WACV, [221] Limin Wang,Yuanjun Xiong,Zhe Wang,Yu Qiao,Dahua\n 2016. 2,3 Lin,Xiaoou Tang,and Luc Van Gool.Temporalsegmentnet-\n [206] David Snyder,Daniel Garcia-Romero,Gregory Sell,Daniel works:Towardsgoodpracticesfordeepactionrecognition.\n Povey,and Sanjeev Khudanpur. X-vectors: Robust DNN In ECCV,2016. 3,7 \n embeddingsforspeakerrecognition. In 2018 IEEEInterna- [222] Qiang Wang,Li Zhang,Luca Bertinetto,Weiming Hu,and\n tional Conferenceon Acoustics,Speechand Signal Process- Philip H.S.Torr. Fastonlineobjecttrackingandsegmenta-\n ing(ICASSP),2018. 57 tion:Aunifyingapproach,2019. 17 \n [207] Khurram Soomro,Amir Roshan Zamir,and Mubarak Shah. [223] Xiaolong Wang,Ali Farhadi,and Abhinav Gupta. Actions˜\n Ucf 101:Adatasetof 101 humanactionclassesfromvideos transformations. In CVPR,2016. 45\n inthewild. In CRCV-TR-12-01,2012. 3 \n [224] Xiaolong Wang,Ross Girshick,Abhinav Gupta,and Kaim-\n [208] Emiliano Spera,Antonino Furnari,Sebastiano Battiato,and \n ing He. Non-localneuralnetworks. In CVPR,2018. 3\n Giovanni Maria Farinella. Egocentricshoppingcartlocal- \n [225] Yuxin Wu,Alexander Kirillov,Francisco Massa,Wan-Yen\n ization. In International Conferenceon Pattern Recognition \n Lo,and Ross Girshick. Detectron 2. 35 \n (ICPR),2018. 3 \n [226] Fanyi Xiao,Yong Jae Lee,Kristen Grauman,Jitendra Malik,\n [209] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, \n and Christoph Feichtenhofer.Audiovisualslowfastnetworks\n Erik Wijmans,Simon Green,Jakob JEngel,Raul Mur-Artal, \n for video recognition. ar Xiv preprint ar Xiv:2001.08740,\n Carl Ren,Shobhit Verma,etal.Thereplicadataset:Adigital \n 2020. 8,51 \n replicaofindoorspaces. ar Xivpreprintar Xiv:1906.05797, \n 2019. 14 [227] SHIXingjian,Zhourong Chen,Hao Wang,Dit-Yan Yeung,\n [210] Yu-Chuan Suand Kristen Grauman. Detectingengagement Wai-Kin Wong,and Wang-chun Woo. Convolutionallstm\n inegocentricvideo. In ECCV,2016. 2,3,45 network: A machine learning approach for precipitation\n [211] Ruijie Tao,Zexu Pan,Rohan Kumar Das,Xinyuan Qian, nowcasting. In Advancesinneuralinformationprocessing\n Mike Zheng Shou,and Haizhou Li. Issomeonespeaking? systems,pages 802–810,2015. 9\n exploringlong-termtemporalfeaturesforaudio-visualactive [228] Jun Xu,Tao Mei,Ting Yao,and Yong Rui. Msr-vtt:Alarge\n speakerdetection. ar Xivpreprintar Xiv:2107.06592,2021. videodescriptiondatasetforbridgingvideoandlanguage.\n 53,58,59 IEEE International Conference on Computer Vision and\n [212] Y.Tian,J.Shi,B.Li,Z.Duan,and C.Xu. Audio-visual Pattern Recognition(CVPR),June 2016. 7\n eventlocalizationinunconstrainedvideos. In ECCV,2018. [229] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet,\n 8,51 and Bernard Ghanem. G-tad: Sub-graphlocalizationfor\n [213] E.Tulving. Episodicandsemanticmemory. In E.Tulv- temporalactiondetection. In Proceedingsofthe IEEE/CVF\n ing and W. Donaldson, editors, Organization of memory. Conferenceon Computer Visionand Pattern Recognition,\n Academic Press,1972. 6 pages 10156–10165,2020. 7,25,32,41 \n 90 "
  },
  {
    "page_num": 91,
    "text": " \n \n \n \n \n \n [230] Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, and \n Yoichi Sato.Futurepersonlocalizationinfirst-personvideos. \n In The IEEEConferenceon Computer Visionand Pattern \n Recognition(CVPR),June 2018. 9 \n [231] Ryo Yonetani,Kris M.Kitani,and Yoichi Sato.Recognizing \n micro-actionsandreactionsfrompairedegocentricvideos. \n In CVPR,2016. 3 \n [232] Ryo Yonetani,Kris MKitani,and Yoichi Sato. Visualmotif \n discoveryviafirst-personvision. In ECCV,2016. 3 \n [233] Fisher Yu,Dequan Wang,Evan Shelhamer,and Trevor Dar- \n rell.Deeplayeraggregation.In Proceedingsofthe IEEEcon- \n ferenceoncomputervisionandpatternrecognition,pages \n 2403–2412,2018. 49 \n [234] Hua Zhang,Xiaochun Cao,and Rui Wang. Audiovisual \n attributediscoveryforfine-grainedobjectrecognition. In \n Proceedings of the AAAI Conference on Artificial Intelli- \n gence,volume 32,2018. 8 \n [235] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. \n Span-basedlocalizingnetworkfornaturallanguagevideo \n localization. In Proceedingsofthe 58 th Annual Meetingof \n the Associationfor Computational Linguistics,pages 6543– \n 6554, Online, July 2020. Association for Computational \n Linguistics. 40 \n [236] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo \n Luo. Learning 2 dtemporaladjacentnetworksformoment \n localizationwithnaturallanguage. In AAAI,2020. 26,31, \n 32,39,40 \n [237] Chen Zhao, Ali K Thabet, and Bernard Ghanem. Video \n self-stitchinggraphnetworkfortemporalactionlocalization. \n In Proceedingsofthe IEEE/CVFInternational Conference \n on Computer Vision,pages 13658–13667,2021. 7,25,32, \n 40,41 \n [238] Hang Zhao,Chuang Gan,Andrew Rouditchenko,Carl Von- \n drick,Josh Mc Dermott,and Antonio Torralba. Thesound \n ofpixels. In ECCV,2018. 51 \n [239] Bolei Zhou,Alex Andonian,Aude Oliva,and Antonio Tor- \n ralba. Temporalrelationalreasoninginvideos. In ECCV, \n 2018. 3 \n [240] Hao Zhou,Chongyang Zhang,Yan Luo,Yanjun Chen,and \n Chuanping Hu. Embracinguncertainty: Decouplingand \n de-biasforrobusttemporalgrounding. In Proceedingsof \n the IEEE/CVFConferenceon Computer Visionand Pattern \n Recognition,pages 8445–8454,2021. 33 \n [241] Xingyi Zhou,Dequan Wang,and Philipp Kra¨henbu¨hl. Ob- \n jectsaspoints. ar Xivpreprintar Xiv:1904.07850,2019. 49 \n [242] Y.Zhouand T.Berg. Learningtemporaltransformations \n fromtime-lapsevideos. In ECCV,2016. 7 \n [243] Yipin Zhouand Tamara LBerg. Temporalperceptionand \n predictioninego-centricvideo. In ICCV,2015. 3,7 \n [244] Yipin Zhouand Tamara LBerg. Learningtemporaltransfor- \n mationsfromtime-lapsevideos. In ECCV,2016. 45 \n [245] Hao Zhu,Man-Di Luo,Rui Wang,Ai-Hua Zheng,and Ran \n He. Deepaudio-visuallearning: Asurvey. International \n Journalof Automationand Computing,pages 1–26,2021. 8 \n 91 "
  }
]