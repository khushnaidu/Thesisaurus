[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n \n \n Humans in 4 D: Reconstructing and Tracking Humans with Transformers \n \n \n Shubham Goel Georgios Pavlakos Jathushan Rajasegaran Angjoo Kanazawa Jitendra Malik\n ∗ ∗ \n shubham-goel, pavlakos, jathushan, kanazawa @berkeley.edu, malik@eecs.berkeley.edu\n { } \n Universityof California,Berkeley \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 1: A “transformerized” view of Human Mesh Recovery. We describe HMR 2.0, a fully transformer-based approach for 3 D\n humanposeandshapereconstructionfromasingleimage.Besidesimpressiveperformanceacrossawidevarietyofposesandviewpoints,\n HMR 2.0 alsoactsasthebackboneofanimprovedsystemforjointlyreconstructingandtracking Humansin 4 D(4 DHumans). Here,we\n seeoutputreconstructionsfrom HMR 2.0 foreach 2 Ddetectionintheleftimage. \n \n Abstract 1.Introduction \n \n In this paper, we present a fully transformer-based ap-\n Wepresentanapproachtoreconstructhumansandtrack proachforrecovering 3 Dmeshesofhumanbodiesfromsin-\n them over time. At the core of our approach, we propose gleimages,andtrackingthemovertimeinvideo.Weobtain\n a fully “transformerized” version of a network for human unprecedentedaccuracyinoursingle-image 3 Dreconstruc-\n meshrecovery. Thisnetwork,HMR 2.0,advancesthestate tions(see Figure 1)evenforunusualposeswhereprevious\n oftheartandshowsthecapabilitytoanalyzeunusualposes approachesstruggle.Invideo,welinkthesereconstructions\n that have in the past been difficult to reconstruct from sin- overtimeby 3 Dtracking,intheprocessbridginggapsdue\n gle images. To analyze video, we use 3 D reconstructions toocclusionordetectionfailures.These 4 Dreconstructions\n from HMR 2.0 as input to a tracking system that operates canbeseenontheprojectwebpage.\n in 3 D. This enables us to deal with multiple people and Ourproblemformulationandapproachcanbeconceived\n maintainidentitiesthroughocclusionevents. Ourcomplete as the “transformerization” of previous work on human\n approach, 4 DHumans, achieves state-of-the-art results for mesh recovery, HMR [30] and 3 D tracking, PHALP [65].\n tracking people from monocular video. Furthermore, we Sincethepioneering Vi Tpaper[15],theprocessof“trans-\n demonstrate the effectiveness of HMR 2.0 on the down- formerization”, i.e., converting models from CNNs or\n streamtaskofactionrecognition,achievingsignificantim- LSTMs to transformer backbones, has advanced rapidly\n provements over previous pose-based action recognition across multiple computer vision tasks, e.g., [8, 16, 24, 40,\n approaches. Our code and models are available on the 61,77]. Specificallyfor 2 Dpose(2 Dbodykeypoints)this\n project website: https://shubham-goel.github. has already been done by Vi TPose [81]. We take that as a\n io/4 dhumans/. startingpointandwedevelopanewversionof HMR,which\n wecall HMR 2.0 toacknowledgeitsantecedent. \n 3202 \n gu A \n 13 \n ]VC.sc[ \n 3 v 19002.5032:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n We use HMR 2.0 to build a system that can simultane- many improvements have been proposed for the original\n ously reconstruct and track humans from videos. We rely method. Notably, many works have proposed alternative\n on the recent 3 D tracking system, PHALP [65], which we methodsforpseudo-groundtruthgeneration, includingus-\n simplifyandimproveusingourposerecovery. Thissystem ingtemporalinformation[3],multipleviews[39],oritera-\n canreconstruct Humansin 4 D,whichgivesthenametoour tive optimization [35, 29, 57]. SPIN [35] proposed an in-\n method, 4 DHumans. 4 DHumans can be deployed on any the-loopoptimizationthatincorporated SMPLify[7]inthe\n videoandcanjointlytrackandreconstructpeopleinvideo. HMR training. Here, we also rely on pseudo-ground truth\n Thefunctionalityofcreatingatrackingentityforeveryper- fitsfortraining,andweuse[37]fortheofflinefitting.\n son is fundamental towards analyzing and understanding Morerecently,therehavebeenworksthatproposemore\n humansinvideo. Besidesachievingstate-of-the-artresults specializeddesignsforthe HMRarchitecture. Py MAF[89,\n for tracking on the Pose Track dataset [1], we also apply 88] incorporates a mesh alignment module for the regres-\n HMR 2.0 onthedownstreamapplicationofactionrecogni- sion of the SMPL parameters. PARE [34] proposes a\n tion. Wefollowthesystemdesignofrecentwork,[63],and body-part-guidedattentionmechanismforbetterocclusion\n we show that the use of HMR 2.0 can achieve impressive handling. HKMR [20] performs a prediction that is in-\n improvements upon the state of the art on action recogni- formedbytheknownhierarchicalstructureof SMPL.Holo-\n tiononthe AVAv 2.2 dataset. Pose [23] proposes a pooling strategy that follows the 2 D\n This paper is unabashedly a systems paper. We make locations of each body joints. Instead, we follow a design\n design choices that lead to the best systems for 3 D human withoutanydomain-specificdecisionsandweshowthatit\n reconstructionandtrackinginthewild. Ourmodelispub- outperformsallpreviousapproaches.\n liclyavailableontheprojectwebpage. Thereisanemerg- \n Many related approaches are making non-parametric\n ing trend, in computer vision as in natural language pro- \n predictions, i.e., instead of estimating the parameters of\n cessing, of large pretrained models which find widespread \n the SMPLmodel,theyexplicitlyregresstheverticesofthe\n downstream applications and thus justify the scaling ef- \n mesh. Graph CMR[36]usesagraphneuralnetworkforthe\n fort. HMR 2.0 is such a large pre-trained model which \n prediction,METRO[42]and Fast METRO[10]useatrans-\n couldpotentiallybeusefulnotjustincomputervision,but \n former, while Mesh Graphormer [43] adopts a hybrid be-\n also in robotics [54, 62, 73], computer graphics [76], bio- \n tween the two. Since we regress the SMPL model param-\n mechanics [60], and other fields where analysis of the hu- \n eters, instead of the locations of mesh vertices, we are not\n man figure and its movement from images or videos is \n directly comparable to these. However, we show how we\n needed. \n canuseafully“transformerized”designfor HMR.\n Ourcontributionscanbesummarizedasfollows: \n Human Mesh & Motion Recovery from Video. To ex-\n 1. Weproposeanend-to-end“transformerized”architec- tend Human Mesh Recovery over time, most methods use\n tureforhumanmeshrecovery,HMR 2.0. Withoutre- the basic backbone of HMR [30] and propose designs for\n lying on domain-specific designs, we outperform ex- the temporal encoder that fuses the per-frame features.\n istingapproachesfor 3 Dbodyposereconstruction. HMMR [31] uses a convolutional encoder on features ex-\n tracted from HMR [30]. VIBE [33], MEVA [48] and\n 2. Buildingon HMR 2.0,wedesign 4 DHumansthatcan TCMR [11] use a recurrent temporal encoder. DSD [71]\n jointlyreconstructandtrackhumansinvideo,achiev- combines convolutional and self-attention layers, while\n ingstate-of-the-artresultsfortracking. MAED[75]andt-HMMR[57]employatransformer-based\n temporal encoder. Baradel et al. [5, 4] also used a trans-\n 3. Weshowthatbetter 3 Dposesfrom HMR 2.0 resultin \n former for temporal pose prediction, while operating di-\n better performance on the downstream task of action \n rectly on SMPL poses. One key limitation of these ap-\n recognition, finally contributing to the state-of-the-art \n proachesisthattheyoftenoperateinscenarioswheretrack-\n result(42.3 m AP)onthe AVAbenchmark. \n ing is simple [31, 90], e.g., videos with a single person\n or minimal occlusions. In contrast to that, our complete\n 2.Related Work \n 4 DHumansapproachisalsosolvingthetrackingproblem.\n Human Mesh Recoveryfroma Single Image. Although, Tracking People in Video. Recently, there have been ap-\n there have been many approaches that estimate 3 D human proaches that demonstrate state-of-the-art performance for\n poseandshaperelyingoniterativeoptimization,e.g.,SM- trackingbyrelyingon 3 Dhumanreconstructionfrom HMR\n PLify [7] and variants [22, 38, 56, 66, 72, 85], for this models, i.e., T 3 DP [64] and PHALP [65]. In these meth-\n analysis we will focus on approaches that directly regress ods, every person detection is lifted to 3 D using an HMR\n the body shape from a single image input. In this case, network [57] and then tracking is performed using the 3 D\n the canonical example is HMR [30], which uses a CNN representationsfromlifting[64]andprediction[65]totrack\n to regress SMPL [45] parameters. Since its introduction, peopleinvideo. Empiricalresultsshowthat PHALPworks"
  },
  {
    "page_num": 3,
    "text": " HMR 2.0 Humans 4 D \n Frame t Frame t+1 \n Patchify \n Vision Transformer \n HMR 2.0 HMR 2.0 \n Input Image \n Pose \n SMPL Transformer \n MLP Shape \n Query w/ Cross Attn \n Token Camera \n Associate using \n HMR 2.0 \n \n \n \n Pose \n SMPL Transformer \n Query w/ Cross Attn MLP Shape \n Token \n Camera \n \n \n \n \n \n \n SMPL \n Query \n Token \n \n Vi T \n Multi-head \n MLP \n Cross Attention \n Pose \n Shape \n Camera \n remrofsnar T \n noisi V \n Tracking \n Frame t Frame t+1 \n HMR 2.0 HMR 2.0 \n Input Image \n Associate using pose, location, appearance\n Figure 2:Overviewofourapproach.Left:HMR 2.0 isafully“transformerized”versionofanetworkfor Human Mesh Recovery.Right:\n Weuse HMR 2.0 asthebackboneofour 4 DHumanssystem,thatbuildson PHALP[65],tojointlyreconstructandtrackhumansin 4 D.\n very well on multiple tracking benchmarks (the main re- space (e.g., joints X) can be projected to the image as\n quirementisthattheimageshaveenoughspatialresolution x=π(X)=Π(K(RX+t)),whereΠisaperspectivepro-\n to permit lifting of the people to 3 D). We use these track- jectionwithcameraintrinsics K.Sinceθalreadyincludesa\n ingpipelines,andparticularly PHALP,asatasktoevaluate globalorientation,inpracticeweassume Rasidentityand\n methodsforhumanmeshrecovery. onlypredictcameratranslationt. \n Action Recognition. Action recognition is typically per- HMR.Thegoalofthehumanmeshreconstruction(HMR)\n formed using appearance features from raw video input. task is to learn a predictor f(I) that given a single im-\n Canonicalexamplesinthiscategoryinclude Slow Fast[18] age I, reconstructs the person in the image by predicting\n and MVi T[16]. Simultaneously, thereare approaches that their 3 D pose and shape parameters. Following the typi-\n usefeaturesextractedfrombodyposeinformation,e.g.,Po- cal parametric approaches [30, 35], we model f to predict\n Tion[12]and JMRN[68]. Arecentapproach,LART[63], Θ = [θ,β,π] = f(I) where θ and β are the SMPL pose\n demonstratesstate-of-the-artperformanceforactionrecog- andshapeparametersandπisthecameratranslation.\n nitionbyfusingvideo-basedfeatureswithfeaturesfrom 3 D \n humanposeestimates. Weusethepipelineofthisapproach 3.2.Architecture \n andemployactionrecognitionasadownstreamtasktoeval- \n We re-imagine HMR [30] as an end-to-end transformer\n uatehumanmeshrecoverymethods. \n architecture that uses no domain specific design choices.\n Yet, it outperforms all existing approaches that have heav-\n 3.Reconstructing People \n ilycustomizedarchitecturesandelaboratedesigndecisions.\n Asshownin Figure 2,weuse(i)a Vi T[15]toextractimage\n 3.1.Preliminaries \n tokens, and (ii) a standard transformer decoder that cross-\n Body Model. The SMPLmodel[46]isalow-dimensional attendstoimagetokenstooutputΘ. \n parametricmodelofthehumanbody. Giveninputparame- Vi T. The Vision Transformer, or Vi T [15] is a trans-\n tersforpose(θ R 24×3×3)andshape(β R 10),itoutputs former [74] that has been modified to operate on an im-\n a mesh M \n R∈ \n 3×N with N = 6890 ve \n ∈ \n rtices. The body age. The input image is first patchified into input tokens\n joints X \n ∈R 3×k \n are defined as a linear combination of and passed through the transformer to get output tokens.\n ∈ \n theverticesandcanbecomputedas X = MW withfixed The output tokens are then passed to the transformer de-\n weights W RN×k. Notethatposeparametersθ include coder. Weusea Vi T-H/16,the“Huge”variantwith 16 16\n thebodypos ∈ eparametersθ b R 23×3×3 andtheglobalori- inputpatchsize. Pleasesee Sup Matformoredetails. ×\n entationθ g R 3×3. ∈ Transformer decoder. We use a standard transformer de-\n ∈ \n Camera. We use a perspective camera model with fixed coder[74]withmulti-headself-attention.Itprocessesasin-\n focal length and intrinsics K. Each camera π = (R,t) gle(zero)inputtokenbycross-attendingtotheoutputimage\n consists of a global orientation R R 3×3 and transla- tokensandendswithalinearreadoutofΘ. Wefollow[35]\n tion t R 3. Given these parameters ∈ , points in the SMPL andregress 3 Drotationsusingtherepresentationof[91].\n ∈ "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n 3.3.Losses \n Followingbestpracticesinthe HMRliterature[30,35], \n we train our predictor f with a combination of 2 D losses, \n 3 Dlosses, andadiscriminator. Sincewetrainwithamix- Pose Predictor \n tureofdatasets,eachhavingdifferentkindsofannotations, \n weemployasubsetoftheselossesforeachimageinamini- \n batch. We use the same losses even with pseudo-ground \n Mask Mask Mask \n truth annotations. Given an input image I, the model pre- Token Token Token \n dicts Θ = [θ,β,π] = f(I). Whenever we have access to \n the ground-truth SMPL pose parameters θ∗ and shape pa- \n rameters β∗, we bootstrap the model predictions using an Past Future \n MSEloss: Figure 3: Pose prediction: We train a BERT-style [13] trans-\n formermodelonover 1 milliontracksobtainedfrom [63].Thisal-\n L smpl = || θ − θ∗ || 2 2 + || β − β∗ || 2 2 . lowustomakefuturepredictionsandamodalcompletionofmiss-\n ingdetectionsusingthesamemodel.Topredictfutureposes(t+1,\n Whentheimagehasaccurateground-truth 3 Dkeypointan- t+2,...),wequerythemodelwithamask-tokenusingcorrespond-\n notations X∗, we additionally supervise the predicted 3 D ingpositionalembeddings. Similarlyforamodalcompletion,we\n keypoints X withan L 1 loss: replacemissingdetectionswithamaskedtoken.\n = X X∗ . \n L kp 3 D || − || 1 “lift” them to 3 D, extracting their 3 D pose, location in 3 D\n When the image has 2 D keypoints annotations x∗, we su- space(derivedfromtheestimatedcamera),and 3 Dappear-\n ance (derived from the texture map). A tracklet represen-\n pervise projections of predicted 3 D keypoints π(X) using \n tation is incrementally built up for each individual person\n an L 1 loss: \n = π(X) x∗ . over time. The recursion step is to predict for each track-\n kp 2 D 1 \n L || − || let, the pose, location and appearance of the person in the\n Furthermore, we want to ensure that our model predicts next frame, all in 3 D, and then find best matches between\n valid 3 Dposesandusetheadversarialpriorin HMR[30].It thesetop-downpredictionsandthebottom-updetectionsof\n factorizesthemodelparametersinto: (i)bodyposeparam- peopleinthatframeafterliftingthemto 3 D.Thestaterep-\n eters θ b , (ii) shape parameters β, and (iii) per-part relative resented by each tracklet is then updated by the incoming\n rotations θ i , which is one 3 D rotation for each of the 23 observation, and the process is iterated. It is possible to\n jointsofthe SMPLmodel. Wetrainadiscriminator D k for trackthroughocclusionsbecausethe 3 Drepresentationofa\n eachfactorofthebodymodel,andthegeneratorlosscanbe trackletcontinuestobeupdatedbasedonpasthistory.\n expressedas: \n Webelievethatarobustposepredictorshouldalsoper-\n = (cid:88) (D (θ ,β) 1)2. formwell,whenevaluatedonthisdownstreamtaskoftrack-\n adv k b \n L − ing,soweusethetrackingmetricsasaproxytoevaluatethe\n k \n qualityof 3 Dreconstructions. Butfirstweneededtomod-\n 3.4.Pseudo-Ground Truthfitting ify the PHALP framework to allow for fair comparison of\n differentposepredictionmodels. Originally, PHALPused\n We scale to unlabelled datasets (i.e., Insta Variety [31], \n posefeaturesbasedonthelastlayerofthe HMRnetwork,\n AVA [21], AI Challenger [78]) by computing pseudo- \n i.e., a 2048-dimensional embedding space. This limits the\n ground truth annotations. Given any image, we first use \n ability of PHALP to be used with different pose models\n anoff-the-shelfdetector[40]andabodykeypointsestima- \n (e.g., HMR 2.0, PARE, Py MAF etc.). To create a more\n tor[81]togetboundingboxesandcorresponding 2 Dkey- \n genericversionof PHALP,weperformthemodificationof\n points. We then fit a SMPL mesh to these 2 D keypoints \n representing pose in terms of SMPL pose parameters, and\n using Pro HMR [37] to get pseudo-ground truth SMPL pa- \n weaccordinglyoptimizethe PHALPcostfunctiontoutilize\n rametersθ∗andβ∗withcameraπ∗. \n the new pose distance. Similarly, we adapt the pose pre-\n dictor to operate on the space of SMPL parameters. More\n 4.Tracking People \n specifically, we train a vanilla transformer model [74] by\n Invideoswithmultiplepeople,weneedtheabilitytoas- masking random pose tokens as shown in the Fig 3. This\n sociate people across time, i.e., perform tracking. For this allowsustopredictfutureposesintime,aswellasamodal\n webuildupon PHALP[65],astate-of-the-arttrackerbased completionofmissingdetections.Withthesemodifications,\n on features derived from HMR-style 3 D reconstructions. wecanpluginanymeshrecoverymethodsandrunthemon\n Thebasicideaistodetectpeopleinindividualframes,and anyvideos. Wecallthismodifiedversion PHALP′."
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n 4 DHumans. Ourfinaltrackingsystem,4 DHumans,usesa 3 DPW Human 3.6 M \n sampling-basedparameter-freeappearanceheadandanew Method \n MPJPEPA-MPJPEMPJPEPA-MPJPE \n posepredictor(Figure 3). Tomodelappearance,wetexture \n visiblepointsonthemeshbyprojectingthemontotheinput \n imageandsamplingcolorfromthecorrespondingpixels. \n Totrackpeopleinvideos,previousapproachesreliedon \n off-the-shelf tracking approaches and used their output to \n reconstructhumansinvideos(e.g.,taketheboundingboxes \n fromtrackingoutputandreconstructpeople). Forexample, \n PHD[90],HMMR[31]canrunonvideoswithonlysingle \n person in the scene. In this work, we combine reconstruc- \n tionandtrackingintoasinglesystemandshowthatbetter \n posereconstructionsresultinbettertrackingandthiscom- \n binedsystemcannowrunonanyvideosinthewild. \n 5.Experiments \n Inthissection,weevaluateourreconstructionandtrack- \n ingsystemqualitativelyandquantitatively. First, weshow \n that HMR 2.0 outperforms previous methods on standard \n 2 Dand 3 Dposeaccuracymetrics(Section 5.2).Second,we \n show 4 DHumans is a versatile tracker, achieving state-of- \n the-artperformance(Section 5.3).Third,wefurtherdemon- \n strate the robustness and accuracy of our recovered poses \n viasuperiorperformanceonthedownstreamapplicationof \n actionrecognition(Section 5.4). Finally,wediscusstheex- \n perimentalinvestigationwhendesigning HMR 2.0 andab- \n lateaseriesofdesignchoices(Section 5.5). \n 5.1.Setup \n Datasets. Following previous work, we use the typi- \n cal datasets for training, i.e., Human 3.6 M [27], MPI-INF- \n 3 DHP[49],COCO[44]and MPII[2]. Additionally,weuse \n Insta Variety[31],AVA[21]and AIChallenger[78]asextra \n datawherewegeneratepseudo-groundtruthfits. \n Baselines. Wereport performanceonbenchmarksthat we \n can compare with many previous works (Section 5.2), but \n we also perform a more detailed comparison with recent \n state-of-the-art methods, i.e., Py MAF [89], CLIFF [41], \n HMAR[65],PARE[34],and Py MAF-X[88]. Forfairness, \n weonlyevaluatethebody-onlyperformanceof Py MAF-X. \n 5.2.Pose Accuracy \n 3 D Metrics. For 3 D pose accuracy, we follow the typical \n protocols of prior work, e.g., [35], and we present results \n on the 3 DPW test split and on the Human 3.6 M val split, \n reporting MPJPE, and PA-MPJPE in Table 1. Please no- \n ticethatweonlycomparewithmethodsthatdonotusethe \n trainingsetof 3 DPWfortraining,similartous.Weobserve \n that with our HMR 2.0 a model, which trains only on the \n typical datasets, we can outperform all previous baselines \n across all metrics. However, we believe that these bench- \n marks are very saturated and these smaller differences in \n pose metrics tend to not be very significant. In fact, we \n laropme T \n Kanazawaetal.[31] 116.5 72.6 - 56.9 \n Doerschetal.[14] - 74.7 - - \n Arnabetal.[3] - 72.2 77.8 54.3 \n DSD[71] - 69.5 59.1 42.4 \n VIBE[33] 93.5 56.5 65.9 41.5 \n desab-emar F \n Pavlakosetal.[59] - - - 75.9 \n HMR[30] 130.0 76.7 88.0 56.8 \n NBF[53] - - 59.9 \n Graph CMR[36] - 70.2 - 50.1 \n Holo Pose[23] - - 60.3 46.5 \n Dense Ra C[82] - - 76.8 48.0 \n SPIN[35] 96.9 59.2 62.5 41.1 \n Deco MR[86] - 61.7† - 39.3† \n Da Net[87] - 56.9 61.5 48.6 \n Songetal.[69] - 55.9 - 56.4 \n I 2 L-Mesh Net[51] 100.0 60.0 55.7† 41.1†\n HKMR[20] - - 59.6 43.2 \n Py MAF[89] 92.8 58.9 57.7 40.5 \n PARE[34] 82.0 50.9 76.8 50.6 \n Py MAF-X[88] 78.0 47.1 54.2 37.2 \n HMR 2.0 a 70.0 44.5 44.8 33.6 \n HMR 2.0 b 81.3 54.3 50.0 32.4 \n Table 1:Reconstructionsevaluatedin 3 D:Reconstructionerrors\n (in mm) on the 3 DPW and Human 3.6 M datasets. † denotes the\n numbers evaluated on non-parametric results. Lower is better.\n ↓ \n Pleaseseethetextfordetails. \n LSP-Extended COCO Pose Track \n Method \n @0.05 @0.1 @0.05@0.1 @0.05@0.1 \n Py MAF[89] - - 0.68 0.86 0.77 0.92 \n CLIFF[41] 0.32 0.66 0.64 0.88 0.75 0.92 \n PARE[34] 0.27 0.60 0.72 0.91 0.79 0.93 \n Py MAF-X[88] - - 0.79 0.93 0.85 0.95 \n HMR 2.0 a 0.38 0.72 0.79 0.95 0.86 0.97 \n HMR 2.0 b 0.53 0.82 0.86 0.96 0.90 0.98 \n Table 2: Reconstructions evaluated in 2 D. PCK scores of pro-\n jected keypoints at different thresholds on the LSP-Extended,\n COCO,and Pose Trackdatasets.Higher isbetter.\n ↑ \n observethatbyasmallcompromiseoftheperformanceon\n 3 DPW, our HMR 2.0 b model, which trains for longer on\n more data (AVA [21], AI Challenger [78], and Insta Vari-\n ety [31]), achieves results that perform better on more un-\n usual poses than what can be found in Human 3.6 M and\n 3 DPW.Weobservethisqualitativelyandfromperformance\n evaluatedon 2 Dposereprojection(Table 2). Furthermore,\n weobservethat HMR 2.0 bisamorerobustmodelanduse\n itforevaluationintherestofthepaper. \n 2 D Metrics. We evaluate 2 D imagealignment of thegen-\n erated poses by reporting PCK of reprojected keypoints\n at different thresholds on LSP-Extended [28], COCO val-\n idation set [44], and Posetrack validation set [1]. Since"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n Py MAF(-X)[89,88]weretrainedusing LSP-Extended,we \n Posetrack \n do not report numbers for that part of the table. Notice in Tracker Pose Engine\n HOTA IDs MOTA IDF 1 \n Table 2, that HMR 2.0 b consistently outperforms all pre- ↑ ↓ ↑ ↑ \n PARE[34] 53.6 510 59.4 76.8 \n vious approaches. On LSP-Extended, which contains un- \n Py MAF-X[88] 53.7 472 59.2 76.9 \n usualposes,HMR 2.0 bachieves PCK@0.05 of 0.53,which \n CLIFF[41] 53.5 551 58.7 76.5 \n is 1.6 betterthanthesecondbest(CLIFF)with 0.32. For PHALP′ \n × Py MAF[89] 53.0 623 58.6 76.1 \n PCK@0.05 on easier datasets like COCO and Pose Track \n HMAR[65] 53.6 482 59.3 77.1 \n with less extreme poses, HMR 2.0 b still outperforms the \n HMR 2.0 54.1 456 59.4 77.4 \n second-bestapproachesbutbynarrowermarginsof 9%and \n 6%respectively. HMR 2.0 aalsooutperformsallbaselines, \n butisworsethan HMR 2.0 b,especiallyonharderposesin \n Table 3: Tracking with different 3 D pose estimators. With\n LSP-Extended. \n themodificationsof PHALP′,wehaveaversatiletrackerthatal-\n Qualitative Results. We show qualitative results of \n lowsdifferent 3 Dposeestimatorstobepluggedintoit.HMR 2.0,\n HMR 2.0 in Figure 4. We are robust to extreme poses \n PARE,and Py MAF-Xperformthebestinthissetting.\n andpartialocclusions.Ourreconstructionsarewell-aligned \n withtheimageandarevalidwhenseenfromanovelview. \n Moreover,wecomparewithourclosestcompetitorsin Fig- Posetrack \n Method \n ure 5. We observe that Py MAF-X and particularly PARE HOTA IDs MOTA IDF 1 \n ↑ ↓ ↑ ↑ \n oftenstrugglewithmoreunusualposes,while HMR 2.0 re- \n Trackformer[50] 46.7 1263 33.7 64.0 \n turnsmorefaithfulreconstructions. \n Tracktor[6] 38.5 702 42.4 65.2 \n 5.3.Tracking Alpha Pose[17] 37.6 2220 36.9 66.9 \n Pose Flow[79] 38.0 1047 15.4 64.2 \n For tracking, we first demonstrate the versatility of T 3 DP[64] 50.6 655 55.8 73.4\n the modifications introduced by PHALP′, which allow us PHALP[65] 52.9 541 58.9 76.4\n to evaluate 3 D pose estimators on the downstream task 4 DHumans 54.3 421 59.8 77.9\n of tracking. Then, we evaluate our complete system, \n 4 DHumans+Vi TDet 57.8 382 61.4 79.1 \n 4 DHumans,withrespecttothestateoftheart. \n Evaluation Setting. Following previous work [64, 65], \n wereportresultsbasedon IDs(IDswitches), MOTA[32], \n Table 4:Comparisonof 4 DHumanswiththestateofthearton\n IDF 1 [67], and HOTA [47] on the Posetrack validation set the Posetrackdataset. 4 DHumansachievestate-of-the-arttrack-\n using the protocol of [65], with detections from Mask R- ingperformanceforallmetrics. Incorporatingabetterdetection\n CNN[25]. system[40]leadstofurtherperformanceimprovements.\n Versatility of PHALP′. With the modifications of \n PHALP′, we abandon the model-specific latent space \n mightstillbelessaccurate(asobservedfrom Table 2). See\n of [65] and instead, we operate in the SMPL space, which \n also Figure 5 and Sup Matformorequalitativecomparisons.\n is shared across most mesh recovery systems. This makes \n PHALP′ more versatile and allows us to plug in different 4 DHumans. Table 4 evaluates tracking performance\n 3 D pose estimators and compare them based on their per- of our complete system, 4 DHumans, on the Pose Track\n formanceonthedownstreamtaskoftracking. Weperform dataset. Usingthesameboundingboxdetectoras[64,65],\n this comparison in Table 3 where we use pose and loca- 4 DHumansoutperformsexistingapproachesonallmetrics,\n tion cues from state-of-the-art 3 D pose estimators (while improving ID Switches by 22%. Using the improved Vi T-\n stillusingappearancefrom HMAR[65]). Weobservethat Detdetector[40]canimproveperformancefurther.Asaby-\n HMR 2.0 performs the best and PARE [34], HMAR [65], productofourtemporalpredictionmodel(Figure 3),wecan\n and Py MAF-X[88]closelyfollowonthe Posetrackdataset, performamodalcompletionandattributeaposetomissing\n withminordifferencesbetweenthem. Notethattrackingis detections. Weshowexamplesofthisinthe Sup Mat.\n often most susceptible to errors in predicted 3 D locations \n 5.4.Action Recognition \n withbodyposehavingasmallereffectinperformance[65]. \n This means that good tracking performance can indicate Evaluationsetting. Theapproachof[63]isthestateofthe\n robustness to occlusions, so it is helpful to consider this artforactionrecognitioninvideos. Givenavideoasinput,\n metric, butitislesshelpfultodistinguishfine-graineddif- the authors propose using per-frame 3 D pose and location\n ferences in pose. As a result, the competitive results of estimates(usingoff-the-shelf HMRmodels[65])asanad-\n PARE[34], HMAR[65], and Py MAF-X[88]indicatethat ditionalfeatureforpredictingactionlabels. Theyalsoshow\n theyhandleocclusionsgracefully,buttheirposeestimation resultsfora“pose-only”baselinethatpredictsactionlabels"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n Input Front view Side view Top view Input Front view Side view Top view \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 4: Qualitativeevaluationof HMR 2.0. Foreachexampleweshow: a)theinputimage,b)thereconstructionoverlay,c)aside\n view,d)thetopview. Todemonstratetherobustnessof HMR 2.0,wevisualizeresultsforavarietyofsettings-forunusualposes(rows\n 1-4),forunusualviewpoints(row 5)andforimageswithpoorvisibility,extremetruncationsandextremeocclusions(rows 6-8).\n \n \n using only 3 D pose and location estimates. We use this Comparisons. Comparing results in Table 5, we observe\n setting to compare our model with baselines on the down- that HMR 2.0 outperforms baselines on the different class\n stream task of action recognition on the AVA dataset [21]. categories (OM, PI, PM) and overall. It achieves an m AP\n In [63], the authors train a transformer that takes SMPL of 22.3 on the AVA test set, which is 14% better than the\n poses as input and predicts action labels. Following their second-best baseline. Since accurate action recognition\n setup, we train a separate action classification transformer fromposesneedsfine-grainedposeestimation,thisisstrong\n foreachbaseline. evidence that HMR 2.0 predicts more accurate poses than\n \n \n \n \n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \nFull Image \n Input Py MAF-X PARE HMR 2.0 \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 5:Qualitativecomparisonofstate-of-the-artmeshrecoverymethods.HMR 2.0 returnsmorefaithfulreconstructionsforunusual\n posescomparedtotheclosestcompetitors,Py MAF-X[88]and PARE[34]. \n \n 0 240 1304 1312 1325 \n \n \n \n 0 19 51 65 122 \n \n \n \n 0 44 55 63 170 \n 0 \n \n \n \n Figure 6: Qualitativetrackingresultsof 4 DHumans. Weuseheadmasks(framenumberisonthetopleft). Firstrow: Wetrackpeople\n skatingonicewithchallengingposesandheavyocclusions,inaminutelongvideowithoutswitchingidentities. Secondrow: Themain\n personistrackedthroughmultipleinteractionswithotherplayers.Thirdrow:Thepersonofinterestistrackedthroughlongocclusions.\n \n \n existing approaches. In fact, when combined with appear-\n Action Pose \n OM PI PM m AP ance features, [63] shows that HMR 2.0 achieves the state\n Model Engine \n oftheartof 42.3 m APon AVAactionrecognition,whichis\n Py MAF[89] 7.3 16.9 34.7 15.4 \n 7%betterthanthesecond-bestof 39.5 m AP. \n CLIFF[41] 9.2 20.0 40.3 18.6 \n HMAR[65] 8.7 20.1 40.3 18.3 \n [63] 5.5.HMR 2.0 Model Design \n PARE[34] 9.2 20.7 41.5 19.1 \n Py MAF-X[88] 10.2 21.4 40.8 19.6 In the process of developing HMR 2.0, we investigated\n HMR 2.0 11.9 24.6 45.8 22.3 aseriesofdesigndecisions. Figure 7 brieflyillustratesthis\n exploration. We experimented with over 100 settings and\n we visualize the performance of 100 checkpoints for each\n Table 5: Action recognition results on the AVA dataset. We run. Forthevisualization,weusetheperformanceofeach\n benchmark different mesh recovery methods on the downstream checkpointonthe 3 DPWandthe LSP-Extendeddataset.\n taskofpose-basedactionrecognition. Here,OM:Object Manip- \n Our investigation focused on some specific aspects of\n ulation,PI:Person Interactions,and PM:Person Movement. \n themodel,whichwedocumenthereasaseriesof“lessons\n learnt”forfutureresearch. Inthefollowingparagraphs,we\n willregularlyreferto Table 6,whichevaluatestheseaspects"
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n \n K \n @0.05 0.6 \n Models MPJPE \n 3 D \n PA \n PW \n -MPJPEMP \n H \n JP \n u \n E \n m \n P \n a \n A \n n 3 \n - \n . \n M \n 6 M \n PJPEPCK \n L \n @ \n SP \n 0 \n - \n .0 \n E \n 5 \n xt \n P \n e \n C \n nd \n K \n e \n @ \n d \n 0.1 \n C \n P-extended - \n P \n 0 \n 0 \n . \n . \n 4 \n 5 \n B \n H \n B \n 2 \n 1 \n MR 2.0 b 8 \n 8 \n 7 \n 1 \n 5 \n 9 \n . \n . \n . \n 3 \n 2 \n 7 \n 5 \n 5 \n 5 \n 4 \n 6 \n 3 \n . \n . \n . \n 3 \n 8 \n 4 \n 5 \n 5 \n 5 \n 0 \n 8 \n 1 \n . \n . \n . \n 0 \n 9 \n 4 \n 3 \n 4 \n 3 \n 2 \n 1 \n 4 \n . \n . \n . \n 4 \n 4 \n 4 \n 0 \n 0 \n 0 \n . \n . \n . \n 5 \n 3 \n 4 \n 3 \n 5 \n 8 \n 0 \n 0 \n 0 \n . \n . \n . \n 8 \n 6 \n 8 \n 2 \n 6 \n 1 \n S \n L 46 48 50 52 54 56 D 1 84.1 54.8 54.5 35.1 0.45 0.79 \n 3 DPW - PA-MPJPE (in mm) \n D 2 80.2 53.3 52.4 34.9 0.46 0.79 \n P 1 98.9 61.7 89.9 58.7 0.24 0.52 \n Figure 7: Extensivemodelsearch. Witheachdot, wevisualize P 2 82.7 55.6 49.3 32.4 0.52 0.81\n the performance of a checkpoint when evaluated on 3 DPW and \n LSP-Extended. Colorsindicatedifferentruns. Weexploremore Table 6:Ablations:Evaluationfordifferentmodeldesignsonthe\n than 100 settings,andvisualize 100 checkpointsfromeachrun. 3 DPW,Human 3.6 M,and LSP-Extendeddatasets.\n ∼ \n outlowqualitypseudo-groundtruthfits(highfittingerror)\n on 3 Dand 2 Dmetrics,usingthe 3 DPW,Human 3.6 M,and \n andpruneimageswithlow-confidence 2 Ddetections.\n LSP-Extendeddatasets. \n Effect of backbone. Unlike the majority of the previous \n 6.Conclusion \n work on Human Mesh Recovery that uses a Res Net back- \n bone, our HMR 2.0 methodreliesona Vi Tbackbone. For Westudytheproblemofreconstructingandtrackinghu-\n a direct comparison of the effect of the backbone, Model mansfromimagesandvideo. First,wepropose HMR 2.0,\n B 1 implements HMR with a Res Net-50 backbone and an afully“transformerized”versionofanetworkfortheprob-\n MLP-based head implementing IEF (Iterative Error Feed- lem of Human Mesh Recovery [30]. HMR 2.0 achieves\n back [9, 30]). In contrast, Model B 2 uses a transformer strongperformanceontheusual 2 D/3 Dposemetrics,while\n backbone(Vi T-H)whilekeepingtheotherdesigndecisions alsoactingasthebackboneforourimprovedvideotracker.\n the same. By updating the backbone, we observe a signif- Thefullsystem,4 DHumans,jointlyreconstructsandtracks\n icant improvement across the 3 D and 2 D metrics, which people in video and achieves state-of-the-art results for\n justifiesthe“transformerization”step. tracking. To further illustrate the benefit of our 3 D pose\n Effect of training data. Besides the architecture, we also estimator,HMR 2.0,weapplyittothetaskofactionrecog-\n investigated the effect of training data. Model D 1 trains nition, where we demonstrate strong improvements upon\n on the typical datasets (H 3.6 M, MPII, COCO, MPI-INF) previouspose-basedbaselines.\n that most of the previous works are leveraging. In com- Ourworkpushestheboundaryofthevideosthatcanbe\n parison, model D 2 adds AVA in the training set, follow- analyzedwithtechniquesfor 3 Dhumanreconstruction. At\n ing[21]. Eventually,wealsotrainusing AI-Challengerand the same time, the improved results also demonstrate the\n Insta-Variety(model B 2),tofurtherexpandthetrainingset. type of limitations that need to be addressed in the future.\n Aswecansee,addingmoretrainingdataleadstoimprove- Forexample, theuseofthe SMPLmodel[45]createscer-\n mentsacrosstheboardforthereportedmetrics,buttheben- tainlimitations,andleveragingimprovedmodelswouldal-\n efitissmallercomparedtothebackboneupdate. low us to model hand pose and facial expressions [56], or\n Vi T pretraining. Another factor that had significant ef- even capture greater age variation, e.g., infants [26] and\n fect on the performance of our model was the pretraining kids[55,70]. Moreover,sinceweconsidereachpersonin-\n of the Vi T backbone. Starting with randomly initialized dependently,ourreconstructionsarelesssuccessfulatcap-\n weights (model P 1) results in slow convergence and poor turing the fine-grained nature of people in close proxim-\n performance. Resultsimproveifourbackboneispretrained ity,e.g.,contact[19,52]. Besidesthis,ourreconstructions\n with MAE [24] on Imagenet (P 2). Eventually, our model “live” in the camera frame, so for proper understanding of\n ofchoice(HMR 2.0 b), whichisfirstpretrainedwith MAE the action in a video, we need to consider everyone in a\n on Image Net and then on the task of 2 D keypoint predic- common world coordinate frame, by reasoning about the\n tion[81],achievesthebestperformance. camera motion too [58, 83, 84]. Finally, lower input reso-\n SMPLhead. Wealsoinvestigatetheeffectofthearchitec- lution can affect the quality of our reconstructions, which\n ture for the head that predicts the SMPL parameters. Our couldbeaddressedbyresolutionaugmentations[80].\n proposed transformer decoder (HMR 2.0 b) improves per- \n Acknowledgements Wethankmembersofthe BAIRcom-\n formancewhenitcomestotheimage-modelalignment(i.e., \n munity for helpful discussions and Stability AI for their\n 2 D metrics) compared to the traditional MLP-based head \n generous compute grant. This work was supported by\n with IEFsteps(B 2). \n BAIR/BDD sponsors, ONR MURI (N 00014-21-1-2801),\n Datasetquality. Similartopreviouswork,e.g.,[35],itwas andthe DARPAMCSprogram. \n crucialtokeepthequalityofthetrainingdatahigh;wefilter "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n References [17] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.\n RMPE: Regional multi-person pose estimation. In ICCV,\n [1] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, \n 2017. \n Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt \n [18] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\n Schiele. Pose Track: Abenchmarkforhumanposeestima- \n Kaiming He. Slowfast networks for video recognition. In\n tionandtracking. In CVPR,2018. \n ICCV,2019. \n [2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and \n [19] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut\n Bernt Schiele. 2 Dhumanposeestimation: Newbenchmark \n Popa, Vlad Olaru, and Cristian Sminchisescu. Three-\n andstateoftheartanalysis. In CVPR,2014. \n dimensionalreconstructionofhumaninteractions.In CVPR,\n [3] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Ex- \n 2020. \n ploitingtemporalcontextfor 3 Dhumanposeestimationin \n [20] Georgios Georgakis,Ren Li,Srikrishna Karanam,Terrence\n thewild. In CVPR,2019. \n Chen,Jana Kosˇecka´,and Ziyan Wu. Hierarchicalkinematic\n [4] Fabien Baradel,Romain Bre´gier,Thibault Groueix,Philippe \n humanmeshrecovery. In ECCV,2020. \n Weinzaepfel,Yannis Kalantidis,and Gre´gory Rogez. Pose- \n [21] Chunhui Gu, Chen Sun, David A Ross, Carl Von-\n BERT: A generic transformer module for temporal 3 D hu- \n drick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\n manmodeling. PAMI,2022. \n narasimhan, George Toderici, Susanna Ricco, Rahul Suk-\n [5] Fabien Baradel, Thibault Groueix, Philippe Weinzaepfel, thankar,Cordelia Schmid,and Jitendra Malik.AVA:Avideo\n Romain Bre´gier, Yannis Kalantidis, and Gre´gory Rogez. datasetofspatio-temporallylocalizedatomicvisualactions.\n Leveraging Mo Capdataforhumanmeshrecovery. In 3 DV, In CVPR,2018. \n 2021. \n [22] Peng Guan, Alexander Weiss, Alexandru O Ba˘lan, and\n [6] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Michael J Black. Estimating human shape and pose from\n Trackingwithoutbellsandwhistles. In ICCV,2019. asingleimage. In ICCV,2009. \n [7] Federica Bogo,Angjoo Kanazawa,Christoph Lassner,Peter [23] Riza Alp Gulerand Iasonas Kokkinos. Holo Pose: Holistic\n Gehler,Javier Romero,and Michael JBlack.Keepit SMPL: 3 Dhumanreconstructionin-the-wild. In CVPR,2019.\n Automatic estimation of 3 D human pose and shape from a [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\n singleimage. In ECCV,2016. Dolla´r,and Ross Girshick.Maskedautoencodersarescalable\n [8] Nicolas Carion,Francisco Massa,Gabriel Synnaeve,Nicolas visionlearners. In CVPR,2022.\n Usunier,Alexander Kirillov,and Sergey Zagoruyko.End-to- [25] Kaiming He,Georgia Gkioxari,Piotr Dolla´r,and Ross Gir-\n endobjectdetectionwithtransformers. In ECCV,2020. shick. Mask R-CNN. In ICCV,2017.\n [9] Joao Carreira,Pulkit Agrawal,Katerina Fragkiadaki,and Ji- [26] Nikolas Hesse, Sergi Pujades, Michael J Black, Michael\n tendra Malik. Human pose estimation with iterative error Arens, Ulrich G Hofmann, and A Sebastian Schroeder.\n feedback. In CVPR,2016. Learningandtrackingthe 3 Dbodyshapeoffreelymoving\n [10] Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Cross- infantsfrom RGB-Dsequences. PAMI,2019.\n attentionofdisentangledmodalitiesfor 3 Dhumanmeshre- [27] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\n coverywithtransformers. In ECCV,2022. Sminchisescu.Human 3.6 M:Largescaledatasetsandpredic-\n [11] Hongsuk Choi,Gyeongsik Moon,Ju Yong Chang,and Ky- tivemethodsfor 3 Dhumansensinginnaturalenvironments.\n oung Mu Lee. Beyond static features for temporally con- PAMI,2013. \n sistent 3 Dhumanposeandshapefromavideo. In CVPR, [28] Sam Johnsonand Mark Everingham. Learningeffectivehu-\n 2021. manposeestimationfrominaccurateannotation. In CVPR,\n [12] Vasileios Choutas, Philippe Weinzaepfel, Je´roˆme Revaud, 2011. \n and Cordelia Schmid. Po Tion: Posemotionrepresentation [29] Hanbyul Joo,Natalia Neverova,and Andrea Vedaldi.Exem-\n foractionrecognition. In CVPR,2018. plarfine-tuningfor 3 Dhumanmodelfittingtowardsin-the-\n [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina wild 3 Dhumanposeestimation. In 3 DV,2021.\n Toutanova. Bert: Pre-training of deep bidirectional [30] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\n transformers for language understanding. ar Xiv preprint Jitendra Malik. End-to-end recovery of human shape and\n ar Xiv:1810.04805,2018. pose. In CVPR,2018. \n [14] Carl Doersch and Andrew Zisserman. Sim 2 real transfer [31] Angjoo Kanazawa,Jason YZhang,Panna Felsen,and Jiten-\n learningfor 3 Dhumanposeestimation: Motiontotheres- dra Malik. Learning 3 D human dynamics from video. In\n cue. Neur IPS,2019. CVPR,2019. \n [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [32] Rangachar Kasturi, Dmitry Goldgof, Padmanabhan\n Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Soundararajan, Vasant Manohar, John Garofolo, Rachel\n Mostafa Dehghani,Matthias Minderer,Georg Heigold,Syl- Bowers, Matthew Boonstra, Valentina Korzhova, and Jing\n vain Gelly,Jakob Uszkoreit,and Neil Houlsby. Animageis Zhang. Frameworkforperformanceevaluationofface,text,\n worth 16 x 16 words: Transformersforimagerecognitionat andvehicledetectionandtrackinginvideo: Data, metrics,\n scale. ICLR,2021. andprotocol. PAMI,2008. \n [16] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, [33] Muhammed Kocabas, Nikos Athanasiou, and Michael J\n Zhicheng Yan,Jitendra Malik,and Christoph Feichtenhofer. Black. VIBE: Video inference for human body pose and\n Multiscalevisiontransformers. In ICCV,2021. shapeestimation. In CVPR,2020. "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n [34] Muhammed Kocabas, Chun-Hao PHuang, Otmar Hilliges, [51] Gyeongsik Moon and Kyoung Mu Lee. I 2 L-Mesh Net:\n and Michael JBlack. PARE:Partattentionregressorfor 3 D Image-to-lixel prediction network for accurate 3 D human\n humanbodyestimation. In ICCV,2021. pose and mesh estimation from a single RGB image. In\n [35] Nikos Kolotouros,Georgios Pavlakos,Michael JBlack,and ECCV,2020. \n Kostas Daniilidis. Learningto reconstruct 3 Dhuman pose [52] Lea Mu¨ller, Vickie Ye, Georgios Pavlakos, Michael Black,\n andshapeviamodel-fittingintheloop. In ICCV,2019. and Angjoo Kanazawa. Generative proxemics: A prior\n [36] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani- for 3 D social interaction from images. ar Xiv preprint\n ilidis. Convolutional mesh regression for single-image hu- ar Xiv:2306.09337,2023.\n manshapereconstruction. In CVPR,2019. [53] Mohamed Omran,Christoph Lassner,Gerard Pons-Moll,Pe-\n [37] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, ter Gehler,and Bernt Schiele. Neuralbodyfitting:Unifying\n and Kostas Daniilidis. Probabilistic modeling for human deeplearningandmodelbasedhumanposeandshapeesti-\n meshrecovery. In ICCV,2021. mation. In 3 DV,2018. \n [38] Christoph Lassner, Javier Romero, Martin Kiefel, Federica [54] Austin Patel,Andrew Wang,Ilija Radosavovic,and Jitendra\n Bogo,Michael JBlack,and Peter VGehler. Unitethepeo- Malik. Learningtoimitateobjectinteractionsfrominternet\n ple:Closingtheloopbetween 3 Dand 2 Dhumanrepresenta- videos. ar Xivpreprintar Xiv:2211.13225,2022.\n tions. In CVPR,2017. [55] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch,\n [39] Vincent Leroy, Philippe Weinzaepfel, Romain Bre´gier, David THoffmann,Shashank Tripathi,and Michael JBlack.\n Hadrien Combaluzier,and Gre´gory Rogez. SMPLybench- AGORA: Avatars in geography optimized for regression\n marking 3 D human pose estimation in the wild. In 3 DV, analysis. In CVPR,2021.\n 2020. [56] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\n [40] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\n Exploringplainvisiontransformerbackbonesforobjectde- Michael JBlack. Expressivebodycapture: 3 Dhands,face,\n tection. In ECCV,2022. andbodyfromasingleimage. In CVPR,2019. \n [41] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, [57] Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa.\n and Youliang Yan. CLIFF:Carryinglocationinformationin Humanmeshrecoveryfrommultipleshots. In CVPR,2022.\n fullframesintohumanposeandshapeestimation.In ECCV, [58] Georgios Pavlakos, Ethan Weber, Matthew Tancik, and\n 2022. Angjoo Kanazawa. The one where they reconstructed 3 D\n [42] Kevin Lin,Lijuan Wang,and Zicheng Liu. End-to-endhu- humansandenvironmentsin TVshows. In ECCV,2022.\n man pose and mesh reconstruction with transformers. In [59] Georgios Pavlakos,Luyang Zhu,Xiaowei Zhou,and Kostas\n CVPR,2021. Daniilidis. Learningtoestimate 3 Dhumanposeandshape\n [43] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh fromasinglecolorimage. In CVPR,2018.\n graphormer. In ICCV,2021. [60] Owen Pearl,Soyong Shin,Ashwin Godura,Sarah Bergbre-\n [44] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, iter,and Eni Halilaj.Fusionofvideoandinertialsensingdata\n Pietro Perona,Deva Ramanan,Piotr Dolla´r,and CLawrence viadynamicoptimizationofabiomechanicalmodel.Journal\n Zitnick. Microsoft COCO:Commonobjectsincontext. In of Biomechanics,155:111617,2023.\n ECCV,2014. [61] William Peeblesand Saining Xie. Scalablediffusionmodels\n [45] Matthew Loper,Naureen Mahmood,Javier Romero,Gerard withtransformers. ar Xivpreprintar Xiv:2212.09748,2022.\n Pons-Moll,and Michael JBlack. SMPL:Askinnedmulti- [62] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter\n personlinearmodel. ACMtransactionsongraphics(TOG), Abbeel,and Sergey Levine. Sfv: Reinforcementlearningof\n 34(6):1–16,2015. physicalskillsfromvideos.ACMTransactions On Graphics\n [46] Matthew Loper, Naureen Mahmood, Javier Romero, Ger- (TOG),37(6):1–14,2018.\n ard Pons-Moll, and Michael J. Black. SMPL: A skinned [63] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\n multi-person linear model. ACM Trans. Graphics (Proc. Kanazawa, Christoph Feichtenhofer, and Jitendra Malik.\n SIGGRAPHAsia),34(6):248:1–248:16,Oct.2015. On the benefits of 3 D tracking and pose for human action\n [47] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip recognition. In CVPR,2023.\n Torr,Andreas Geiger,Laura Leal-Taixe´,and Bastian Leibe. [64] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\n HOTA: A higher order metric for evaluating multi-object Kanazawa, and Jitendra Malik. Tracking people with 3 D\n tracking. IJCV,2021. representations. In Neur IPS,2021. \n [48] Zhengyi Luo,SAlireza Golestaneh,and Kris MKitani. 3 D [65] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\n human motion estimation via motion compression and re- Kanazawa, and Jitendra Malik. Tracking people by pre-\n finement. In ACCV,2020. dicting 3 D appearance, location and pose. In CVPR,\n [49] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal 2022. \n Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian [66] Davis Rempe,Tolga Birdal,Aaron Hertzmann,Jimei Yang,\n Theobalt. Monocular 3 Dhumanposeestimationinthewild Srinath Sridhar,and Leonidas JGuibas.Hu Mo R:3 Dhuman\n usingimproved CNNsupervision. In 3 DV,2017. motionmodelforrobustposeestimation. In ICCV,2021.\n [50] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and [67] Ergys Ristani,Francesco Solera,Roger Zou,Rita Cucchiara,\n Christoph Feichtenhofer. Track Former: Multi-objecttrack- and Carlo Tomasi. Performancemeasuresandadatasetfor\n ingwithtransformers. In CVPR,2022. multi-target,multi-cameratracking. In ECCV,2016."
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n [68] Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng [85] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis-\n Chen,Rama Chellappa,and Abhinav Shrivastava. Poseand escu. Monocular 3 Dposeandshapeestimationofmultiple\n joint-awareactionrecognition. In WACV,2022. peopleinnaturalscenes-Theimportanceofmultiplescene\n [69] Jie Song,Xu Chen,and Otmar Hilliges. Humanbodymodel constraints. In CVPR,2018.\n fittingbylearnedgradientdescent. In ECCV,2020. [86] Wang Zeng,Wanli Ouyang,Ping Luo,Wentao Liu,and Xi-\n [70] Yu Sun,Wu Liu,Qian Bao,Yili Fu,Tao Mei,and Michael J aogang Wang. 3 Dhumanmeshregressionwithdensecorre-\n Black. Puttingpeopleintheirplace: Monocularregression spondence. In CVPR,2020.\n of 3 Dpeopleindepth. In CVPR,2022. [87] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and\n [71] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Zhenan Sun. Da Nnet: Decompose-and-aggregate network\n Mei. Human mesh recovery from monocular images via a for 3 D human shape and pose estimation. In Proceedings\n skeleton-disentangledrepresentation. In ICCV,2019. of the 27 th ACM International Conference on Multimedia,\n [72] Garvita Tiwari,Dimitrije Antic´,Jan Eric Lenssen,Nikolaos 2019. \n Sarafianos,Tony Tung,and Gerard Pons-Moll. Pose-NDF: [88] Hongwen Zhang,Yating Tian,Yuxiang Zhang,Mengcheng\n Modelinghumanposemanifoldswithneuraldistancefields. Li,Liang An,Zhenan Sun,and Yebin Liu. Py MAF-X:To-\n In ECCV,2022. wardswell-alignedfull-bodymodelregressionfrommonoc-\n [73] Vasileios Vasilopoulos,Georgios Pavlakos,Sean LBowman, ularimages. PAMI,2023.\n J Diego Caporale, Kostas Daniilidis, George J Pappas, and [89] Hongwen Zhang,Yating Tian,Xinchi Zhou,Wanli Ouyang,\n Daniel EKoditschek. Reactivesemanticplanninginunex- Yebin Liu,Limin Wang,and Zhenan Sun. Py MAF:3 Dhu-\n plored semantic environments using deep perceptual feed- manposeandshaperegressionwithpyramidalmeshalign-\n back. IEEE Robotics and Automation Letters, 5(3):4455– mentfeedbackloop. In ICCV,2021.\n 4462,2020. \n [90] Jason YZhang,Panna Felsen,Angjoo Kanazawa,and Jiten-\n [74] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko- \n dra Malik. Predicting 3 Dhumandynamicsfromvideo. In\n reit,Llion Jones,Aidan NGomez,Łukasz Kaiser,and Illia \n ICCV,2019. \n Polosukhin. Attentionisallyouneed. In NIPS,2017. \n [91] Yi Zhou,Connelly Barnes,Jingwan Lu,Jimei Yang,and Hao\n [75] Ziniu Wan, Zhengjia Li, Maoqing Tian, Jianbo Liu, Shuai \n Li. On the continuity of rotation representations in neural\n Yi, and Hongsheng Li. Encoder-decoder with multi-level \n networks. In CVPR,2019. \n attentionfor 3 Dhumanshapeandposeestimation.In ICCV, \n 2021. \n [76] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, \n Jonathan TBarron,and Ira Kemelmacher-Shlizerman. Hu- \n man Ne RF:Free-viewpointrenderingofmovingpeoplefrom \n monocularvideo. In CVPR,2022. \n [77] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph \n Feichtenhofer, and Georgia Gkioxari. Multiview compres- \n sivecodingfor 3 Dreconstruction. In CVPR,2023. \n [78] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, \n Rui Liang,Wenjia Wang,Shipei Zhou,Guosen Lin,Yanwei \n Fu,Yizhou Wang,and Yonggang Wang. AIChallenger: A \n large-scaledatasetforgoingdeeperinimageunderstanding. \n ar Xivpreprintar Xiv:1711.06475,2017. \n [79] Yuliang Xiu,Jiefeng Li,Haoyu Wang,Yinghong Fang,and \n Cewu Lu. Pose Flow: Efficient online pose tracking. In \n BMVC,2018. \n [80] Xiangyu Xu,Hao Chen,Francesc Moreno-Noguer,Laszlo A \n Jeni,and Fernando Dela Torre. 3 Dhumanpose,shapeand \n texturefromlow-resolutionimagesandvideos.PAMI,2021. \n [81] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. \n Vi TPose: Simple vision transformer baselines for human \n poseestimation. In Neur IPS,2022. \n [82] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. Dense Ra C: \n Joint 3 D pose and shape estimation by dense render-and- \n compare. In ICCV,2019. \n [83] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo \n Kanazawa. Decoupling human and camera motion from \n videosinthewild. In CVPR,2023. \n [84] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and \n Jan Kautz. GLAMR:Globalocclusion-awarehumanmesh \n recoverywithdynamiccameras. In CVPR,2022. "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n \n \n \n \n Supplementary Material for: \n \n “Humans in 4 D: Reconstructing and Tracking Humans with Transformers” \n \n \n Shubham Goel Georgios Pavlakos Jathushan Rajasegaran Angjoo Kanazawa Jitendra Malik\n ∗ ∗ \n shubham-goel, pavlakos, jathushan, kanazawa @berkeley.edu, malik@eecs.berkeley.edu\n { } \n Universityof California,Berkeley \n Weprovidemoredetailsabout HMR 2.0,i.e.,thearchi- pointdetection, whilefittinghappensusing Pro HMR[10].\n tectureweuse(Section S.1),thedata(Section S.2)andthe We discard detections with very few 2 D detected key-\n training pipeline (Section S.3). Furthermore, we describe points(lessthanfive)andlowdetectionconfidence(thresh-\n the aspect of pose prediction (Section S.4) and we discuss old 0.5). We also discard fits with unnatural body shapes\n the metrics we use for evaluation (Section S.5). Then, we (i.e., body shape parameters outside [ 3,3]), unnatural\n − \n discusstheexperimentalsettingsfortracking(Section S.6), bodyposes(computedusingaper-jointhistogramofposes\n and action recognition (Section S.7). Finally, we provide on AMASS [17]), and large fitting errors (i.e., which indi-\n additionalqualitativeresults(Section S.8). catesthatthereconstructionwasnotsuccessful). Fortrain-\n ingour HMR 2.0 bmodel,wesamplewithdifferentproba-\n S.1.HMR 2.0 architecturedetails bilitiesfromeachdataset,i.e.,Human 3.6 M:0.1,MPII:0.1,\n MPI-INF-3 DHP: 0.1, AVA: 0.15, AI Challenger: 0.15, In-\n The architecture of our HMR 2.0 model is based on a \n sta Variety: 0.2,COCO:0.2. \n Vi T image encoder and a transformer decoder. We use \n a Vi T-H/16 (“huge”) pre-trained on the task of 2 D key- \n S.3.Trainingdetails \n pointlocalization[25]. Ithas 50 transformerlayers,takesa \n 256 192 sizedimageasinput,andoutputs 16 12 image Wetrainourmainmodelusing 8 A 100 GPUswithanef-\n × × \n tokens,eachofdimension 1280.Ourtransformerdecoderis fective batch size of 8 48 = 384. We use an Adam W\n × \n astandardtransformerdecoderarchitecture[23]with 6 lay- optimizer [15] with a learning rate of 1 e-5, β = 0.9,\n 1 \n ers, each containing multi-head self-attention, multi-head β = 0.999,andaweightdecayof 1 e-4. Traininglastsfor\n 2 \n cross-attention, and feed-forward blocks, with layer nor- 1 Miterations, whichtakesroughlysixdays. Forourmain\n malization[2]. Ithasa 2048 hiddendimension,8(64-dim) model HMR 2.0 b, we train the network end-to-end. How-\n headsforself-andcross-attention,andahiddendimension ever, for the HMR 2.0 a variant, the Vi T encoder remains\n of 1024 inthefeed-forward MLPblock.Itoperatesonasin- frozen,allowingalargereffectivebatchsizeof 8 512 =\n × \n glelearnable 2048-dimensional SMPLquerytokenasinput 4096,learningrateof 1 e-4,andfewertrainingiterationsof\n and cross-attends to the 16 12 image tokens. Finally, a 100 K(i.e.,roughlyequivalentnumberofepochs).\n × \n linearreadoutontheoutputtokenfromthetransformerde- While training, we weigh the different losses. ,\n kp 3 D \n L \n codergivesposeθ,shapeβ,andcameraπ. , and have weights 0.05, 0.01, and 0.0005 re-\n kp 2 D adv \n L L \n spectively. The terms within are also weighed dif-\n smpl \n L \n S.2.Datadetails ferently,theθandβ termsweigh 0.001 and 0.0005 respec-\n tively. \n Inourtraining,weadoptthetrainingdataconventionsof \n previous works [10], using images from Human 3.6 M [4], \n S.4.Poseprediction \n COCO[13],MPII[1]and MPI-INF-3 DHP[18].Thisforms \n thetrainingsetfortheversionwerefertoas HMR 2.0 ain For the pose prediction model, we train a vanilla trans-\n the main manuscript. For the eventual HMR 2.0 b version, formermodel[23]fromthetrackletsobtainedby[19].Each\n we additionally generate pseudo-ground truth SMPL [14] tracklet at every time instance contains 3 D pose and 3 D\n fitsforimagesfrom AVA[3],Insta Variety[6]and AIChal- location information, where the pose is parameterized by\n lenger[24]. Since AVAand Insta Varietyincludevideos,we the SMPLmodel[14]andthelocationisrepresentedasthe\n collect frames by sampling at 1 fps and 5 fps respectively. translationinthecameraframe. Thetransformerhas 6 lay-\n For pseudo-ground truth generation, we use Vi TDet [11] ers and 8 self-attention heads with a hidden dimension of\n for bounding box detection and Vi TPose [25] for key- 256. Eachoutputtokenregressesthe 3 Dposeand 3 Dloca-"
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n tion of the person at the specified time-step. We train this ofthemainmanuscript). Tomodelappearance,wetexture\n model by randomly masking input pose tokens and apply- visiblepointsonthemeshbyprojectingthemontotheinput\n ing the loss on the masked tokens. During inference, to imageandsamplingcolorfromthecorrespondingpixels.\n predict a future 3 D pose, we query the model by reading \n outfromafuturetime-step, usingalearnedmask-tokenas S.7.Actionrecognition \n input to that time-step. Similarly for amodal completion, \n we replace the missing detections with the learned mask- As an alternative way to assess the quality of 3 D hu-\n token and read out from the output at the corresponding man reconstruction, we evaluate various human mesh re-\n time-step. Themodelistrainedwithabatchsizeof 64 se- covery systems on the downstream task of action recogni-\n quences and a sequence length of 128 tokens. We use the tion on AVA (please refer to [19] for more details on the\n Adam W optimizer [15] with a learning rate of 0.001 and task definition). More specifically, we take the tracklets\n β =0.9,β =0.95. from [19], which were generated by running PHALP [21]\n 1 2 \n on the Kinetics [8] and AVA [3] datasets. Then, we re-\n S.5.Metrics placetheposesfromvarioushumanmeshrecoverymodels\n (i.e.,Py MAF[28],Py MAF-X[27],PARE[9],CLIFF[12],\n Forourevaluation,weusethemetricsthatarecommon HMAR[21], HMR 2.0)andevaluatetheirperformanceon\n intheliterature: the action recognition task. In this pose-only setting, the\n 3 DPose:Wefollow[5]andweuse MPJPEand PA-MPJPE. action recognition model has access only to the 3 D poses\n MPJPErefersto Mean Per Joint Position Erroranditisthe (inthe SMPLformat)and 3 Dlocationandistrainedtopre-\n average L 2 erroracrossalljoint,afteraligningwiththeroot dict the action of each person. For a fair comparison and\n node. PA-MPJPE is similar but is computed after aligning toachievethebestperformanceforeach 3 Dposeregressor,\n the predicted pose with the ground-truth pose using Pro- weretraintheactionrecognitionmodelspecificallyforeach\n crustes Alignment. 3 Dposemethod. \n 2 D Pose: We use PCK as defined in [26]. This is the Per- \n centageof Correctlylocalized Keypoints,whereakeypoint S.8.Additionalqualitativeresults\n is considered as correctly localized if its L 2 distance from \n theground-truthkeypointislessthanathresholdt. Were- We have already provided a lot of qualitative results of\n portresultsusingdifferentthresholds(@0.05 and@0.1 of HMR 2.0, both in the main manuscript and in videos on\n imagesize). the project webpage. Here, we provide additional results,\n Tracking: Following [20, 21], we use standard tracking including comparisons with our closest competitors (Fig-\n metrics. This includes ID switches (IDs), MOTA [7], ure S.1), and a demonstration of our results in a variety of\n IDF 1[22],and HOTA[16]. challengingcases,includingsuccesses(Figure S.2)andfail-\n Action Recognition: Wereportresultsusingm APmetrics urecases(Figure S.3). \n as defined in the AVA dataset [3]. We further provided a \n morefine-grainedanalysisreportingresultsondifferentac- References \n tion categories: actions that involve Object Manipulation \n [1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\n (OM),actionsthatinvolve Person Interactions(PI),andac- \n Bernt Schiele. 2 Dhumanposeestimation: Newbenchmark\n tions that involve Person Movement (PM). The results in \n andstateoftheartanalysis. In CVPR,2014. \n thesecategoriesarealsoreportedusingm AP. \n [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\n ton. Layernormalization. ar Xivpreprintar Xiv:1607.06450,\n S.6.Trackingwith PHALP \n ′ 2016. \n In the main manuscript, we compare different human [3] Chunhui Gu, Chen Sun, David A Ross, Carl Von-\n drick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\n mesh recovery systems on the downstream problem of \n narasimhan, George Toderici, Susanna Ricco, Rahul Suk-\n tracking (Table 3 of the main manuscript). For this, we \n thankar,Cordelia Schmid,and Jitendra Malik.AVA:Avideo\n modifythe PHALPapproach[21], sothatposedistanceis \n datasetofspatio-temporallylocalizedatomicvisualactions.\n computedonthe SMPLspacethatallthemodelsshare. To \n In CVPR,2018. \n make this comparison fair, we keep other variables simi- \n [4] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\n lar to the original PHALP (e.g., same appearance embed- \n Sminchisescu.Human 3.6 M:Largescaledatasetsandpredic-\n ding). Note that this comparison is generous to baselines tivemethodsfor 3 Dhumansensinginnaturalenvironments.\n that do not model appearance themselves. Eventually, our PAMI,2013. \n final 4 DHumanssystemusesasampling-basedappearance [5] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\n headandournewposeprediction, whichleadtothestate- Jitendra Malik. End-to-end recovery of human shape and\n of-the-art performance for tracking on Pose Track (Table 4 pose. In CVPR,2018. "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n Input Py MAF-X PARE CLIFF HMR 2.0 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure S.1:Qualitativecomparisonofourapproachwithstate-of-the-artmethods.Wecompare HMR 2.0 withourclosestcompetitors,\n Py MAF-X[27], PARE[9]and CLIFF[12]. Foreachexample, weshowtheinputimage, andresultsfromeachmethod(includingthe\n frontal and a side view). HMR 2.0 is significantly more robust in a variety of settings, including images with unusual poses, unusual\n viewpointsandheavyperson-personoverlap. \n \n \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n \n Input Front view Side view Top view Input Front view Side view Top view \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure S.2:Qualitativeresultsofourapproachonchallengingexamples.Foreachexampleweshowtheinputimage,thereconstruction\n overlay, a side view and the top view. The examples include unusual poses, unusual viewpoints, people in close interaction, extreme\n truncationsandocclusions,aswellasblurryimages. \n \n \n \n \n "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n Input Front view Side view Top view Input Front view Side view Top view \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure S.3: Failuresofsingleframe 3 Dhumanreconstructionwith HMR 2.0. Despitetheincreasedrobustnessofourmethod, we\n observethat HMR 2.0 occasionallyrecoverserroneousreconstructionsincaseswithveryunusualarticulation(firstrow),heavyperson-\n personinteraction(secondrow),andverychallengingdepthorderingforthedifferentbodyparts(thirdrow).\n [6] Angjoo Kanazawa,Jason YZhang,Panna Felsen,and Jiten- [15] Ilya Loshchilovand Frank Hutter. Decoupledweightdecay\n dra Malik. Learning 3 D human dynamics from video. In regularization. ar Xivpreprintar Xiv:1711.05101,2017.\n CVPR,2019. [16] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip\n [7] Rangachar Kasturi, Dmitry Goldgof, Padmanabhan Torr,Andreas Geiger,Laura Leal-Taixe´,and Bastian Leibe.\n Soundararajan, Vasant Manohar, John Garofolo, Rachel HOTA: A higher order metric for evaluating multi-object\n Bowers, Matthew Boonstra, Valentina Korzhova, and Jing tracking. IJCV,2021.\n Zhang. Frameworkforperformanceevaluationofface,text, [17] Naureen Mahmood,Nima Ghorbani,Nikolaus FTroje,Ger-\n andvehicledetectionandtrackinginvideo: Data, metrics, ard Pons-Moll, and Michael JBlack. AMASS:Archiveof\n andprotocol. PAMI,2008. motioncaptureassurfaceshapes. In ICCV,2019.\n [8] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, [18] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal\n Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\n Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Theobalt. Monocular 3 Dhumanposeestimationinthewild\n and Andrew Zisserman. The kinetics human action video usingimproved CNNsupervision. In 3 DV,2017.\n dataset. ar Xivpreprintar Xiv:1705.06950,2017. \n [19] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\n [9] Muhammed Kocabas, Chun-Hao PHuang, Otmar Hilliges, Kanazawa, Christoph Feichtenhofer, and Jitendra Malik.\n and Michael JBlack. PARE:Partattentionregressorfor 3 D On the benefits of 3 D tracking and pose for human action\n humanbodyestimation. In ICCV,2021. recognition. In CVPR,2023. \n [10] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, [20] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\n and Kostas Daniilidis. Probabilistic modeling for human Kanazawa, and Jitendra Malik. Tracking people with 3 D\n meshrecovery. In ICCV,2021. representations. In Neur IPS,2021. \n [11] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. [21] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo\n Exploringplainvisiontransformerbackbonesforobjectde- Kanazawa, and Jitendra Malik. Tracking people by pre-\n tection. In ECCV,2022. dicting 3 D appearance, location and pose. In CVPR,\n [12] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, 2022. \n and Youliang Yan. CLIFF:Carryinglocationinformationin [22] Ergys Ristani,Francesco Solera,Roger Zou,Rita Cucchiara,\n fullframesintohumanposeandshapeestimation.In ECCV, and Carlo Tomasi. Performancemeasuresandadatasetfor\n 2022. multi-target,multi-cameratracking. In ECCV,2016.\n [13] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays, [23] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszko-\n Pietro Perona,Deva Ramanan,Piotr Dolla´r,and CLawrence reit,Llion Jones,Aidan NGomez,Łukasz Kaiser,and Illia\n Zitnick. Microsoft COCO:Commonobjectsincontext. In Polosukhin. Attentionisallyouneed. In NIPS,2017.\n ECCV,2014. [24] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan,\n [14] Matthew Loper,Naureen Mahmood,Javier Romero,Gerard Rui Liang,Wenjia Wang,Shipei Zhou,Guosen Lin,Yanwei\n Pons-Moll,and Michael JBlack. SMPL:Askinnedmulti- Fu,Yizhou Wang,and Yonggang Wang. AIChallenger: A\n personlinearmodel. ACMtransactionsongraphics(TOG), large-scaledatasetforgoingdeeperinimageunderstanding.\n 34(6):1–16,2015. ar Xivpreprintar Xiv:1711.06475,2017. "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n \n \n [25] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. \n Vi TPose: Simple vision transformer baselines for human \n poseestimation. In Neur IPS,2022. \n [26] Yi Yang and Deva Ramanan. Articulated human detection \n withflexiblemixturesofparts. PAMI,2012. \n [27] Hongwen Zhang,Yating Tian,Yuxiang Zhang,Mengcheng \n Li,Liang An,Zhenan Sun,and Yebin Liu. Py MAF-X:To- \n wardswell-alignedfull-bodymodelregressionfrommonoc- \n ularimages. PAMI,2023. \n [28] Hongwen Zhang,Yating Tian,Xinchi Zhou,Wanli Ouyang, \n Yebin Liu,Limin Wang,and Zhenan Sun. Py MAF:3 Dhu- \n manposeandshaperegressionwithpyramidalmeshalign- \n mentfeedbackloop. In ICCV,2021. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n "
  }
]