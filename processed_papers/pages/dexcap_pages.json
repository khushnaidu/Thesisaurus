[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n Dex Cap: Scalable and Portable Mocap Data \n \n Collection System for Dexterous Manipulation \n \n \n Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu\n Stanford University \n https://dex-cap.github.io \n \n \n \n \n \n \n \n \n \n (a)Dex Cap: Portable motion capturesystem (b)Mocapdataand 3 Dscene (c)Dex IL: Dexterous imitation learning\n Fig. 1: DEXCAP facilitates the in-the-wild collection of high-quality human hand motion capture data and 3 D observations.\n Leveraging this data, DEXIL adapts it to the robot embodiment and trains control policy to perform the same task.\n \n Abstract—Imitation learning from human hand motion data supervised training using human demonstration data. One\n presentsapromisingavenueforimbuingrobotswithhuman-like commonly used way to collect data is to teleoperate robot\n dexterityinreal-worldmanipulationtasks.Despitethispotential, \n hands to perform the tasks. However, due to the requirement\n substantialchallengespersist,particularlywiththeportabilityof \n ofarealrobotsystemandslowrobotmotion,thisapproachis\n existinghandmotioncapture(mocap)systemsandthecomplexity \n oftranslatingmocapdataintoeffectiveroboticpolicies.Totackle expensive to scale up. An alternative way is to directly track\n these issues, we introduce DEXCAP, a portable hand motion human hand motions during manipulation without controlling\n capture system, alongside DEXIL, a novel imitation algorithm the robot. Current system is primarily vision-based with a\n for training dexterous robot skills directly from human hand \n single-viewcamera.However,besidesthequestionofwhether\n mocapdata. DEXCAP offersprecise,occlusion-resistanttracking \n the tracking algorithm can provide accurate 3 D information\n ofwristandfingermotionsbasedon SLAMandelectromagnetic \n fieldtogetherwith 3 Dobservationsoftheenvironment.Utilizing which is critical for robot policy learning, these systems are\n this rich dataset, DEXIL employs inverse kinematics and point vulnerable to visual occlusions that frequently occur during\n cloud-based imitation learning to seamlessly replicate human hand-object interactions.\n actions with robot hands. Beyond direct learning from human \n A better alternative to vision-based methods for gathering\n motion, DEXCAP also offers an optional human-in-the-loop \n dexterous manipulation data is through motion capture (mo-\n correctionmechanismduringpolicyrolloutstorefineandfurther \n improve task performance. Through extensive evaluation across cap). Mocap systems provides accurate 3 D information and\n six challenging dexterous manipulation tasks, our approach not are robust to visual occlusions. Hence human operators can\n only demonstrates superior performance but also showcases the directly interact with the environment with their hands, which\n system’s capability to effectively learn from in-the-wild mocap \n is fast and easier to scale up since no robot hardware is\n data, paving the way for future data collection methods in the \n required.Toscaleuphandmocapsystemstodatacollectionin\n pursuit of human-level robot dexterity. \n everydaytasksandenvironmentsforrobotlearning,asuitable\n I. INTRODUCTION system should ideally be portable and robust for long capture\n Buildingroboticsystemstoperformeverydaymanipulation sessions, provide accurate finger and wrist poses, as well as\n tasks is a long-standing challenge. Our living environments 3 D environment information. Most hand mocap systems are\n and daily objects are designed with human hand functionality not portable and rely on well-calibrated third-view cameras.\n in mind, posing a substantial challenge for developing future Whileelectromagneticfield(EMF)glovesovercomethisissue,\n home robots. Recent breakthroughs in robotic dexterity, espe- they cannot track the 6-Do F wrist pose in the world frame,\n ciallyinthecontrolofmulti-fingeredmechanicalhandswitha which is important for end-effectors policy learning. Devices\n high degree of freedom, have shown remarkable potential [1– like IMU-based whole-body suits can monitor wrist position\n 3]. However, enabling robotic hands to emulate human-level but are prone to drift over time.\n dexterity in manipulation tasks remains unsolved, due to both In addition to hardware challenges, there are also algorith-\n hardware and algorithmic challenges. mic challenges to use motion capture data for robot imitation\n Imitation Learning (IL) [4, 5] has recently made con- learning. While dexterous robot hands enable the possibility\n siderable strides toward this goal [6, 7], especially through of learning directly from human hand data, the inherent dif-\n 4202 \n lu J \n 4 \n ]OR.sc[ \n 2 v 88770.3042:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n ferences in size, proportion, and kinematic structure between II. RELATEDWORKS\n the robot hand and human hand call for innovative algorithms \n A. Dexterous manipulation \n to overcome these embodiment gaps. Towards solving these \n challenges,ourworksimultaneouslyintroducesanewportable Dexterous manipulation has been a long-standing research\n hand mocap system, DEXCAP, and an imitation algorithm, area in robotics [15–19], posing significant challenges to\n DEXIL, that allows the robot to learn dexterous manipulation planning and control due to the high degrees-of-freedom. The\n policies directly from the human hand mocap data. traditional optimal control methods [17–19] often necessitate\n simplification of the contacts, which is usually not tenable\n DEXCAP (Fig. 1) is a portable hand mocap system that \n in more complex tasks. Recently, reinforcement learning has\n tracks the 6-Do F poses of the wrist and the finger motions in \n been explored to learn dexterous policies in simulation with\n real-time (60 Hz). The system includes a mocap glove to track \n minimalassumptionsaboutthetaskortheenvironment[2,20–\n finger joints, a camera mounted on top of each glove to track \n 29]. The learned policies can solve complex tasks, including\n the 6-Do F poses of the wrists with SLAM, and an RGB-D \n in-hand object re-orientatation [2, 20, 23–25, 28], bimanual\n Li DAR camera on the chest to observe the 3 D environments. \n manipulation[26,30],andlong-horizonmanipulation[22,27].\n Besides the hardware challenges, research efforts on de- However, due to the sim-to-real gap, deploying the learned\n veloping algorithms to utilize mocap data for robot learning policy on a real-world robot remains challenging. Imitation\n have been missing due to the lack of such a data collection learning, on the other hand, focuses on learning directly\n system and collected data. Prior algorithms that learn from fromreal-worlddemonstrationdata,whichisobtainedthrough\n human motion focus on learning the rewards [8, 9], high- either teleportation [1, 6, 31, 32] or human videos [3, 33, 34].\n levelplans[10,11],andvisualrepresentations[12,13],which DIME [31] uses VR to teleoperate a dexterous hand for data\n oftenrequireadditionalrobotdataandcannotbedirectlyused collection; Qin et al. [35] uses an RGB camera to track hand\n for low-level control. In this work, we argue that the main pose for teleoperation; Dex Transfer [36] uses human mocap\n challenge of learning low-level control from human motion is data to guide dexterous grasping; Dex MV [33], Dex VIP [34]\n that the data is missing precise 3 D information of the hand and Video Dex [3] leverages human video data for learning\n motion (e.g., 6-Do F hand pose, 3 D finger positioning), which the motion priors but often require additional training in\n are exactly what DEXCAP can provide. simulation or real robot teleoperation data. Our work focuses\n To leverage data collected by DEXCAP for learning dex- on dexterous imitation learning, which relies on DEXCAP to\n terous robot policies, we propose imitation learning from collect high-quality hand mocap data grounded in 3 D point\n mocap data, DEXIL, which consists of two major steps — cloud observation, which can be directly used to train low-\n dataretargetingandtraininggenerative-basedbehaviorcloning level positional control on robots with single or dual hands.\n policywithpointcloudinputs,withanoptionalhuman-in-the- \n B. Hand motion capture system \n loop motion correction step. For retargeting, we use inverse \n kinematics (IK) to retarget the robotic hand’s fingertips to the Human hand mocap is an important technique for appli-\n same 3 D location as the human’s fingertips. The 6-Do F pose cations in computer vision and graphics. Most previous sys-\n ofthewristisusedtoinitializethe IKtoensurethesamewrist tems are camera-based, IMU-based, or electromagnet(EMF)-\n motion between the human and the robots. Then we convert based. Camera-based systems utilize monocular camera [37–\n RGB-Dobservationstopointcloud-basedrepresentations.We 39], RGB-D camera [40–42], VR headset [43], or multi-view\n thenuseapointcloud-basedbehaviorcloningalgorithmbased camera with markers [44, 45]. However, the quality of hand\n on Diffusion Policy [14]. In more challenging tasks when IK motion tracking quickly deteriorates in scenarios involving\n is insufficient to fulfill the embodiment gap between human heavy occlusions, which happen frequently in hand-object\n and robot hands, we propose a human-in-the-loop motion interactions. Some of these systems also require third-view\n correction mechanism. During policy rollouts, humans can calibrated cameras which are not portable or scalable. More\n wear the DEXCAP and interrupt the robot’s motion when recently, Inertia Measurement Unit (IMU) has been used for\n unexpected behavior occurs, and such interruption data can in-the-wildhumanmocap[46–50].Nevertheless,mostofthem\n be further used for policy finetuning. focus on whole-body motion capture and miss fine-grained\n In summary, the main contributions of this work include: finger motions. EMF-based mocap gloves are designed for\n capturing finger motion, which is widely used for dexterous\n • DEXCAP: a novel portable human hand mocap system, teleoperation [51–53]. However, the glove does not track the\n enablingreal-timetrackingofwristandfingermovements 6-Do F palm poses grounded in the environment and misses\n for dexterous manipulation tasks. visual observations for training robot policies. DEXCAP is a\n • DEXIL:animitationlearningframeworkleveraginghand mocapglovesystemthatisdesignedtocollectdatafortraining\n mocap data for directly learning dexterous manipulation visuomotor manipulation policies. Through novel engineering\n skills from human hand motions. designs, our system stays robust to occlusions, captures fine-\n • Human-in-the-Loop Correction:ahuman-in-the-loopcor- grained finger motion, tracks palm poses using SLAM, and\n rection mechanism with DEXCAP, significantly enhanc- records RGB-D images to reconstruct the scene with a wear-\n ing robot performance in complex tasks. able camera vest. "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n Calibrationphase Datacollectionphase\n \n \n \n \n \n \n Intel NUC \n \n Power bank \n 1 2 \n (a)Dex Capfrontview (b)Dex Capbackview (c)Detailsofthe camerasetup \n Fig. 2: Details of the human system. (a) Our setup includes a 3 D-printed rack on a chest harness, featuring a Realsense\n L 515 Li DAR camera on top and three Realsense T 265 tracking cameras below. (b) An Intel NUC and power bank in a\n backpack power the system for approximately 40 minutes of data collection. (c) The T 265 cameras, initially in a known pose\n for calibration, are relocated to hand mounts during data collection to monitor palm positions, ensuring consistency through a\n click-in design. Finger motions are captured by Rokoko gloves, accurately tracking the finger joint positions.\n \n C. Robot learning with human demonstration designed and used for the parallel-gripper data collection\n process, while in this work we aim to collect multi-finger\n Imitation Learning (IL) has enabled robots to successfully \n handmotiondatafordexterousmanipulationtasks(e.g.,using\n perform various manipulation tasks [4, 54–60]. Traditional \n scissors and unscrewing bottle caps). \n IL algorithms such as DMP and Pr MP [61–64] enjoy high \n learning sample efficiency but are limited in their ability to \n III. HARDWARESYSTEM:DEXCAP \n handle high-dimensional observations. In contrast, recent IL \n methods built upon deep neural networks can learn policies Inthissection,weintroducethesystemdesignincluding(1)\n with raw image observation inputs [65, 66], even for high- a portable human hand motion capture system DEXCAP that\n degree robot systems with bimanual arms [67, 68]. Despite is used for data collection (Sec. III-A) and (2) a bimanual\n their effectiveness, one key challenge for imitation learning robot system equipped with dexterous hands for testing the\n is how to scale up the training data. Prior works focus on policies learned from the collected data (Sec. III-B).\n teleoperation data [66, 69–77] which is expensive to collect \n A. Dex Cap \n due to the requirement of the robot hardware. More recently, \n learning from human motion data has started to receive To capture the fine-grained hand motion data suitable to\n more attention because it allows collecting data without robot train dexterous robot policies, DEXCAP is designed with four\n hardware [78]. By leveraging human videos [11, 79], hand key objectives in mind: (1) detailed finger motion tracking,\n trajectories [10, 80–82], promising results have been shown (2) accurate 6-Do F wrist pose estimation, (3) aligned 3 D ob-\n to train policies with less manual human effort. However, servations recording in a unified coordinate frame with hands,\n these human motions are in 2 D image space [80, 83, 84], and (4) outstanding portability for data collection in various\n which fails to directly train 6-Do F manipulation policies in real-world environments. We achieved these objectives with\n 3 Denvironmentsandusuallyrequiresadditionalteleoperation zero compromise on scalability—DEXCAP must be simple to\n data to bridge the gap [10, 11, 79]. Recently, human-in- calibrate, inexpensive to build, and robust for data collection\n the-loop correction algorithms have also shown promising of daily activities in the wild.\n results in robot learning [85–87]. Our DEXCAP provides Tracking finger motions. Our system uses electromag-\n tracking of 6-Do F hand poses together with finger motions netic field (EMF) gloves, offering a significant advantage\n grounded in 3 D point cloud observations, which is portable over vision-based finger tracking systems, particularly in the\n fordatacollectionwithoutarobot.Basedonthedatacollected robustness to visual occlusions that frequently occur in hand-\n with DEXCAP, we introduce DEXIL which is a point cloud- object interactions. In our system, finger motions are tracked\n based imitation learning algorithm for learning fine-grained using Rokokomotioncaptureglovesasillustratedin Figure 2.\n dexterous manipulation policies, with an optional human-in- Each glove’s fingertip is embedded with a tiny magnetic\n the-loop correction step for more challenging tasks. sensor, while a signal receiver hub is placed on the glove’s\n dorsal side. The 3 D location of each fingertip is measured\n D. Portable data collection systems for manipulation \n as the relative 3 D translation from the hub to the sensors.\n Recentlyadvancementsinlow-costhand-heldgrippershave In appendix we included a qualitative comparison between\n shown promising results in collecting robot manipulation data our EMF glove system and state-of-the-art vision-based hand-\n without robot hardware [88–94]. All of these systems are tracking methods across different manipulation scenarios."
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n Front view Side view \n (a)Retargetingwithfingertip IK (b)Bimanualdexterousrobotsetup (c)Human-in-the-loopcorrectionsetup\n \n Fig. 3: Details of the robot system. Mirroring the human system, the robot system reuses the same chest cameras and mount.\n (a) Once the motion is captured by Dex Cap, it’s retargeted to LEAP hand through discarding pinky finger and IK to match\n fingertiplocation.(c)Anoptionalhuman-in-the-loopcorrectionstepcanbeperformedtofurtherrefinethemotionstransferred.\n Specifically, the human will provide the delta input in real time when the robot system is carrying out the task. Note the hand\n T 265 is only used at correction time, as the robot arm already knows the exact location of fingers.\n Tracking 6-Do F wrist pose. Beyond finger motion, know- a constant transformation between the camera frames. Then,\n ing the precise positioning of a robot’s end-effector in a we take off the tracking cameras from the rack and insert\n 3 D space is crucial for robot manipulation. This necessitates them into the camera slot attached to each glove. In this way,\n DEXCAPtoestimateandrecordthe 6-Do Fposetrajectoriesof wecaneasilytransformthehandposetrackingresultsintothe\n human hands during data collection. While camera-based and observationframeofthechestcamerawiththeconstantinitial\n IMU-based methods are commonly used, each has its limita- transformation. The full calibration process is demonstrated\n tions. Camera-based systems, often non-portable and limited in Appendix Figure 13 and supplementary videos, which\n in their ability to estimate wrist orientation, are less suited takesaround 10 seconds.Tofurtherensurestableobservations\n for data collection in manipulation tasks. IMU-based systems, amidst human movement, another fisheye tracking camera\n although wearable, tend to suffer from position drifting when (markedredin Fig.2(c))ismountedunderthe Li DARcamera,\n used for long recording sessions. To address these challenges, which provides a more robust SLAM performance than the\n we develop a 6-Do F wrist tracking system based on the Li DARcamerawithitswidefieldofview.Wedefinetheinitial\n SLAM algorithm, as shown in Figure 2(c). This system uses pose frame of this tracking camera as the world frame for all\n an Intel Realsense T 265 camera, mounted on each glove’s stream data. Figure 6 is the visualization of the collected data\n dorsal side. It combines images from two fisheye cameras by transforming the observations into colored point clouds in\n and IMU sensor signals to construct an environment map the world frame alongside the captured hand motions.\n using the SLAM algorithm, enabling consistent tracking of System Portability. Central to the portability of DEXCAP\n the wrist’s 6-Do F pose. This design has three key advantages: is a compact mini-PC (Intel NUC 13 Pro), carried in a\n it is portable, allowing for wrist pose tracking without the backpack, which serves as the primary computation unit for\n need for hands to be visible in third-person camera frames; data recording. This PC is powered by a portable power bank\n SLAM can autonomously correct position drift with the built with a 40000 m Ah battery, enabling approximately 40 minutes\n map for long-time use; and the IMU sensor provides crucial of continuous data collection (Fig. 2(b)). The total weight of\n wrist orientation information to train the robot policy in the the backpack is 3.96 pounds. The supplementary video shows\n subsequent pipeline. that donning and calibrating DEXCAP is fast and simple,\n takinglessthan 10 seconds.Additionally,DEXCAP’shardware\n Recording 3 D observations and calibration. Capturing designismodularandinexpensivetobuild—norestrictionto\n the data necessary for training robot policies requires not brandsormodelsofcameras,motioncapturegloves,andmini-\n only the tracking of hand movement but also recording ob- PCs. We will open-source the code and instruction videos for\n servations of the 3 D environment as the policy input. As builders, along with a range of hardware options. The overall\n depicted in Figure 2(a), we design a wearable camera vest cost of the DEXCAP is kept within a $4 k USD budget.\n for this purpose. It incorporates an Intel Realsense L 515 \n B. Bimanual dexterous robot \n RGB-D Li DAR camera, mounted on the top of the chest, to \n capture the observations during human data collection. The To validate the robot policy trained by the data from\n nextcriticalquestionthenbecomeshowtoeffectivelyintegrate DEXCAP, we establish a bimanual dexterous robot setup.\n the tracked hand motion data with the 3 D observations. To This setup comprises two Franka Emika robot arms, each\n simplify the calibration process, we designed a 3 D-printed equippedwitha LEAPdexterousrobotichand(afour-fingered\n camera rack underneath the chest camera mount as illustrated hand with 16 joints) [95], as depicted in Figure 3(b). For\n in Figure 2(c). At the beginning of the data collection, all policy evaluation, the chest Li DAR camera used in human\n tracking cameras are placed in the rack slots, which secures data collection is detached from the vest and mounted on"
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n Timestep! Dataretargeting \n ! \n \" \n \n Policyrollouts \n Transformto Remove \n RGB-Dimage Pointcloud \n robotspace redundantpoints \n (in-the-wild) \n !' \n # \" \" \n \" \" \n Residual \n Humancorrection action \n Handmocap \n Fingertip IK \n Futuresteps inrobotspace Policy \n [!+#:!+%+#] ! Correction \n dataset \n Policy \n ! \n ! Original \n [\":\"$%] \n … … MSELoss 46-dim action space dataset Merge&finetuning \n (a)Dex ILoverview (b)Human-in-the-loopcorrection \n Fig. 4: Algorithm overview. (a) DEXIL first retargets the DEXCAP data to the robot embodiment by first constructing 3 D\n point clouds from RGB-D observations and transforming it into robot operation space. Meanwhile, the hand motion capture\n data is retargeted to the dexterous hand and robot arm with fingertip IK. Based on the data, a robot policy is learned to output\n a sequence of future goal positions as the robot actions. (b). DEXCAP also offers an optional human-in-the-loop correction\n mechanism,wherehumansapplydeltaresidualactiontothepolicy-generatedactionstocorrectrobotbehavior.Thecorrections\n are stored in a new dataset and uniformly sampled with the original dataset for fine-tuning the robot policy.\n a stand positioned between the robot arms. To simplify the A. Data re-targeting\n process of switching the camera system between the human Actionre-targeting.Asillustratedin Figure 3(a),anotable\n and robot, a quick-release buckle has been integrated into the challengeemergesduetothesizedisparitybetweenthehuman\n back of the camera rack, allowing for swift camera swaps – handandthe LEAPhand,withthelatterabout 50%larger[95].\n in less than 5 seconds. In this way, the robot utilizes the same Thissizedifferencemakesithardtodirectlytransferthefinger\n observation camera employed during human data collection. motionstotherobotichardware.Thefirststepistoretargetthe\n Note that, for robot setups, only the Li DAR camera is used human hand motion capture data into the robot embodiment,\n and wrist cameras are not needed. Both the robot arms and which requires mapping the finger position and 6-Do F palm\n the LEAP hands operate at a control frequency of 20 Hz. We pose with inverse kinematics (IK).\n useend-effectorpositioncontrolforbothrobotarmsandjoint One critical finding in prior research is that fingertips\n position control for both LEAP hands. are the most frequently contacted areas on a hand when\n interacting with objects (as evidenced in studies like HO-\n IV. LEARNINGALGORITHM:DEXIL \n 3 D [41], GRAB [44], ARCTIC [45]). Motivated by this,\n Our goal is to use the human hand motion capture data we re-target finger motion by matching fingertip positions\n recorded by DEXCAP to train dexterous robot policies. There using inverse kinematics (IK). Specifically, we deploy an IK\n are several research questions along the way - (1) How can algorithmthatgeneratessmoothandaccuratefingertipmotion\n we re-target the human hand motion to the robotic hand? (2) in real time [96–98] to determine the 16-dimensional joint\n What algorithm can learn dexterous policies, especially when positions for the robotic hand. This ensures the alignment\n the action space is high-dimensional in the bimanual setup? between robot fingertips and the human fingertips in the\n (3) In addition, we would like to investigate the failure cases DEXCAP data. Considering the design of the LEAP hand,\n forlearningdirectlyfromhumanmotioncapturedataandtheir whichfeaturesfourfingers,weadaptourprocessbyexcluding\n potential solutions. littlefingerinformationduring IKcomputations.Additionally,\n To tackle these challenges, we introduce DEXIL, a three- the 6-Do F wrist pose captured in the mocap data serves as an\n step framework to train dexterous robots using human hand initial reference for wrist pose in the IK algorithm. Figure 6\n motioncapturedata.Thefirststepistore-targetthe DEXCAP demonstrates the final result of re-targeting. The 6-Do F pose\n data into the action and observation spaces of the robot em- of the wrist p =[R |T ] and the finger joint positions J of\n t t t t \n bodiment (Sec. IV-A). Second step trains a point-cloud-based the LEAP hands are then used as the robot’s proprioception\n diffusion policy using the re-targeted data (Sec. IV-B). The state s = (p ,J ). We use position control in our setup\n t t t \n final step involves an optional human-in-the-loop correction and the robot’s action labels are defined as next future states\n mechanism, designed to address unexpected behaviors that a =s . \n t t+1 \n emerge during the policy execution (Sec. IV-C). Observation post-processing. Observation and state rep-"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n resentation choice are critical for training robot policies. We where we empirically find it outperforms traditional MLP-\n convert the RGB-D images captured by the Li DAR camera based architecture for learning dexterous robot policies.\n in the DEXCAP data into point clouds using the camera \n parameters. This additional conversion offers two significant C. Human-in-the-loop correction\n benefits compared to RGB-D input. First, because DEXCAP \n allows the human torso to move naturally during data acqui- \n With the design presented above, DEXIL can learn chal-\n lengingdexterousmanipulationskills(e.g.,pick-and-placeand\n sition, directly using RGB-D input would need to account \n for the moving camera frame. By transforming point cloud \n bimanual coordination) directly from DEXCAP data without\n the need for on-robot data. However, our simple retargeting\n observations into a consistent world frame—defined as the \n method does not address all aspects of the human-robot\n coordinate frame of the main SLAM camera at the start of \n embodiment gap. For example, when using a pair of scissors,\n the mocap (the main camera is marked in red in Fig. 2(c))— \n astableholdofscissorsrequiresinsertingthefingersdeepinto\n we isolate and remove torso movements, resulting in a stable \n the handle. Due to the differences in finger length proportion,\n robot observation. Second, point clouds provide flexibility \n directly matching the fingertips and the joint motion does not\n in editing and alignment with the robot’s operational space. \n guarantee the same force exerted on the scissors.\n Given that some motions captured in the wild may extend \n To address this issue, we offer a human-in-the-loop mo-\n beyond the robot’s reachability, adjusting the placement of \n tion correction mechanism, which consists of two modes -\n point cloud observations and motion trajectories ensures their \n residualcorrectionandteleoperation.Duringpolicyexecution,\n feasibilitywithintherobot’soperationalrange.Basedonthese \n we allow humans to provide corrective actions to robots\n findings,all RGB-Dframesfromthemocapdataareprocessed \n into point clouds aligned with the robot’s space, and the \n in real-time by wearing DEXCAP. In residual mode, DEX-\n task-irrelevant elements, such as the table surface points, are \n CAP measures the delta position changes of human hands\n (∆p H,∆JH) relative to hands’ initial states (p H,JH) at\n excluded. This refined point cloud data thus becomes the t t 0 0 \n the beginning of the policy roll-out. The delta position is\n observation inputs o fed into the robot policy π. \n t \n applied as a residual action ar = (∆p H,∆JH) to the\n t t t \n robot policy action a = (p ,J ), scaled by α and\n t t+1 t+1 \n B. Point cloud-based diffusion policy \n β. The corrected robot action can then be formalized as\n With the transformed robot’s state s t , action a t and cor- a′ t = (p t+1 (cid:76) α·∆p H t ,J t+1 +β ·∆J t H). We empirically\n responding 3 D point cloud observation o , we formalize the find that setting β with a small scale (< 0.1) offers the best\n t \n robot policy learning process as a trajectory generation task. user experience, which avoids fingers moving too fast.\n More specifically, a policy model π, processes the point In the case when a large position change is desired, a\n cloud observations o and the robot’s current proprioception pressonthefootpedalwillswitchthesystemtoteleoperation\n t \n state s into an action trajectory (a ,a ,...,a ) (as in mode. DEXCAP now ignores the policy rollout and applies\n t t t+1 t+d \n Fig. 4). Given point cloud observation with N points o human wrist delta directly to the robot wrist pose. The robot\n t \n in RN×3, we uniformly down-sample it into K points and fingertipsarenowdirectlyfollowinghumanfingertips.Inother\n concatenate the RGB pixel color corresponding to each point words, the robot fingertip will track the human fingertip in\n into the final policy input in RK×6. To bridge the visual gap their respective wrist frame through IK. Users can also switch\n between human hands and the robot’s hand, we use forward back to the residual mode after correcting the robot’s mistake\n kinematics to transform the links of the robot model with by pressing the foot pedal again.\n the proprioception state s and merge the point clouds of the Since the robot has already learned an initial policy, typi-\n t \n transformed links into the observation o . During training, we cally the correction happens in a small portion of the rollout,\n t \n alsousedataaugmentationovertheinputsbyapplyingrandom greatly reducing the human effort. The corrected actions and\n 2 D translations to the point clouds and motion trajectories observations are stored in a new dataset D′. Training data\n within the robot’s operational space. is sampled with equal probability from D′ and the original\n One challenge of learning dexterous robot policies, espe- dataset D tofine-tunethepolicymodel,similarto IWR[101].\n cially for bimanual dexterous robots, is handling the large \n dimensional action outputs. In our setup, the action output V. EXPERIMENTS \n includes two 7-Do F robot arms and two 16-Do F dexterous \n We aim to answer the following research questions:\n hands for d steps, which forms a high-dimensional regression \n problem. Similar challenges have also been studied in image Q 1: What is the quality of DEXCAP data?\n generation tasks, which aim to regress all pixel values in a Q 2: Can DEXIL directlylearndexterousrobotpoliciesfrom\n high-resolution frame. Recently, diffusion model [99, 100], DEXCAP data without any on-robot data?\n with its step-by-step diffusion process, has shown success in Q 3: What model architecture choices are critical to improv-\n modeling complex data distributions with high-dimensional ing the performance?\n data. For robotics, diffusion policy [14] follows the same idea Q 4: Can DEXIL learn from in-the-wild DEXCAP data?\n and formalizes the control problem into an action generation Q 5: How does human-in-the-loop correction help when\n task. Thus we use a diffusion policy as the action decoder, DEXCAP data is insufficient?"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n gnikcipegnop S \n \n \n \n \n \n \n \n gnipiwetal P \n \n \n \n \n \n gnigakca P \n \n \n \n gnittucrossic S \n gnitcelloclla B \n a b \n c \n d \n e \n f \n gniraperpae T \n \n \n Fig. 5: Experiment Tasks. (a) Sponge Picking: Pick and lift the sponge. (b) Ball Collecting: Pick up a ball and drop it into a\n basket. (c) Plate Wiping: Use both hands to pick up a plate and sponge, then wipe the plate vertically twice. (d) Packaging:\n Place items into a box with one hand while using the other to either push or stabilize them, before securely closing the box\n lid. (e) Scissor Cutting: Secure paper with one hand and use scissors in the other to cut through the paper. (f) Tea Preparing:\n Grasp the tea bottle with one hand, use the other hand to uncap, then pick up tweezers to extract tea and pour it into the pot.\n \n Q 6: Can the whole framework handle extremely challeng- b) Data: We utilize two data types: (1) DEXCAP data\n ing bimanual dexterous manipulation tasks (e.g., using capturing human hand motion (In-the-wild data refers to a\n scissors and preparing tea)? mixture of data collected in more than 10 scenes) and (2)\n human-in-the-loop correction data for adjusting robot actions\n A. Experiment setups \n or enabling teleoperation to correct errors, collected using a\n a) Tasks: we evaluate DEXIL using six tasks of varying foot pedal. Data were initially recorded at 60 Hz and then\n difficulty to assess its performance with DEXCAP data. These downsampled to 20 Hz to match the robot’s control speed,\n tasksrangefrombasic,suchas Spongepicking,Ballcollecting, except for correction data, which was collected directly at\n and Plate wiping, which test single-handed and dual-handed 20 Hz.Fordatacollection,wegathered 30 minutesof DEXCAP\n coordination, to more complex ones like Packaging, which data across the first three tasks, resulting in 251, 179, and\n looksatbimanualtasksandgeneralizationusingbothfamiliar 102 demos respectively. An hour of in-the-wild DEXCAP\n and new objects. Scissor cutting focuses on the effectiveness data provided 104 demos for Packaging. Scissor Cutting and\n ofthehuman-in-the-loopcorrectionmechanisminprecisetool Tea Preparing tasks each received an hour of DEXCAP data,\n use,whereas Teapreparingchallengesthesystemwithalong- yielding 96 and 55 demos respectively.\n horizon task requiring intricate actions. To further analyze \n performance, we introduce the Subtask metric for multi-step c) Baselines: We evaluate multiple baselines to deter-\n tasks, indicating the completion of task subgoals, such as mine the model architecture with the best performance, fo-\n placing an object inside a box in Packaging, or picking up cusing on three key aspects using DEXCAP data: identifying\n scissors in Scissor Cutting. the best imitation learning framework for bimanual dexterous\n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n gnipiwetal P \n \n \n \n \n gnittucrossic S \n \n \n \n \n \n gnigakca P \n Raw Observation Right View Middle View Left View \n (Right View) \n \n \n Fig. 6: Data Retargeting for Tasks. DEXIL effectively retargets human mocap data for activities like plate wiping, scissor\n cutting, and packaging. The initial column displays the raw point cloud scene. Columns 2-7 offer three views—right, middle,\n left—withbluebackgroundcolumnsdepictinghumandataandyellowforrobothandretargeting.Thisside-by-sidearrangement\n highlights the precision of our fingertip IK in translating human to robot hand motions.\n \n \n tupn I \n \n \n \n \n Re Ma H \n \n \n \n \n sru O \n total trials) and 9 unseen objects (45 total trials).\n B. Results \n DEXCAP delivers high-quality 3 D mocap data (Q 1).\n Figure 6 showcases DEXCAP’sabilitytocapturedetailedhand\n motionin 3 D,aligninghumanactionswithobjectpointclouds\n across all views, such as in Plate wiping and Scissor cutting\n tasks (blue columns). The retargeted robot hand motions,\n depictedintheyellowcolumns,demonstrateprecisealignment\n inthesame 3 Dspace.In Figure 7,wecompare DEXCAP with\n the state-of-the-art vision-based hand pose estimation method\n Ha Me R [39], observing their performance from similar view-\n points. We find that the vision-based approach is vulnerable\n to self-occlusion, particularly when the fingers are obscured.\n As depicted in Figure 7, Ha Me R struggles in instances of\n significant occlusion, either failing to detect the hand (as seen\n Fig.7:Comparewithvision-basedmethod.Wedemonstrate \n in the second column) or inaccurately estimating fingertip\n that motion capture gloves provide more stable hand pose \n positions (noted in the first, third, and fourth columns). In\n estimation results compared to vision-based methods and are \n contrast, DEXCAP demonstrates good robustness under these\n not affected by visual occlusion. \n conditions. Beyond the challenge of occlusion, most vision-\n based methods rely on 2 D hand estimation, predicated on\n manipulation between BC-RNN [102] and diffusion policy learningfrom 2 Dimageprojectionlosses.Consequently,these\n (DP)[14], assessing the most effective observation type to methods are inherently limited in their ability to discern the\n bridge the visual gap between human and robot hands (com- precise 3 D hand positioning, as they are trained based on\n paringimageinputs[14,65]andapointcloudmethod[103]), presumed, fixed camera intrinsic parameters, which do not\n and determining the most suitable encoder for point cloud necessarily match the actual camera used for experiments. In\n inputs by comparing Point Net[104] and Perceiver [105, 106] Figure 8,weshowcasethedatacollectionthroughputof DEX-\n encoders.Implementationdetailsareincludedintheappendix. CAP,whichisthreetimesfasterthantraditionalteleoperation.\n d) Metric: Each model variant is tested for 20 trials in DEXCAPdatacandirectlytraindexterousrobotpolicies\n eachtaskwithrandomizedinitialplacements.Thetasksuccess (Q 2).Table Iistheexperimentresultoftrainingrobotpolicies\n rateisreportedin Table IIIIII.Forthemulti-object Packaging onlyusing DEXCAPdata.Within 30-minutehandmotioncap-\n task, each object is tested with 5 trials - 6 trained objects (30 turedemonstrationscollectedby DEXCAP,thelearnedpolicies"
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n (a). Human motion (b). Dex Cap (c). Teleoperation \n \n Fig.8:Datacollectionthroughputcomparison. DEXCAP’sdatacollectionspeedinthe Ballcollectingtaskisclosetonatural\n human motion and is three times faster than traditional teleoperation. \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 9: Visualization of human-in-the-loop corrections. DEXCAP supports teleoperation and residual correction for human-\n in-the-loop adjustments. Teleoperation directly translates human hand movements to the robot end-effector actions, indicated\n bycolor-fadingtrajectoriesfrombluetogreen(human)andredtoyellow(robot)over 20 timesteps.Residualcorrectionadjusts\n the robot’s end-effector based on changes from the human hand’s initial pose, enabling minimal movement but requiring more\n precise control. Users can switch between correction modes with a foot pedal.\n \n achieve up to 72% average task success rate in single-hand raw, DP-point, DP-prec), on the other hand, do not require\n pick-and-place(Spongepicking,Ballcollecting)andbimanual masking over observations and achieve more than 60% task\n coordination (Plate wiping) tasks. This result highlights the successrate.Thisresulthighlightstheadvantageofusingpoint\n effectiveness of DEXCAP data on training dexterous robot cloud inputs, which allow us to add robot hand points to the\n policies without on-robot data, which introduces a new way observationwithoutlosingthedetailsintheoriginalinputs.We\n for training robot dexterous manipulation. also observe that, even without adding robot hand points, DP-\n point-rawachievescloseperformanceto DP-point.Thismight\n Generative-based algorithm with point cloud inputs \n because the downsampling process of the point cloud inputs\n shows advantages (Q 3). In Table I, we compare the per- \n lowers the appearance gap between human gloves and robot\n formance of multiple model architectures. We first observe \n hands. Furthermore, compared to the Point Net, the model\n that, due to the visual appearance gap between human and \n with Perceiver encoder has higher performance, especially in\n robothands,thepolicieswithfullimageinputsfailcompletely \n bimanual tasks with multiple task objects (20% improvement\n (BC-RNN-img,DP-img).Wethentrymaskingouthumanand \n ontasksuccessratein Platewiping).Basedonthesefindings,\n robothandswithwhitecirclesintrainingandevaluation.This \n we use DP-perc as the default model architecture for DEXIL.\n setting brings improvements, where DP-img-mask achieves \n more than 30% success rate in all tasks. Meanwhile, diffusion DEXIL canpurelylearnfromin-the-wild DEXCAP data\n policy works better than MLP-based BC-RNN policies (25% (Q 4). The first three columns of Table II are the results of\n higher in averaged task success rate). This result verifies training policies using in-the-wild DEXCAP data. We first\n our hypothesis that generative-based policy is more suitable notice that image-input baselines (BC-RNN-img-mask, DP-\n for learning dexterous policies. Although getting promising img-mask)haveclosetozeroperformancewhenlearningwith\n results, masking out the end-effector loses details for in-hand in-the-wild data. This observation verifies our hypothesis that\n manipulation. This hypothesis is verified by the low success the viewpoint changes caused by human body movements\n rate in the Plate wiping task, which requires the robot to during in-the-wild data collection bring challenges to learn-\n use fine-grained finger motion to grab the plate from the ing image-based policies. Our DEXIL transforms the point\n edge. Our point cloud-based learning algorithms (DP-point- cloud inputs into a consistent world frame, resulting in stable"
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n DEXCAPData Only \n Scissorcutting \n DEXCAPData Only 30 humancorrections\n Spongepicking Ballcollecting Platewiping Overall Subtask All Subtask All\n BC-RNN-point[103] 0.00 0.00 0.10 0.00 \n BC-RNN-img 0.00 0.00 0.00 0.00 \n Ours 0.00 0.00 0.45 0.20 \n BC-RNN-img-mask[65] 0.25 0.10 0.10 0.15 \n BC-RNN-point[103] 0.45 0.30 0.25 0.33 \n BC-RNN-prec[105] 0.50 0.30 0.35 0.38 TABLE III: Quantitative results for the Scissor cutting task.\n DP-img 0.00 0.00 0.00 0.00 \n DP-img-mask[14] 0.55 0.40 0.30 0.42 DEXCAPData Only 30 humancorrections\n Teapreparing \n DP-point-raw 0.70 0.70 0.40 0.60 Subtask All Subtask All \n DP-point 0.75 0.65 0.50 0.63 \n Ours(DP-perc) 0.85 0.60 0.70 0.72 Ours 0.30 0.00 0.65 0.25 \n TABLEI:Quantitativeresultsforlearningwith DEXCAPdata. TABLE IV: Quantitative results for the Tea preparing task.\n In-the-wild DEXCAP 30 humancorrections \n Packaging the same setup used for the evaluations. This result further\n Subtask All Unseen Subtask All Unseen \n supports our conclusion: image-based approaches are more\n BC-RNN-img-mask[65] 0.00 0.00 0.00 0.23 0.07 0.00 \n effective in learning with fixed third-view cameras compared\n BC-RNN-point[103] 0.33 0.23 0.16 0.40 0.27 0.22 \n DP-img-mask[14] 0.17 0.00 0.00 0.47 0.33 0.00 to the in-the-wild scenarios with moving cameras. Human\n Ours 0.70 0.47 0.40 0.83 0.57 0.42 corrections also result in a 10% improvement in our approach\n that utilizes point cloud inputs. However, we’ve observed that\n TABLE II: Quantitative results for the Packaging task. \n fine-tuning with human corrections has a minor effect on the\n resultsforunseenobjects,primarilyduetothelimitedamount\n of correction data (30 trials in total). \n Ourwholeframeworkcanhandleextremelychallenging\n tasks (Q 6). DEXIL together with human-in-the-loop correc-\n tion is able to solve extremely challenging tasks such as\n Scissor cutting and Tea preparing. In Table III, we showcase\n that our system can achieve a 45% success rate on picking up\n Trainedobjects Unseenobjects the scissor from the container and 20% in cutting a piece of\n papertape.Inoursupplementaryvideo,wealsoshowcasehow\n Fig. 10: Objects used in the Packaging task \n the robot performs the long-horizon Tea preparing task which\n includes unscrewing a bottle cap and pouring tea into the pot.\n observations and thus getting better results (70% in Subtask Table IV presents the evaluation results of our approach (DP-\n and 47% in full task setup). Please refer to our video results perc) in the Tea preparing task. The subtask is defined as\n for more visualization of the stabilized input point clouds. By successfully unscrewing the cap of the tea bottle. We found\n training the policy with multiple task objects using in-the- thatevenwithhumanmocapdataonly(DEXCAP Data Only),\n wild (Fig. 10), our model can already generalize to unseen our model can achieve a 30% success rate in uncapping.\n object instances, with a 40% success rate. During evaluation, Most of the failures occur during the task of picking up\n we identified two primary issues with the policy learned the tweezers, which requires high-precision control over the\n from in-the-wild DEXCAP data: firstly, the absence of force fingertip. In such cases, human-in-the-loop correction signifi-\n informationin DEXCAP datacausestherighthandtostruggle cantly improves performance. With 30 human corrections, we\n with stabilizing the box during box closure attempts by the achievea 35%improvementintheuncappingsuccessrateand\n left hand. Secondly, the box lid occasionally moves out of the attain a 25% success rate for the entire task. Please refer to\n chest camera’s view due to human movements, hindering the our video submission for more qualitative results of this task.\n robot’s ability to learn precise lid grasping. These challenges These tasks showcase the high potential of our framework in\n prompt us to seek improvement strategies. learning extremely challenging dexterous manipulation tasks.\n Human-in-the-loop correction greatly help when DEX- \n CAP data is insufficient (Q 5). Figure 9 illustrates two types \n VI. CONCLUSIONANDLIMITATIONS \n of human-in-the-loop correction mode with DEXCAP. Users We present DEXCAP, a portable hand motion capture\n can switch between the two modes by stepping on the foot system, and DEXIL, an imitation algorithm enabling robots\n pedal and the whole trajectory is stored and used for fine- to learn dexterous manipulation directly from human mocap\n tuning the policy. The last three columns of Table II show- data.DEXCAP,designedtoovercomeocclusions,capturefine-\n case the effectiveness of using human-in-the-loop correction grained 3 D hand motion, record RGB-D observations, and\n together with policy fine-tuning to improve the model perfor- allow data collection outside the lab. DEXIL applies this data\n mance.Withjust 30 humancorrectiontrialsduringpolicyroll- toteachrobotscomplexdexterousmanipulationtasks,withan\n out, the fine-tuned policy with image inputs (DP-img-mask) optional human-in-the-loop correction mechanism to further\n achieves a 33% improvement in the full task success rate for improve performance. Demonstrating proficiency in tasks like\n trained objects. This significant boost is mainly because the scissor cutting and tea preparation, DEXCAP and DEXIL\n human correction data is collected using a fixed camera - significantlyadvanceroboticdexterity.Wehope DEXCAP can"
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n pave the path for future research on scaling up dexterous [6] Irmak Guzey, Ben Evans, Soumith Chintala, and Ler-\n manipulationdatawithportabledevices.Allhardwaredesigns rel Pinto. Dexterity from touch: Self-supervised pre-\n and code will be open-source. training of tactile representations with robotic play.\n While DEXCAP collects high-quality mocap data in-the- ar Xiv preprint ar Xiv:2303.12076, 2023.\n wild for learning challenging dexterous manipulation tasks, [7] Aravind Sivakumar,Kenneth Shaw,and Deepak Pathak.\n it has several limitations that need future research: (1) The Robotic telekinesis: Learning a robotic hand imita-\n system’s power consumption currently restricts the collection tor by watching humans on youtube. ar Xiv preprint\n time to be at most 40 minutes. Future improvements will ar Xiv:2202.10448, 2022.\n focus on enhancing power efficiency to extend the collection [8] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter\n time. (2) Our learning algorithm DEXIL utilizes fingertip Abbeel, and Sergey Levine. Avid:Learning multi-stage\n inverse kinematics to retarget human hand motion to various tasks via pixel-level translation of human videos. ar Xiv\n robotic hands. However, the size difference between human preprint ar Xiv:1912.04443, 2019.\n and robotic hands (with some robotic fingers being thicker) [9] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak.\n can make some tasks difficult to perform, such as playing the Human-to-robot imitation in the wild. ar Xiv preprint\n piano.Futuredevelopmentswillaimtointegrateadvancements ar Xiv:2207.09450, 2022.\n in robotic hand design to minimize these size differences and [10] Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang,\n fullydemonstratethesystem’spotential.(3)Current DEXCAP Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anand-\n collectsonly 3 Dobservationsandmotioncapturedata,lacking kumar. Mimicplay: Long-horizon imitation learning by\n force sensing. One promising direction we plan to explore watchinghumanplay.ar Xivpreprintar Xiv:2302.12422,\n involves the use of conformal tactile textiles, as introduced in 2023. \n [107], to gather tactile information during data collection. [11] Homanga Bharadhwaj, Abhinav Gupta, Vikash Kumar,\n and Shubham Tulsiani. Towardsgeneralizablezero-shot\n ACKNOWLEDGMENTS \n manipulation via translating human interaction plans.\n This research was supported by National Science Founda- ar Xiv preprint ar Xiv:2312.00775, 2023.\n tion NSF-FRR-2153854 and Stanford Institute for Human- [12] Suraj Nair, Aravind Rajeswaran, Vikash Kumar,\n Centered Artificial Intelligence,SUHAI.Thisworkispartially Chelsea Finn, and Abhinav Gupta. R 3 m: A universal\n supported by ONR MURI N 00014-21-1-2801. We would visual representation for robot manipulation. ar Xiv\n like to thank Yunfan Jiang, Albert Wu, Paul de La Sayette, preprint ar Xiv:2203.12601, 2022.\n Ruocheng Wang, Sirui Chen, Josiah Wong, Wenlong Huang, [13] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jiten-\n Yanjie Ze,Christopher Agia,Jingyun Yangandthe SVLPAIR dra Malik.Maskedvisualpre-trainingformotorcontrol.\n groupforprovidinghelpandfeedback.Wealsothank Zhenjia ar Xiv preprint ar Xiv:2203.06173, 2022.\n Xu, Cheng Chi, Yifeng Zhu for their suggestions on the [14] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric\n robot controller. We especially thank Kenneth Shaw, Ananye Cousineau, Benjamin Burchfiel, and Shuran Song. Dif-\n Agrawal, Deepak Pathak for open-sourcing the LEAP Hand. fusion policy: Visuomotor policy learning via action\n diffusion. ar Xiv preprint ar Xiv:2303.04137, 2023.\n REFERENCES \n [15] J Kenneth Salisbury and John J Craig. Articulated\n [1] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, hands: Force control and kinematic issues. The Inter-\n Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan national journal of Robotics research, 1(1):4–17, 1982.\n Ratliff, and Dieter Fox. Dexpilot: Vision-based tele- [16] Matthew T Mason and J Kenneth Salisbury Jr. Robot\n operation of dexterous robotic hand-arm system. In hands and the mechanics of manipulation. 1985.\n 2020 IEEE International Conference on Robotics and [17] Igor Mordatch, Zoran Popovic´, and Emanuel Todorov.\n Automation (ICRA), pages 9164–9170. IEEE, 2020. Contact-invariant optimization for hand manipulation.\n [2] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, In Proceedings of the ACM SIGGRAPH/Eurographics\n Edward Adelson, and Pulkit Agrawal. Visual dexterity: symposium on computer animation, pages 137–144,\n In-hand dexterous manipulation from depth. ar Xiv 2012. \n preprint ar Xiv:2211.11744, 2022. [18] Yunfei Bai and C Karen Liu. Dexterous manipulation\n [3] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak. usingbothpalmandfingers.In 2014 IEEEInternational\n Videodex: Learning dexterity from internet videos. Conference on Robotics and Automation (ICRA), pages\n Co RL, 2022. 1560–1565. IEEE, 2014. \n [4] Stefan Schaal. Is imitation learning the route to hu- [19] Vikash Kumar, Yuval Tassa, Tom Erez, and Emanuel\n manoid robots? Trends in cognitive sciences, 3(6):233– Todorov. Real-time behaviour synthesis for dynamic\n 242, 1999. hand-manipulation. In 2014 IEEEInternational Confer-\n [5] Ahmed Hussein,Mohamed Medhat Gaber,Eyad Elyan, ence on Robotics and Automation (ICRA), pages 6808–\n and Chrisina Jayne. Imitation learning: A survey of 6815. IEEE, 2014. \n learning methods. ACM Computing Surveys (CSUR), [20] Ankur Handa, Arthur Allshire, Viktor Makoviychuk,\n 50(2):1–35, 2017. Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys"
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, terity with immersive mixed reality. In 2023 IEEE\n Balakumar Sundaralingam, et al. Dextreme: Transfer International Conference on Robotics and Automation\n ofagilein-handmanipulationfromsimulationtoreality. (ICRA), pages 5962–5969. IEEE, 2023.\n ar Xiv preprint ar Xiv:2210.13702, 2022. [33] Yuzhe Qin,Yueh-Hua Wu,Shaowei Liu,Hanwen Jiang,\n [21] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for Ruihan Yang, Yang Fu, and Xiaolong Wang. Dexmv:\n general in-hand object re-orientation. Conference on Imitation learning for dexterous manipulation from hu-\n Robot Learning, 2021. man videos. In European Conference on Computer\n [22] Abhishek Gupta, Justin Yu, Tony Z. Zhao, Vikash Ku- Vision, pages 570–587. Springer, 2022.\n mar, Aaron Rovinsky, Kelvin Xu, Thomas Devlin, and [34] Priyanka Mandikal and Kristen Grauman. Dexvip:\n Sergey Levine. Reset-free reinforcement learning via Learning dexterous grasping with human hand pose\n multi-task learning: Learning dexterous manipulation priors from video. In Conference on Robot Learning,\n behaviors without human intervention. In ICRA, pages pages 651–661. PMLR, 2022.\n 6664–6671. IEEE, 2021. [35] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one\n [23] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng handtomultiplehands:Imitationlearningfordexterous\n Chen, and Xiaolong Wang. Rotating without seeing: manipulation from single-camera teleoperation. IEEE\n Towardsin-handdexteritythroughtouch.ar Xivpreprint Robotics and Automation Letters, 7(4):10873–10881,\n ar Xiv:2303.10880, 2023. 2022. \n [24] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, [36] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao, Wei\n and Jitendra Malik. In-Hand Object Rotation via Rapid Yang, Arsalan Mousavian, Abhishek Gupta, and Dieter\n Motor Adaptation. In Conference on Robot Learning Fox. Dextransfer: Real world multi-fingered dexterous\n (Co RL), 2022. grasping with minimal human demonstrations. ar Xiv\n [25] Gagan Khandate, Siqi Shang, Eric T Chang, Tristan L preprint ar Xiv:2209.14284, 2022.\n Saidi, Johnson Adams, and Matei Ciocarlie. Sampling- [37] Christian Zimmermann, Duygu Ceylan, Jimei Yang,\n based Exploration for Reinforcement Learning of Dex- Bryan Russell, Max Argus, and Thomas Brox. Frei-\n terous Manipulation. In Proceedings of Robotics: Sci- hand: A dataset for markerless capture of hand pose\n enceand Systems,Daegu,Republicof Korea,July 2023. and shape from single rgb images. In Proceedings of\n doi: 10.15607/RSS.2023.XIX.020. the IEEE/CVF International Conference on Computer\n [26] Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Vision, pages 813–822, 2019.\n Qin, Yaodong Yang, Nikolay Atanasov, and Xiaolong [38] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shira-\n Wang. Dynamic handover: Throw and catch with bi- tori, and Kyoung Mu Lee. Interhand 2. 6 m: A dataset\n manual hands. ar Xiv preprint ar Xiv:2309.05655, 2023. and baseline for 3 d interacting hand pose estimation\n [27] Yuanpei Chen, Chen Wang, Li Fei-Fei, and C Karen from a single rgb image. In Computer Vision–ECCV\n Liu. Sequential dexterity: Chaining dexterous poli- 2020:16 th European Conference,Glasgow,UK,August\n cies for long-horizon manipulation. ar Xiv preprint 23–28, 2020, Proceedings, Part XX 16, pages 548–564.\n ar Xiv:2309.00987, 2023. Springer, 2020. \n [28] Johannes Pitz, Lennart Ro¨stel, Leon Sievers, and [39] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic,\n Berthold Ba¨uml. Dextrous tactile in-hand manipulation Angjoo Kanazawa, David Fouhey, and Jitendra Malik.\n using a modular reinforcement learning architecture. Reconstructing hands in 3 d with transformers. ar Xiv\n ar Xiv preprint ar Xiv:2303.04705, 2023. preprint ar Xiv:2312.05251, 2023. \n [29] Kelvin Xu, Zheyuan Hu, Ria Doshi, Aaron Rovinsky, [40] Tanner Schmidt,Richard ANewcombe,and Dieter Fox.\n Vikash Kumar, Abhishek Gupta, and Sergey Levine. Dart: Dense articulated real-time tracking. In Robotics:\n Dexterous manipulation from images: Autonomous Science and systems, volume 2, pages 1–9. Berkeley,\n real-world rl via substep guidance. In 2023 IEEE CA, 2014. \n International Conference on Robotics and Automation [41] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and\n (ICRA), pages 5938–5945. IEEE, 2023. Vincent Lepetit.Honnotate:Amethodfor 3 dannotation\n [30] Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, of hand and object poses. In Proceedings of the\n and Jitendra Malik. Twisting lids off with two hands. IEEE/CVF conference on computer vision and pattern\n ar Xiv:2403.02338, 2024. recognition, pages 3196–3206, 2020. \n [31] Sridhar Pandian Arunachalam, Sneha Silwal, Ben [42] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov,\n Evans, and Lerrel Pinto. Dexterous imitation made Ankur Handa, Jonathan Tremblay, Yashraj S Narang,\n easy: A learning-based framework for efficient dexter- Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al.\n ousmanipulation. In 2023 ieeeinternationalconference Dexycb: A benchmark for capturing hand grasping of\n on robotics and automation (icra), pages 5954–5961. objects.In Proceedingsofthe IEEE/CVFConferenceon\n IEEE, 2023. Computer Visionand Pattern Recognition,pages 9044–\n [32] Sridhar Pandian Arunachalam, Irmak Gu¨zey, Soumith 9053, 2021. \n Chintala, and Lerrel Pinto. Holo-dex: Teaching dex- [43] Shangchen Han, Beibei Liu, Randi Cabezas, Christo-"
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n pher D Twigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho operation for legged robot loco-manipulation using\n Yu, Chun-Jung Tai, Muzaffer Akbay, Zheng Wang, wearable imu-based motion capture. ar Xiv preprint\n et al. Megatrack: monochrome egocentric articulated ar Xiv:2209.10314, 2022.\n hand-tracking for virtual reality. ACM Transactions on [54] Sylvain Calinon, Florent D’halluin, Eric L Sauser, Dar-\n Graphics (To G), 39(4):87–1, 2020. win G Caldwell, and Aude G Billard. Learning and\n [44] Omid Taheri, Nima Ghorbani, Michael J Black, and reproduction of gestures by imitation. IEEE Robotics\n Dimitrios Tzionas. Grab: A dataset of whole-body & Automation Magazine, 17(2):44–54, 2010.\n human grasping of objects. In Computer Vision–ECCV [55] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Move-\n 2020:16 th European Conference,Glasgow,UK,August ment imitation with nonlinear dynamical systems in\n 23–28, 2020, Proceedings, Part IV 16, pages 581–600. humanoid robots. In Proceedings 2002 IEEE Interna-\n Springer, 2020. tional Conference on Robotics and Automation (Cat.\n [45] Zicong Fan, Omid Taheri, Dimitrios Tzionas, No.02 CH 37292), volume 2, pages 1398–1403 vol.2,\n Muhammed Kocabas, Manuel Kaufmann, Michael J 2002. doi: 10.1109/ROBOT.2002.1014739.\n Black, and Otmar Hilliges. Arctic: A dataset [56] Jens Koberand Jan Peters. Imitationandreinforcement\n for dexterous bimanual hand-object manipulation. In learning. IEEE Robotics & Automation Magazine, 17\n Proceedingsofthe IEEE/CVFConferenceon Computer (2):55–62, 2010. \n Vision and Pattern Recognition, pages 12943–12954, [57] Peter Englert and Marc Toussaint. Learning manipu-\n 2023. lation skills from a single demonstration. The Inter-\n [46] Yinghao Huang, Manuel Kaufmann, Emre Aksan, national Journal of Robotics Research, 37(1):137–154,\n Michael JBlack,Otmar Hilliges,and Gerard Pons-Moll. 2018. \n Deepinertialposer:Learningtoreconstructhumanpose [58] Chelsea Finn,Tianhe Yu,Tianhao Zhang,Pieter Abbeel,\n from sparse inertial measurements in real time. ACM and Sergey Levine. One-shot visual imitation learning\n Transactions on Graphics (TOG), 37(6):1–15, 2018. via meta-learning. In Conference on robot learning,\n [47] Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam pages 357–368. PMLR, 2017.\n Won, Alexander W Winkler, and C Karen Liu. Trans- [59] Aude Billard,Sylvain Calinon,Ruediger Dillmann,and\n former inertial poser: Real-time human motion recon- Stefan Schaal. Robot programming by demonstration.\n struction from sparse imus with simultaneous terrain In Springer handbook of robotics, pages 1371–1394.\n generation. In SIGGRAPH Asia 2022 Conference Pa- Springer, 2008. \n pers, pages 1–9, 2022. [60] Brenna D Argall, Sonia Chernova, Manuela Veloso,\n [48] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shi- and Brett Browning. A survey of robot learning from\n mada,Vladislav Golyanik,Christian Theobalt,and Feng demonstration. Robotics and autonomous systems, 57\n Xu. Physical inertial poser (pip): Physics-aware real- (5):469–483, 2009. \n time human motion tracking from sparse inertial sen- [61] Stefan Schaal. Dynamic movement primitives-a frame-\n sors. In Proceedings of the IEEE/CVF Conference work for motor control in humans and humanoid\n on Computer Vision and Pattern Recognition, pages robotics. In Adaptive motion of animals and machines,\n 13167–13178, 2022. pages 261–280. Springer, 2006. \n [49] Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, [62] Jens Kober and Jan Peters. Learning motor primitives\n Scott Delp, and C Karen Liu. Diffusion inertial poser: for robotics. In 2009 IEEE International Conference\n Humanmotionreconstructionfromarbitrarysparseimu on Robotics and Automation, pages 2112–2118. IEEE,\n configurations. ar Xiv preprint ar Xiv:2308.16682, 2023. 2009. \n [50] Fabian C Weigend, Xiao Liu, and Heni Ben Amor. [63] Alexandros Paraschos, Christian Daniel, Jan R Pe-\n Probabilistic differentiable filters enable ubiquitous ters, and Gerhard Neumann. Probabilistic move-\n robot control with smartwatches. ar Xiv preprint ment primitives. In C.J. Burges, L. Bottou,\n ar Xiv:2309.06606, 2023. M. Welling, Z. Ghahramani, and K.Q. Weinberger,\n [51] Lars Fritsche, Felix Unverzag, Jan Peters, and Roberto editors, Advances in Neural Information Process-\n Calandra. First-person tele-operation of a humanoid ing Systems, volume 26. Curran Associates, Inc.,\n robot.In 2015 IEEE-RAS 15 th International Conference 2013. URL https://proceedings.neurips.cc/paper/2013/\n on Humanoid Robots (Humanoids), pages 997–1002. file/e 53 a 0 a 2978 c 28872 a 4505 bdb 51 db 06 dc-Paper.pdf.\n IEEE, 2015. [64] Alexandros Paraschos, Christian Daniel, Jan Peters,\n [52] Bin Fang,Di Guo,Fuchun Sun,Huaping Liu,and Yupei and Gerhard Neumann. Using probabilistic movement\n Wu. A robotic hand-arm teleoperation system using primitives in robotics. Autonomous Robots, 42(3):529–\n humanarm/handwithanoveldataglove. In 2015 IEEE 551, 2018. \n International Conference on Robotics and Biomimetics [65] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\n (ROBIO), pages 2483–2488. IEEE, 2015. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,\n [53] Chengxu Zhou, Christopher Peers, Yuhui Wan, Robert Silvio Savarese,Yuke Zhu,and Roberto Mart´ın-Mart´ın.\n Richardson, and Dimitrios Kanoulas. Teleman: Tele- Whatmattersinlearningfromofflinehumandemonstra-"
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n tions for robot manipulation. In 5 th Annual Conference [76] Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, and\n on Robot Learning, 2021. URL https://openreview.net/ Dorsa Sadigh. Efficient data collection for robotic\n forum?id=Jrsf BJt DFd I. manipulation via compositional generalization. ar Xiv\n [66] Peter Florence, Lucas Manuelli, and Russ Tedrake. preprint ar Xiv:2403.05110, 2024.\n Self-supervised correspondence in visuomotor policy [77] Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent\n learning. IEEE Robotics and Automation Letters, 5(2): Yi, Sergey Levine, and Jitendra Malik. Learn-\n 492–499, 2019. ing visuotactile skills with two multifingered hands.\n [67] Tony Z Zhao, Vikash Kumar, Sergey Levine, and ar Xiv:2404.16823, 2024. \n Chelsea Finn. Learning fine-grained bimanual ma- [78] Jiafei Duan, Yi Ru Wang, Mohit Shridhar, Dieter Fox,\n nipulation with low-cost hardware. ar Xiv preprint and Ranjay Krishna. Ar 2-d 2: Training a robot without\n ar Xiv:2304.13705, 2023. a robot. ar Xiv preprint ar Xiv:2306.13818, 2023.\n [68] Jennifer Grannen, Yilin Wu, Brandon Vu, and Dorsa [79] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso,\n Sadigh. Stabilize to act: Learning to coordinate for and Shuran Song. Xskill: Cross embodiment skill\n bimanualmanipulation. In Conferenceon Robot Learn- discovery. In Conference on Robot Learning, pages\n ing, pages 563–576. PMLR, 2023. 3536–3555. PMLR, 2023. \n [69] Tianhao Zhang,Zoe Mc Carthy,Owen Jow,Dennis Lee, [80] Jingyun Yang, Junwu Zhang, Connor Settle, Akshara\n Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep Rai, Rika Antonova, and Jeannette Bohg. Learning\n imitation learning for complex manipulation tasks from periodic tasks from human demonstrations. In 2022\n virtualrealityteleoperation. In 2018 IEEEinternational International Conference on Robotics and Automation\n conference on robotics and automation (ICRA), pages (ICRA), pages 8658–8665. IEEE, 2022.\n 5628–5635. IEEE, 2018. [81] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu,\n [70] Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao\n Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo\n Savarese, and Li Fei-Fei. Scaling robot supervision to Xu, et al. Rt-trajectory: Robotic task generaliza-\n hundreds of hours with roboturk: Robotic manipulation tion via hindsight trajectory sketches. ar Xiv preprint\n dataset through human reasoning and dexterity. In ar Xiv:2311.01977, 2023. \n 2019 IEEE/RSJ International Conference on Intelligent [82] Haoyu Xiong, Haoyuan Fu, Jieyi Zhang, Chen Bao,\n Robots and Systems (IROS), pages 1048–1055. IEEE, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh\n 2019. Garg, and Cewu Lu. Robotube: Learning household\n [71] Liyiming Ke, Ajinkya Kamat, Jingqiang Wang, Tapo- manipulation from human videos with simulated twin\n mayukh Bhattacharjee, Christoforos Mavrogiannis, and environments. In Conference on Robot Learning, pages\n Siddhartha S Srinivasa. Telemanipulation with chop- 1–10. PMLR, 2023. \n sticks:Analyzinghumanfactorsinuserdemonstrations. [83] Dima Damen, Hazel Doughty, Giovanni Maria\n In 2020 IEEE/RSJ International Conference on Intelli- Farinella, Sanja Fidler, Antonino Furnari, Evangelos\n gent Robots and Systems (IROS), pages 11539–11546. Kazakos, Davide Moltisanti, Jonathan Munro, Toby\n IEEE, 2020. Perrett, Will Price, et al. Scaling egocentric vision:\n [72] Chen Wang, Rui Wang, Ajay Mandlekar, Li Fei- The epic-kitchens dataset. In Proceedings of the\n Fei, Silvio Savarese, and Danfei Xu. Generaliza- European conference on computer vision (ECCV),\n tion through hand-eye coordination: An action space pages 720–736, 2018. \n for learning spatially-invariant visuomotor control. In [84] Kristen Grauman, Andrew Westbury, Eugene Byrne,\n 2021 IEEE/RSJ International Conference on Intelligent Zachary Chavis,Antonino Furnari,Rohit Girdhar,Jack-\n Robots and Systems (IROS), pages 8913–8920. IEEE, son Hamburger,Hao Jiang,Miao Liu,Xingyu Liu,etal.\n 2021. Ego 4 d: Around the world in 3,000 hours of egocentric\n [73] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke video. In Proceedings of the IEEE/CVF Conference\n Zhu. Viola: Imitation learning for vision-based ma- on Computer Vision and Pattern Recognition, pages\n nipulation with object proposal priors. ar Xiv preprint 18995–19012, 2022. \n ar Xiv:2210.11339, 2022. [85] Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao\n [74] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- Bao,and Yuke Zhu. Robotlearningonthejob:Human-\n gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana in-the-loop autonomy and learning during deployment.\n Gopalakrishnan,Karol Hausman,Alex Herzog,Jasmine ar Xiv preprint ar Xiv:2211.08416, 2022.\n Hsu, et al. Rt-1: Robotics transformer for real-world [86] Zhenghao Peng, Wenjie Mo, Chenda Duan, Quanyi\n controlatscale.ar Xivpreprintar Xiv:2212.06817,2022. Li, and Bolei Zhou. Learning from active human in-\n [75] Philipp Wu,Yide Shentu,Zhongke Yi,Xingyu Lin,and volvement through proxy value propagation. In Thirty-\n Pieter Abbeel. Gello: A general, low-cost, and intuitive seventh Conference on Neural Information Processing\n teleoperation framework for robot manipulators. ar Xiv Systems, 2023. \n preprint ar Xiv:2309.13037, 2023. [87] Jonathan Spencer, Sanjiban Choudhury, Matthew"
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n Barnes, Matthew Schmittle, Mung Chiang, Peter Ra- Michael Gleicher. Rangedik: An optimization-based\n madge, and Siddhartha Srinivasa. Learning from in- robot motion generation method for ranged-goal tasks.\n terventions. In Robotics: Science and Systems (RSS), pages 9700–9706, 2023.\n 2020. [99] Jascha Sohl-Dickstein, Eric Weiss, Niru\n [88] Shuran Song, Andy Zeng, Johnny Lee, and Thomas Maheswaranathan, and Surya Ganguli. Deep\n Funkhouser. Grasping in the wild: Learning 6 dof unsupervised learning using nonequilibrium\n closed-loop grasping from low-cost demonstrations. thermodynamics. In International conference on\n IEEE Robotics and Automation Letters, 5(3):4978– machine learning, pages 2256–2265. PMLR, 2015.\n 4985, 2020. [100] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\n [89] Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Ab- diffusion probabilistic models. Advances in neural\n hinav Gupta, Pieter Abbeel, and Lerrel Pinto. Visual information processing systems, 33:6840–6851, 2020.\n imitationmadeeasy. In Conferenceon Robot Learning, [101] Ajay Mandlekar, Danfei Xu, Roberto Mart´ın-Mart´ın,\n pages 1992–2005. PMLR, 2021. Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Human-in-\n [90] Kiran Doshi, Yijiang Huang, and Stelian Coros. On the-loop imitation learning using remote teleoperation.\n hand-held grippers and the morphological gap in ar Xiv preprint ar Xiv:2012.06733, 2020.\n human manipulation demonstration. ar Xiv preprint [102] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\n ar Xiv:2311.01832, 2023. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,\n [91] Felipe Sanches, Geng Gao, Nathan Elangovan, Ri- Silvio Savarese,Yuke Zhu,and Roberto Mart´ın-Mart´ın.\n cardo V Godoy, Jayden Chapman, Ke Wang, Patrick What matters in learning from offline human demon-\n Jarvis, and Minas Liarokapis. Scalable. intuitive hu- strations for robot manipulation. ar Xiv preprint\n man to robot skill transfer with wearable human ma- ar Xiv:2108.03298, 2021.\n chine interfaces: On complex, dexterous tasks. In [103] Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Hao Su,\n 2023 IEEE/RSJ International Conference on Intelligent and Xiaolong Wang. Dexpoint: Generalizable point\n Robots and Systems (IROS), pages 6318–6325. IEEE, cloud reinforcement learning for sim-to-real dexterous\n 2023. manipulation. In Conference on Robot Learning, pages\n [92] Hongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren, 594–605. PMLR, 2023.\n Jingjing Chen, Ruo Zhang, Weiming Wang, and Cewu [104] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J.\n Lu. Low-cost exoskeletons for learning whole-arm ma- Guibas. Pointnet: Deep learning on point sets for 3 d\n nipulationinthewild.ar Xivpreprintar Xiv:2309.14975, classification and segmentation. ar Xiv preprint ar Xiv:\n 2023. Arxiv-1612.00593, 2016. \n [93] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja [105] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew\n Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, Zisserman, Oriol Vinyals, and Joao Carreira. Per-\n and Lerrel Pinto. On bringing robots home. ar Xiv ceiver:Generalperceptionwithiterativeattention.ar Xiv\n preprint ar Xiv:2311.16098, 2023. preprint ar Xiv: Arxiv-2103.03206, 2021.\n [94] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, [106] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Ko-\n Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and siorek, Seungjin Choi, and Yee Whye Teh. Set trans-\n Shuran Song. Universal manipulation interface: In-the- former: A framework for attention-based permutation-\n wild robot teaching without in-the-wild robots. ar Xiv invariant neural networks. ar Xiv preprint ar Xiv: Arxiv-\n preprint ar Xiv:2402.10329, 2024. 1810.00825, 2018. \n [95] Kenneth Shaw, Ananye Agarwal, and Deepak Pathak. [107] Yiyue Luo, Yunzhu Li, Pratyusha Sharma, Wan Shou,\n LEAP Hand: Low-Cost, Efficient, and Anthropomor- Kui Wu, Michael Foshey, Beichen Li, Toma´s Palacios,\n phic Hand for Robot Learning. In Proceedings of Antonio Torralba, and Wojciech Matusik. Learning\n Robotics: Science and Systems, Daegu, Republic of human–environment interactions using conformal tac-\n Korea, July 2023. doi: 10.15607/RSS.2023.XIX.089. tile textiles. Nature Electronics, 4(3):193–201, 2021.\n [96] Daniel Rakita, Bilge Mutlu, and Michael Gleicher. Re- [108] Oussama Khatib. A unified approach for motion and\n laxed IK: Real-time Synthesis of Accurate and Feasible force control of robot manipulators: The operational\n Robot Arm Motion. In Proceedings of Robotics: Sci- space formulation. IEEE Journal on Robotics and\n ence and Systems, Pittsburgh, Pennsylvania, June 2018. Automation, 3(1):43–53, 1987.\n doi: 10.15607/RSS.2018.XIV.043. [109] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\n [97] Daniel Rakita, Haochen Shi, Bilge Mutlu, and Michael Sun. Deep residual learning for image recognition. In\n Gleicher. Collisionik: A per-instant pose optimization Proceedingsofthe IEEEconferenceoncomputervision\n method for generating robot motions with environment and pattern recognition, pages 770–778, 2016.\n collisionavoidance.In 2021 IEEEInternational Confer- [110] Jiaming Song, Chenlin Meng, and Stefano Ermon.\n ence on Robotics and Automation (ICRA), pages 9995– Denoising diffusion implicit models. ar Xiv preprint\n 10001. IEEE, 2021. ar Xiv:2010.02502, 2020. \n [98] Yeping Wang, Pragathi Praveena, Daniel Rakita, and "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n APPENDIXA \n IMPLEMENTATIONDETAILS \n \n A. DEXCAP hardware implementations \n Figure 11 illustrates the hardware design of DEXCAP. All \n models are 3 D-printed with PLA material. The chest camera \n mount is equipped with four slots for cameras: at the top, \n an L 515 RGB-D Li DAR camera, followed by three T 265 \n fisheye SLAM tracking cameras. The Li DAR camera and \n the uppermost T 265 camera are securely fixed to the camera \n Fig. 11: Detailed view of chest mount and glove mount\n rack, while the two lower T 265 cameras are designed to be \n The glove mount follows the contour of the hump on the\n detachable and can be affixed to the glove’s back for hand 6- \n top of the Rokoko glove, and an opening is added to route\n Do F pose tracking. The design features of the camera mounts \n the USB-C cable to the glove. The angle of the camera is\n on both the chest and gloves include a locking mechanism \n set to 45 degrees facing upwards so that the camera view is\n to prevent the cameras from accidentally slipping out. On the \n less obstructed from the back of the hand. The slide guide\n glove, the camera mount is positioned over the magnetic hub \n has an indentation matching the position of the back plate\n onitsdorsalside,ensuringafirmattachmentbetweenthehub \n to ensure the same insertion position across experiments. The\n and the mount. For powering and data storage, the user wears \n chest mount houses 3 identical slots following the contour of\n a backpack containing a 40000 m Ah portable power bank and \n the T 265. An additional slot is added to fit in the slide plate\n a mini-PC with 64 GB RAM and 2 TB SSD. The system’s \n of the T 265. \n total weight is 3.96 pounds, optimized for ease of mobility, \n supporting up to 40 minutes of continuous data collection. \n The power bank’s rapid recharge capability, requiring only 30 \n 10 minutes of each session prioritized for high-quality data\n minutes for a full charge, enables extensive data collection \n capture. After collection, transferring the data from RAM to\n sessions over several hours. \n SSD is efficiently completed within 3-5 minutes using multi-\n threading. \n B. Data collection details \n Inthisstudy,weprimarilyinvestigatetwotypesof DEXCAP\n Figure 13 andthesupplementaryvideoillustratethebegin- data:(1)datacapturedintherobotspaceand(2)datacollected\n ning steps of a data collection session. Initially, all cameras inthewild.Forthefirstcategory,wepositionthechestcamera\n are mounted on the chest. Upon initiating the program, the setup on a stand between two robot arms. The robots are then\n participant moves within the environment for several sec- adjusted to a resting position, clearing the operational space\n onds, allowing the SLAM algorithm to build the map of for human interaction. This arrangement allows for the direct\n the surroundings. Subsequently, the bottom T 265 cameras are use of DEXCAP to collect data within the robot’s operational\n relocated to the glove mounts, initiating the data collection area. Such data underpins basic experiments for tasks like\n phase. This preparatory phase is completed in approximately Sponge picking, Ball collecting, and Plate wiping, alongside\n 15 seconds, as demonstrated in the video submission. more complex challenges, including Scissor cutting and Tea\n The data collection encompasses four data types, recorded preparing. For the second category, individuals don DEXCAP\n at 60 frames per second: (1) the 6-Do F pose of the chest- togatherdataoutsidethelabsetting,focusingonthesystem’s\n mounted Li DAR camera, as tracked by the top T 265 camera; zero-shotlearningperformancewithin-the-wild DEXCAPdata\n (2) the 6-Do F wrist poses, as captured by the two lower T 265 and its ability to generalize to unseen objects, particularly in\n cameras attached to the gloves; (3) the positions of finger the Packaging task.\n joints within each glove’s reference frame, detected by the \n motion capture gloves; and (4) RGB-D image frames from C. Data retargeting details\n the Li DAR camera. The initial pose of the top T 265 camera To adapt the collected raw DEXCAP data for training robot\n establishes the world frame for all data, allowing for the policies (commonly known as retargeting). This involves two\n integration of all streamed data—RGB-D point clouds, hand key steps: (1) retargeting the observations and (2) retargeting\n 6-Do F poses, and finger joint locations—into a unified world the actions. \n frame. This configuration permits unrestricted movement by For observation retargeting, the initial step is to convert the\n the participant, enabling easy isolation and removal of body RGB-D inputs into 3 D point clouds, ensuring each pixel’s\n movements from the dataset. colorispreserved.Thesepointcloudsarethenalignedwiththe\n Data are initially buffered in the mini-PC’s RAM, support- worldframe,definedbytheinitialposeofthemain T 265 cam-\n ing a 15-minute collection at peak frame rate (60 fps). Once era. Subsequently, a point cloud visualization UI is launched,\n the RAM is full, data capture slows to 20 fps due to storage displaying the aligned input point clouds alongside the robot\n shifting to the SSD. We empirically find that this reduction operation space’s point clouds within a unified coordinate\n in frame rate may affect SLAM tracking accuracy, potentially frame. Through this UI, users can adjust the point cloud’s\n leading to jumping tracking results. Thus, we use the first positionwithintherobotoperationspaceusingthekeyboard’s"
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n directionalkeys.Thisadjustmentprocessisrequiredonlyonce Hyperparameter Default\n for all data collected in the same location and is completed in Batch Size 16 \n under a minute. After aligning the point clouds with the robot Learning Rate(LR) 1 e-4\n Num Epoch 3000 \n space, points below the robot’s table surface are eliminated, \n LRDecay None \n refining the observation data for policy development. Image Encoder Res Net-18 \n Action retargeting begins with applying a consistent trans- Image Feature Dim 64\n RNNType LSTM \n formation between the T 265 cameras on the chest mount to RNNHorizon 3 \n translate the hand joint locations into the world frame. Then, GMM None \n we use the previously calculated point cloud transformation \n TABLE V: Hyperparameters - BC-RNN-img\n matrix to transform the hand joints to the robot operation \n space.Theresultsofthisprocessarevisualizedin Figure 12 by \n Hyperparameter Default \n depicting the transformed hand joints together with the point \n Batch Size 16 \n cloudasaskeletalmodelofthehand.Thefinalphaseemploys \n Learning Rate(LR) 1 e-4 \n inverse kinematics to map the fingertip positions between the Num Epoch 3000 \n LRDecay None \n robot hand (LEAP hand) and the human hand. We use the \n Point Cloud Encoder Point Net \n hand’s 6-Do F pose to initialize the LEAP hand’s orientation Point Cloud Downsample 1000\n for IKcalculation.Figure 12 illustratesthe IKresults,showing Pooling Type Max Pooling\n UNet Embed Dim 256 \n the robot hand model integrated with the observational point UNet Downdims [256,512,1024]\n clouds,therebygeneratingtheactionsrequiredfortrainingthe UNet Kernel Size 5 \n Diffusion Type DDIM \n robot policy. Diffusion Num Train 100 \n All of the point cloud observations are downsampled uni- Diffusion Num Infer 10\n Input Horizon 3 \n formly to 5000 points and stored together with robot propri- \n oception states and actions into an hdf 5 file. We manually TABLE VI: Hyperparameters - DP-point\n annotate the start and end frames of each task demonstration \n from the entire recording session (10 minutes each). The \n motion for resetting the task environment is not included in the inputs is set to three. For pointcloud-based methods, the\n the training dataset. input point cloud is uniformly downsampled to 1000 points.\n We list the hyperparameters for each architecture in Table V,\n D. Robot controller details VI, VII. \n Position control is employed throughout our experiments, \n F. Task implementations \n structured hierarchically: (1) At the high level, the learned \n policy generates the goal position for the next step, which In this section, we introduce the details of each task design\n encompasses the 6-Do F pose of the end-effector for both • Sponge Picking: A sponge is randomly placed on the\n robotarmsanda 16-dimensionalfingerjointpositionforboth table within a 40×70 centimeter area. The objective is\n hands. (2) At the low level, an Operational Space Controller to grasp the sponge and lift it upwards by more than 30\n (OSC) [108], continuously interpolates the arm’s trajectory centimeters. \n towardsthehigh-levelspecifiedgoalpositionandrelaysinter- • Ball Collecting: A ball is randomly positioned on the\n polated OSC actions to the robot for execution. Meanwhile, right side of the table within a 40×30 centimeter area,\n finger movements are directly managed by a joint impedance while a basket is similarly placed randomly on the left\n controller. Following each robot action, we calculate the dis- side within the same dimensions. The task is completed\n tancebetweentherobot’scurrentproprioceptionandthetarget whentheballisgraspedandthendroppedintothebasket.\n pose.Ifthedistancebetweenthemissmallerthanathreshold, • Plate Wiping: In a setup akin to the Ball Collecting task,\n we regard that the robot has reached the goal position and aplateandaspongearerandomlyplacedontherightand\n will query the policy for the next action. To prevent the robot left sides of the table, respectively, each within a 40×30\n from becoming idle, if it fails to reach the goal pose within centimeter area. The goal involves using both hands to\n h steps, the policy is queried anew for the subsequent action. pick up the plate and sponge separately, then utilizing\n We designate h=10 in our experiments. We empirically find the sponge to wipe the plate twice. This task demands\n that for tasks that consist of physical contact with objects or coordinationbetweenthetwohands,positioningtheplate\n applyingforce,thissituationhappensmoreoftenandasmaller in the table’s middle area to facilitate the wiping action.\n h will have a smoother robot motion. • Packaging: An empty paper box and a target object are\n randomly positioned on the table, with the object within\n E. Policy model and training details \n a 40×30 centimeter area on the right and the box within\n For all image-input methods, we use Res Net-18 [109] as a 10×10 centimeter area on the left. This task aims to\n the image encoder. For models based on diffusion policy, we assess the model’s ability to generalize across various\n use Denoising Diffusion Implicit Models (DDIM) [110] for objects, including unseen ones not present in the training\n the denoising iterations. For all baselines, the time horizon of dataset. Success involves using one hand to pick up the"
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n 𝑡 \n \n \n \n gnipiw \n \n etal P \n \n \n \n \n \n \n gnittuc \n \n \n rossic S \n \n \n \n \n \n \n gnigakca P \n Human \n Robot \n Human \n Robot \n Human \n Robot \n Fig.12:Visualizationofcollectedhumandataandretargetedrobotdata.DEXILsuccessfullyadaptshumanmotioncapture\n data for tasks such as plate wiping, scissor cutting, and packaging. We demonstrate the entire workflow of executing these\n tasks. \n \n \n Hyperparameter Default • Scissor Cutting: A container is fixed at the table’s center,\n Batch Size 16 with scissors on the left and a strip of paper tape on\n Learning Rate(LR) 1 e-4 \n the right. The task begins with the left hand function-\n Num Epoch 3000 \n LRDecay None ally grasping the scissors—inserting the thumb into one\n Point Cloud Encoder Perceiver \n handle and the index and middle fingers into the other.\n Point Cloud Downsample 1000 \n Pooling Type Max Pooling Simultaneously,therighthandgraspsthepapertape.Both\n UNet Embed Dim 256 \n scissors and tape are then lifted and moved towards the\n UNet Downdims [256,512,1024] \n UNet Kernel Size 5 center, with the left hand operating the scissors to cut\n Diffusion Type DDIM \n the tape. A cut exceeding 3 millimeters deems the task\n Diffusion Num Train 100 \n Diffusion Num Infer 10 successful. \n Input Horizon 3 \n • Tea Preparing:Ateatableiscentrallyplacedwithafixed\n TABLE VII: Hyperparameters - Ours (DP-prec) orientation, accompanied by a tea bottle, tweezers, and\n a teapot. The robot must first grasp the tea bottle with\n the left hand and unscrew the cap with the right hand,\n completing two rotations. The cap is then taken off and\n objectandtheothertomovetheboxtothetable’scenter. \n placedontherightsideoftheteatable.Subsequently,the\n The object is then placed into the box, followed by \n righthandpicksupthetweezersfromthetoprightcorner\n stabilizing the box with one hand while the other closes \n oftheteatable.Therobotthenattemptstopourteafrom\n it by grasping and moving the lid. "
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 13: Prepration of data collection in the wild. The first row illustrates data collection conducted in a laboratory setting,\n and the second row depicts in-the-wild data collection. (a) Initially, the human data collector moves around in the environment\n totrack 6-Do Fwristposeswith SLAM.(b)-(d)Subsequently,thedatacollectordetachesthetwocamerasfromthechestmount\n and secures them onto the glove mount. (e) With this setup, the human is prepared to begin data collection.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig.14:Switching DEXCAP fromthehumantotherobot.Weillustrate,frombothfirst-personandfrontviews,theseamless\n transition of DEXCAP from a human data collector to a bimanual dexterous robot system. This process involves effortlessly\n detaching the cameras from the chest mount and inserting them into a stationary mount on the robot’s table.\n \n the bottle into the teapot with the left hand, while the additionalcorrectiondata,whichisusedinfurtherrefiningthe\n right hand uses the tweezers to aid the pouring process. policyforenhancedtaskperformance.Detaileddescriptionsof\n Finally, the robot returns the tweezers and the tea bottle these algorithms and their implementation are provided in the\n to their corresponding positions on the table. The task is main paper. In the human-in-the-loop process, we employ the\n deemedsuccessfulifteamakesitintotheteapotandboth mini-PC to live stream data from all T 265 tracking cameras.\n theteabottleandtweezersarereturnedtotheirrespective Thistrackinginformationisthentransmittedtoa Redisserver\n places.Forthetasktobeconsideredfullysuccessful,the configured on the local network. Concurrently, the robot,\n teabottlemustbecompletelyreleasedfromthelefthand. operating the learned policy on a workstation, receives delta\n movements of the human hands from the Redis server. These\n G. Human-in-the-loop implementations \n deltasserveasresidualcorrectionsandareintegratedintoeach\n DEXCAP incorporates two human-in-the-loop correction \n robot action. The RGB-D Li DAR camera, positioned on the\n methodologies: teleoperation and residual correction. Both \n centralbarbetweentherobotarms,connectstotheworkstation\n methods can be utilized during policy rollouts to gather "
  },
  {
    "page_num": 20,
    "text": " \n \n \n \n \n \n desab-UMI \n \n \n \n \n )sru O( \n \n \n UMI-MALS \n Trajectory overview Zoom-in result \n Start \n End \n Start & End \n Fig. 15: Compare with IMU-based mocap system. We \n disablethe SLAMmappingandpose-correctionfeaturesofthe \n T 265 tracking camera, forcing it to rely on IMU information \n totrackthepose.Thehumanoperatorheldthecamera,started \n from a fixed location, moved it along a predefined trajectory, \n and then returned to the starting position. IMU-based method \n (first row) fails to match the endpoint with the start point, \n which indicates that there is pose drift during tracking. Our \n SLAM-IMU method (second row) doesn’t drift and captures \n smooth trajectory during the tracking. \n Drifting error (cm) Trajectory 1 Trajectory 2 \n IMU-based 8.0±3.1 11.3±4.7 \n SLAM-IMU (Ours) 0.4±0.2 0.8±0.3 \n TABLE VIII: Drifting error of different tracking methods. \n \n \n to capture observation data. Instead of recording the robot’s \n actual positional changes, we log the action commands dis- \n patchedtotherobotcontroller.Thisdesigniscrucialfortasks \n involving physical contact with the environment and objects. \n APPENDIXB \n SUPPLEMENTARYEXPERIMENTRESULTS \n A. Tracking accuracy \n Figure 15 and Table VIIIpresentqualitativeandquantitative \n results, respectively. We observe that the IMU-based method \n suffers from pose drifting during tracking, while our SLAM- \n IMU approach more accurately tracks hand poses, with an \n average error of 0.8 cm compared to the 11.3 cm error of the \n IMU-based method. \n \n \n \n \n \n \n \n \n \n \n \n "
  }
]