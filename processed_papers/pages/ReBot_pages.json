[
  {
    "page_num": 1,
    "text": " \n \n \n Teaser \n \n Real-to-Sim \n Trajectory \n Re Bot: Scaling Robot Learning with \n Replay \n Real-to Rea-l-Swoirldm-to-Real Robotic Video Synthesis \n Background \n Inpainting \n Real-world Real-to-Sim Trajectory Replay\n Yu Fang 1, Yue Yang 1, X Baicnkggrhouanod In Zpahinuti 2 ng, Kaiyuan Zheng 3, Gedas Bertasius 1, Daniel Szafir 1, Mingyu Ding 1\n Sim-to-Real VLA\n Abstract—Vision-language-action Rea ( l V -to L -S A im ) models present a Open VLA Video Models\n Trajectory Replay Synthesis\n promising paradigm by training policies directly on real robot \n Octo w/o Re Bot \n datasets like Open X-Embodimen Rte.al H-woorwldever, the high cost w/ Re Bot \n of real-world data collection Bhacikngdroeurnsd Infpuarintthinegr data scaling, Bridge Data V 2 DROID Re Bot Finetuned\n thereby restricting the generalizability of VLAs. In this paper, Real Robot Datasets VLA performance VLA Models\n we introduce Re Bot, a novel real-to-sim-Rteoa-lr-teoa-Slimap proach for \n Trajectory Replay \n scaling real robot datasets and adapting VLA models to \n Episode: Move the yellow mug to New Episode:\n target domains, which is the last-mile deployment challenge in the front right side of the table Put the spoon on the towel\n robotmanipulation.Specifically,Re Botrep R l e a a y l s -w r o e r a ld l- worldrobot Put spatula on cutting board\n Background Inpainting \n trajectoriesinsimulationtodiversifymanipulatedobjects(real- Real-to-Sim Trajectory Real-world Robot Trajectories\n to-sim),andintegratesthesimulatedmovementswithinpainted Replay Sim-to-Real \n real-world background to synthesize p Shiyms-itcoa-Rlleyal realistic and Video\n Video Synthesis Synthesis \n temporally consistent robot videos (sim-to-real). Our approach \n has several advantages: 1) it enjoys the benefit of real data Real Video Real-world Real-to-Sim-to-Real\n Background \n to minimize the sim-to-real gap; 2) it leverages the scalability Synthetic Video\n Inpainting \n of simulation; and 3) it can generalize a pretrained VLA to a Take banana out of colander\n target domain with fully automated data pipelines. Extensive Mug Spoon \n experiments in both simulation and real-world environments \n show that Re Bot significantly enhances the performance and \n robustness of VLAs. For example, in Simpler Env with the Fig. 1. An overview of Re Bot. We propose Re Bot, a novel real-to-\n Widow X robot, Re Bot improved the in-domain performance sim-to-real approach for scaling real robot datasets. Re Bot replays real-\n of Octo by 7.2% and Open VLA by 21.8%, and out-of-domain worldrobottrajectoriesinasimulationenvironmenttodiversifymanipulated\n generalizationby 19.9%and 9.4%,respectively.Forreal-world objects(real-to-sim),andintegratesthesimulatedmovementswithinpainted\n evaluation with a Franka robot, Re Bot increased the success real-world background to produce realistic synthetic videos (sim-to-real),\n effectivelyadapting VLAmodelstotargetdomains.\n ratesof Octoby 17%and Open VLAby 20%.Moreinformation \n can be found at our project page. \n generalizing to real-world applications [12, 13], limiting the\n I. INTRODUCTION effectiveness of simulated data for advancing VLAs.\n Large-scale real robot datasets have demonstrated their To tackle these challenges, a straightforward strategy for\n significant contribution to the rapid advances of robot learn- scaling robot learning is generating synthetic robot videos\n ing [1–3], enabling vision-language-action (VLA) models to from real robot datasets. With the rapid development of\n learn across various tasks, environments, and embodiments. foundation models in computer vision and generative AI,\n Despite these achievements, VLAs still face challenges researchers have introduced generative models for synthetic\n in effectively generalizing to new scenarios, spurring the robot video generation [14–16]. For example, methods [17–\n need for scaling data to enhance their performance in new 19] have leveraged text-to-image inpainting to scale real\n target domains. However, collecting large-scale real robot robotic images to diverse scenarios. However, they typically\n datasets is very costly and often demands extensive effort face the issue of AI-generated artifacts such as visible\n and resources, e.g., robots and human teleoperators, which imperfections or inconsistent textures, failing to produce\n significantly limits the availability and scalability [4, 5]. physically realistic and temporally consistent robot videos.\n On the other hand, simulated datasets are more accessible Such distortions introduce new domain gaps, making it dif-\n and cost-effective alternatives, as they can be generated ficult for VLAs to learn stable and continuous robot actions\n in simulation environments without real-world setups [6– while raising reliability concerns. Additionally, generated\n 11]. Unfortunately, the sim-to-real gap in both the action images may not adhere precisely to instruction conditions,\n space and the observation space hinders robot policies from limitingtheeffectivenessofsuchmethodsinadapting VLAs\n to specific target domains, leaving the last-mile deployment\n 1 Yu Fang,Yue Yang,Gedas Bertasius,Daniel Szafir,and Mingyu Ding challenge in robot manipulation unresolved.\n arewith Departmentof Computer Science,Universityof North Carolinaat To mitigate these issues, we propose Re Bot, a novel real-\n Chapel Hill,201 SColumbia St,Chapel Hill,NC 27599,USA.{yufang, \n yygx, gedas, dszafir, md}@cs.unc.edu to-sim-to-real approach for scaling real robot datasets and\n 2 Xinghao Zhu is with Robotics and AI Institute, 145 Broadway, adapting VLA models to target domains. Our key insight\n Cambridge,MA 02142,USA.xizhu@rai-inst.com \n is to replay real-world robot trajectories in simulation to\n 3 Kaiyuan Zhengiswith Departmentof Electricaland Computer Engi- \n diversify manipulated objects (real-to-sim), and integrate\n neering,Universityof Washington,1410 NECampus Parkway,Seattle,WA \n 98195,USA.kaiyuan 5@uw.edu the simulated movements with inpainted real-world back-\n 5202 \n ra M \n 51 \n ]VC.sc[ \n 1 v 62541.3052:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n ground (sim-to-real) to synthesize physically realistic and real robot datasets demands extensive resources, making it\n temporallyconsistentrobotvideos.Notably,Re Botcombines highly challenging to scale across diverse environments and\n the advantages of both sim and real, i.e., leveraging the tasks.Thislimitationhindersthegeneralizationperformance\n scalability of simulation, while minimizing the sim-to-real of VLA models. On the other hand, simulated datasets\n gap by grounding both the action and observation spaces offer a more scalable alternative. Well-developed simulation\n from real robot data. Particularly, in contrast to generation- platforms [31–34] facilitate rapid data collection in con-\n based scaling approaches, Re Bot ensures physical realism trolled environments without the high cost of real-world\n and temporal consistency, and enables effective adaptation experiments. Unfortunately, these datasets often introduce\n of VLA models to target domains. significant sim-to-real gap [12], limiting their effectiveness\n Specifically, as shown in Fig. 1, Re Bot includes three in real-world applications. Notably, recent works have ex-\n key components: 1) Real-to-Sim Trajectory Replay. For ploredgenerativemodelstoscalerealrobotdatasets[17–19].\n each real-world episode, we automatically set up digital Yet, these approaches often struggle to provide physically\n twins in a simulation environment, and replay the real- realistic and temporal consistent robot videos, making them\n world robot trajectory to obtain simulated movements for unreliable and ineffective for developing VLA models. In\n manipulating new objects. We validate the scalability of this paper, we propose a real-to-sim-to-real approach for\n our approach by demonstrating that real-world trajectories scalingrealrobotdatasets,offeringanovelsolutionforthese\n can be successfully reused to manipulate different shapes of longstanding challenges.\n objects in simulation. 2) Real-world Background Inpainting. Real-to-sim and Sim-to-real. Real-to-sim and sim-to-real\n To obtain task-agnostic real-world background for video strategies have been explored in many applications in\n synthesis,weintroduceanautomatedinpaintingmodulewith robotics [13, 35–38]. Notably, recent work has leveraged\n Grounded SAM 2 [20] to segment and track the robot and real-to-sim-to-real strategy to develop simulated evaluation\n object (i.e., task-specific elements) in original real-world platforms for robotics [39], demonstrating a strong cor-\n videos, and remove them with Pro Painter [21]. 3) Sim-to- relation with real-world robot evaluations. These studies\n Real Video Synthesis. We eventually integrate simulated highlight the significant potential of real-to-sim-to-real ap-\n movements with task-agnostic real-world background, pro- proaches in bridging the gap between simulation and real-\n ducing synthetic videos with realistic physics and excellent world environments. However, existing methods often face\n temporal consistency. scalability challenges due to limited scene and object diver-\n In summary, our key contributions are three-fold. sity, primarily due to the substantial manual effort for con-\n • We introduce Re Bot, which, to our knowledge, is the first structing digital twins in simulation environments [37, 39].\n real-to-sim-to-real approach for scaling real robot datasets In this paper, we explore a new application of this strategy,\n and adapting VLA models to target domains, addressing i.e., for scaling real robot datasets, enabling realistic robotic\n the last-mile deployment challenge in robot manipulation. video generation without manual intervention.\n • Re Bot combines the advantages of both sim and real, i.e., \n leveraging the scalability of simulation, while minimizing III. METHOD \n the sim-to-real gap by grounding both the action and \n In this paper, we propose a novel real-to-sim-to-real ap-\n observation spaces from real robot data. Notably, Re Bot \n proach for scaling real robot datasets. We define a real robot\n is fully automated and requires no manual intervention. \n dataset as D = {τ }M , where M episodes are represented\n • Extensive evaluations confirm Re Bot’s effectiveness in as τ = {o ,a ,L i } i T =1 . Here, t denotes the timestep, o\n both simulation and real-world settings, e.g., it improves i t t t=1 t \n is the video frame, a is the action, L is the language\n t \n Open VLA’s in-domain and generalization performance by \n instruction. Our goal is to produce new synthetic episodes\n 21.8% and 9.4% on Simpler Env and achieves a 20% gain \n τ′ ={o′,a ,L′}T basedonτ ,tobuildasyntheticdataset\n inreal-worldtasks,significantlyoutperformingpriorstate- j t t t=1 i \n D′ = {τ′}N for adapting VLA models to target domains.\n of-the-art ROSIE [18]. j j=1 \n As illustrated in Fig. 2, Re Bot has three key steps: A) Real-\n II. RELATEDWORK to-Sim Trajectory Replay to obtain simulated movements\n {osim}T inasimulationenvironment(Sec.III-A);B)Real-\n Scaling Robot Learning. Although many research insti- t t=1 \n world Background Inpaintingonvideoframe{o }T toob-\n tutes have collaborated to construct large-scale real robot t t=1 \n taintask-agnosticreal-worldbackground{oreal}T (Sec.III-\n datasets [4, 5], data scale remains a fundamental bottle- t t=1 \n B);andeventually C)Sim-to-Real Video Synthesistoobtain\n neck for VLA models. To address this issue, recent works \n new frame {o′}T (Sec. III-C). \n have explored three primary strategies: 1) collecting data t t=1 \n in real-world environments, 2) collecting data in simulation \n A. Real-to-Sim Trajectory Replay \n environments, and 3) scaling real robot datasets with gen- \n erative models. Real robot datasets can be acquired using The real-to-sim process involves: 1) Creating spatially\n various methods, including kinesthetic teaching [22, 23], aligned digital twins of the scene in the simulation environ-\n teleoperation [5, 24–26], or mixed reality devices [27, 28], ment, 2) Replaying real-world robot trajectory to produce\n and have significantly contributed to the recent progress simulated robot movements {osim}T , 3) Validating each\n t t=1 \n in VLA models [29, 30]. However, collecting large-scale replayed trajectory to ensure successful object manipulation."
  },
  {
    "page_num": 3,
    "text": " Framework \n Latest version \n \n \n Real-to-Sim Trajectory Replay (Sec. III-A) Real-world Background Inpainting (Sec. III-B) Sim-to-Real Video Synthesis (Sec. III-C)\n \n (1) Scene Parsing and Alignment (2) Trajectory Replay Object and Robot Segmentation\n Real-world Episode Object Container \"robot\" Pr T o e m xt pt \n Time (e.g., spoon) (e.g., towel) \n Frames P P ro o m in p t t \n Grounded SAM 2 \n Action \n Instruction : Move the yellow mug to Simulated Movements Synthesize \n the front right side of the table \n (3) Replay Validation \n Synthetic Episode \n Robot Camera Semantic Masks Time \n Pro Painter New \n Table Height Frames \n Action \n Digital Twins \n (tableis invisible later) Task-agnostic Real-world Instruction : Put the spoon on the towel\n Point Cloud Filtered Points Background \n Fig.2. An overview of our framework.Re Botincludesthreekeycomponents:A) Real-to-Sim Trajectory Replay:Foreachreal-worldepisode,we\n automatically set up digital twins and replay the real-world trajectory to obtain simulated movements for manipulating new objects. Each trajectory can\n bereusedfordifferentobjects.B)Real-world Background Inpainting:Toobtaintask-agnosticreal-worldbackgroundforvideosynthesis,weintroduce\n an automated inpainting module to segment and remove the robot and object from the original real-world video. C) Sim-to-Real Video Synthesis: We\n eventuallyintegratesimulatedmovementswithtask-agnosticreal-worldbackgroundtoproducesyntheticvideos.Re Botisfullyautomatedandrequiresno\n manualintervention. \n Scene Parsing and Alignment.Toensurefaithfultrajectory distance between the object and gripper from t to t .\n TODO list: start end \n replay, we construct digital twins of the robot, cameras, and We present a representative example in Fig. 2, showing that\n - Notation \n table, and align them to the initial video frame o . The despite the disparity of object shapes, real-world trajectories\n - Integration in Sim-to-real 1 \n prototypes o-f Cthhaengreo Ibmoatgeasnd cameras are prepared ahead, can be successfully reused to manipulate various objects,\n only requiring pose adjustments to complete their setup. demonstrating the scalability of our approach.\n To determine the table height, we acquire the metric depth \n B. Real-world Background Inpainting \n from the initial video frame o and create a point cloud \n 1 \n of the scene. Using Grounding DINO [40], we automatically In this step, we prepare task-agnostic real-world back-\n segment the table with the text prompt (“table”), and extract ground{or t eal}T t=1 forintegrationwithsimulatedmovements,\n the subset of the point cloud after removing outliers using by removing task-specific elements (i.e., the original real\n the interquartile range. We eventually set the average height object and robot) in the original real robot video {o t }T t=1 .\n of the filtered points as the table height. Object and Robot Segmentation. We automatically seg-\n ment and track the original real object and robot by using\n Trajectory Replay. We reuse the real-world trajectory to \n Grounded SAM 2[20],whichcombines Grounding DINO[40]\n diversify manipulated objects. First, to ensure the robot can \n and SAM 2 [41]. More specifically, we first use Ground-\n successfully reach the simulated object, we need to place it \n ing DINO to identify and segment the robot using the text\n exactlywheretheoriginalrealobjectwasplaced.Weanalyze \n prompt (“robot”) on o , as we empirically observe the\n the gripper action sequence to determine t (when the \n tstart \n start best performance when the robot is most visible. However,\n gripperclosestograsptheobject)andt (whenthegripper \n end automaticallyidentifyingtheoriginalrealobjectisextremely\n opens to place the object). To estimate the object position, \n challenging, as a detailed description of its appearance,\n weacquirethegripperpositionatt byreplaying{a }tstart, \n start t t=1 which is essential for effective text prompts, is typically\n and place the simulated object accordingly. Similarly, and \n unavailableinrealrobotdatasets.Moreover,textpromptsare\n optionally, we place a container on the table at the gripper \n highly susceptible to distractors or similar instances, mak-\n position at t . Finally, we replay the robot trajectory \n end ing them unreliable for accurately locating the manipulated\n using the action sequence {a }T , and record simulated \n t t=1 object. Fortunately, the object position at t is already\n movements{osim}T formanipulatingthenewobject.Note start \n t t=1 estimated during real-to-sim trajectory replay, now serving\n that all digital twins are faithfully aligned to the real-world \n as a crucial cue for segmenting the real object on o .\n scene, this ensure the recorded movements remain aligned \n tstart \n Using the camera pose, we project the 3 D object position\n with the real-world background. \n onto o , providing a 2 D point prompt for real object\n tstart \n Replay Validation. Notably, trajectory replay may succeed segmentationwith SAM 2.Afterobtainingthesemanticmask\n orfailinmanipulatinganewobject,dependingontheaffor- m (i.e.,therobotandobjectmasksatt ),wepropagate\n tstart start \n dance compatibility between the new object and the original it to all video frames {o }T using SAM 2, generating the\n t t=1 \n real-world object. We automatically validate whether the corresponding semantic masks {m }T .\n t t=1 \n objectissuccessfullymanipulatedineachsyntheticepisode, Objectand Robot Removal.Given{o ,m }T ,weeventu-\n t t t=1 \n and discard failed episodes by monitoring the Cartesian allyapply Pro Painter[21],astate-of-the-artvideoinpainting"
  },
  {
    "page_num": 4,
    "text": " \n Results \n \n \n Original Video ROSIE Re Bot (Ours) Original Video ROSIE Re Bot (Ours) Original Video ROSIE Re Bot (Ours)\n \n \n \n \n \n \n \n \n \n \n \n em \n i T \n Move red bull can to the left → Move coke can to the left Put spatula on cutting board →Put spoon on cutting board Put grape in pink bowl →Put carrot in pink bowl\n Fig. 3. Comparison of synthetic videos. We show examples from three datasets: DROID (left), Bridge Data V 2 (mid), and our dataset (right). Re Bot\n generatesrealisticvideoswithphysicallyplausiblemovementsandexcellenttemporalconsistency,significantlyoutperforming ROSIE.\n \n model, to remove the original real object and robot from Implementation Details. We use Isaac Sim 4.1 as our sim-\n the original video, obtaining the task-agnostic background ulation environment for its excellent rendering quality and\n {oreal}T . Notice that we also remove the real robot in flexibility. We implement the real-to-sim trajectory replay\n t t=1 \n this step and later use the virtual robot in our synthetic based on Isaac Lab [34]. We pre-build digital twins of the\n videos{o′}T .Thisensurescorrectocclusionsandrealistic robotsin Isaac Sim,matchingthesamerobotplatformsasper\n t t=1 \n physical interactions during object manipulation. realrobotdatasets,i.e.,using Widow X 2506 DOFrobotarm\n for Bridge Data V 2 and Franka Panda 7 Do F robot arm with\n C. Sim-to-Real Video Synthesis \n Robotiq 2 F-85 gripperfor DROIDandourdataset.Following\n We eventually combine simulated movements {osim}T \n t t=1 the official guidelines of Octo and Open VLA, we use 100\n withtask-agnosticreal-worldbackground{oreal}T tobuild \n t t=1 synthetic episodes per task as the optimal data volume for\n new video frames {o′}T . Specifically, to obtain o′, we \n t t=1 t finetuning. We use four NVIDIA A 6000 GPUs, using full\n extract the robot and the manipulated object from osim, and \n t finetuning with a batch size of 256 and a learning rate of\n merge them onto or t eal. We then assign a new language 4×10−5 for Octo, and Lo RA finetuning with a batch size\n instruction L′ by replacing the object (e.g., “yellow mug” to of 32 and a learning rate of 5×10−4 for Open VLA.\n “spoon”)andcontainer(e.g.,“table”to“towel”)intheorigi- \n Methods for Comparison. We compare Re Bot with\n nalinstruction Ltotheonesweusedduringtrajectoryreplay. \n ROSIE [18], a state-of-the-art generation-based method for\n Eventually,weconstructanewepisodeτ′ ={o′,a ,L′}T . \n j t t t=1 scalingrealrobotvideos.ROSIEemploysimage-basedfoun-\n Note that, since we faithfully replay real-world robot trajec- \n dation models, using Imagen [44] to inpaint manipulated\n tories, the real-world actions remain unchanged in synthetic \n objects directly on original real robot videos. In contrast,\n episodes. In our experiments (see Sec. IV), we validate the \n Re Botintroducesanovelreal-to-sim-to-realscalingstrategy,\n effectiveness of our method for adapting VLA models with \n producingphysicallyrealisticandtemporallyconsistentsyn-\n our synthetic dataset D′ ={τ′}N . \n j j=1 thetic robot videos. Since ROSIE is not open-source, we use\n IV. EXPERIMENTS ourimplementationbasedonthestablediffusionmodel[45].\n Evaluation with VLA Models. We evaluate the effec-\n In this section, we evaluate and demonstrate that Re- \n tiveness of synthetic videos for adapting VLA models to\n Bot effectively produces high-fidelity synthetic robot videos \n target domains. We mainly discuss two state-of-the-art VLA\n (Sec. IV-B), and comprehensively enhances the performance \n models, Octo [29] and Open VLA [30], both of which\n of VLAmodelsinbothsimulation(Sec.IV-C)andreal-world \n are trained on large and diverse datasets involving various\n environments (Sec. IV-D). \n robotic embodiments [4]. To compare scaling methods, we\n A. Experimental Setups evaluate three versions of each VLA model: 1) Octo and\n Datasets. Forrealrobotdatasets,weleveragetabletoppick- Open VLA (zero-shot evaluation, i.e., pre-trained models\n and-place episodes in Bridge Data V 2 [42] and DROID [5]. without finetuning), 2) Octo+ROSIE and Open VLA+ROSIE\n For evaluation in real-world environments in Sec. IV-D, we (finetuned with episodes from ROSIE), and 3) Octo+Re Bot\n collect 220 real-world episodes to build our dataset. In the and Open VLA+Re Bot(finetunedwithepisodesfrom Re Bot).\n DROID dataset, we leverage two exterior videos captured \n B. Evaluation of Video Quality \n from opposite sides of the robot. For simulated objects used \n inreal-to-simtrajectoryreplay,wefollow[11,39]andcollect We compare the generated video quality of ROSIE [18]\n kitchen assets from Objaverse [43]. and Re Bot across three aspects: Temporal Quality, Imaging"
  },
  {
    "page_num": 5,
    "text": " Results \n \n \n \n \n \u0000/\u0000ŵ\u0000Ă\u0000Ő\u0000ŝ\u0000Ŷ\u0000Ő \n \u0000ϱ\u0000ϯ\u0000͘\u0000ϰ\u0000й \n \u0000ϲ\u0000ϲ\u0000͘\u0000ϰ\u0000й \n \u0000Z \n \u0000Z \n \u0000K \n \u0000Ğ\u0000\u0011 \n \u0000^ \n \u0000Ž \n \u0000/\u0000\u001c \n \u0000ƚ\u0000\u0003\u0000;\u0000K\u0000Ƶ\u0000ƌ\u0000Ɛ\u0000Ϳ \n o ed \n i V \n \u0000Y\u0000Ƶ\u0000Ă\u0000ů\u0000ŝ\u0000ƚ\u0000Ǉ \u0000ϳ\u0000Ϭ\u0000͘\u0000ϭ\u0000й \u0000K\u0000ƌ\u0000ŝ\u0000Ő\u0000ŝ\u0000Ŷ\u0000Ă\u0000ů\u0000\u0003\u0000s\u0000ŝ\u0000Ě\u0000Ğ\u0000Ž lan \n \u0000^\u0000Ƶ\u0000ď\u0000ũ\u0000Ğ\u0000Đ\u0000ƚ \u0000ϲ\u0000ϱ\u0000͘\u0000ϲ\u0000й \u0000ϴ\u0000ϳ\u0000͘\u0000ϳ\u0000й \n ig \n ir O \n \u0000\u0012\u0000Ž\u0000Ŷ\u0000Ɛ\u0000ŝ\u0000Ɛ\u0000ƚ\u0000Ğ\u0000Ŷ\u0000Đ\u0000Ǉ \u0000ϵ\u0000ϯ\u0000͘\u0000Ϯ\u0000й \n \u0000ϴ\u0000ϯ\u0000͘\u0000ϳ\u0000й \n \u0000\u0011\u0000Ă\u0000Đ\u0000Ŭ\u0000Ő\u0000ƌ\u0000Ž\u0000Ƶ\u0000Ŷ\u0000Ě \u0000ϵ\u0000Ϯ\u0000͘\u0000Ϯ\u0000й \n \u0000\u0012\u0000Ž\u0000Ŷ\u0000Ɛ\u0000ŝ\u0000Ɛ\u0000ƚ\u0000Ğ\u0000Ŷ\u0000Đ\u0000Ǉ \u0000ϵ\u0000ϲ\u0000͘\u0000Ϯ\u0000й E IS \n O \n \u0000ϴ\u0000ϱ\u0000͘\u0000Ϯ\u0000й R \n \u0000D\u0000Ž\u0000ƚ\u0000ŝ\u0000Ž\u0000Ŷ \u0000ϵ\u0000ϵ\u0000͘\u0000Ϯ\u0000й \n \u0000^\u0000ŵ\u0000Ž\u0000Ž\u0000ƚ\u0000Ś\u0000Ŷ\u0000Ğ\u0000Ɛ\u0000Ɛ \u0000ϵ\u0000ϵ\u0000͘\u0000Ϭ\u0000й \n \u0000Ϭ \u0000Ϯ\u0000Ϭ \u0000ϰ\u0000Ϭ \u0000ϲ\u0000Ϭ \u0000ϴ\u0000Ϭ \u0000ϭ\u0000Ϭ\u0000Ϭ \n \u0000s\u0000\u0011\u0000Ğ\u0000Ŷ\u0000Đ\u0000Ś\u0000\u0003\u0000^\u0000Đ\u0000Ž\u0000ƌ\u0000Ğ\u0000\u0003\u0000;\u0000й\u0000Ϳ )sru \n O \n F \n re \n i \n p \n g \n o \n . \n rt \n 4. \n VBen \n Q \n c \n u \n h \n a \n s \n n \n c \n t \n o \n i \n r \n t \n e \n a \n s \n ti \n a \n v \n s \n e \n ev \n c \n a \n o \n l \n m \n ua \n p \n ti \n a \n o \n r \n n \n is \n m \n on \n etr \n o \n ic \n f \n s. \n g \n R \n e \n e \n n \n B \n er \n o \n a \n t \n t \n o \n e \n u \n d \n tp \n v \n er \n id \n fo \n e \n r \n o \n ms \n q \n R \n ua \n O \n l \n S \n it \n I \n y \n E \n . \n a \n W \n nd \n e \n to \n B \n ( \n achievesvideoqualitycomparabletooriginalreal-worldvideos. \n e R \n Fig.5. Comparisonsofmulti-viewconsistency.Wepresenttwoexamples\n Quality, and Multi-view Consistency. We present a qualita- from the DROID dataset, each captured from two different camera views.\n tive comparison in Fig. 3. Meanwhile, as shown in Fig. 4, While ROSIE lacks multi-view consistency, Re Bot naturally preserves\n this capability inherited from 3 D simulation, ensuring the same object in\n we use VBench [46], a comprehensive benchmark tool for \n differentcameraviews,asintherealworld. \n assessing video generation quality, to evaluate two key as- \n pectsacrossfourdimensions(pleasereferto[46]fordetailed \n Multi-view Consistency. Additionally, as shown in Fig. 5,\n definitions): 1) Temporal Quality - including Subject Con- \n Re Bot inherently preserves multi-view consistency across\n sistency, Background Consistency, and Motion Smoothness; \n multiple camera views, since the synthetic videos are pro-\n and 2) Frame-wise Quality, i.e., Imaging Quality. We also \n duced within a 3 D environment. Notably, this crucial at-\n evaluate original real videos for reference. \n tribute is uniquely achievable through our real-to-sim-to-real\n Temporal Quality. Although ROSIE offers a straight- \n scaling approach. \n forward solution, it fails to generate temporally consistent \n videos, which hinders VLA models from learning stable C. Evaluation in Simulation Environment\n actions. As shown in the first example of Fig. 3, ROSIE We first evaluate VLA models and their two finetuned\n initiallygeneratesaplausiblecokecaninthefirsttwoframes, versions (“+ROSIE” and “+Re Bot”) in Simpler Env [39].\n but then fails to maintain consistency, producing irrelevant For fair comparisons, we use ROSIE and Re Bot to scale\n bottles in later frames. This limitation is further reflected in the same data volume exclusively for evaluation tasks (i.e.,\n its low subject consistency score of only 65.6%, as reported 100 episodes per task), adapting VLA models to the same\n in Fig. 4. Therefore, although observation history has been target domain. We demonstrate that Re Bot effectively im-\n shown to enhance VLA models [1, 29], ROSIE remains un- proves VLA performance across three key aspects: 1) In-\n suitableforimprovingtheirabilitytolearnfromconsecutive domain Performance: Direct evaluation on the given tasks;\n frames. In contrast, Re Bot inherently ensures excellent tem- 2) Generalization Performance (following [30, 47]): Eval-\n poral consistency through the simulation process, achieving uating variations of in-domain tasks across unseen object\n 99.2%inmotionsmoothness.Surprisingly,thisevenslightly sizes (physical), unseen instructions (semantics), and unseen\n outperformsrealrobotvideosby 0.2%,possiblybecausethe objects (subject); 3) Cross-embodiment Performance: Eval-\n simulationprocessreducesartifactssuchasmotionblur(see uating on one embodiment while finetuning on another.\n thesecondframeinthesecondexamplein Fig.3).Moreover, In-domain Performance. In Table I, we report the grasp\n real-world background inpainting faithfully uses temporal rates(percentageofsuccessfulobjectgraspsduringthetask)\n context to recover the occlusions, contributing to a 92.2% andsuccessrates(percentageofcompletedtasks)forthefour\n backgroundconsistency.Notably,ourtemporalqualityacross Simpler Env tasks on the Widow X robot. When used out-of-\n all dimensions, with an average score of 93.0%, is highly the-box, both Octo and Open VLA struggle to report decent\n comparable to real robot videos (96.1%), indicating that our performance on most tasks. Particularly, Open VLA entirely\n synthetic videos achieve lifelike temporal consistency. fails on challenging tasks, showing 0.0% success rates (e.g.,\n Imaging Quality. In Fig. 3, ROSIE struggles to generate stack green cube on yellow cube). This demonstrates their\n high-quality manipulated objects, especially in the last two poor performance in the target domain without data scaling,\n examples. This issue becomes particularly evident when the despiteextensivetrainingon SOTA-scaledatasets[3].Mean-\n newobjectshapepotentiallydeviatesfromtheoriginalobject while, ROSIE performs poorly across most tasks with 0.0%\n shape. This is because generative models tend to rely more success rates, as it fails to generate realistic manipulated\n on the inpainting mask, while paying less attention to the objects and, more importantly, lacks temporal consistency.\n guidance of the text prompt. By comparison, Re Bot ensures This limitation is particularly problematic for Octo, which\n physically plausible movements through simulation, while reliesonobservationhistorywithtwoconsecutiveframes.In\n demonstratingexcellentimagingqualityin Fig.4,withonlya contrast, Re Bot achieves the best performance improving all\n 3.7%decreasecomparedtooriginalvideos,whilesurpassing models,increasingtheaveragesuccessrateby 7.2%for Octo\n ROSIE by 13.0%. and 21.8%for Open VLA.Notably,Re Botbooststheaverage"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n TABLEI \n COMPARISONOFEVALUATIONRESULTSONTHEWIDOWXROBOTINSIMPLERENV. \n Putspoon Putcarrot Stackgreencube Puteggplant \n Average \n ontowel onplate onyellowcube inbasket \n Model \n Grasp Success Grasp Success Grasp Success Grasp Success Grasp Success\n Octo [29] 34.7% 12.5% 52.8% 8.3% 31.9% 0.0% 66.7% 43.1% 46.5% 16.0% \n Octo+ROSIE [18] 20.8% 2.8% 27.8% 0.0% 18.1% 0.0% 22.3% 0.0% 22.3% 0.7% \n Octo+Re Bot (Ours) 61.1% 54.2% 41.1% 22.0% 63.9% 4.2% 52.8% 12.5% 54.7% 23.2%\n Open VLA [30] 4.2% 0.0% 33.3% 0.0% 12.5% 0.0% 8.3% 4.2% 14.6% 1.1% \n Open VLA+ROSIE [18] 12.5% 0.0% 41.7% 0.0% 50.0% 0.0% 20.8% 0.0% 31.3% 0.0% \n Open VLA+Re Bot (Ours) 58.3% 20.8% 45.8% 12.5% 66.7% 4.2% 66.7% 54.2% 59.4% 22.9%\n \u0000ϳ\u0000Ϭ \n \u0000ϲ\u0000Ϭ \n \u0000ϱ\u0000Ϭ \n \u0000ϰ\u0000Ϭ \n \u0000ϯ\u0000Ϭ \u0000Ϯ\u0000Ϭ \n \u0000ϭ\u0000Ϭ \u0000Ϭ \n \u0000W\u0000Ś\u0000Ǉ\u0000Ɛ\u0000ŝ\u0000Đ\u0000Ă\u0000ů \u0000^\u0000Ğ\u0000ŵ\u0000Ă\u0000Ŷ\u0000ƚ\u0000ŝ\u0000Đ\u0000Ɛ \u0000^\u0000Ƶ\u0000ď\u0000ũ\u0000Ğ\u0000Đ\u0000ƚ \u0000\u0004\u0000ǀ\u0000Ğ\u0000ƌ\u0000Ă\u0000Ő\u0000Ğ \n \u0000Ϳ\u0000й\u0000;\u0000\u0003\u0000Ğ\u0000ƚ\u0000Ă\u0000Z\u0000\u0003\u0000Ɛ\u0000Ɛ\u0000Ğ\u0000Đ\u0000Đ\u0000Ƶ\u0000^\u0000ͬ\u0000Ɖ\u0000Ɛ\u0000Ă\u0000ƌ\u0000' \n \u0000K\u0000Đ\u0000ƚ\u0000Ž \n \u0000ϱ\u0000ϭ\u0000͘\u0000ϳ\u0000й \u0000ϱ\u0000ϯ\u0000͘\u0000ϱ\u0000й \n \u0000ϰ\u0000ϴ\u0000͘\u0000ϴ\u0000й \n \u0000ϰ\u0000Ϭ\u0000͘\u0000ϱ\u0000й \u0000ϰ\u0000Ϭ\u0000͘\u0000ϯ\u0000й \u0000ϰ\u0000ϭ\u0000͘\u0000ϯ\u0000й \u0000ϰ\u0000ϭ\u0000͘\u0000Ϭ\u0000й \u0000ϰ\u0000Ϭ\u0000͘\u0000ϲ\u0000й \n \u0000ϯ\u0000ϲ\u0000͘\u0000Ϭ\u0000й \n \u0000Ϯ\u0000Ϯ\u0000͘\u0000ϰ\u0000й \u0000Ϯ\u0000ϲ\u0000͘\u0000ϳ\u0000й \u0000Ϯ\u0000Ϭ\u0000͘\u0000ϭ\u0000й \u0000Ϯ\u0000ϯ\u0000͘\u0000ϲ\u0000й\u0000Ϯ\u0000ϯ\u0000͘\u0000ϯ\u0000й \u0000Ϯ\u0000ϰ\u0000͘\u0000ϯ\u0000й\u0000Ϯ\u0000ϲ\u0000͘\u0000ϰ\u0000й \u0000ϭ\u0000Ϭ\u0000͘\u0000ϴ\u0000й\n \u0000ϰ\u0000͘\u0000ϱ\u0000й \u0000ϰ\u0000͘\u0000Ϯ\u0000й \u0000ϲ\u0000͘\u0000ϱ\u0000й \u0000Ϭ\u0000͘\u0000ϰ\u0000й \u0000Ϭ\u0000͘\u0000Ϭ\u0000й \u0000Ϭ\u0000͘\u0000Ϭ\u0000й \u0000Ϭ\u0000͘\u0000ϭ\u0000й \n \u0000W\u0000Ś\u0000Ǉ\u0000Ɛ\u0000ŝ\u0000Đ\u0000Ă\u0000ů \u0000^\u0000Ğ\u0000ŵ\u0000Ă\u0000Ŷ\u0000ƚ\u0000ŝ\u0000Đ\u0000Ɛ \u0000^\u0000Ƶ\u0000ď\u0000ũ\u0000Ğ\u0000Đ\u0000ƚ \u0000\u0004\u0000ǀ\u0000Ğ\u0000ƌ\u0000Ă\u0000Ő\u0000Ğ\n \u0000Ϳ\u0000й\u0000;\u0000\u0003\u0000Ğ\u0000ƚ\u0000Ă\u0000Z\u0000\u0003\u0000Ɛ\u0000Ɛ\u0000Ğ\u0000Đ\u0000Đ\u0000Ƶ\u0000^\u0000ͬ\u0000Ɖ\u0000Ɛ\u0000Ă\u0000ƌ\u0000'\n \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004 \u0000ϳ\u0000ϭ\u0000͘\u0000ϵ\u0000й \n \u0000ϲ\u0000ϯ\u0000͘\u0000Ϭ\u0000й \u0000ϲ\u0000ϱ\u0000͘\u0000ϲ\u0000й \u0000ϲ\u0000ϲ\u0000͘\u0000ϴ\u0000й \n \u0000ϯ\u0000ϳ\u0000͘\u0000ϱ\u0000й \n \u0000ϯ\u0000Ϭ\u0000͘\u0000ϳ\u0000й \u0000Ϯ\u0000ϴ\u0000͘\u0000ϭ\u0000й \u0000ϯ\u0000Ϯ\u0000͘\u0000ϭ\u0000й \u0000Ϯ\u0000ϰ\u0000͘\u0000Ϭ\u0000й \u0000ϭ\u0000ϱ\u0000͘\u0000ϲ\u0000й \u0000ϭ\u0000Ϯ\u0000͘\u0000ϱ\u0000й \u0000ϭ\u0000ϯ\u0000͘\u0000ϱ\u0000й \u0000ϭ\u0000ϱ\u0000͘\u0000ϲ\u0000й \u0000ϭ\u0000ϭ\u0000͘\u0000ϭ\u0000й\n \u0000ϳ\u0000͘\u0000ϯ\u0000й \u0000ϳ\u0000͘\u0000ϯ\u0000й \u0000ϭ\u0000͘\u0000ϭ\u0000й\u0000ϭ\u0000͘\u0000ϲ\u0000й \u0000Ϭ\u0000͘\u0000Ϭ\u0000й\u0000Ϭ\u0000͘\u0000Ϭ\u0000й \u0000ϭ\u0000͘\u0000ϭ\u0000й\u0000Ϯ\u0000͘\u0000ϭ\u0000й \u0000Ϭ\u0000͘\u0000ϳ\u0000й\u0000ϭ\u0000͘\u0000Ϯ\u0000й\n \u0000н\u0000\u0003\u0000ͬ\u0000\u0003\u0000;\u0000'\u0000ƌ\u0000Ă\u0000Ɛ\u0000Ɖ\u0000Ϳ \u0000н\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000\u001c\u0000\u0003\u0000;\u0000'\u0000ƌ\u0000Ă\u0000Ɛ\u0000Ɖ\u0000Ϳ \u0000н\u0000\u0003\u0000Z\u0000Ğ\u0000\u0011\u0000Ž\u0000ƚ\u0000\u0003\u0000;\u0000'\u0000ƌ\u0000Ă\u0000Ɛ\u0000Ɖ\u0000Ϳ\n \u0000н\u0000\u0003\u0000ͬ\u0000\u0003\u0000;\u0000^\u0000Ƶ\u0000Đ\u0000Đ\u0000Ğ\u0000Ɛ\u0000Ɛ\u0000Ϳ \u0000н\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000\u001c\u0000\u0003\u0000;\u0000^\u0000Ƶ\u0000Đ\u0000Đ\u0000Ğ\u0000Ɛ\u0000Ɛ\u0000Ϳ \u0000н\u0000\u0003\u0000Z\u0000Ğ\u0000\u0011\u0000Ž\u0000ƚ\u0000\u0003\u0000;\u0000^\u0000Ƶ\u0000Đ\u0000Đ\u0000Ğ\u0000Ɛ\u0000Ɛ\u0000Ϳ\n Fig.6. Evaluationofgeneralizationperformance.Re Botimprovesthegeneralizationperformanceof Octo(left)and Open VLA(right)acrossallthree\n generalizationtypes(physical,semantics,andsubject)on Widow XRobotin Simpler Env. \n \u0000Ϯ\u0000Ϭ \n \u0000ϭ\u0000Ϭ \n \u0000Ϭ \n \u0000W\u0000Ƶ\u0000ƚ\u0000\u0003\u0000Ɛ\u0000Ɖ\u0000Ž\u0000Ž\u0000Ŷ \u0000W\u0000Ƶ\u0000ƚ\u0000\u0003\u0000Đ\u0000Ă\u0000ƌ\u0000ƌ\u0000Ž\u0000ƚ \u0000^\u0000ƚ\u0000Ă\u0000Đ\u0000Ŭ\u0000\u0003\u0000Ő\u0000ƌ\u0000Ğ\u0000Ğ\u0000Ŷ\u0000\u0003\u0000Đ\u0000Ƶ\u0000ď\u0000Ğ \u0000W\u0000Ƶ\u0000ƚ\u0000\u0003\u0000Ğ\u0000Ő\u0000Ő\u0000Ɖ\u0000ů\u0000Ă\u0000Ŷ\u0000ƚ\n \u0000Ž\u0000Ŷ\u0000\u0003\u0000ƚ\u0000Ž\u0000ǁ\u0000Ğ\u0000ů \u0000Ž\u0000Ŷ\u0000\u0003\u0000Ɖ\u0000ů\u0000Ă\u0000ƚ\u0000Ğ \u0000Ž\u0000Ŷ\u0000\u0003\u0000Ǉ\u0000Ğ\u0000ů\u0000ů\u0000Ž\u0000ǁ\u0000\u0003\u0000Đ\u0000Ƶ\u0000ď\u0000Ğ \u0000ŝ\u0000Ŷ\u0000\u0003\u0000ď\u0000Ă\u0000Ɛ\u0000Ŭ\u0000Ğ\u0000ƚ\n \u0000Ϳ\u0000й\u0000;\u0000\u0003\u0000Ğ\u0000ƚ\u0000Ă\u0000Z\u0000\u0003\u0000Ɛ\u0000Ɛ\u0000Ğ\u0000Đ\u0000Đ\u0000Ƶ\u0000^ \u0000t\u0000ŝ\u0000Ě\u0000Ž\u0000ǁ\u0000y \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004 \n \u0000Ϯ\u0000Ϭ\u0000͘\u0000ϴ\u0000й \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000н\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000\u001c \n \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000н\u0000\u0003\u0000Z\u0000Ğ\u0000\u0011\u0000Ž\u0000ƚ \n \u0000ϭ\u0000Ϯ\u0000͘\u0000ϱ\u0000й \n \u0000ϴ\u0000͘\u0000ϯ\u0000й \n \u0000ϰ\u0000͘\u0000Ϯ\u0000й \u0000ϰ\u0000͘\u0000Ϯ\u0000й\u0000ϰ\u0000͘\u0000Ϯ\u0000й \n \u0000Ϭ\u0000͘\u0000Ϭ\u0000й \u0000Ϭ\u0000͘\u0000Ϭ\u0000й\u0000Ϭ\u0000͘\u0000Ϭ\u0000й\u0000Ϭ\u0000͘\u0000Ϭ\u0000й \u0000Ϭ\u0000͘\u0000Ϭ\u0000й\u0000Ϭ\u0000͘\u0000Ϭ\u0000й \n \u0000ϱ\u0000Ϭ \n \u0000ϰ\u0000Ϭ \n \u0000ϯ\u0000Ϭ \n \u0000Ϯ\u0000Ϭ \n \u0000ϭ\u0000Ϭ \n \u0000Ϭ \n \u0000W\u0000ŝ\u0000Đ\u0000Ŭ\u0000\u0003\u0000Đ\u0000Ž\u0000Ŭ\u0000Ğ\u0000\u0003\u0000Đ\u0000Ă\u0000Ŷ \u0000W\u0000ŝ\u0000Đ\u0000Ŭ\u0000\u0003\u0000Đ\u0000Ž\u0000Ŭ\u0000Ğ\u0000\u0003\u0000Đ\u0000Ă\u0000Ŷ \u0000W\u0000ŝ\u0000Đ\u0000Ŭ\u0000\u0003\u0000Đ\u0000Ž\u0000Ŭ\u0000Ğ\u0000\u0003\u0000Đ\u0000Ă\u0000Ŷ\n \u0000;\u0000^\u0000ƚ\u0000Ă\u0000Ŷ\u0000Ě\u0000ŝ\u0000Ŷ\u0000Ő\u0000Ϳ \u0000;\u0000,\u0000Ž\u0000ƌ\u0000ŝ\u0000ǌ\u0000Ž\u0000Ŷ\u0000ƚ\u0000Ă\u0000ů\u0000Ϳ \u0000;\u0000s\u0000Ğ\u0000ƌ\u0000ƚ\u0000ŝ\u0000Đ\u0000Ă\u0000ů\u0000Ϳ \n \u0000Ϳ\u0000й\u0000;\u0000\u0003\u0000Ğ\u0000ƚ\u0000Ă\u0000Z\u0000\u0003\u0000Ɛ\u0000Ɛ\u0000Ğ\u0000Đ\u0000Đ\u0000Ƶ\u0000^ \n although Open VLA faces greater challenges in Simpler Env,\n it benefits significantly from Re Bot, with the average grasp\n raterisingfrom 15.6%to 66.8%,andtheaveragesuccessrate\n increasingfrom 0.7%to 11.1%.Theseresultsfurtherconfirm\n the effectiveness of Re Bot in improving the generalization\n performance of VLA models. \n Cross-embodiment Performance. We also investigate\n \u0000'\u0000Ž\u0000Ž\u0000Ő\u0000ů\u0000Ğ\u0000\u0003\u0000Z\u0000Ž\u0000ď\u0000Ž\u0000ƚ \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004 whether Re Bot can enhance the cross-embodiment perfor-\n \u0000ϰ\u0000ϵ\u0000͘\u0000Ϭ\u0000й \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000н\u0000\u0003\u0000Z\u0000K\u0000^\u0000/\u0000\u001c \n \u0000ϰ\u0000ϭ\u0000͘\u0000Ϭ\u0000й \u0000K\u0000Ɖ\u0000Ğ\u0000Ŷ\u0000s\u0000>\u0000\u0004\u0000\u0003\u0000н\u0000\u0003\u0000Z\u0000Ğ\u0000\u0011\u0000Ž\u0000ƚ mance of VLA models. Specifically, we use ROSIE and\n Re Bot to scale the DROID dataset for the Franka Panda\n \u0000Ϯ\u0000ϰ\u0000͘\u0000Ϭ\u0000й \u0000Ϯ\u0000ϱ\u0000͘\u0000Ϭ\u0000й robot, then finetune Open VLA and evaluate its performance\n \u0000ϭ\u0000ϴ\u0000͘\u0000Ϭ\u0000й \u0000ϭ\u0000ϲ\u0000͘\u0000Ϭ\u0000й on the Widow X robot and Google Robot in Simpler Env.\n \u0000ϵ\u0000͘\u0000Ϭ\u0000й We report the success rates in Fig. 7. On the Widow X\n \u0000ϯ\u0000͘\u0000Ϭ\u0000й \u0000ϰ\u0000͘\u0000Ϭ\u0000й \n robot,while ROSIEonlyprovidesamarginalincreaseinthe\n average success rate from 1.4% to 3.1%, Re Bot achieves a\n substantialboostto 12.5%.Forthe“pickcokecan”taskwith\n Fig.7. Evaluationofcross-embodimentperformance. Re Botenhances \n thecross-embodimentperformanceof Open VLAonthe Widow Xrobot(top) varying object poses on Google Robot, Re Bot demonstrates\n and Google Robot(bottom)in Simpler Env. consistent improvements across all poses, whereas ROSIE\n fails to achieve such robustness. This highlights that Re Bot\n grasp rate from 14.6% to 59.4% on Open VLA, further enables Open VLA to learn more precise and adaptable\n demonstrating its effectiveness. These results highlight that manipulation strategies across diverse object poses. Notably,\n both VLA models benefit greatly from Re Bot because of its Re Bot consistently improves the performance of Open VLA\n temporal consistent and physically realistic synthetic videos. despitescalingforadifferentembodiment,demonstratingits\n Generalization Performance. While current VLA models ability to enhance cross-embodiment performance.\n often face generalization challenges, we further validate \n D. Evaluation in Real-world Environment \n Re Bot as an effective scaling solution for enhancing their \n generalization performance. As shown in Fig. 6, ROSIE In real-world experiments, we demonstrate that Re Bot\n remains ineffective on Octo, while Re Bot consistently im- consistently enhances the effectiveness of VLA models,\n proves both Octo and Open VLA across all three generaliza- delivering superior performance over ROSIE. As shown in\n tion types. Specifically, Re Bot increases the average success Tab. II, we leverage both ROSIE and Re Bot to scale our\n rate from 6.5% to 26.4% on Octo. On the other hand, real robot dataset for four evaluation tasks (see examples"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n TABLEII \n COMPARISONOFEVALUATIONRESULTSONTHEFRANKAPANDAROBOTINTHEREALWORLDENVIRONMENT.\n Results \n Put carrot Put grape Put fanta can Put black cube \n Average \n in blue plate in yellow plate in blue plate in yellow plate\n Model \n Grasp Success Grasp Success Grasp Success Grasp Success Grasp Success\n Octo [29] 0% 0% 30% 20% 10% 0% 20% 10% 15% 8% \n Octo+ROSIE [18] 30% 20% 0% 0% 20% 20% 10% 0% 15% 10% \n Octo+Re Bot (Ours) 40% 20% 40% 30% 30% 20% 30% 30% 35% 25% \n Open VLA [30] 30% 20% 30% 20% 60% 30% 40% 30% 40% 25% \n Open VLA+ROSIE [18] 10% 0% 10% 0% 30% 10% 20% 10% 18% 5% \n Open VLA+Re Bot (Ours) 40% 40% 50% 40% 50% 50% 60% 50% 50% 45% \n \n Put carrot in blue plate Put grape in yellow plate Put fanta can in blue plate Put black cube in yellow plate\n \n at the bottom of Tab. II), and compare the performance example, extending Re Bot to diverse data settings (e.g.,\n of their finetuned VLA models. To ensure better adaptation varying camera setups and robots) could potentially benefit\n to our real-world scene, we also incorporate our real robot cross-embodiment learning. Additionally, exploring more\n dataset (i.e., 220 real-world episodes) during the finetuning challenging scenarios beyond tabletop manipulation is also\n process for all models. We conduct 10 trials per task, and interestingwithpotentialbroaderreal-worldapplications.We\n report both the grasp rate and success rate as evaluation take these directions for future work.\n metrics. While ROSIE provides a marginal improvement, \n increasing the average success rate of Octo from 8% to \n REFERENCES \n 10%, it fails entirely on some tasks (e.g., put the grape in [1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,\n K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al., “Rt-1:\n yellow plate), and does not show meaningful enhancement \n Robotics transformer for real-world control at scale,” ar Xiv preprint\n for Open VLA. In contrast, Re Bot consistently achieves sub- ar Xiv:2212.06817,2022.\n stantial performance gains across diverse tasks, improving [2] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choro-\n manski,T.Ding,D.Driess,A.Dubey,C.Finn,etal.,“Rt-2:Vision-\n the average success rates of Octo by 17% and Open VLA \n language-action models transfer web knowledge to robotic control,”\n by 20%. Notably, for challenging tasks where Octo initially ar Xivpreprintar Xiv:2307.15818,2023.\n has a 0% grasp rate and success rate (e.g., put carrot in [3] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Ir-\n pan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, et al., “Open\n blue plate), Re Bot boosts the grasping rate to 40% and the \n x-embodiment: Robotic learning datasets and rt-x models,” ar Xiv\n success rate to 20%, highlighting its robust effectiveness in preprintar Xiv:2310.08864,2023.\n real-world applications. [4] A. O’Neill, A. Rehman, A. Gupta, A. Maddukuri, A. Gupta,\n A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, et al.,\n V. CONCLUSIONANDDISCUSSION “Open x-embodiment: Robotic learning datasets and rt-x models,”\n ar Xivpreprintar Xiv:2310.08864,2023. \n We propose Re Bot, a novel real-to-sim-to-real approach [5] A.Khazatsky,K.Pertsch,S.Nair,A.Balakrishna,S.Dasari,S.Karam-\n for scaling real robot datasets and adapting VLA models to cheti,S.Nasiriany,M.K.Srirama,L.Y.Chen,K.Ellis,etal.,“Droid:\n A large-scale in-the-wild robot manipulation dataset,” ar Xiv preprint\n target domains. Re Bot replays real-world robot trajectories \n ar Xiv:2403.12945,2024. \n in simulation to diversify manipulated objects, and inte- [6] E.Kolve,R.Mottaghi,W.Han,E.Vander Bilt,L.Weihs,A.Herrasti,\n grates the simulated movements with inpainted real-world M.Deitke,K.Ehsani,D.Gordon,Y.Zhu,etal.,“Ai 2-thor:Aninter-\n active 3 denvironmentforvisualai,”ar Xivpreprintar Xiv:1712.05474,\n background to synthesize physically realistic and temporally \n 2017. \n consistentrobotvideos.Re Botachievesexcellentvideogen- [7] T.Mu,Z.Ling,F.Xiang,D.Yang,X.Li,S.Tao,Z.Huang,Z.Jia,and\n eration quality, with a VBench temporal consistency score H. Su, “Maniskill: Generalizable manipulation skill benchmark with\n large-scaledemonstrations,”ar Xivpreprintar Xiv:2107.14483,2021.\n of 93.0% and imaging quality score of 66.4%, which are \n [8] J.Gu,F.Xiang,X.Li,Z.Ling,X.Liu,T.Mu,Y.Tang,S.Tao,X.Wei,\n comparable to 96.1% and 70.1% for real robot videos. In Y. Yao, et al., “Maniskill 2: A unified benchmark for generalizable\n Simpler Env with the Widow X robot, Re Bot improved the manipulationskills,”ar Xivpreprintar Xiv:2302.04659,2023.\n [9] Y. Wang, Z. Xian, F. Chen, T.-H. Wang, Y. Wang, K. Fragkiadaki,\n in-domain performance of Octo by 7.2% and Open VLA by \n Z. Erickson, D. Held, and C. Gan, “Robogen: Towards unleashing\n 21.8%, and enhanced generalization performance by 19.9% infinitedatafor automatedrobotlearningvia generativesimulation,”\n and 9.4%, respectively. In a real-world environment with a ar Xivpreprintar Xiv:2311.01455,2023.\n [10] B.Liu,Y.Zhu,C.Gao,Y.Feng,Q.Liu,Y.Zhu,and P.Stone,“Libero:\n physical Franka Panda, Re Bot increased the success rates of \n Benchmarkingknowledgetransferforlifelongrobotlearning,”ar Xiv\n Octo by 17% and Open VLA by 20%. preprintar Xiv:2306.03310,2023. \n Wehope Re Botcouldserveasavaluableassetandinspire [11] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi,\n A.Mandlekar,and Y.Zhu,“Robocasa:Large-scalesimulationofev-\n future research on real-to-sim-to-real for robot learning. It \n erydaytasksforgeneralistrobots,”ar Xivpreprintar Xiv:2406.02523,\n opens several exciting avenues for future exploration. For 2024. "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n [12] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in ar Xiv:2406.09246,2024.\n deep reinforcement learning for robotics: a survey,” in 2020 IEEE [31] M.Savva,A.Kadian,O.Maksymets,Y.Zhao,E.Wijmans,B.Jain,\n symposiumseriesoncomputationalintelligence(SSCI). IEEE,2020, J.Straub,J.Liu,V.Koltun,J.Malik,etal.,“Habitat:Aplatformfor\n pp.737–744. embodiedairesearch,”in Proceedingsofthe IEEE/CVFinternational\n [13] F. Muratore, F. Ramos, G. Turk, W. Yu, M. Gienger, and J. Peters, conferenceoncomputervision,2019,pp.9339–9347.\n “Robotlearningfromrandomizedsimulations:Areview,”Frontiersin [32] M.Shridhar,J.Thomason,D.Gordon,Y.Bisk,W.Han,R.Mottaghi,\n Roboticsand AI,vol.9,p.799893,2022. L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting\n [14] Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and grounded instructions for everyday tasks,” in Proceedings of the\n V. Kumar, “Cacti: A framework for scalable multi-task multi-scene IEEE/CVF conference on computer vision and pattern recognition,\n visualimitationlearning,”ar Xivpreprintar Xiv:2212.05711,2022. 2020,pp.10740–10749.\n [15] S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, “Robo- [33] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang,\n dreamer:Learningcompositionalworldmodelsforrobotimagination,” Y.Yuan,H.Wang,etal.,“Sapien:Asimulatedpart-basedinteractive\n ar Xivpreprintar Xiv:2404.12377,2024. environment,” in Proceedings of the IEEE/CVF conference on com-\n [16] Y.Du,S.Yang,B.Dai,H.Dai,O.Nachum,J.Tenenbaum,D.Schu- putervisionandpatternrecognition,2020,pp.11097–11107.\n urmans, and P. Abbeel, “Learning universal policies via text-guided [34] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan,\n video generation,” Advances in Neural Information Processing Sys- R. Singh, Y. Guo, H. Mazhar, et al., “Orbit: A unified simulation\n tems,vol.36,2024. frameworkforinteractiverobotlearningenvironments,”IEEERobotics\n [17] Z. Chen, S. Kiami, A. Gupta, and V. Kumar, “Genaug: Retargeting and Automation Letters,vol.8,no.6,pp.3740–3747,2023.\n behaviors to unseen situations via generative augmentation,” ar Xiv [35] L. Wang, R. Guo, Q. Vuong, Y. Qin, H. Su, and H. Christensen,\n preprintar Xiv:2302.06671,2023. “A real 2 sim 2 real method for robust object grasping with neural\n [18] T.Yu,T.Xiao,A.Stone,J.Tompson,A.Brohan,S.Wang,J.Singh, surfacereconstruction,”in 2023 IEEE 19 th International Conference\n C. Tan, J. Peralta, B. Ichter, et al., “Scaling robot learning with on Automation Science and Engineering (CASE). IEEE, 2023, pp.\n semanticallyimaginedexperience,”ar Xivpreprintar Xiv:2302.11550, 1–8. \n 2023. [36] M. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta,\n [19] L. Y. Chen, C. Xu, K. Dharmarajan, M. Z. Irshad, R. Cheng, and P. Agrawal, “Reconciling reality through simulation: A real-\n K. Keutzer, M. Tomizuka, Q. Vuong, and K. Goldberg, “Rovi-aug: to-sim-to-real approach for robust manipulation,” ar Xiv preprint\n Robotandviewpointaugmentationforcross-embodimentrobotlearn- ar Xiv:2403.03949,2024.\n ing,” in Conference on Robot Learning (Co RL), Munich, Germany, [37] Y. Mu, T. Chen, S. Peng, Z. Chen, Z. Gao, Y. Zou, L. Lin, Z. Xie,\n 2024. and P. Luo, “Robotwin: Dual-arm robot benchmark with generative\n [20] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, digitaltwins(earlyversion),”ar Xivpreprintar Xiv:2409.02920,2024.\n Y.Chen,F.Yan,Z.Zeng,H.Zhang,F.Li,J.Yang,H.Li,Q.Jiang, [38] X.Li,J.Li,Z.Zhang,R.Zhang,F.Jia,T.Wang,H.Fan,K.-K.Tseng,\n and L. Zhang, “Grounded sam: Assembling open-world models for and R.Wang,“Robogsim:Areal 2 sim 2 realroboticgaussiansplatting\n diversevisualtasks,”2024. simulator,”2024.[Online].Available:https://arxiv.org/abs/2411.11839\n [21] S. Zhou, C. Li, K. C. Chan, and C. C. Loy, “Pro Painter: Improving [39] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu,\n propagationandtransformerforvideoinpainting,”in Proceedingsof I. Lunawat, I. Sieh, S. Kirmani, et al., “Evaluating real-world robot\n IEEEInternational Conferenceon Computer Vision(ICCV),2023. manipulationpoliciesinsimulation,”ar Xivpreprintar Xiv:2405.05941,\n [22] H.Ravichandar,A.S.Polydoros,S.Chernova,and A.Billard,“Recent 2024. \n advances in robot learning from demonstration,” Annual review of [40] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li,\n control, robotics, and autonomous systems, vol. 3, no. 1, pp. 297– J. Yang, H. Su, J. Zhu, et al., “Grounding dino: Marrying dino with\n 330,2020. grounded pre-training for open-set object detection,” ar Xiv preprint\n [23] Y.Yang,L.Chen,Z.Zaidi,S.van Waveren,A.Krishna,and M.Gom- ar Xiv:2303.05499,2023.\n bolay, “Enhancing safety in learning from demonstration algorithms [41] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr,\n via control barrier function shielding,” in Proceedings of the 2024 R.Ra¨dle,C.Rolland,L.Gustafson,E.Mintun,J.Pan,K.V.Alwala,\n ACM/IEEE International Conference on Human-Robot Interaction, N. Carion, C.-Y. Wu, R. Girshick, P. Dolla´r, and C. Feichtenhofer,\n 2024,pp.820–829. “Sam 2: Segment anything in images and videos,” 2024. [Online].\n [24] A. Mandlekar, J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, Available:https://arxiv.org/abs/2408.00714\n A. Garg, S. Savarese, and L. Fei-Fei, “Scaling robot supervision to [42] H.Walke,K.Black,A.Lee,M.J.Kim,M.Du,C.Zheng,T.Zhao,\n hundredsofhourswithroboturk:Roboticmanipulationdatasetthrough P.Hansen-Estruch,Q.Vuong,A.He,V.Myers,K.Fang,C.Finn,and\n human reasoning and dexterity,” in 2019 IEEE/RSJ International S. Levine, “Bridgedata v 2: A dataset for robot learning at scale,” in\n Conferenceon Intelligent Robotsand Systems(IROS). IEEE,2019, Conferenceon Robot Learning(Co RL),2023.\n pp.1048–1055. [43] M.Deitke,D.Schwenk,J.Salvador,L.Weihs,O.Michel,E.Vander-\n [25] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, Bilt,L.Schmidt,K.Ehsani,A.Kembhavi,and A.Farhadi,“Objaverse:\n K.Daniilidis,C.Finn,and S.Levine,“Bridgedata:Boostinggener- Auniverseofannotated 3 dobjects,”ar Xivpreprintar Xiv:2212.08051,\n alizationofroboticskillswithcross-domaindatasets,”ar Xivpreprint 2022. \n ar Xiv:2109.13396,2021. [44] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,\n [26] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans,\n S. Levine, and C. Finn, “Bc-z: Zero-shot task generalization with et al., “Photorealistic text-to-image diffusion models with deep lan-\n roboticimitationlearning,”in Conferenceon Robot Learning. PMLR, guage understanding,” Advances in neural information processing\n 2022,pp.991–1002. systems,vol.35,pp.36479–36494,2022. \n [27] D. Whitney, E. Rosen, E. Phillips, G. Konidaris, and S. Tellex, [45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n “Comparing robot grasping teleoperation across desktop and virtual “High-resolution image synthesis with latent diffusion models,” in\n realitywithrosreality,”in Robotics Research:The 18 th International Proceedings of the IEEE/CVF conference on computer vision and\n Symposium ISRR. Springer,2019,pp.335–350. patternrecognition,2022,pp.10684–10695.\n [28] Y. Yang, B. Ikeda, G. Bertasius, and D. Szafir, “Arcade: Scalable [46] Z.Huang,Y.He,J.Yu,F.Zhang,C.Si,Y.Jiang,Y.Zhang,T.Wu,\n demonstration collection and generation via augmented reality for Q.Jin,N.Chanpaisit,Y.Wang,X.Chen,L.Wang,D.Lin,Y.Qiao,and\n imitation learning,” in 2024 IEEE/RSJ International Conference on Z.Liu,“VBench:Comprehensivebenchmarksuiteforvideogenerative\n Intelligent Robotsand Systems(IROS). IEEE,2024,pp.2855–2861. models,” in Proceedings of the IEEE/CVF Conference on Computer\n [29] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, Visionand Pattern Recognition,2024.\n O.Mees,S.Dasari,J.Hejna,C.Xu,J.Luo,T.Kreiman,Y.Tan,L.Y. [47] Z. Zhang, K. Zheng, Z. Chen, J. Jang, Y. Li, C. Wang, M. Ding,\n Chen,P.Sanketi,Q.Vuong,T.Xiao,D.Sadigh,C.Finn,and S.Levine, D.Fox,and H.Yao,“Grape:Generalizingrobotpolicyviapreference\n “Octo: An open-source generalist robot policy,” in Proceedings of alignment,”ar Xivpreprintar Xiv:2411.19309,2024.\n Robotics:Scienceand Systems,Delft,Netherlands,2024. \n [30] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, \n S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al., “Open- \n vla: An open-source vision-language-action model,” ar Xiv preprint "
  }
]