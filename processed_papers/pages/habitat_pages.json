[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n \n \n Habitat: A Platform for Embodied AI Research \n \n \n Manolis Savva 1,4*,Abhishek Kadian 1*,Oleksandr Maksymets 1*,Yili Zhao 1, \n Erik Wijmans 1,2,3,Bhavana Jain 1,Julian Straub 2,Jia Liu 1,Vladlen Koltun 5, \n \n Jitendra Malik 1,6,Devi Parikh 1,3,Dhruv Batra 1,3 \n \n 1 Facebook AIResearch,2 Facebook Reality Labs,3 Georgia Instituteof Technology,\n 4 Simon Fraser University,5 Intel Labs,6 UCBerkeley \n https://aihabitat.org \n \n 1.Introduction \n Abstract \n Theembodimenthypothesisistheideathatintelligenceemerges\n intheinteractionofanagentwithanenvironmentandasaresult\n Wepresent Habitat,aplatformforresearchinembodied ofsensorimotoractivity. \n artificialintelligence(AI).Habitatenablestrainingembod- \n Smithand Gasser[26] \n iedagents(virtualrobots)inhighlyefficientphotorealistic \n 3 Dsimulation. Specifically,Habitatconsistsof: \n Imaginewalkinguptoahomerobotandasking‘Hey–\n (i) Habitat-Sim: a flexible, high-performance 3 D sim- \n canyougocheckifmylaptopisonmydesk?Andifso,bring\n ulator with configurable agents, sensors, and generic 3 D \n ittome.’ Inordertobesuccessful,sucharobotwouldneed\n datasethandling. Habitat-Simisfast–whenrendering \n arangeofskills–visualperception(torecognizescenesand\n a scene from Matterport 3 D, it achieves several thousand \n objects),languageunderstanding(totranslatequestionsand framespersecond(fps)runningsingle-threaded,andcan\n instructionsintoactions),andnavigationincomplexenviron-\n reachover 10,000 fpsmulti-processonasingle GPU. \n ments(tomoveandfindthingsinachangingenvironment).\n (ii)Habitat-API:amodularhigh-levellibraryforend-to- \n While there has been significant progress in the vision\n enddevelopmentofembodied AIalgorithms–definingtasks \n andlanguagecommunitiesthankstorecentadvancesindeep\n (e.g.navigation,instructionfollowing,questionanswering), \n representations [14, 11], much of this progress has been\n configuring,training,andbenchmarkingembodiedagents. \n on‘internet AI’ratherthanembodied AI.Thefocusofthe\n formerispatternrecognitioninimages,videos,andtexton\n Theselarge-scaleengineeringcontributionsenableusto \n datasetstypicallycuratedfromtheinternet[10,18,4]. The\n answerscientificquestionsrequiringexperimentsthatwere \n focusofthelatteristoenableactionbyanembodiedagent\n tillnowimpracticableor‘merely’impractical. Specifically, \n (e.g.arobot)inanenvironment.Thisbringstotheforeactive\n in the context of point-goal navigation: (1) we revisit the \n perception, long-termplanning, learningfrominteraction,\n comparisonbetweenlearningand SLAMapproachesfrom \n andholdingadialoggroundedinanenvironment.\n tworecentworks[20,16]andfindevidencefortheoppo- \n siteconclusion–thatlearningoutperforms SLAMifscaled Astraightforwardproposalistotrainagentsdirectlyin\n to an order of magnitude more experience than previous thephysicalworld–exposingthemtoallitsrichness. This\n investigations, and (2) we conduct the first cross-dataset isvaluableandwillcontinuetoplayanimportantroleinthe\n generalizationexperiments{train,test}×{Matterport 3 D, developmentof AI.However,wealsorecognizethattrain-\n Gibson}formultiplesensors{blind,RGB,RGBD,D}and \n ingrobotsintherealworldisslow(therealworldrunsno\n findthatonlyagentswithdepth(D)sensorsgeneralizeacross \n fasterthanrealtimeandcannotbeparallelized),dangerous\n datasets. Wehopethatouropen-sourceplatformandthese (poorly-trainedagentscanunwittinglyinjurethemselves,the\n findingswilladvanceresearchinembodied AI. \n environment,orothers),resourceintensive(therobot(s)and\n theenvironment(s)inwhichtheyexecutedemandresources\n andtime),difficulttocontrol(itishardtotestcorner-case\n scenarios as these are, by definition, infrequent and chal-\n lengingtorecreate),andnoteasilyreproducible(replicating\n conditionsacrossexperimentsandinstitutionsisdifficult).\n *Denotesequalcontribution. Weaimtosupportacomplementaryresearchprogram:\n 9102 \n vo N \n 52 \n ]VC.sc[ \n 2 v 10210.4091:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n Habitat Platform \n \n Tasks Habitat API \n Embodied QA Language grounding Interactive QA Vision-Language Navigation Visual Navigation\n (Das et al., 2018) (Hill et al., 2017) (Gordon et al., 2018) (Anderson et al., 2018) (Zhu et al., 2017, Gupta et al., 2017)\n \n Simulators Habitat Sim \n \n House 3 D AI 2-THOR MINOS Gibson CHALET \n (Wu et al., 2017) (Kolveet al., 2017) (Savva et al., 2017) (Zamir et al., 2018) (Yan et al., 2018)\n \n Generic Dataset \n Datasets \n Support \n Replica(Straubet al., 2019) Matterport 3 D(Chang et al., 2017) 2 D-3 D-S(Armeniet al., 2017)\n Figure 1: The‘softwarestack’fortrainingembodiedagentsinvolves(1)datasetsproviding 3 Dassetswithsemanticannotations, (2)\n simulatorsthatrendertheseassetsandwithinwhichanembodiedagentmaybesimulated,and(3)tasksthatdefineevaluatableproblemsthat\n enableustobenchmarkscientificprogress.Priorwork(highlightedinblueboxes)hascontributedavarietyofdatasets,simulationsoftware,\n andtaskdefinitions. Weproposeaunifiedembodiedagentstackwiththe Habitatplatform,includinggenericdatasetsupport,ahighly\n performantsimulator(Habitat-Sim),andaflexible API(Habitat-API)allowingthedefinitionandevaluationofabroadsetoftasks.\n trainingembodiedagents(e.g.virtualrobots)inrichrealistic question answering), configuring and training embodied\n simulatorsandthentransferringthelearnedskillstoreality. agents(viaimitationorreinforcementlearning,orviaclassic\n Simulations have a long and rich history in science and SLAM),andbenchmarkingusingstandardmetrics[2].\n engineering(fromaerospacetozoology). Inthecontextof The Habitat architecture and implementation combine\n embodied AI,simulatorshelpovercometheaforementioned modularityandhighperformance. Whenrenderingascene\n challenges–theycanrunordersofmagnitudefasterthan from the Matterport 3 D dataset, Habitat-Sim achieves\n real-time and can be parallelized over a cluster; training several thousand frames per second (fps) running single-\n in simulation is safe, cheap, and enables fair comparison threaded, and can reach over 10,000 fps multi-process on\n andbenchmarkingofprogressinaconcertedcommunity- asingle GPU,whichisordersofmagnitudefasterthanthe\n wideeffort. Onceapromisingapproachhasbeendeveloped closest simulator. Habitat-API allows us to train and\n and tested in simulation, it can be transferred to physical benchmarkembodiedagentswithdifferentclassesofmeth-\n platformsthatoperateintherealworld[6,15]. odsandindifferent 3 Dscenedatasets. \n Datasetshavebeenakeydriverofprogressincomputer Theselarge-scaleengineeringcontributionsenableusto\n vision, NLP, and other areas of AI [10, 18, 4, 1]. As the answerscientificquestionsrequiringexperimentsthatwere\n communitytransitionstoembodied AI,webelievethatsim- tillnowimpracticableor‘merely’impractical. Specifically,\n ulatorswillassumetheroleplayedpreviouslybydatasets. in the context of point-goal navigation [2], we make two\n Tosupportthistransition,weaimtostandardizetheentire scientificcontributions: \n ‘software stack’ for training embodied agents (Figure 1): 1. We revisit the comparison between learning and\n scanningtheworldandcreatingphotorealistic 3 Dassets,de- SLAMapproachesfromtworecentworks[20,16]andfind\n velopingthenextgenerationofhighlyefficientandparalleliz- evidence for the opposite conclusion – that learning out-\n ablesimulators,specifyingembodied AItasksthatenable performs SLAM if scaled to an order of magnitude more\n us to benchmark scientific progress, and releasing modu- experiencethanpreviousinvestigations.\n larhigh-levellibrariesfortraininganddeployingembodied 2. Weconductthefirstcross-datasetgeneralizationexper-\n agents. Specifically,Habitatconsistsofthefollowing: iments{train,test}×{Matterport 3 D,Gibson}formultiple\n 1. Habitat-Sim: a flexible, high-performance 3 D sensors{Blind 1,RGB,RGBD,D}×{GPS+Compass}and\n simulator with configurable agents, multiple sensors, and findthatonlyagentswithdepth(D)sensorsgeneralizewell\n generic 3 Ddatasethandling(withbuilt-insupportfor Mat- acrossdatasets. \n terport 3 D,Gibson,and Replicadatasets). \n Wehopethatouropen-sourceplatformandthesefindings\n 2. Habitat-API:amodularhigh-levellibraryforend- \n willadvanceandguidefutureresearchinembodied AI.\n to-enddevelopmentofembodied AIalgorithms–defining \n embodied AItasks(e.g.navigation, instructionfollowing, 1 Blindreferstoagentswithnovisualsensoryinputs.\n "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n 2.Related Work \n Realityissomethingyouriseabove. \n \n Liza Minnelli \n \n Theavailabilityoflarge-scale 3 Dscenedatasets[5,27,8] \n andcommunityinterestinactivevisiontasksledtoarecent \n surgeofworkthatresultedinthedevelopmentofavariety \n ofsimulationplatformsforindoorenvironments[17,7,13, \n 24,29,3,30,31,23]. Theseplatformsvarywithrespectto \n the 3 Dscenedatatheyuse,theembodiedagenttasksthey Figure 2:Examplerenderedsensorobservationsforthreesensors\n (colorcamera,depthsensor,semanticinstancemask)intwodiffer-\n address,andtheevaluationprotocolstheyimplement. \n entenvironmentdatasets. AMatterport 3 D[8]environmentisin\n Thissurgeofactivityisboththrillingandalarming. On \n thetoprow,anda Replica[28]environmentinthebottomrow.\n theonehand,itisclearlyasignoftheinterestinembodied \n AIacrossdiverseresearchcommunities(computervision, \n naturallanguageprocessing,robotics,machinelearning).On \n – thousands vs. one hundred frames per second – allows\n theotherhand,theexistenceofmultipledifferingsimulation \n us to evaluate agents that have been trained with signifi-\n environmentscancausefragmentation,replicationofeffort, \n cantlylargeramountsofexperience(75 millionstepsvs.five\n anddifficultyinreproductionandcommunity-wideprogress. \n million steps). The trends we observe demonstrate that\n Moreover,existingsimulatorsexhibitseveralshortcomings: \n learnedagentscanbegintomatchandoutperformclassical\n – Tightcouplingoftask(e.g.navigation),simulationplat- \n approacheswhenprovidedwithlargeamountsoftraining\n form(e.g.Gibson Env),and 3 Ddataset(e.g.Gibson). Ex- \n experience.Otherrecentworkby Koijimaand Deng[16]has\n perimentswithmultipletasksordatasetsareimpractical. \n alsocomparedhand-engineerednavigationagentsagainst\n – Hard-codedagentconfiguration(e.g.size,action-space). \n learnedagentsbuttheirfocusisondefiningadditionalmet-\n Ablationsofagentparametersandsensortypesarenot \n ricstocharacterizetheperformanceofagentsandtoestablish\n supported,makingresultshardtocompare. \n measuresofhardnessfornavigationepisodes. Toourknowl-\n – Suboptimalrenderingandsimulationperformance. Most \n edge,ourexperimentsarethefirsttotrainnavigationagents\n existingindoorsimulatorsoperateatrelativelylowframe \n provided with multi-month experience in realistic indoor\n rates (10-100 fps), becoming a bottleneck in training \n environmentsandcontrastthemagainstclassicalmethods.\n agentsandmakinglarge-scalelearninginfeasible. Take- \n awaymessagesfromsuchexperimentsbecomeunreliable 3.Habitat Platform \n –hasthelearningconvergedtotrustthecomparisons? \n – Limitedcontrolofenvironmentstate. Thestructureofthe What Icannotcreate Idonotunderstand.\n 3 Dsceneintermsofpresentobjectscannotbeprogram- \n Richard Feynman \n maticallymodified(e.g.totesttherobustnessofagents). \n Mostcritically,workbuiltontopofanyoftheexisting \n platformsishardtoreproduceindependentlyfromtheplat- Thedevelopmentof Habitatisalong-termefforttoen-\n form, and thus hard to evaluate against work based on a able the formation of a common task framework [12] for\n differentplatform,evenincaseswherethetargettasksand researchintoembodiedagents,therebysupportingsystem-\n datasetsarethesame. Thisstatusquoisundesirableandmo- aticresearchprogressinthisarea.\n tivatesthe Habitateffort. Weaimtolearnfromthesuccesses Designrequirements. Theissuesdiscussedintheprevious\n ofpreviousframeworksanddevelopaunifyingplatformthat sectionleadustoasetofrequirementsthatweseektofulfill.\n combines their desirable characteristics while addressing – Highly performant rendering engine: resource-\n their limitations. A common, unifying platform can sig- efficientrenderingenginethatcanproducemultiplechan-\n nificantlyaccelerateresearchbyenablingcodere-useand nels of visual information (e.g. RGB, depth, semantic\n consistentexperimentalmethodology. Moreover,acommon instancesegmentation,surfacenormals,opticalflow)for\n platformenablesustoeasilycarryoutexperimentstesting multipleconcurrentlyoperatingagents.\n agentsbasedondifferentparadigms(learnedvs. classical) – Scenedatasetingestion API:makestheplatformagnos-\n andgeneralizationofagentsbetweendatasets. ticto 3 Dscenedatasetsandallowsuserstousetheirown\n The experiments we carry out contrasting learned and datasets. \n classicalapproachestonavigationaresimilartotherecent – Agent API: allows users to specify parameterized em-\n work of Mishkin et al. [20]. However, the performance bodiedagentswithwell-definedgeometry,physics,and\n of the Habitat stack relative to MINOS [24] used in [20] actuationcharacteristics."
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n – Sensorsuite API:allowsspecificationofarbitrarynum- 1 process 5 processes \n bersofparameterizedsensors(e.g.RGB,depth,contact, Sensors/Resolution 128 256 512 128 256 512\n GPS,compasssensors)attachedtoeachagent. \n RGB 4,093 1,987 848 10,592 3,574 2,629\n – Scenario and task API: allows portable definition of RGB+depth 2,050 1,042 423 5,223 1,774 1,348\n tasksandtheirevaluationprotocols. \n – Implementation: C++ backend with Python API and Table 1: Performance of Habitat-Sim in frames per second\n interoperationwithcommonlearningframeworks,mini- foranexample Matterport 3 Dscene(id 17 DRP 5 sb 8 fy)onan Intel\n mizesentrythreshold. Xeon E 5-2690 v 4 CPUand Nvidia Titan Xp GPU,measuredat\n – Containerization:enablesdistributedtraininginclusters differentframeresolutionsandwithavaryingnumberofconcur-\n rentsimulatorprocessessharingthe GPU.Seethesupplementfor\n andremote-serverevaluationofuser-providedcode. \n additionalbenchmarkingresults. \n – Humans-as-agents: allowshumanstofunctionasagents \n insimulationinordertocollecthumanbehaviorandin- \n vestigatehuman-agentorhuman-humaninteractions. \n threedatasetsbysimplyspecifyingadifferentinputscene.\n – Environment state manipulation: programmatic con- \n Performance. Habitat-Sim achieves thousands of\n troloftheenvironmentconfigurationintermsoftheob- \n framespersecondpersimulatorthreadandisordersofmag-\n jectsthatarepresentandtheirrelativelayout. \n nitude faster than previous simulators for realistic indoor\n Designoverview. Theabovedesignrequirementscutacross \n environments(whichtypicallyoperateattensorhundredsof\n severallayersinthe‘softwarestack’in Figure 1. Amono- \n framespersecond)–see Table 1 forasummaryandthesup-\n lithicdesignisnotsuitableforaddressingrequirementsat \n plementformoredetails. Bycomparison,AI 2-THOR[17]\n alllevels. We,therefore,structurethe Habitatplatformto \n and CHALET[31]runattensoffps,MINOS[24]and Gib-\n mirrorthismulti-layerabstraction. \n son[30]runataboutahundred,and House 3 D[29]runsat\n Atthelowestlevelis Habitat-Sim, aflexible, high- \n about 300 fps. Habitat-Simis 2-3 ordersofmagnitude\n performance 3 Dsimulator,responsibleforloading 3 Dscenes \n faster. Byoperatingat 10,000 framespersecondweshift\n intoastandardizedscene-graphrepresentation,configuring \n thebottleneckfromsimulationtooptimizationfornetwork\n agentswithmultiplesensors,simulatingagentmotion,and \n training. Basedon Tensor Flowbenchmarks,manypopular\n returning sensory data from an agent’s sensor suite. The \n network architectures run at frame rates that are 10-100 x\n sensorabstractionin Habitatallowsadditionalsensorssuch \n loweronasingle GPU 3.Inpractice,wehaveobservedthat\n as LIDARand IMUtobeeasilyimplementedasplugins. \n itisoftenfastertogenerateimagesusing Habitat-Sim\n Generic 3 D dataset API using scene graphs. thantoloadimagesfromdisk. \n Habitat-Sim employs a hierarchical scene graph \n Efficient GPU throughput. Currently, frames rendered\n torepresentallsupported 3 Denvironmentdatasets,whether \n by Habitat-Simareexposedas Pythontensorsthrough\n synthetic or based on real-world reconstructions. The \n shared memory. Future development will focus on even\n use of a uniform scene graph representation allows us to \n higher rendering efficiency by entirely avoiding GPU-to-\n abstractthedetailsofspecificdatasets,andtotreatthemina \n CPUmemorycopyoverheadthroughtheuseof CUDA-GL\n consistentfashion. Scenegraphsallowustocompose 3 D \n interoperationanddirectsharingofrenderbuffersandtex-\n environmentsthroughproceduralscenegeneration,editing, \n turesastensors. Ourpreliminaryinternaltestingsuggests\n orprogrammaticmanipulation. \n thatthiscanleadtoaspeedupbyafactorof 2. \n Renderingengine. The Habitat-Simbackendmodule Abovethesimulationbackend,the Habitat-APIlayer\n isimplementedin C++andleveragesthe Magnumgraphics isamodularhigh-levellibraryforend-to-enddevelopment\n middlewarelibrary 2 tosupportcross-platformdeployment \n inembodied AI.Settingupanembodiedtaskinvolvesspeci-\n on a broad variety of hardware configurations. The simu- fyingobservationsthatmaybeusedbytheagent(s),using\n latorbackendemploysanefficientrenderingpipelinethat environmentinformationprovidedbythesimulator,andcon-\n implements visual sensor frame rendering using a multi- nectingtheinformationwithatask-specificepisodedataset.\n attachment‘uber-shader’combiningoutputsforcolorcam- – Task: this class extends the simulator’s\n erasensors,depthsensors,andsemanticmasksensors. By Observations class and action space with task-\n allowingalloutputstobeproducedinasinglerenderpass, specific ones. The criteria of episode termination and\n weavoidadditionaloverheadwhensensorparametersare measures of success are provided by the Task. For\n sharedandthesamerenderpasscanbeusedforalloutputs. example, in goal-driven navigation, Task provides\n Figure 2 showsexamplesofvisualsensorsrenderedinthree the goal and evaluation metric [2]. To support this\n different supported datasets. The same agent and sensor kind of functionality the Task has read-only access to\n configurationwasinstantiatedinascenefromeachofthe \n 3 https://www.tensorflow.org/guide/performance/\n 2 https://magnum.graphics/ benchmarks "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n Simulatorand Episode-Dataset. continuousstatespace 4 andmotioncanproducecollisions\n – Episode: aclassforepisodespecificationthatincludes resultinginpartial(orno)progressalongthedirectionin-\n theinitialpositionandorientationofan Agent,sceneid, tended – simply put, it is possible for the agent to ‘slide’\n goalposition,andoptionallytheshortestpathtothegoal. alongawallorobstacle. Crucially,theagentmaychoose\n Anepisodeisadescriptionofaninstanceofthetask. move_forward(0.25 m)andendupinalocationthatis\n – Environment: thefundamentalenvironmentconcept not 0.25 mforwardofwhereitstarted;thus,odometryisnot\n for Habitat, abstracting all the information needed for trivialevenintheabsenceofactuationnoise.\n workingonembodiedtaskswithasimulator. Goalspecification: staticordynamic? Oneconspicuous\n More details about the architecture of the Habitat plat- underspecificationinthe Point Goaltask[2]iswhetherthe\n form,performancemeasurements,andexamplesof APIuse goalcoordinatesarestatic(i.e.providedonceatthestartof\n areprovidedinthesupplement. theepisode)ordynamic(i.e.providedateverytimestep).\n Theformerismorerealistic–itisdifficulttoimagineareal\n taskwhereanoraclewouldprovideprecisedynamicgoalco-\n 4.Point Goal Navigationat Scale \n ordinates.However,intheabsenceofactuationnoiseandcol-\n lisions,everysteptakenbytheagentresultsinaknownturn\n To demonstrate the utility of the Habitat platform de- \n ortranslation,andthiscombinedwiththeinitialgoalloca-\n sign,wecarryoutexperimentstotestforgeneralizationof \n tionisfunctionallyequivalenttodynamicgoalspecification.\n goal-directedvisualnavigationagentsbetweendatasetsof \n Wehypothesizethatthisiswhyrecentworks[16,20,13]\n differentenvironmentsandtocomparetheperformanceof \n useddynamicgoalspecification. Wefollowandprescribe\n learning-basedagentsagainstclassicagentsastheamount \n thefollowingconceptualdelineation–asatask,weadopt\n ofavailabletrainingexperienceisincreased. \n static Point Goalnavigation;asforthesensorsuite,weequip\n Taskdefinition. Weusethe Point Goaltask(asdefinedby \n ouragentswithanidealized GPS+Compasssensor. Thisori-\n Andersonetal.[2])asourexperimentaltestbed. Thistaskis \n entsustowardsarealistictask(static Point Goalnavigation),\n ostensiblysimpletodefine–anagentisinitializedataran- \n disentanglessimulatordesign(actuationnoise,collisiondy-\n domstartingpositionandorientationinanenvironmentand \n namics)fromthetaskdefinition,andallowsustocompare\n askedtonavigatetotargetcoordinatesthatareprovidedrela- \n techniques by sensors used (RGB, depth, GPS, compass,\n tivetotheagent’sposition;noground-truthmapisavailable \n contactsensors). \n and the agent must only use its sensory input to navigate. \n Sensoryinput. Theagentsareendowedwithasinglecolor\n However, in the course of experiments, we realized that \n visionsensorplacedataheightof 1.5 mfromthecenterof\n thistaskleavesspaceforsubtlechoicesthat(a)canmakea \n theagent’sbaseandorientedtoface‘forward’. Thissensor\n significantdifferenceinexperimentaloutcomesand(b)are \n provides RGBframesataresolutionof 2562 pixelsandwith\n either not specified or inconsistent across papers, making \n afieldofviewof 90 degrees. Inaddition,anidealizeddepth\n comparisondifficult. Weattempttobeasdescriptiveaspos- \n sensorisavailable,inthesamepositionandorientationas\n sibleabouttheseseeminglylow-levelchoices;wehopethe \n the color vision sensor. The field of view and resolution\n Habitatplatformwillhelpironouttheseinconsistencies. \n ofthedepthsensormatchthoseofthecolorvisionsensor.\n Agentembodimentandactionspace. Theagentisphysi- We designate agents that make use of the color sensor by\n callyembodiedasacylindricalprimitiveshapewithdiame- RGB,agentsthatmakeuseofthedepthsensorby Depth,\n ter 0.2 mandheight 1.5 m. Theactionspaceconsistsoffour and agents that make use of both by RGBD. Agents that\n actions: turn_left, turn_right, move_forward, use neither sensor are denoted as Blind. All agents are\n and stop. These actions are mapped to idealized actua- equipped with an idealized GPS and compass – i.e., they\n tions that result in 10 degree turns for the turning actions haveaccesstotheirlocationcoordinates,andimplicitlytheir\n andlineardisplacementof 0.25 mforthemove_forward \n orientationrelativetothegoalposition. \n action. Thestopactionallowstheagenttosignalthatit \n Episode specification. We initialize the agent at a start-\n hasreachedthegoal. Habitatsupportsnoisyactuationsbut \n ingpositionandorientationthataresampleduniformlyat\n experiments in this paper are conducted in the noise-free \n randomfromallnavigablepositionsontheflooroftheenvi-\n settingasouranalysisfocusesonotherfactors. \n ronment. Thegoalpositionischosensuchthatitliesonthe\n Collisiondynamics. Somepreviousworks[3]useacoarse samefloorandthereexistsanavigablepathfromtheagent’s\n irregularnavigationgraphwhereanagenteffectively‘tele- startingposition. Duringtheepisode,theagentisallowedto\n ports’fromonelocationtoanother(1-2 mapart). Others[9] takeupto 500 actions. Thisthresholdsignificantlyexceeds\n useafine-grainedregulargrid(0.01 mresolution)wherethe thenumberofstepsanoptimalagentrequirestoreachall\n agentmovesonunoccupiedcellsandtherearenocollisions goals (see the supplement). After each action, the agent\n or partial steps. In Habitat and our experiments, we use \n amorerealisticcollisionmodel–theagentnavigatesina 4 Uptomachineprecision. "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n receivesasetofobservationsfromtheactivesensors. differenceofstaticgoalanddynamic GPScoordinates).\n Evaluation. Anavigationepisodeisconsideredsuccessful – Forwardonlyalwayscallsthemove_forwardaction,\n ifandonlyiftheagentissuesastopactionwithin 0.2 mof andcallsthestopactionwhenwithin 0.2 mofthegoal.\n thetargetcoordinates,asmeasuredbyageodesicdistance – Goalfollowermovestowardsthegoaldirection. Ifitis\n alongtheshortestpathfromtheagent’spositiontothegoal not facing the goal (more than 15 degrees off-axis), it\n position. Iftheagenttakes 500 actionswithouttheabove performsturn_leftorturn_righttoalignitself;\n conditionbeingmettheepisodeendsandisconsideredun- otherwise,itcallsmove_forward. Theagentcallsthe\n successful. Performance is measured using the ‘Success \n stopactionwhenwithin 0.2 mofthegoal. \n weightedby Path Length’(SPL)metric[2]. Foranepisode – RL(PPO)isanagenttrainedwithreinforcementlearn-\n wherethegeodesicdistanceoftheshortestpathislandthe ing,specificallyproximalpolicyoptimization[25]. We\n agenttraversesadistancep,SPLisdefinedas S·l/max(p,l), experimentwith RLagentsequippedwithdifferentvisual\n where S isabinaryindicatorofsuccess. sensors: no visual input (Blind), RGB input, Depth\n input,and RGBwithdepth(RGBD).Themodelconsists\n Episodedatasetpreparation. Wecreate Point Goalnaviga- \n ofa CNNthatproducesanembeddingforvisualinput,\n tionepisode-datasetsfor Matterport 3 D[8]and Gibson[30] \n whichtogetherwiththerelativegoalvectorisusedbyan\n scenes. For Matterport 3 Dwefollowedthepubliclyavailable \n actor(GRU)andacritic(linearlayer). The CNNhasthe\n train/val/testsplits. Notethatasinrecentworks[9,20,16], \n following architecture: {Conv 8×8, Re LU, Conv 4×4,\n thereisnooverlapbetweentrain,val,andtestscenes. For \n Re LU,Conv 3×3,Re LU,Linear,Re LU}(seesupplement\n Gibsonscenes,weobtainedtextured 3 Dsurfacemeshesfrom \n fordetails). Letr denotetherewardattimestept,d be\n the Gibsonauthors[30],manuallyannotatedeachsceneon t t \n the geodesic distance to goal at timestep t, s a success\n itsreconstructionquality(small/bigholes,floating/irregular \n rewardandλatimepenalty(toencourageefficiency). All\n surfaces,poortextures),andcuratedasubsetof 106 scenes \n modelsweretrainedwiththefollowingrewardfunction:\n (outof 572);seethesupplementfordetails.Anepisodeisde- \n finedbytheuniqueidofthescene,thestartingpositionand \n (cid:40) \n orientationoftheagent,andthegoalposition. Additional r = s+d t−1 −d t +λ ifgoalisreached\n t \n metadata such as the geodesic distance along the shortest d −d +λ otherwise \n t−1 t \n path(GDSP)fromstartpositiontogoalpositionisalsoin- \n cluded. While generating episodes, we restrict the GDSP In our experiments s is set to 10 and λ is set to −0.01.\n to be between 1 m and 30 m. An episode is trivial if there Notethatrewardsareonlyprovidedintrainingenviron-\n isanobstacle-freestraightlinebetweenthestartandgoal ments;thetaskischallengingastheagentmustgeneralize\n positions. A good measure of the navigation complexity tounseentestenvironments.\n of an episode is the ratio of GDSP to Euclidean distance – SLAM[20]isanagentimplementingaclassicrobotics\n betweenstartandgoalpositions(noticethat GDSPcanonly navigationpipeline(includingcomponentsforlocaliza-\n be larger than or equal to the Euclidean distance). If the tion,mapping,andplanning),using RGBanddepthsen-\n ratioisnearly 1,therearefewobstaclesandtheepisodeis sors.Weusetheclassicagentby Mishkinetal.[20]which\n easy;iftheratioismuchlargerthan 1,theepisodeisdifficult leverages the ORB-SLAM 2 [21] localization pipeline,\n becausestrategicnavigationisrequired. Tokeepthenavi- withthesameparametersasreportedintheoriginalwork.\n gationcomplexityoftheprecomputedepisodesreasonably \n high,weperformrejectionsamplingforepisodeswiththe Trainingprocedure. Whentraininglearning-basedagents,\n aboveratiofallingintherange[1,1.1]. Followingthis,there wefirstdividethescenesinthetrainingsetequallyamong\n isasignificantdecreaseinthenumberofnear-straight-line 8(Gibson),6(Matterport 3 D)concurrentlyrunningsimula-\n episodes (episodes with a ratio in [1,1.1]) – from 37% to torworkerthreads. Eachthreadestablishesblocksof 500\n 10%forthe Gibsondatasetgeneration. Thisstepwasnot trainingepisodesforeachsceneinitstrainingsetpartition\n performedinanypreviousstudies. Wefindthatwithoutthis andshufflestheorderingoftheseblocks. Trainingcontinues\n filtering, all metrics appear inflated. Gibson scenes have throughshuffledcopiesofthisarray.Wedonothardcodethe\n smallerphysicaldimensionscomparedtothe Matterport 3 D stopactiontoretaingeneralityandallowforcomparison\n scenes. Thisisreflectedintheresulting Point Goaldataset– withfutureworkthatdoesnotassume GPSinputs. Forthe\n average GDSPofepisodesin Gibsonscenesissmallerthan \n experimentsreportedhere,wetrainuntil 75 millionagent\n thatof Matterport 3 Dscenes. steps are accumulated across all worker threads. This is\n Baselines. Wecomparethefollowingbaselines: 15 x larger than the experience used in previous investiga-\n – Random chooses an action randomly among tions[20,16]. Trainingagentsto 75 millionstepstook(in\n turn_left, turn_right, and move_forward sum over all three datasets): 320 GPU-hours for Blind,\n with uniform distribution. The agent calls the stop 566 GPU-hoursfor RGB,475 GPU-hoursfor Depth,and\n actionwhenwithin 0.2 mofthegoal(computedusingthe 906 GPU-hoursfor RGBD(overall 2267 GPU-hours)."
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n 1.0 \n \n 0.8 \n \n 0.6 \n 0.4 \n \n 0.2 \n \n 0 10 20 30 40 50 60 70 \n Number of training steps taken (experience) in million \n LPS \n Performance on Gibson validation split \n 1.0 \n 0.8 \n 0.6 \n 0.4 \n RGB \n Depth \n RGBD 0.2 \n Blind \n SLAM \n 0 10 20 30 40 50 60 70 \n Number of training steps taken (experience) in million\n LPS \n Performance on Matterport 3 D validation split\n RGB \n Depth \n RGBD \n Blind \n SLAM \n Figure 3: Average SPLofagentsonthevalsetoverthecourseoftraining. Previouswork[20,16]hasanalyzedperformanceat 5-10\n millionsteps.Interestingtrendsemergewithmoreexperience:i)Blindagentsinitiallyoutperform RGBand RGBDbutsaturatequickly;\n ii)Learning-based Depthagentsoutperformclassic SLAM.Theshadedareasaroundcurvesshowthestandarderrorof SPLoverfiveseeds.\n Gibson MP 3 D Matterport 3 D).All RL(PPO)agentsstartoutwithfarworse\n SPL, but RL (PPO) Depth, in particular, improves dra-\n Sensors Baseline SPL Succ SPL Succ \n maticallyandmatchestheclassicbaselineatapproximately\n Random 0.02 0.03 0.01 0.01 10 Mframes(Gibson)or 30 Mframes(Matterport 3 D)ofex-\n Forwardonly 0.00 0.00 0.00 0.00 perience, continuing to improve thereafter. Notice that if\n Blind \n Goalfollower 0.23 0.23 0.12 0.12 weterminatedtheexperimentat 5 Mframesasin[20]we\n RL(PPO) 0.42 0.62 0.25 0.35 wouldalsoconcludethat SLAM[20]dominates. Interest-\n ingly, RGBagentsdonotsignificantlyoutperform Blind\n RGB RL(PPO) 0.46 0.64 0.30 0.42 \n agents;wehypothesizebecausebothareequippedwith GPS\n Depth RL(PPO) 0.79 0.89 0.54 0.69 sensors. Indeed,qualitativeresults(Figure 4 andvideoin\n RL(PPO) 0.70 0.80 0.42 0.53 supplement) suggest that Blind agents ‘hug’ walls and\n RGBD \n SLAM[20] 0.51 0.62 0.39 0.47 implement‘wallfollowing’heuristics. Incontrast,RGBsen-\n sorsprovideahigh-dimensionalcomplexsignalthatmaybe\n pronetooverfittingtotrainenvironmentsduetothevariety\n Table 2:Performanceofbaselinemethodsonthe Point Goaltask[2] \n testedonthe Gibson[30]and MP 3 D[8]testsetsundermultiple acrossscenes(evenwithinthesamedataset). Wealsonotice\n sensorconfigurations.RLmodelshavebeentrainedfor 75 million in Figure 3 thatallmethodsperformbetteron Gibsonthan\n steps.Wereportaveragerateofepisodesuccessand SPL[2]. Matterport 3 D.Thisisconsistentwithourpreviousanalysis\n that Gibsoncontainssmallerscenesandshorterepisodes.\n 5.Resultsand Findings Next, for each agent and dataset, we select the best-\n performingcheckpointonvalidationandreportresultson\n We seek to answer two questions: i) how do learning- testin Table 2.Weobservethatuniformlyacrossthedatasets,\n based agents compare to classic SLAM and hand-coded RL(PPO)Depthperformsbest,outperforming RL(PPO)\n baselinesastheamountoftrainingexperienceincreasesand RGBD(by 0.09-0.16 SPL),SLAM(by 0.15-0.28 SPL),and\n ii)howwelldolearnedagentsgeneralizeacross 3 Ddatasets. RGB(by 0.13-0.33 SPL)inthatorder(seethesupplementfor\n Itshouldbetacitlyunderstood,buttobeexplicit–‘learn- additionalexperimentsinvolvingnoisydepth). Webelieve\n ing’and‘SLAM’arebroadfamiliesoftechniques(andnot Depthperformsbetterthan RGBDbecausei)the Point Goal\n a single method), are not necessarily mutually exclusive, navigationtaskrequiresreasoningonlyaboutfreespaceand\n andarenot‘settled’intheirdevelopment. Wecomparerep- depth provides relevant information directly, and ii) RGB\n resentative instances of these families to gain insight into hassignificantlymoreentropy(differenthouseslookvery\n questionsofscalingandgeneralization,anddonotmakeany different),thusitiseasiertooverfitwhenusing RGB.Weran\n claimsaboutintrinsicsuperiorityofoneortheother. ourexperimentswith 5 randomseedsperrun,toconfirmthat\n Learning vs SLAM. To answer the first question we plot thesedifferencesarestatisticallysignificant. Thedifferences\n agentperformance(SPL)onvalidation(i.e.unseen)episodes areaboutanorderofmagnitudelargerthanthestandarddevi-\n overthecourseoftrainingin Figure 3(top: Gibson,bottom: ationofaverage SPLforallcases(e.g.onthe Gibsondataset\n Matterport 3 D). SLAM [20] does not require training and errorsare,Depth: ±0.015,RGB:±0.055,RGBD:±0.028,\n thushasaconstantperformance(0.59 on Gibson,0.42 on Blind: ±0.005). Randomandforward-onlyagentshave"
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n Gibson MP 3 D performance degradation, while the Blind agent is least\n Blind SPL=0.28 RGBSPL=0.57 Blind SPL=0.35 RGBSPL=0.88 \n affected(aswewouldexpect). \n Second, we find a potentially counter-intuitive trend –\n agentstrainedon Gibsonconsistentlyoutperformtheircoun-\n terpartstrainedon Matterport 3 D,evenwhenevaluatedon\n RGBDSPL=0.91 Depth SPL=0.98 RGBDSPL=0.90 Depth SPL=0.94 Matterport 3 D.Webelievethereasonisthepreviouslynoted\n observationthat Gibsonscenesaresmallerandepisodesare\n shorter(lower GDSP)than Matterport 3 D.Gibsonagentsare\n trainedon‘easier’episodesandencounterpositivereward\n moreeasilyduringrandomexploration,thusbootstrapping\n learning. Consequently,forafixedcomputationbudget Gib-\n Figure 4:Navigationexamplesfordifferentsensoryconfigurations \n of the RL (PPO) agent, visualizing trials from the Gibson and sonagentsarestrongeruniversally(notjuston Gibson).This\n MP 3 Dvalsets. Abluedotandreddotindicatethestartingand findingsuggeststhatvisualnavigationagentscouldbenefit\n goalpositions,andthebluearrowindicatesfinalagentposition. fromcurriculumlearning.\n Theblue-green-redlineistheagent’strajectory.Colorshiftsfrom Theseinsightsareenabledbytheengineeringof Habitat,\n bluetoredasthemaximumnumberofagentstepsisapproached. whichmadetheseexperimentsassimpleasachangeinthe\n Seethesupplementalmaterialsformoreexampletrajectories. evaluationdatasetname.\n Gibson MP 3 D 6.Habitat Challenge \n Blind Gibson 0.42 0.34 \n MP 3 D 0.28 0.25 \n Nobattleplaneversurvivescontactwiththeenemy.\n RGB Gibson 0.46 0.40 \n MP 3 D 0.25 0.30 Helmuth Karl Bernhardvon Moltke \n Depth Gibson 0.79 0.68 \n MP 3 D 0.56 0.54 Challengesdriveprogress. Thehistoryof AIsub-fields\n RGBD Gibson 0.70 0.53 \n indicates that the formulation of the right questions, the\n MP 3 D 0.44 0.42 \n creationoftherightdatasets,andthecoalescenceofcommu-\n Figure 5: Generalizationofagentsbetweendatasets. Wereport nitiesaroundtherightchallengesdrivesscientificprogress.\n average SPLforamodeltrainedonthesourcedatasetineachrow, Ourgoalistosupportthisprocessforembodied AI.Habitat\n asevaluatedontestepisodesforthetargetdatasetineachcolumn. Challengeisanautonomousnavigationchallengethataims\n to benchmark and advance efforts in goal-directed visual\n navigation. \n verylowperformance,whilethehand-codedgoalfollower Onedifficultyincreatingachallengearoundembodied AI\n and Blindbaselineseemodestperformance.Seethesup- tasksisthetransitionfromstaticpredictions(asinpassive\n plementforadditionalanalysisoftrainedagentbehavior. perception) to sequential decision making (as in sensori-\n In Figure 4 weplotexampletrajectoriesforthe RL(PPO) motorcontrol). Intraditional‘internet AI’challenges(e.g.\n agents,toqualitativelycontrasttheirbehaviorinthesame Image Net[10],COCO[18],VQA[4]),itispossibletore-\n episode. Consistentwiththeaggregatestatistics,weobserve leaseastatictestingdatasetandaskparticipantstosimply\n that Blindcollideswithobstaclesandfollowswalls,while uploadtheirpredictionsonthisset.Incontrast,embodied AI\n Depth is the most efficient. See the supplement and the taskstypicallyinvolvesequentialdecisionmakingandagent-\n videoformoreexampletrajectories. drivencontrol,makingitinfeasibletopre-packageatesting\n Generalization across datasets. Our findings so far are dataset. Essentially,embodied AIchallengesrequirepartici-\n that RL(PPO)agentssignificantlyoutperform SLAM[20]. pantstouploadcodenotpredictions. Theuploadedagents\n This prompts our second question – are these findings canthenbeevaluatedinnovel(unseen)testenvironments.\n dataset specific or do learned agents generalize across Challenge infrastructure. We leverage the frontend and\n datasets? We report exhaustive comparisons in Figure 5 challengesubmissionprocessofthe Eval AIplatform,and\n – specifically, average SPL for all combinations of {train, buildbackendinfrastructureourselves. Participantsin Habi-\n test} × {Matterport 3 D, Gibson} for all agents {Blind, tat Challenge are asked to upload Docker containers [19]\n RGB,RGBD,Depth}. Rowsindicate(agent,trainset)pair, withtheiragentsvia Eval AI.Thesubmittedagentsarethen\n columnsindicatetestset. Wefindanumberofinteresting evaluatedonalive AWSGPU-enabledinstance.Specifically,\n trends. First,nearlyallagentssufferadropinperformance contestantsarefreetotraintheiragentshowevertheywish\n whentrainedononedatasetandtestedonanother,e.g.RGBD (any language, any framework, any infrastructure). In or-\n Gibson→Gibson 0.70 vs RGBDGibson→Matterport 3 D 0.53 dertoevaluatetheseagents,participantsareaskedtoderive\n (drop of 0.17). RGB and RGBD agents suffer a significant fromabase Habitat Dockercontainerandimplementaspe-"
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n cificinterfacetotheirmodel–agent’sactiontakengivenan sensorsgeneralizewellbetweendifferent 3 Denvironment\n observationfromtheenvironmentateachstep. Thisdocker- datasetsincomparisontoagentsequippedwithonly RGB.\n izedinterfaceenablesrunningtheparticipantcodeonnew Feature roadmap. Our near-term development roadmap\n environments. willfocusonincorporatingphysicssimulationandenabling\n More details regarding the Habitat Challenge held at physics-based interaction between mobile agents and ob-\n CVPR 2019 areavailableatthehttps://aihabitat. jects in 3 D environments. Habitat-Sim’s scene graph\n org/challenge/ website. In a future iteration of this representationiswell-suitedforintegrationwithphysicsen-\n challengewewillintroducethreemajordifferencesdesigned gines,allowingustodirectlycontrolthestateofindividual\n tobothreducethegapbetweensimulationandrealityandto objectsandagentswithinascenegraph. Anotherplanned\n increasethedifficultyofthetask. avenueoffutureworkinvolvesproceduralgenerationof 3 D\n – Inthe 2019 challenge,therelativecoordinatesspecifying environmentsbyleveragingacombinationof 3 Dreconstruc-\n the goal were continuously updated during agent tionandvirtualobjectdatasets. Bycombininghigh-quality\n movement–essentiallysimulatinganagentwithperfect reconstructions of large indoor spaces with separately re-\n localizationandheadingestimation(e.g. anagentwith constructedormodelledobjects,wecantakefulladvantage\n an idealized GPS+Compass). However, high-precision ofourhierarchicalscenegraphrepresentationtointroduce\n localizationinindoorenvironmentscannotbeassumedin controlledvariationinthesimulated 3 Denvironments.\n realisticsettings–GPShaslowprecisionindoors,(visual) Lastly,weplantofocusondistributedsimulationsettings\n odometry may be noisy, SLAM-based localization can thatinvolvelargenumbersofagentspotentiallyinteracting\n fail,etc. Hence,wewillinvestiageonlyprovidingtothe withoneanotherincompetitiveorcollaborativescenarios.\n agent a fixed relative coordinate for the goal position \n fromthestartlocation. Acknowledgments. Wethankthereviewersfortheirhelp-\n fulsuggestions. The Habitatprojectwouldnothavebeen\n – Likewise, the 2019 Habitat Challenge modeled agent \n actions (e.g. forward, turn 10◦ left,...) deter- possiblewithoutthesupportandcontributionsofmanyin-\n dividuals. Wearegratefulto Mandeep Baines,Angel Xuan\n ministically. However in real settings, agent intention \n Chang, Alexander Clegg, Devendra Singh Chaplot, Xin-\n (e.g.goforward 1 m)andtheresultrarelymatchperfectly \n lei Chen,Wojciech Galuba,Georgia Gkioxari,Daniel Gor-\n –actuationerror,differingsurfacematerials,andamyriad \n don,Leonidas Guibas,Saurabh Gupta,Jerry(Zhi-Yang)He,\n ofothersourcesoferrorintroducesignificantdriftovera \n Rishabh Jain,Or Litany,Joel Marcey,Dmytro Mishkin,Mar-\n longtrajectory. Tomodelthis,weintroduceanoisemodel \n cus Rohrbach,Amanpreet Singh,Yuandong Tian,Yuxin Wu,\n acquired by benchmarking a real robotic platform [22]. \n Fei Xia,Deshraj Yadav,Amir Zamir,and Jiazhi Zhangfor\n Visual sensing is an excellent means of combating this \n theirhelp. \n “dead-reckoning”driftandthischangeallowsparticipants \n tostudymethodologiesthatarerobusttoandcancorrect \n Licensesforreferenceddatasets. \n forthisnoise. \n Gibson: https://storage.googleapis. \n – Finally,wewillintroducerealisticmodelsofsensornoise com/gibson_material/Agreement%20 GDS%\n for RGBanddepthsensors–narrowingthegapbetween 2006-04-18.pdf \n perceptualexperiencesagentswouldhaveinsimulation Matterport 3 D: http://kaldir.vc.in.tum.de/\n andreality. matterport/MP_TOS.pdf. \n Welookforwardtosupportingthecommunityinestab- \n lishingabenchmarktoevaluatethestate-of-the-artinmeth- \n odsforembodiednavigationagents. \n 7.Future Work \n Wedescribedthedesignandimplementationofthe Habi- \n tatplatform. Ourgoalistounifyexistingcommunityefforts \n andtoaccelerateresearchintoembodied AI.Thisisalong- \n termeffortthatwillsucceedonlybyfullengagementofthe \n broaderresearchcommunity. \n Experimentsenabledbythegenericdatasetsupportand \n the high performance of the Habitat stack indicate that \n i)learning-basedagentscanmatchandexceedtheperfor- \n mance of classic visual navigation methods when trained \n forlongenoughandii)learnedagentsequippedwithdepth "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n References [17] Eric Kolve,Roozbeh Mottaghi,Daniel Gordon,Yuke Zhu,\n Abhinav Gupta,and Ali Farhadi. AI 2-THOR:Aninteractive\n [1] Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana \n 3 Denvironmentforvisual AI. ar Xiv:1712.05474,2017.\n Košecká,and Alexander CBerg. Adatasetfordeveloping \n [18] Tsung-Yi Lin,Michael Maire,Serge Belongie,James Hays,\n andbenchmarkingactivevision. In ICRA,2017. \n Pietro Perona,Deva Ramanan,Piotr Dollár,and C.Lawrence\n [2] Peter Anderson,Angel X.Chang,Devendra Singh Chaplot, \n Zitnick. Microsoft COCO:Commonobjectsincontext. In\n Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana \n ECCV,2014. \n Kosecka,Jitendra Malik,Roozbeh Mottaghi,Manolis Savva, \n [19] Dirk Merkel. Docker:Lightweight Linuxcontainersforcon-\n and Amir Roshan Zamir. Onevaluationofembodiednaviga- \n sistentdevelopmentanddeployment. Linux Journal,2014.\n tionagents. ar Xiv:1807.06757,2018. \n [20] Dmytro Mishkin,Alexey Dosovitskiy,and Vladlen Koltun.\n [3] Peter Anderson,Qi Wu,Damien Teney,Jake Bruce,Mark \n Benchmarkingclassicandlearnednavigationincomplex 3 D\n Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and \n environments. ar Xiv:1901.10915,2019. \n Antonvanden Hengel. Vision-and-languagenavigation:In- \n [21] Raúl Mur-Artal and Juan D. Tardós. ORB-SLAM 2: An\n terpretingvisually-groundednavigationinstructionsinreal \n environments. In CVPR,2018. open-source SLAMsystemformonocular,stereoand RGB-D\n cameras. IEEETransactionson Robotics,33(5),2017.\n [4] Stanislaw Antol,Aishwarya Agrawal,Jiasen Lu,Margaret \n Mitchell,Dhruv Batra,C.Lawrence Zitnick,and Devi Parikh. [22] Adithyavairavan Murali,Tao Chen,Kalyan Vasudev Alwala,\n VQA:Visual Question Answering. In ICCV,2015. Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav\n [5] Iro Armeni,Ozan Sener,Amir R.Zamir,Helen Jiang,Ioannis Gupta. Pyrobot:Anopen-sourceroboticsframeworkforre-\n Brilakis,Martin Fischer,and Silvio Savarese. 3 Dsemantic searchandbenchmarking. ar Xivpreprintar Xiv:1906.08236,\n parsingoflarge-scaleindoorspaces. In CVPR,2016. 2019. \n [6] Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, [23] Xavier Puig,Kevin Ra,Marko Boben,Jiaman Li,Tingwu\n Richard Shen,Vinh-Dieu Lam,and Alex Kendall. Learning Wang,Sanja Fidler,and Antonio Torralba.Virtual Home:Sim-\n todrivefromsimulationwithoutrealworldlabels. In ICRA, ulatinghouseholdactivitiesviaprograms. In CVPR,2018.\n 2019. [24] Manolis Savva, Angel X. Chang, Alexey Dosovitskiy,\n [7] Simon Brodeur,Ethan Perez,Ankesh Anand,Florian Golemo, Thomas Funkhouser, and Vladlen Koltun. MINOS: Mul-\n Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, timodalindoorsimulatorfornavigationincomplexenviron-\n and Aaron C.Courville. Ho ME:Ahouseholdmultimodal ments. ar Xiv:1712.03931,2017.\n environment. ar Xiv:1711.11017,2017. [25] John Schulman,Filip Wolski,Prafulla Dhariwal,Alec Rad-\n [8] Angel Chang,Angela Dai,Thomas Funkhouser,Maciej Hal- ford,and Oleg Klimov. Proximalpolicyoptimizationalgo-\n ber,Matthias Niessner,Manolis Savva,Shuran Song,Andy rithms. ar Xiv:1707.06347,2017.\n Zeng,and Yinda Zhang. Matterport 3 D:Learningfrom RGB- [26] Linda Smithand Michael Gasser. Thedevelopmentofem-\n Ddatainindoorenvironments. In International Conference bodiedcognition: Sixlessonsfrombabies. Artificial Life,\n on 3 DVision(3 DV),2017. 11(1-2),2005. \n [9] Abhishek Das,Samyak Datta,Georgia Gkioxari,Stefan Lee, [27] Shuran Song,Fisher Yu,Andy Zeng,Angel XChang,Mano-\n Devi Parikh,and Dhruv Batra. Embodied Question Answer- lis Savva,and Thomas Funkhouser. Semanticscenecomple-\n ing. In CVPR,2018. tionfromasingledepthimage. In CVPR,2017.\n [10] Jia Deng,Wei Dong,Richard Socher,Li-Jia Li,Kai Li,and \n [28] Julian Straub,Thomas Whelan,Lingni Ma,Yufan Chen,Erik\n Fei-Fei Li. Image Net: A large-scale hierarchical image \n Wijmans,Simon Green,Jakob J.Engel,Raul Mur-Artal,Carl\n database. In CVPR,2009. \n Ren,Shobhit Verma,Anton Clarkson,Mingfei Yan,Brian\n [11] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina \n Budge,Yajie Yan,Xiaqing Pan,June Yon,Yuyang Zou,Kim-\n Toutanova. BERT:Pre-trainingofdeepbidirectionaltrans- \n berly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham,\n formers for language understanding. ar Xiv:1810.04805, \n Elias Mueggler,Luis Pesqueira,Manolis Savva,Dhruv Batra,\n 2018. \n Hauke M.Strasdat,Renzo De Nardi,Michael Goesele,Steven\n [12] David Donoho. 50 yearsofdatascience. In Tukey Centennial \n Lovegrove,and Richard Newcombe. The Replicadataset:A\n Workshop,2015. \n digitalreplicaofindoorspaces. ar Xiv:1906.05797,2019.\n [13] Saurabh Gupta,James Davidson,Sergey Levine,Rahul Suk- \n [29] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian.\n thankar,and Jitendra Malik. Cognitivemappingandplanning \n Building generalizable agents with a realistic and rich 3 D\n forvisualnavigation. In CVPR,2017. \n environment. ar Xiv:1801.02209,2018. \n [14] Kaiming He,Xiangyu Zhang,Shaoqing Ren,and Jian Sun. \n [30] Fei Xia,Amir R.Zamir,Zhiyang He,Alexander Sax,Jiten-\n Deepresiduallearningforimagerecognition.In CVPR,2016. \n dra Malik, and Silvio Savarese. Gibson env: Real-world\n [15] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario \n perceptionforembodiedagents. In CVPR,2018.\n Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco \n [31] Claudia Yan,Dipendra Misra,Andrew Bennnett,Aaron Wals-\n Hutter. Learningagileanddynamicmotorskillsforlegged \n man,Yonatan Bisk,and Yoav Artzi.CHALET:Cornellhouse\n robots. Science Robotics,2019. \n agentlearningenvironment. ar Xiv:1801.07357,2018.\n [16] Noriyuki Kojima and Jia Deng. To learn or not to learn: \n Analyzingtheroleoflearningfornavigationinvirtualenvi- \n ronments. ar Xiv:1907.11770,2019. "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n A.Habitat Platform Details • Flexible,structuredrepresentationof 3 Denvironments\n using Scene Graphs,allowingforprogrammaticma-\n Asdescribedinthemainpaper,Habitatconsistsofthe \n nipulationofobjectstate,andcombinationofobjects\n followingcomponents: \n fromdifferentenvironments. \n • Habitat-Sim: a flexible, high-performance 3 D \n • High-efficiencyrenderingenginewithmulti-attachment\n simulator with configurable agents, multiple sensors, \n renderpasstoreduceoverheadformultiplesensors.\n and generic 3 D dataset handling (with built-in sup- \n • Arbitrary numbers of Agents and corresponding\n port for Matterport 3 D [8], Gibson [30], and other \n Sensorsthatcanbelinkedtoa 3 Denvironmentby\n datasets). Habitat-Simisfast–whenrenderinga \n attachmenttoa Scene Graph. \n realisticscannedscenefromthe Matterport 3 Ddataset, \n Theperformanceofthesimulationbackendsurpassesthat\n Habitat-Simachievesseveralthousandframesper \n ofpriorworkoperatingonrealisticreconstructiondatasets\n second (fps) running single-threaded, and can reach \n byalargemargin. Table 3 reportsperformancestatisticson\n over 10,000 fpsmulti-processonasingle GPU. \n atestscenefromthe Matterport 3 Ddataset. Single-thread\n • Habitat-API:amodularhigh-levellibraryforend- \n performance reaches several thousand frames per second\n to-enddevelopmentofembodied AI–definingembod- \n (fps),whilemulti-processoperationwithseveralsimulation\n ied AI tasks (e.g. navigation [2], instruction follow- \n backends can reach over 10,000 fps on a single GPU. In\n ing[3],questionanswering[9]),configuringembodied \n addition,byemploying Open GL-CUDAinteroperationwe\n agents(physicalform,sensors,capabilities),training \n enable direct sharing of rendered image frames with ML\n theseagents(viaimitationorreinforcementlearning, \n frameworkssuchas Py Torchwithoutameasurableimpact\n orviaclassic SLAM),andbenchmarkingtheirperfor- \n on performance as the image resolution is increased (see\n manceonthedefinedtasksusingstandardmetrics[2]. \n Figure 7). \n Habitat-APIcurrentlyuses Habitat-Simasthe \n Habitat-API. The second layer of the Habitat platform\n coresimulator,butisdesignedwithamodularabstrac- \n (Habitat-API) focuses on creating a general and flex-\n tionforthesimulatorbackendtomaintaincompatibility \n ible APIfordefiningembodiedagents,tasksthattheymay\n overmultiplesimulators. \n carryout,andevaluationmetricsforthosetasks. Whende-\n Key abstractions. The Habitat platform relies on a num- \n signingsuchan API,akeyconsiderationistoallowforeasy\n berofkeyabstractionsthatmodelthedomainofembodied \n extensibilityofthedefinedabstractions. Thisisparticularly\n agentsandtasksthatcanbecarriedoutinthree-dimensional \n importantsincemanyoftheparametersofembodiedagent\n indoorenvironments. Hereweprovideabriefsummaryof \n tasks, specific agent configurations, and 3 D environment\n keyabstractions: \n setupscanbevariedininterestingways. Futureresearchis\n • Agent: aphysicallyembodiedagentwithasuiteof \n likelytoproposenewtasks,newagentconfigurations,and\n Sensors.Canobservetheenvironmentandiscapable \n new 3 Denvironments. \n oftakingactionsthatchangeagentorenvironmentstate. \n The API allows for alternative simulator backends to\n • Sensor: associatedwithaspecific Agent, capable \n beused,beyondthe Habitat-Simmodulethatweimple-\n ofreturningobservationdatafromtheenvironmentata \n mented.Thismodularityhastheadvantageofallowingincor-\n specifiedfrequency. \n porationofexistingsimulatorbackendstoaidintransitioning\n • Scene Graph: ahierarchicalrepresentationofa 3 D \n fromexperimentsthatpreviousworkhasperformedusing\n environment that organizes the environment into re- \n legacyframeworks. Thearchitectureof Habitat-APIis\n gionsandobjectswhichcanbeprogrammaticallyma- \n illustratedin Figure 8,indicatingcore APIfunctionalityand\n nipulated. \n functionalityimplementedasextensionstothecore.\n • Simulator: an instance of a simulator backend. \n Abovethe APIlevel,wedefineaconcreteembodiedtask\n Given actions for a set of configured Agents and \n suchasvisualnavigation. Thisinvolvesdefiningaspecific\n Scene Graphs,canupdatethestateofthe Agents \n datasetconfiguration, specifyingthestructureofepisodes\n and Scene Graphs,andprovideobservationsforall \n (e.g.numberofstepstaken,terminationconditions),training\n active Sensorspossessedbythe Agents. \n curriculum(progressionofepisodes,difficultyramp),and\n These abstractions connect the different layers of the \n evaluationprocedure(e.g.testepisodesetsandtaskmetrics).\n platform.Theyalsoenablegenericandportablespecification \n Anexampleofloadingapre-configuredtask(Point Nav)and\n ofembodied AItasks. \n stepping through the environment with a random agent is\n Habitat-Sim.Thearchitectureofthe Habitat-Simback- \n showninthecodebelow. \n end module is illustrated in Figure 6. The design of this \n 5 Note:Thesemanticsensorin Matterport 3 Drequiresusingadditional\n moduleensuresafewkeyproperties: \n 3 Dmesheswithsignificantlymoregeometriccomplexity,leadingtore-\n • Memory-efficientmanagementof 3 Denvironmentre- \n ducedperformance. Weexpectthistobeaddressedinfutureversions,\n sources(trianglemeshgeometry,textures,shaders)en- leadingtospeedscomparableto RGB+depth.\n suringsharedresourcesarecachedandreused. "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n Resource Manager Simulator Agent \n \n \n Scene Manager \n \n Texture Material Shader \n Scene Graph \n \n \n Mesh Scene Node Sensor \n \n Figure 6:Architectureof Habitat-Simmainclasses.The Simulatordelegatesmanagementofallresourcesrelatedto 3 Denvironments\n toa Resource Managerthatisresponsibleforloadingandcaching 3 Denvironmentdatafromavarietyofon-diskformats.Theseresources\n areusedwithin Scene Graphsatthelevelofindividual Scene Nodesthatrepresentdistinctobjectsorregionsinaparticular Scene.Agents\n andtheir Sensorsareinstantiatedbybeingattachedto Scene Nodesinaparticular Scene Graph.\n GPU→CPU→GPU GPU→CPU GPU→GPU \n Sensors/numberofprocesses 1 3 5 1 3 5 1 3 5 \n \n RGB 2,346 6,049 7,784 3,919 8,810 11,598 4,538 8,573 7,279 \n RGB+depth 1,260 3,025 3,730 1,777 4,307 5,522 2,151 3,557 3,486 \n RGB+depth+semantics 5 378 463 470 396 465 466 464 455 453 \n \n Table 3:Performanceof Habitat-Siminframespersecondforanexample Matterport 3 Dscene(id 17 DRP 5 sb 8 fy)ona Xeon E 5-2690\n v 4 CPUand Nvidia Titan Xp GPU,measuredataframeresolutionof 128 x 128,underdifferentframememorytransferstrategiesandwitha\n varyingnumberofconcurrentsimulatorprocessessharingthe GPU.‘GPU-CPU-GPU’indicatespassingofrenderedframesfrom Open GL\n contextto CPUhostmemoryandbackto GPUdevicememoryforuseinoptimization,‘GPU-CPU’onlyreportscopyingfrom Open GL\n contextto CPUhostmemory,whereas‘GPU-GPU’indicatesdirectsharingthrough Open GL-CUDAinteroperation.\n \n reporttheaveragegeodesicdistancealongtheshortestpath\n (GDSP)betweenstartingpointandgoalposition. Asnoted\n inthemainpaper,Gibsonepisodesaresignificantlyshorter\n than Matterport 3 D ones. Figure 9 visualizes the episode\n distributionsovergeodesicdistance(GDSP),Euclideandis-\n tancebetweenstartandgoalposition, andtheratioofthe\n two(anapproximatemeasureofcomplexityfortheepisode).\n Weagainnotethat Gibsonepisodeshavemoreepisodeswith\n shorterdistances,leadingtothedatasetbeingoveralleasier\n thanthe Matterport 3 Ddataset. \n import habitat \n # Load embodied AI task (Point Nav) \n # and a pre-specified virtual robot \n config = habitat.get_config(config_file= \n \"pointnav.yaml\") \n Figure 7:Performanceof Habitat-Simunderdifferentsensor env = habitat.Env(config)\n framememorytransferstrategiesforincreasingimageresolution. \n observations = env.reset() \n Weseethat‘GPU->GPU’isunaffectedbyimageresolutionwhile \n otherstrategiesdegraderapidly. # Step through environment with random actions\n while not env.episode_over: \n observations = \\ \n B.Additional Dataset Statistics \n env.step(env.action_space.sample()) \n In Table 5 wesummarizethetrain,validationandtestsplit \n sizesforallthreedatasetsusedinourexperiments. Wealso "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n \n \n Sensor API \n RL Environment RL baselines \n Habitat-Sim Simulator API \n SLAM \n . . . \n Environment \n Embodied QA \n Imitation \n Task \n learning \n Navigation \n Episodes \n Episode Baselines \n Dataset \n Gibson Point Nav Matterport 3 D Point Nav Replica Point Nav Matterport 3 D EQA Replica EQA\n \n use inherit core API extensions and implementations \n \n Figure 8:Architectureof Habitat-API.Thecorefunctionalitydefinesfundamentalbuildingblockssuchasthe APIforinteractingwith\n thesimulatorbackendandreceivingobservationsthrough Sensors. Concretesimulationbackends,3 Ddatasets,andembodiedagent\n baselinesareimplementedasextensionstothecore API. \n \n Dataset scenes(#) episodes(#) average GDSP(m) C.1.Analysisof Collisions \n Matterport 3 D 58/11/18 4.8 M/495/1008 11.5/11.1/13.2 To further characterize the behavior of learned agents\n Gibson 72/16/10 4.9 M/1000/1000 6.9/6.5/7.0 \n duringnavigationweplottheaveragenumberofcollisions\n in Figure 10. We see that Blind incurs a much larger\n Table 4: Statistics of the Point Goal navigation datasets that we numberofcollisionsthanotheragents,providingevidence\n precomputeforthe Matterport 3 Dand Gibsondatasets:totalnumber \n for‘wall-following’behavior. Depth-equippedagentshave\n ofscenes,totalnumberofepisodes,andaveragegeodesicdistance \n the lowest number of collisions, while RGB agents are in\n betweenstartandgoalpositions.Eachcellreportstrain/val/test \n between. \n splitstatistics. \n C.2.Noisy Depth \n Dataset Min Median Mean Max \n Toinvestigatetheimpactofnoisydepthmeasurementson\n Matterport 3 D 18 90.0 97.1 281 \n agentperformance,were-evaluateddepthagents(without\n Gibson 15 60.0 63.3 207 \n re-training)onnoisydepthgeneratedusingasimplenoise\n Table 5: Statisticsofpathlength(inactions)foranoraclewhich model: iid Gaussiannoise(µ = 0,σ = 0.4)ateachpixel\n greedily fits actions to follow the negative of geodesic distance in inverse depth (larger depth = more noise). We observe\n gradientonthe Point Goalnavigationvalidationsets.Thisprovides a drop of 0.13 and 0.02 SPL for depth-RL and SLAM on\n expectedhorizonlengthsforanear-perfectagentandcontextualizes Gibson-val(depth-RLstilloutperforms SLAM).Notethat\n thedecisionforamax-steplimitof 500. SLAM from [20] utilizes ORB-SLAM 2, which is quite\n robusttonoise,whiledepth-RLwastrainedwithoutnoise.\n C.Additional Experimental Results If we increase σ to 0.1, depth-RL gets 0.12 SPL whereas\n SLAMsufferscatastrophicfailures. \n In order to confirm that the trends we observe for the \n experimentalresultspresentedinthepaperholdformuch D.Gibson Dataset Curation \n largeramountsofexperience,wescaledourexperimentsto \n 800 M steps. We found that (1) the ordering of the visual Wemanuallycuratedthefulldatasetof Gibson 3 Dtex-\n inputs stays Depth > RGBD > RGB > Blind; (2) RGB turedmeshes[30]toselectmeshesthatdonotexhibitsignif-\n is consistently better than Blind (by 0.06/0.03 SPL on icantreconstructionartifactssuchasholesortexturequality\n Gibson/Matterport 3 D),and(3)RGBDoutperforms SLAM issues. Akeyissuethatwetriedtoavoidisthepresenceof\n on Matterport 3 D(by 0.16 SPL). "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 9:Statisticsof Point Goalnavigationepisodes.Fromleft:distributionover Euclideandistancebetweenstartandgoal,distribution\n overgeodesicdistancealongshortestpathbetweenstartandgoal,anddistributionovertheratioofgeodesicto Euclideandistance.\n \n Gibson Blind habitat-api/habitat_baselines. Below is the\n RGB \n shellscriptweusedforour RLexperiments: \n RGBD \n Depth \n MP 3 D Blind # Note: parameters in {} are experiment specific.\n RGB # Note: use 8, 6 processes for Gibson, MP 3 D\n RGBD # respectively. \n Depth \n 0 5 10 15 20 25 30 35 40 python habitat_baselines/train_ppo.py \\ \n Avg. Collisions --sensors {RGB_SENSOR,DEPTH_SENSOR} \\ \n --blind {0,1} --use-gae --lr 2.5 e-4 \\ \n Figure 10: Averagenumberofcollisionsduringsuccessfulnavi- --clip-param 0.1 --use-linear-lr-decay \\\n gationepisodesforthedifferentsensoryconfigurationsofthe RL --num-processes {8,6} --num-steps 128 \\\n (PPO)baselineagentontestsetepisodesforthe Gibsonand Matter- --num-mini-batch 4 --num-updates 135000 \\\n --use-linear-clip-decay \\ \n port 3 Ddatasets.The Blindagentexperiencesthehighestnumber \n ofcollisions,whileagentspossessingdepthsensors(Depthand \n For running SLAM please refer to habitat-\n RGBD)havethefewestcollisionsonaverage. \n api/habitat_baselines/slambased. \n holesorcracksinfloorsurfaces.Thisisparticularlyproblem- \n F.Example Navigation Episodes \n aticfornavigationtasksasitdividesseeminglyconnected \n navigable areasintonon-traversabledisconnectedcompo- \n Figure 12 visualizes additional example navigation\n nents. Wemanuallyannotatedthescenes(usingthe 0 to 5 \n episodesforthedifferentsensoryconfigurationsofthe RL\n qualityscaleshownin Figure 11)andonlyusesceneswitha (PPO) agents that we describe in the main paper. Blind\n ratingof 4 orhigher,i.e.,noholes,goodreconstruction,and \n agentshavethelowestperformance,collidingmuchmore\n negligibletextureissuestogeneratethedatasetepisodes. \n frequentlywiththeenvironmentandadoptinga‘wallhug-\n ging’ strategy for navigation. RGB agents are less prone\n E.Reproducing Experimental Results \n tocollisions but stillstruggle tonavigatetothe goalposi-\n Our experimental results can be reproduced us- tionsuccessfullyinsomecases. Incontrast,depth-equipped\n ing the Habitat-API (commit ec 9557 a) and agentsaremuchmoreefficient,exhibitingfewercollisions,\n Habitat-Sim (commit d 383 c 20) repositories. The andnavigatingtogoalsmoresuccessfully(asindicatedby\n code for running experiments is present under the folder theoverallhigher SPLvalues).\n \n \n "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 0:criticalreconstructionartifacts,holes,ortextureissues 1:bigholesorsignificanttextureissuesandreconstructionartifacts\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 2:bigholesorsignificanttextureissues,butgoodreconstruction 3:smallholes,sometextureissues,goodreconstruction\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 4:noholes,sometextureissues,goodreconstruction 5:noholes,uniformtextures,goodreconstruction\n \n Figure 11:Ratingscaleusedincurationof 3 Dtexturedmeshreconstructionsfromthe Gibsondataset.Weuseonlymesheswithratingsof 4\n orhigherforthe Habitat Challengedataset. \n \n \n \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n Gibson \n \n \n \n \n \n \n \n \n \n \n \n Blind SPL=0.00 RGBSPL=0.45 \n \n \n \n \n \n \n \n \n \n RGBDSPL=0.82 Depth SPL=0.88 \n \n \n \n \n \n \n \n \n \n \n Blind SPL=0.00 RGBSPL=0.29 \n \n \n \n \n \n \n \n \n \n \n RGBDSPL=0.49 Depth SPL=0.96 \n \n Figure 12:Additionalnavigationexampleepisodesforthedifferentsensoryconfigurationsofthe RL(PPO)agent,visualizingtrialsfrom\n the Gibsonand MP 3 Dvalsets.Abluedotandreddotindicatethestartingandgoalpositions,andthebluearrowindicatesfinalagent\n position.Theblue-green-redlineistheagent’strajectory.Colorshiftsfrombluetoredasthemaximumnumberofallowedagentstepsis\n approached. \n \n \n \n \n "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP 3 D \n \n \n \n \n \n \n \n \n \n \n Blind SPL=0.00 RGBSPL=0.40 \n \n \n \n \n \n \n \n \n \n \n RGBDSPL=0.92 Depth SPL=0.98 \n \n Figure 12:Additionalnavigationexampleepisodesforthedifferentsensoryconfigurationsofthe RL(PPO)agent,visualizingtrialsfrom\n the Gibsonand MP 3 Dvalsets.Abluedotandreddotindicatethestartingandgoalpositions,andthebluearrowindicatesfinalagent\n position.Theblue-green-redlineistheagent’strajectory.Colorshiftsfrombluetoredasthemaximumnumberofallowedagentstepsis\n approached. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n "
  }
]