[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n OKAMI: Teaching Humanoid Robots Manipulation \n \n Skills through Single Video Imitation \n \n \n Jinhan Li 1† Yifeng Zhu 1∗ Yuqi Xie 1,2∗ Zhenyu Jiang 1,2∗ Mingyo Seo 1 \n Georgios Pavlakos 1 Yuke Zhu 1,2 \n UTAustin 1 NVIDIAResearch 2 \n \n Abstract:Westudytheproblemofteachinghumanoidrobotsmanipulationskills\n byimitatingfromsinglevideodemonstrations. Weintroduce OKAMI,amethod\n that generates a manipulation plan from a single RGB-D video and derives a\n policy for execution. At the heart of our approach is object-aware retargeting,\n which enables the humanoid robot to mimic the human motions in an RGB-D\n video while adjusting to different object locations during deployment. OKAMI\n uses open-world vision models to identify task-relevant objects and retarget the\n body motions and hand poses separately. Our experiments show that OKAMI\n achievesstronggeneralizationsacrossvaryingvisualandspatialconditions,out-\n performing the state-of-the-art baseline on open-world imitation from observa-\n tion. Furthermore,OKAMIrollouttrajectoriesareleveragedtotrainclosed-loop\n visuomotorpolicies,whichachieveanaveragesuccessrateof 79.2%withoutthe\n needforlabor-intensiveteleoperation. Morevideoscanbefoundonourwebsite\n https://ut-austin-rpl.github.io/OKAMI/. \n Keywords:Humanoid Manipulation,Imitation From Videos,Motion Retargeting\n \n \n \n \n \n \n \n \n Single Video \n H D u e m m a o n Imitation H D u e m m a o n \n \n \n \n \n Human Human \n Demo Demo \n Figure 1: OKAMIenablesahumanusertoteachthehumanoidrobothowtoperformanewtaskbyproviding\n asinglevideodemonstration. \n \n 1 Introduction \n Deployinggeneralistrobotstoassistwitheverydaytasksrequiresthemtooperateautonomouslyin\n natural environments. With recent advances in hardware designs and increased commercial avail-\n ability,humanoidrobotsemergeasapromisingplatformtodeployinourlivingandworkingspaces.\n Despitetheirgreatpotential,theystillstruggletooperateautonomouslyanddeployrobustlyinthe\n †Thisworkwasdonewhile Jinhan Liwasavisitingresearcherat UTAustin. \n *Equalcontribution. \n \n 8 th Conferenceon Robot Learning(Co RL 2024),Munich,Germany. \n 4202 \n tc O \n 51 \n ]OR.sc[ \n 1 v 29711.0142:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n unstructured world. A burgeoning line of work has resorted to deep imitation learning methods\n forhumanoidmanipulation[1–3]. However,theyrelyonlargeamountsofdemonstrationsthrough\n whole-body teleoperation, requiring domain expertise and strenuous efforts. In contrast, humans\n have the innate ability to watch their peers do a task once and mimic the behaviors. Equipping\n robotswiththeabilitytoimitatefromvisualobservationswillmoveusclosertothegoaloftraining\n roboticfoundationmodelsfrom Internet-scalehumanactivityvideos. \n We explore teaching humanoid robots to manipulate objects by watching humans. We consider a\n problemsettingrecentlyformulatedas“open-worldimitationfromobservation,”wherearobotim-\n itates a manipulation skill from a single video of human demonstration [4–6]. This setting would\n facilitateusersineffortlesslydemonstratingtasksandenableahumanoidrobottoacquirenewskills\n quickly. Enabling humanoids to imitate from single videos presents a significant challenge — the\n videodoesnothaveactionlabels, butyettherobothastolearntoperformtasksinnewsituations\n beyondwhat’sdemonstratedinthevideo. Priorworksonone-shotvideolearninghaveattemptedto\n optimizerobotactionstoreconstructthefutureobjectmotiontrajectories[4,5]. However,theyhave\n been applied to single-arm manipulators and are computationally prohibitive for humanoid robots\n due to their high degrees of freedom and joint redundancy [7]. Meanwhile, the similar kinematic\n structuresharedbyhumansandhumanoidsmakesdirectlyretargetinghumanmotionstorobotsfea-\n sible[8,9]. Nonetheless,existingretargetingtechniquesfocusonfree-spacebodymotions[10–14],\n lacking the contextual awareness of objects and interactions needed for manipulation. To address\n thisshortcoming, weintroducetheconceptof“object-awareretargeting”. Byincorporatingobject\n contextual information into the retargeting process, the resulting humanoid motions can be effi-\n cientlyadaptedtothelocationsofobjectsinopen-endedenvironments. \n Tothisend,weintroduce OKAMI(Object-aware Kinematicret Argetingforhu Manoid Imitation),\n anobject-awareretargetingmethodthatenablesabimanualhumanoidwithtwodexteroushandsto\n imitate manipulation behaviors from a single RGB-D video demonstration. OKAMI uses a two-\n stage process to retarget the human motions to the humanoid robot to accomplish the task across\n varyinginitialconditions. Thefirststageprocessesthevideotogenerateareferencemanipulation\n plan.Thesecondstageusesthisplantosynthesizethehumanoidmotionsthroughmotionretargeting\n thatadaptstotheobjectlocationsintargetenvironments. \n OKAMI consists of two key designs. The first design is an open-world vision pipeline that iden-\n tifiestask-relevantobjects,reconstructshumanmotionsfromthevideo,andlocalizestask-relevant\n objects during evaluation. Localizing objects at test time also enables motion retargeting to adapt\n todifferentbackgroundsornewobjectinstancesofthesamecategories. Theseconddesignisthe\n factorized process for retargeting, where we retarget the body motions and hand poses separately.\n Wefirstretargetthebodymotionsfromthereferenceplaninthetaskspace,andthenwarptheretar-\n getedtrajectorygiventhelocationoftask-relevantobjects. Thetrajectoryofbodyjointsisobtained\n throughinversekinematics. Thejointanglesoffingersaremappedfromtheplanontothedexterous\n hands, reproducing hand-object interaction. With object-aware retargeting, OKAMI policies sys-\n tematicallygeneralizeacrossvariousspatiallayoutsofobjectsandsceneclutters. Finally,wetrain\n visuomotorpoliciesontherollouttrajectoriesfrom OKAMI throughbehavioralcloningtoobtain\n vision-basedmanipulationskills. \n Weevaluate OKAMI onhumanvideodemonstrationsofdiversetasksthatcoverrichobjectinter-\n actions, suchaspicking, placing, pushing, andpouring. Weshowthatitsobject-awareretargeting\n achieves 71.7%tasksuccessratesaveragedacrossalltasksandoutperformsthe ORION[4]baseline\n by 58.3%. Wethentrainclosed-loopvisuomotorpoliciesonthetrajectoriesgeneratedby OKAMI,\n achievinganaveragesuccessrateof 79.2%. Ourcontributionsof OKAMIarethree-fold:\n 1. OKAMI enables a humanoid robot to mimic human behaviors from a single video for\n dexterousmanipulation. Itsobject-awareretargetingprocessgeneratesfeasiblemotionsof\n thehumanoidrobotwhileadaptingthemotionstotargetobjectlocationsattesttime;\n 2. OKAMI uses vision foundation models [15, 16] to identify task-relevant objects with-\n outadditionalhumaninputs. Theircommon-sensereasoningabilityhelpsrecognizetask-\n 2 \n \n "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n relevantobjectseveniftheyarenotdirectlyincontactwithotherobjectsortherobothands,\n allowingourmethodtoimitatemorediversetasksthanpriorwork; \n 3. Wevalidate OKAMI’sstrongspatialandvisualgeneralizationabilitiesonhumanoidhard-\n ware. OKAMIenablesreal-robotdeploymentinnaturalenvironmentswithunseenobject\n layouts,varyingvisualbackgrounds,andnewobjectinstances. \n \n 2 Related Work \n \n Humanoid Robot Control. Methods like motion planning and optimal control have been devel-\n oped for humanoid locomotion and manipulation [10, 12, 17]. These model-based approaches\n rely on precise physical modeling and expensive computation [11, 12, 18]. To mitigate the\n stringent requirements, researchers have explored policy training in simulation and sim-to-real\n transfer [10, 19]. However, thesemethods still require a significant amount of labor and expertise\n indesigningsimulationtasksandrewardfunctions,limitingtheirsuccessestolocomotiondomains.\n In parallel to automated methods, a variety of human control mechanisms and devices have been\n developed for humanoid teleoperation using motion capture suits [9, 12, 20–24], telexistence\n cockpits[25–29],VRdevices[1,30,31],orvideosthattrackhumanbodies[17,32]. Whilethese\n systemscancontroltherobotstogeneratediversebehaviors,theyrequirereal-timehumaninputthat\n posessignificantcognitiveandphysicalburdens. Incontrast,OKAMIonlyrequiressingle RGB-D\n humanvideostoteachthehumanoidrobotnewskills,significantlyreducingthehumancost.\n Imitation Learning for Robot Manipulation. Imitation Learning has significantly advanced\n vision-basedrobotmanipulationwithhighsampleefficiency[33–44]. Priorworkshaveshownthat\n robotscanlearnvisuomotorpoliciestocompletevarioustaskswithjustdozensofdemonstrations,\n ranging from long-horizon manipulation [34–36] to dexterous manipulation [37–39]. However,\n collecting demonstrations often requires domain expertise and high costs, creating challenges to\n scale. Another line of work focuses on one-shot imitation learning [40–44], yet they demand\n excessive data collection for meta-training tasks. Recently, researchers have looked into a new\n problem setting of imitating from a single video demonstration [4–6], referred to as “open-world\n imitation from observation” [4]. Unlike prior works that abstract away embodiment motions\n due to kinematic differences between the robot and the human, we exploit embodiment motion\n information owing to the kinematic similarity between humans and humanoids. Specifically, we\n introduceobject-awareretargetingthatadaptshumanmotionstohumanoidrobots.\n Motion Retargeting. Motion retargeting has wide applications in computer graphics and 3 D\n vision[8],whereextensiveliteraturestudieshowtoadapthumanmotionstodigitalavatars[45–47].\n This technique has been adopted in robotics for recreating human-like motions on humanoid or\n anthropomorphic robots through various retargeting methods, including optimization-based ap-\n proaches[11,12,20,48],geometric-basedmethods[49],andlearning-basedtechniques[10,13,17].\n However, in manipulation tasks, these retargeting methods have been used within teleoperation\n systems,lackingavisionpipelineforautomaticadaptationtoobjectlocations. OKAMIintegrates\n theretargetingprocesswithopen-worldvision,endowingitwithobjectawarenesssothattherobot\n canmimichumanmotionsfromvideodemonstrationsandadapttoobjectlocationsattesttime.\n 3 OKAMI \n In this work, we introduce OKAMI, a two-staged method that tackles open-world imitation from\n observationforhumanoidrobots. OKAMIfirstgeneratesareferenceplanusingtheobjectlocations\n andreconstructedhumanmotionsfromagiven RGB-Dvideo. Then,itretargetsthehumanmotion\n trajectories to the humanoid robot while adapting the trajectories based on new locations of the\n objects. Figure 2 illustratesthewholepipeline. \n Problem Formulation We formulate a humanoid manipulation task as a discrete-time Markov\n Decision Process defined by a tuple: M = (S,A,P,R,γ,µ), where S is the state space, A is the\n action space, P(·|s,a) is the transition probability, R(s) is the reward function, γ ∈ [0,1) is the\n \n 3 \n \n "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n GPT 4 V \n “bottle” \n “bowl” \n Identify Keyframes Through \n Returnalistoftask- Trackobjectsacross Changepoint Detections\n relevantobjectnames thevideo \n Reference Plan \n Human \n RGB-DVideo \n Reconstruction \n Reference Plan Model l<latexit sha 1_base 64=\"L 8 w Fmirg Yiq Iex Nq 3 ulkz L 02 s EI=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cnfar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Av 0+Oq A==</latexit> 0 l<latexit sha 1_base 64=\"9 L/YXDXQXOLHv 1 NEo N 5 OIo Od PBM=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cm/ar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Aw NSOq Q==</latexit> 1 l<latexit sha 1_base 64=\"j Afv Ax TKm Ay IZRV+Bm Wd/Jpc Kv I=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 mkq Mei F 09 Swb SFNp TNdtsu 3 Wz C 7 k Qoob/Biwd Fv Pq Dv Plv 3 LY 5 a Ou Dgcd 7 M 8 z MCx Mp DLrut 1 NYW 9/Y 3 Cpul 3 Z 29/YPyod HTROnmn Gfx TLW 7 ZAa Lo Xi Pgq Uv J 1 o Tq NQ 8 l Y 4 vp 35 r Seuj Yj VI 04 SHk R 0 q MRAMIp W 8 m Uvu 5/2 yh W 36 s 5 BVom Xkwrka PTKX 91+z NKIK 2 SSGt Px 3 ASDj Go UTPJpq Zsanl A 2 pk Pes VTRi Jsgmx 87 JWd W 6 ZNBr G 0 p JHP 190 RGI 2 Mm UWg 7 I 4 ojs+z Nx P+8 Toq D 6 y ATKkm RK 7 ZYNEglw Zj MPid 9 o Tl DOb GEMi 3 sr YSNq KYMb T 4 l G 4 K 3/PIqa V 5 Uvctq 7 a FWqd/kc RTh BE 7 h HDy 4 gjrc QQN 8 YCDg GV 7 hz VHOi/Puf Cxa C 04+cwx/4 Hz+AOzljs Y=</latexit> N\n Generation SMPL-Htrajectory Identifytarget/refobjectsand\n generatereferenceplan \n Estimatetransformation Inverse \n Robot Observation Localizerelevantobjects betweenpointclouds Kinematics\n attesttime \n Reference Plan Hand Finger \n l<latexit sha 1_base 64=\"L 8 w Fmirg Yiq Iex Nq 3 ulkz L 02 s EI=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cnfar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Av 0+Oq A==</latexit> 0 l<latexit sha 1_base 64=\"9 L/YXDXQXOLHv 1 NEo N 5 OIo Od PBM=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 l E 1 GPRi 8 c Kpi 20 o Wy 2 m 3 bp Zh N 2 J 0 IJ/Q 1 e PCji 1 R/kz X/jts 1 BWx 8 MPN 6 b YWZem Eph 0 HW/nd La+sbm Vnm 7 sr O 7 t 39 QPTxqm STTj Pssk Ynuh NRw KRT 3 Ua Dkn VRz Goe St 8 Px 3 cxv P 3 Ft RKIec ZLy IKZDJSLBKFr Jl/3 cm/ar Nbfuzk FWi Ve QGh Ro 9 qtfv UHCspgr ZJIa 0/Xc FIOcah RM 8 mmllxme Ujam Q 961 VNGYmy Cf Hzsl Z 1 YZk Cj Rth SSufp 7 Iqex MZM 4 t J 0 xx ZFZ 9 mbif 143 w+gmy IVKM+SKLRZFm SSYk Nnn ZCA 0 Zygnll Cmhb 2 Vs BHVl KHNp 2 JD 8 JZf Xi Wti 7 p 3 Vb 98 u Kw 1 bos 4 yn ACp 3 AOHlx DA+6 h CT 4 w EPAMr/Dm KOf Fe Xc+Fq 0 lp 5 g 5 hj 9 w Pn 8 Aw NSOq Q==</latexit> 1 l<latexit sha 1_base 64=\"j Afv Ax TKm Ay IZRV+Bm Wd/Jpc Kv I=\">AAAB 7 Hicb VBNS 8 NAEJ 3 Ur 1 q/qh 69 LBb BU 0 mkq Mei F 09 Swb SFNp TNdtsu 3 Wz C 7 k Qoob/Biwd Fv Pq Dv Plv 3 LY 5 a Ou Dgcd 7 M 8 z MCx Mp DLrut 1 NYW 9/Y 3 Cpul 3 Z 29/YPyod HTROnmn Gfx TLW 7 ZAa Lo Xi Pgq Uv J 1 o Tq NQ 8 l Y 4 vp 35 r Seuj Yj VI 04 SHk R 0 q MRAMIp W 8 m Uvu 5/2 yh W 36 s 5 BVom Xkwrka PTKX 91+z NKIK 2 SSGt Px 3 ASDj Go UTPJpq Zsanl A 2 pk Pes VTRi Jsgmx 87 JWd W 6 ZNBr G 0 p JHP 190 RGI 2 Mm UWg 7 I 4 ojs+z Nx P+8 Toq D 6 y ATKkm RK 7 ZYNEglw Zj MPid 9 o Tl DOb GEMi 3 sr YSNq KYMb T 4 l G 4 K 3/PIqa V 5 Uvctq 7 a FWqd/kc RTh BE 7 h HDy 4 gjrc QQN 8 YCDg GV 7 hz VHOi/Puf Cxa C 04+cwx/4 Hz+AOzljs Y=</latexit> N Targetand Mapping Send Joint\n referenceobjects Commands \n Object-Aware \n Retargeting SMPL-Htrajectory Retargetmotions \n segment Using SMPL-H Warpedmotions Robotexecution \n Figure 2: Overviewof OKAMI.OKAMIisatwo-stagedmethodthatenablesahumanoidrobottoimitatea\n manipulationtaskfromasinglehumanvideo.Inthefirststage,OKAMIgeneratesareferenceplanusing GPT-\n 4 Vandlargevisionmodelsforsubsequentmanipulation. Inthesecondstage,OKAMIfollowsthereference\n plan,whereitretargetshumanmotionsontothehumanoidwithobjectawareness. Theretargetedmotionsare\n convertedintoasequenceofrobotjointcommandsfortherobottofollow. \n discountfactor,andµistheinitialstatedistribution. Inourcontext,S isthespaceofraw RGB-D\n observationsthatcaptureboththerobotandobjectstates, Aisthespaceofthemotioncommands\n forthehumanoidrobot,Risthesparserewardfunctionthatreturns 1 whenataskiscomplete. The\n objectiveofsolvingataskistofindapolicyπthatmaximizestheexpectedtasksuccessratesfrom\n awiderangeofinitialconfigurationsdrawnfromµattesttime. \n We consider the setting of “open-world imitation from observation” [4], where the robot system\n takesarecorded RGB-Dhumanvideo, V asinput, andreturnsahumanoidmanipulationpolicyπ\n thatcompletesthetaskasdemonstratedin V.Thissettingis“open-world”astherobotdoesnothave\n priorknowledgeorground-truthaccesstothecategoriesorphysicalstatesofobjectsinvolvedinthe\n task, and it is “from observation” in the sense that video V does not come with any ground-truth\n robotactions. Apolicyexecutionisconsideredsuccessfulifthestatematchesthestateofthefinal\n framefrom V. Thesuccessconditionsofalltestedtasksaredescribedin Appendix B.1. Notably,\n two assumptions are made about V in this paper: all the image frames in V capture the human\n bodies,andthecameraviewofshooting V isstaticthroughouttherecording. \n 3.1 Reference Plan Generation \n Toenableobject-awareretargeting,OKAMIfirstgeneratesareferenceplanforthehumanoidrobot\n to follow. Plan generation involves understanding what task-relevant objects are and how humans\n manipulatethem. \n Identifying and Localizing Task-Relevant Objects. To imitate manipulation tasks from videos\n V,OKAMImustidentifythetask-relevantobjectstointeractwith.Whilepriormethodsrelyonun-\n supervisedapproacheswithsimplebackgroundsorrequireadditionalhumanannotations[50–53],\n OKAMIusesanoff-the-shelf Vision-Language Models(VLMs),GPT-4 V,toidentifytask-relevant\n objects in V by leveraging the commonsense knowledge internalized in the model. Concretely,\n OKAMI obtains the names of task-relevant objects by sampling RGB frames from the video\n demonstration V and prompting GPT-4 V with the concatenation of these images (details in\n Appendix A.2). Using these object names, OKAMI employs Grounded-SAM [16] to segment\n the objects in the first frame and track their locations throughout the video using a Vidoe Object\n 4 "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n Segmentationmodel,Cutie[54]. Thisprocessenables OKAMItolocalizetask-relevantobjectsin\n V,formingthebasisforsubsequentsteps. \n Reconstructing Human Motions. To retarget human motions to the humanoid robot, OKAMI\n reconstructs human motions from V to obtain motion trajectories. We adopt an improved version\n of SLAHMR [55], an iterative optimization algorithm that reconstructs human motion sequences.\n While SLAHMR assumes flat hands, our extension optimizes the hand poses of the SMPL-H\n model [56], which are initialized using estimated hand poses from Ha Me R [57] (More details\n in Appendix A.1). This modification allows us to jointly optimize body and hand poses from\n monocularvideo. Theoutputisasequenceof SMPL-Hmodelscapturingfull-bodyandhandposes,\n enabling OKAMI to retarget human motions to humanoids (See Section 3.2). Additionally, the\n SMPL-Hmodelcanrepresenthumanposesacrossdemographicdifferences,allowingeasymapping\n ofmotionsfromhumandemonstratorstothehumanoid. \n Generatinga Planfrom Video. Havingidentifiedtask-relevantobjectsandreconstructedhuman\n motions,OKAMIgeneratesareferenceplanfrom V forrobotstocompleteeachsubgoal. OKAMI\n identifies subgoals by performing temporal segmentation on V with the following procedure: We\n first track keypoints using Co Tracker [58] and detect velocity changes of keypoints to determine\n keyframes, which correspond to subgoal states. For each subgoal, we identify a target object (in\n motion due to manipulation) and a reference object (serving as a spatial reference for the target\n object’smovementsthrougheithercontactornon-contactrelations). Thetargetobjectisdetermined\n basedontheaveragedkeypointvelocitiesperobject,whilethereferenceobjectisidentifiedthrough\n geometric heuristics or semantic relations predicted by GPT-4 V (More implementation details of\n plangenerationin Appendix A.4). \n Withsubgoalsandassociatedobjectsdetermined,wegenerateareferenceplanl ,l ,...,l ,where\n 0 1 N \n eachstepl correspondstoakeyframeandincludesthepointcloudsofthetargetobjecto ,the\n i target \n reference object o , and the SMPL-H trajectory segment τSMPL . If no reference object is\n reference ti:ti+1 \n required (e.g., grasping an object), o is null. Point clouds are obtained by back-projecting\n reference \n segmentedobjectsfrom RGBimagesusingdepthimages[59]. \n 3.2 Object-Aware Retargeting \n Givenareferenceplanfromthevideodemonstration,OKAMIenablesthehumanoidrobottoimi-\n tatethetaskin V. Therobotfollowseachstepl intheplanbylocalizingtask-relevantobjectsand\n i \n retargetingthe SMPL-Htrajectorysegmentontothehumanoid. Theretargetedtrajectoriesarethen\n convertedintojointcommandsthroughinversekinematics. Thisprocessrepeatsuntilallthesteps\n areexecuted,andsuccessisevaluatedbasedontask-specificconditions(see Appendix B.1).\n Localizing Objectsat Test Time. Toexecutetheplaninthetest-timeenvironment,OKAMImust\n localize the task-relevant objects in the robot’s observations, extracting 3 D point clouds to track\n objectlocations. Byattendingtotask-relevantobjects, OKAMI policiesgeneralizeacrossvarious\n visualconditions,includingdifferentbackgroundsorthepresenceofnovelinstancesoftask-relevant\n objects. \n Retargeting Human Motionstothe Humanoid. Thekeyaspectofobject-awarenessisadapting\n motions to new object locations. After localizing the objects, we employ a factorized retargeting\n processthatsynthesizesarmandhandmotionsseparately. OKAMIfirstadaptsthearmmotionsto\n theobjectlocationssothatthefingersofthehandsareplacedwithintheobject-centriccoordinate\n frame. Then OKAMI only needs to retarget fingers in the joint configuration to mimic how the\n demonstratorinteractswithobjectswiththeirhands. \n Concretely,wefirstmaphumanbodymotionstothetaskspaceofthehumanoid,scalingandadjust-\n ingtrajectoriestoaccountfordifferencesinsizeandproportion. OKAMIthenwarpstheretargeted\n trajectorysothattherobot’sarmreachesthenewobjectlocations(Moredetailsin Appendix A.5).\n Weconsidertwocasesintrajectorywarping—whentherelationalstatebetweentargetandrefer-\n enceobjectsisunchangedandwhenitchanges,adjustingthewarpingaccordingly. Inthefirstcase,\n 5 "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n Sprinkle-salt Plush-toy-in-basket Close-the-laptop \n \n \n \n \n \n \n \n \n Close-the-drawer Place-snacks-on-plate Bagging \n \n \n \n \n \n \n \n \n Figure 3:Visualizationofinitialandfinalframesofbothhumandemonstrationsandrobotrolloutsforalltasks.\n \n weonlywarpthetrajectorybasedonthetargetobjectlocations;inthesecondcase,thetrajectoryis\n warpedbasedonthereferenceobjectlocation. \n After warping, we use inverse kinematics to compute a sequence of joint configurations for the\n arms while balancing the weights of position and rotation targets in inverse kinematics computa-\n tiontomaintainnaturalpostures. Simultaneously,weretargetthehumanhandposestotherobot’s\n finger joints, allowing the robot to perform fine-grained manipulations (Implementation details in\n Appendix A.3). Intheend,weobtainafull-bodyjointconfigurationtrajectoryforexecution. Since\n armmotionretargetingisaffine,ourprocessnaturallyscalesandadjustsmotionsfromdemonstrators\n withvarieddemographiccharacteristics. Byadaptingarmtrajectoriestoobjectlocationsandretar-\n getinghandposesindependently,OKAMIachievesgeneralizationacrossvariousspatiallayouts.\n 4 Experiments \n \n Our experiments are designed to answer the following research question: 1) Is OKAMI effective\n forahumanoidrobottoimitatediversemanipulationtasksfromsinglevideosofhumandemonstra-\n tion?2)Isitcriticalin OKAMItoretargetthebodymotionsofdemonstratorstothehumanoidrobot\n insteadofonlyretargetingbasedonobjectlocations? 3)Can OKAMIretainitsperformancescon-\n sistentlyonvideosdemonstratedbyhumansofdiversedemographics?4)Cantherolloutsgenerated\n by OKAMIbeusedfortrainingclosed-loopvisuomotorpolicies? \n \n 4.1 Experimental Setup \n Task Designs.Wedescribethesixtasksweuseintheexperiments:1)Plush-toy-in-basket:\n placing a plush toy in the basket; 2) Sprinkle-salt: sprinkling a bit of salt into the bowl;\n 3) Close-the-drawer: pushing the drawer in to close it; 4) Close-the-laptop: closing\n the lid of the laptop; 5) Place-snacks-on-plate: placing a bag of snacks on the plate. 6)\n Bagging: placing a chip bag into a shopping bag. We select these six tasks that cover a diverse\n rangeofmanipulationbehaviors: Plush-toy-in-basketand Place-snacks-on-plate\n requirepick-and-placebehaviorsofdailyobjects; Sprinkle-saltisthetaskthatcoverspour-\n ingbehavior;Close-the-drawerand Close-the-laptoprequirethehumanoidtointeract\n with articulated objects, a prevalent type of interaction in daily environments; Bagging involves\n dexterous, bimanualmanipulationandincludesmultiplesubgoals. Whilewemainlyfocusonreal\n \n 6 \n \n "
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n Successrate Missedgrasping Failed Completion Demonstrator 1 Demonstrator 2 Demonstrator 3\n 83.3% 83.3% 83.3% \n 75.0% 75.0% 75.0% 75.0% 75.0% \n 66.7% 66.7% \n 58.3% 58.3% 58.3% \n Sprinkle-salt Plush-toy-in- Close-the- Close-the- Place-snacks- Bagging Place-snacks- Close-the-\n basket laptop drawer on-plate on-plate laptop \n (a) (b) \n Figure 4: (a)Evaluationof OKAMI overallsixtasks, includingthesuccessratesandthequantificationof\n failedtrials,separatedbyfailuremode.(b)Evaluationof OKAMIusingvideosfromdifferentdemonstrations.\n Demonstrator 1 isthemainpersonrecordingvideosforallevaluationsin(a). \n robot experiments, we also implement Sprinkle-salt and Close-the-drawer in simula-\n tionusing Robo Suite[60]foreasyreproducibilityof OKAMI.See Appendix B.4. \n Hardware Setup. Weusea Fourier GR 1 robotasourhardwareplatform,equippedwithtwo 6-Do F\n Inspiredexteroushandsanda D 435 i Intel Real Sensecameraforvideorecordingandtest-timeobser-\n vation. Weimplementajointpositioncontrollerthatoperatesat 400 Hz. Toavoidjerkymovements,\n wecomputejointpositioncommandsat 40 Hzandinterpolatethecommandsto 400 Hztrajectories.\n Evaluation Protocol. We run 12 trials for each task. The locations of the objects are randomly\n initialized within the intersection of the robot camera’s view and the humanoid arms’ reachable\n range. The tasks are evaluated on a tabletop workspace with multiple objects, including both\n task-relevant objects and various other objects. Further, we test new object generalization on\n Place-snacks-on-plate,Plush-toy-in-basket,and Sprinkle-salttasks,chang-\n ingtheinvolvedplate,snackbag,plushtoy,andbowltootherinstancesofthesametype.\n Baselines. Wecompareourresultwithabaseline ORION [4]. Since ORION wasproposedfor\n parallel-jawgrippers,itisnotdirectlyapplicableinourexperimentsandweadoptitwithminimal\n modifications: we estimate the palm trajectory using the SMPL-H trajectories, and warp the tra-\n jectory conditioning on the new object locations. The warped trajectory is used in the subsequent\n inversekinematicsforcomputingrobotjointconfigurations. \n 4.2 Quantitative Results \n \n Toanswerquestion(1), weevaluatethepoliciesof OKAMI acrossallthetasks, coveringdiverse\n behaviorssuchasdailypick-place,pouring,andmanipulationofarticulatedobjects. Theresultsare\n presentedin Figure 4(a). Inourexperiment,werandomlyinitializetheobjectlocationssothatthe\n robotneedstoadapttothelocationsoftheobjects. Thisresultshowstheeffectivenessof OKAMI\n ingeneralizingoverdifferentvisualandspatialconditions. \n To answer question (2), we compare OKAMI against ORION on two representative tasks,\n Place-snacks-on-plate and Close-the-laptop. In the comparison experiment,\n OKAMI differs from ORION in that ORION does not condition on the human body poses.\n OKAMIachieves 75.0%and 83.3%successrates,respectively,while ORIONonlyachieves 0.0%\n and 41.2%,respectively. Additionally,wecompare OKAMIagainst ORIONonthetwosimulated\n versionsof Sprinkle-saltand Close-the-drawertasks. Insimulation,OKAMIachieves\n 82.0% and 84.0% success rates in two tasks while ORION only achieves 0.0% and 10.0%. Most\n failuresof ORIONpoliciesareduetofailingtoapproachobjectswithreliablegraspingposes(e.g.,\n in Place-snacks-on-plate task, ORION tries to grasp the snack from the sides instead of\n thetop-downgraspinhumanvideo),andfailingtorotatethewristfullytoachievebehaviorssuchas\n pouring. Thesebehaviorsoriginatefromthefactthat ORIONignorestheembodimentinformation,\n thusfallingshortinperformancecomparedto OKAMI.Thesuperiorperformanceof OKAMIsug-\n geststheimportanceofretargetingthebodymotionofthehumandemonstratorsontothehumanoid\n whenimitatingfromhumanvideos. \n To answer question (3), we conduct a controlled experiment of recording videos of different\n demonstrators and test if OKAMI policies maintain strong performance across the video inputs.\n 7 \n \n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n Sameasthepreviousexperiment,weevaluate OKAMIonthe Place-snacks-on-plateand\n Close-the-laptoptasks. Theresultsarepresentedin Figure 4(b). Weshowthatforthetask\n Close-the-laptop, there is no statistical significance in performance change. As for task\n Place-snacks-on-plate, while the evaluation maintains above 50%, the worst policy per-\n formanceis 16.7%worsethanthebestpolicyperformance. Afterlookingintothevideorecording,\n wefindthatthemotionofdemonstrator 2 isrelativelyfasterthantheothertwodemonstrators,and\n fastermotionscreateanoisyestimationofmotionwhendoinghumanmodelreconstruction. Over-\n all,OKAMIcanmaintainreasonablygoodperformancegivenvideosfromdifferentdemonstrators,\n butthereisroomforimprovementsonourvisionpipelinetohandlesuchvariety. \n 4.3 Learning Visuomotor Policy With OKAMIRollout Data \n \n We address question (4) by training neural \n visuomotor policies on OKAMI rollouts. We 50 Trajectories 100 Trajectories\n first run OKAMI over randomly initialized 83.3% \n 75.0% \n object layouts to generate multiple rollouts 66.7% \n 58.3% \n and collect a dataset of successful trajectories \n while discarding the failed ones. We train \n neuralnetworkpoliciesonthisdatasetthrough \n a behavioral cloning algorithm. Since smooth \n execution is critical for humanoid manipu- \n Sprinkle-salt Bagging \n lation, we implement the behavioral cloning \n Figure 5: Successratesoflearnedvisuomotorpolicies\n with ACT[61], whichpredictssmoothactions \n on Sprinkle-saltand Baggingusing 50 and 100 \n via its temporal ensemble design, a trajectory trajectories,respectively.\n smoothing component (more implementation \n details in Appendix B.5). We train visuomotor policies for Sprinkle-salt and Bagging.\n Figure 5 illustrates the success rates of these policies, demonstrating that OKAMI rollouts are\n effectivedatasourcesfortraining. Wealsoshowthatthelearnedpoliciesimproveasmorerollouts\n are collected. These results hold the promise of scaling up data collection for learning humanoid\n manipulationskillswithoutlaboriousteleoperation. \n 5 Conclusion \n This paper introduces OKAMI that enables a humanoid robot to imitate a single RGB-D human\n videodemonstration.Atthecoreof OKAMIisobject-awareretargeting,whichretargetsthehuman\n motions onto the humanoid robot and adapts the motions to the target object locations. OKAMI\n consists of two stages to realize object-aware retargeting. The first stage is generating a reference\n plan for manipulation from the video. The second stage is used for retargeting, where OKAMI\n retargetsthearmmotionsinthetaskspaceandthefingermotionsinthejointconfigurationspace.\n Ourexperimentsvalidatethedesignof OKAMI,showingthesystematicgeneralizationof OKAMI\n policies. OKAMI enables efficient collection of trajectory data based on a single human video\n demonstration. OKAMI-baseddatacollectionsignificantlyreducesthehumancostforpolicytrain-\n ingcomparedtothatrequiredbyteleoperation. \n Limitations and Future Work. The current focus of OKAMI is on the upper body motion\n retargetingofhumanoidrobots, particularlyformanipulationtaskswithintabletopworkspaces. A\n promising future direction is to include lower body retargeting that enables locomotion behaviors\n during video imitation. To enable full-body loco-manipulation, a whole-body motion controller\n needstobeimplementedasopposedtothejointpositioncontrollerusedin OKAMI.Additionally,\n we rely on RGB-D videos in OKAMI, which limits us from using in-the-wild Internet videos\n recorded in RGB. Extending OKAMI to use web videos will be another promising direction for\n futureworks. Atlast,thecurrentimplementationofretargetinghaslimitedrobustnessagainstlarge\n variationsinobjectshapes. Afutureimprovementwouldbeintegratingmorepowerfulfoundation\n modelsthatendowtherobotwithageneralunderstandingofhowtointeractwithaclassofobjects\n inspiteoftheirlargeshapechanges. \n 8 "
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n Acknowledgments \n We would like to thank William Yue for providing the initial implementation of the behavioral\n cloningpolicies, Peter Stoneforhisvaluablesupportwithtaskdesignsanddemoshooting, Yuzhe\n Qin for sharing the dex-retargeting codebase, and Zhenjia Xu for his advice on developing the\n humanoidrobotinfrastructure. \n \n References \n \n [1] M.Seo,S.Han,K.Sim,S.H.Bang,C.Gonzalez,L.Sentis,and Y.Zhu. Deepimitationlearn-\n ingforhumanoidloco-manipulationthroughhumanteleoperation. In IEEE-RASInternational\n Conferenceon Humanoid Robots(Humanoids),2023. \n [2] Y.Matsuura,K.Kawaharazuka,N.Hiraoka,K.Kojima,K.Okada,and M.Inaba.Development\n ofawhole-bodyworkimitationlearningsystembyabipedandbi-armedhumanoid. In 2023\n IEEE/RSJInternational Conferenceon Intelligent Robotsand Systems(IROS),pages 10374–\n 10381.IEEE,2023. \n [3] T.Asfour,P.Azad,F.Gyarfas,and R.Dillmann. Imitationlearningofdual-armmanipulation\n tasksinhumanoidrobots. Internationaljournalofhumanoidrobotics,5(02):183–202,2008.\n \n [4] Y. Zhu, A. Lim, P. Stone, and Y. Zhu. Vision-based manipulation from single human video\n withopen-worldobjectgraphs. ar Xivpreprintar Xiv:2405.20321,2024. \n [5] N.Heppert,M.Argus,T.Welschehold,T.Brox,and A.Valada.Ditto:Demonstrationimitation\n bytrajectorytransformation. ar Xivpreprintar Xiv:2403.15203,2024. \n \n [6] D. Guo. Learning multi-step manipulation tasks from a single human demonstration. ar Xiv\n preprintar Xiv:2312.15346,2023. \n [7] T.Asfourand R.Dillmann. Human-likemotionofahumanoidrobotarmbasedonaclosed-\n formsolutionoftheinversekinematicsproblem. In Proceedings 2003 IEEE/RSJInternational\n Conferenceon Intelligent Robotsand Systems(IROS 2003)(Cat.No.03 CH 37453),volume 2,\n pages 1407–1412.IEEE,2003. \n [8] M.Gleicher.Retargettingmotiontonewcharacters.Proceedingsofthe 25 thannualconference\n on Computergraphicsandinteractivetechniques,1998. \n \n [9] K.Darvish,Y.Tirupachuri,G.Romualdi,L.Rapetti,D.Ferigo,F.J.A.Chavez,and D.Pucci.\n Whole-bodygeometricretargetingforhumanoidrobots.In 2019 IEEE-RAS 19 th International\n Conferenceon Humanoid Robots(Humanoids),pages 679–686.IEEE,2019. \n [10] X.Cheng,Y.Ji,J.Chen,R.Yang,G.Yang,and X.Wang. Expressivewhole-bodycontrolfor\n humanoidrobots. ar Xivpreprintar Xiv:2402.16796,2024. \n \n [11] S.Nakaoka,A.Nakazawa,F.Kanehiro,K.Kaneko,M.Morisawa,and K.Ikeuchi.Taskmodel\n oflowerbodymotionforabipedhumanoidrobottoimitatehumandances. In 2005 IEEE/RSJ\n International Conferenceon Intelligent Robotsand Systems,pages 3157–3162.IEEE,2005.\n [12] K.Hu,C.Ott,and D.Lee. Onlinehumanwalkingimitationintaskandjointspacebasedon\n quadraticprogramming. In 2014 IEEEInternational Conferenceon Roboticsand Automation\n (ICRA),pages 3458–3464.IEEE,2014. \n [13] S. Choi, M. K. Pan, and J. Kim. Nonparametric motion retargeting for humanoid robots on\n sharedlatentspace. In Robotics: Scienceand Systems,2020. \n \n [14] E.Demircan,T.Besier,S.Menon,and O.Khatib. Humanmotionreconstructionandsynthesis\n ofhumanskills. In Advancesin Robot Kinematics: Motionin Manand Machine: Motionin\n Manand Machine,pages 283–292.Springer,2010. \n \n 9 \n \n "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n [15] Open AI. Gpt-4 technicalreport,2023. \n [16] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,H.Su,J.Zhu,etal.Grounding\n dino: Marryingdinowithgroundedpre-trainingforopen-setobjectdetection. ar Xivpreprint\n ar Xiv:2303.05499,2023. \n \n [17] T.He,Z.Luo,W.Xiao,C.Zhang,K.Kitani,C.Liu,and G.Shi.Learninghuman-to-humanoid\n real-timewhole-bodyteleoperation. Inar Xiv,2024. \n [18] A.Escande,N.Mansard,and P.-B.Wieber. Hierarchicalquadraticprogramming: Fastonline\n humanoid-robot motion generation. The International Journal of Robotics Research, 33(7):\n 1006–1028,2014. \n \n [19] Q. Liao, B. Zhang, X. Huang, X. Huang, Z. Li, and K. Sreenath. Berkeley humanoid: A\n researchplatformforlearning-basedcontrol. ar Xivpreprintar Xiv:2407.21781,2024.\n [20] L.Penco,N.Scianca,V.Modugno,L.Lanari,G.Oriolo,and S.Ivaldi. Amultimodeteleop-\n erationframeworkforhumanoidloco-manipulation: Anapplicationfortheicubrobot. IEEE\n Robotics&Automation Magazine,26(4):73–82,2019. \n \n [21] D. Kim, B.-J. You, and S.-R. Oh. Whole body motion control framework for arbitrarily and\n simultaneously assigned upper-body tasks and walking motion. Modeling, Simulation and\n Optimizationof Bipedal Walking,pages 87–98,2013. \n [22] A.Di Fava,K.Bouyarmane,K.Chappellet,E.Ruffaldi,and A.Kheddar.Multi-contactmotion\n retargetingfromhumantohumanoidrobot. In 2016 IEEE-RAS 16 thinternationalconference\n onhumanoidrobots(humanoids),pages 1081–1086.IEEE,2016. \n \n [23] M.Arduengo,A.Arduengo,A.Colome´,J.Lobo-Prat,and C.Torras. Humantorobotwhole-\n bodymotiontransfer. In 2020 IEEE-RAS 20 th International Conferenceon Humanoid Robots\n (Humanoids),pages 299–305.IEEE,2021. \n [24] R.Cisneros,M.Benallegue,K.Kaneko,H.Kaminaga,G.Caron,A.Tanguy,R.Singh,L.Sun,\n A. Dallard, C. Fournier, et al. Team janus humanoid avatar: A cybernetic avatar to embody\n human telepresence. In Toward Robot Avatars: Perspectives on the ANA Avatar XPRIZE\n Competition,RSSWorkshop,volume 3,2022. \n \n [25] S. Tachi, K. Komoriya, K. Sawada, T. Nishiyama, T. Itoko, M. Kobayashi, and K. Inoue.\n Telexistencecockpitforhumanoidrobotcontrol. Advanced Robotics,17(3):199–217,2003.\n [26] J.Ramosand S.Kim. Humanoiddynamicsynchronizationthroughwhole-bodybilateralfeed-\n backteleoperation. IEEETransactionson Robotics,34(4):953–965,2018. \n \n [27] Y.Ishiguro,T.Makabe,Y.Nagamatsu,Y.Kojio,K.Kojima,F.Sugai,Y.Kakiuchi,K.Okada,\n and M.Inaba. Bilateralhumanoidteleoperationsystemusingwhole-bodyexoskeletoncockpit\n tablis. IEEERoboticsand Automation Letters,5(4):6419–6426,2020. \n [28] F.Abi-Farrajl,B.Henze,A.Werner,M.Panzirsch,C.Ott,and M.A.Roa.Humanoidteleoper-\n ationusingtask-relevanthapticfeedback.In 2018 IEEE/RSJInternational Conferenceon Intel-\n ligent Robotsand Systems(IROS),pages 5010–5017,2018. doi:10.1109/IROS.2018.8593521.\n [29] M. Schwarz, C. Lenz, A. Rochow, M. Schreiber, and S. Behnke. Nimbro avatar: Interactive\n immersivetelepresencewithforce-feedbacktelemanipulation.In 2021 IEEE/RSJInternational\n Conferenceon Intelligent Robotsand Systems(IROS),pages 5312–5319.IEEE,2021.\n \n [30] M. Hirschmanner, C. Tsiourti, T. Patten, and M. Vincze. Virtual reality teleoperation of a\n humanoid robot using markerless human upper body pose imitation. in 2019 ieee-ras 19 th\n internationalconferenceonhumanoidrobots(humanoids),2019. \n \n 10 \n \n "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n [31] D.Lim,D.Kim,and J.Park.Onlinetelemanipulationframeworkonhumanoidforbothmanip-\n ulationandimitation. 202219 th International Conferenceon Ubiquitous Robots(UR),pages\n 8–15,2022. URLhttps://api.semanticscholar.org/Corpus ID:250577582. \n [32] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn. Humanplus: Humanoid shadowing and\n imitationfromhumans. ar Xivpreprintar Xiv:2406.10454,2024. \n \n [33] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese,\n Y. Zhu, and R. Mart´ın-Mart´ın. What matters in learning from offline human demonstrations\n forrobotmanipulation. ar Xivpreprintar Xiv:2108.03298,2021. \n [34] A. Mandlekar, D. Xu, R. Mart´ın-Mart´ın, S. Savarese, and L. Fei-Fei. Learning to general-\n izeacrosslong-horizontasksfromhumandemonstrations. ar Xivpreprintar Xiv:2003.06085,\n 2020. \n \n [35] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu,Y.Zhu,and A.Anandkumar.Mimicplay:\n Long-horizonimitationlearningbywatchinghumanplay. ar Xivpreprintar Xiv:2302.12422,\n 2023. \n [36] Y.Zhu,A.Joshi,P.Stone,and Y.Zhu. Viola:Imitationlearningforvision-basedmanipulation\n withobjectproposalpriors. ar Xivpreprintar Xiv:2210.11339,2022. \n \n [37] C.Wang,H.Shi,W.Wang,R.Zhang,L.Fei-Fei,and C.K.Liu.Dexcap:Scalableandportable\n mocap data collection system for dexterous manipulation. ar Xiv preprint ar Xiv:2403.07788,\n 2024. \n [38] T.Lin,Y.Zhang,Q.Li,H.Qi,B.Yi,S.Levine,and J.Malik. Learningvisuotactileskillswith\n twomultifingeredhands. ar Xivpreprintar Xiv:2404.16823,2024. \n \n [39] Y.Ze,G.Zhang,K.Zhang,C.Hu,M.Wang,and H.Xu. 3 ddiffusionpolicy. ar Xivpreprint\n ar Xiv:2403.03954,2024. \n [40] M.Changand S.Gupta. One-shotvisualimitationviaattributedwaypointsanddemonstration\n augmentation. ar Xivpreprintar Xiv:2302.04856,2023. \n \n [41] T. Yu, P. Abbeel, S. Levine, and C. Finn. One-shot composition of vision-based skills from\n demonstration. In 2019 IEEE/RSJInternational Conferenceon Intelligent Robotsand Systems\n (IROS),pages 2643–2650.IEEE,2019. \n [42] E. Valassakis, G. Papagiannis, N. Di Palo, and E. Johns. Demonstrate once, imitate imme-\n diately (dome): Learning visual servoing for one-shot imitation learning. In 2022 IEEE/RSJ\n International Conferenceon Intelligent Robotsand Systems(IROS),pages 8614–8621.IEEE,\n 2022. \n [43] E.Johns. Coarse-to-fineimitationlearning: Robotmanipulationfromasingledemonstration.\n In 2021 IEEEinternationalconferenceonroboticsandautomation(ICRA),pages 4613–4619.\n IEEE,2021. \n \n [44] N.Di Paloand E.Johns. Learningmulti-stagetaskswithonedemonstrationviaself-replay. In\n Conferenceon Robot Learning,pages 1180–1189.PMLR,2022. \n [45] Z. Luo, J. Cao, K. Kitani, W. Xu, et al. Perpetual humanoid control for real-time simulated\n avatars. In Proceedingsofthe IEEE/CVFInternational Conferenceon Computer Vision,pages\n 10895–10904,2023. \n \n [46] X.B.Peng,Z.Ma,P.Abbeel,S.Levine,and A.Kanazawa. Amp: Adversarialmotionpriors\n for stylized physics-based character control. ACM Transactions on Graphics (To G), 40(4):\n 1–20,2021. \n \n 11 \n \n "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n [47] B.Jiang,X.Chen,W.Liu,J.Yu,G.Yu,and T.Chen. Motiongpt: Humanmotionasaforeign\n language. Advancesin Neural Information Processing Systems,36,2024. \n [48] S.Kuindersma,R.Deits,M.Fallon,A.Valenzuela,H.Dai,F.Permenter,T.Koolen,P.Marion,\n and R.Tedrake. Optimization-basedlocomotionplanning,estimation,andcontroldesignfor\n theatlashumanoidrobot. Autonomousrobots,40:429–455,2016. \n \n [49] Y. Liang, W. Li, Y. Wang, R. Xiong, Y. Mao, and J. Zhang. Dynamic movement primitive\n based motion retargeting for dual-arm sign language motions. In 2021 IEEE International\n Conferenceon Roboticsand Automation(ICRA),pages 8195–8201.IEEE,2021. \n [50] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, and L. Van Gool. The\n 2019 davis challenge on vos: Unsupervised multi-object segmentation. ar Xiv preprint\n ar Xiv:1905.00737,2019. \n [51] Y. Huang, J. Yuan, C. Kim, P. Pradhan, B. Chen, L. Fuxin, and T. Hermans. Out of sight,\n still in mind: Reasoning and planning about unobserved objects with video tracking enabled\n memorymodels. ar Xivpreprintar Xiv:2309.15278,2023. \n \n [52] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu. Learning generalizable manipulation policies with\n object-centric 3 drepresentations. In 7 th Annual Conferenceon Robot Learning,2023.\n [53] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,S.Kirmani,\n B.Zitkovich,F.Xia,etal. Open-worldobjectmanipulationusingpre-trainedvision-language\n models. ar Xivpreprintar Xiv:2303.00905,2023. \n [54] H.K.Cheng,S.W.Oh,B.Price,J.-Y.Lee,and A.Schwing. Puttingtheobjectbackintovideo\n object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n Pattern Recognition,pages 3151–3161,2024. \n \n [55] V.Ye,G.Pavlakos,J.Malik,and A.Kanazawa. Decouplinghumanandcameramotionfrom\n videosinthewild. In CVPR,2023. \n [56] J.Romero,D.Tzionas,and M.J.Black. Embodiedhands. ACMTransactionson Graphics,36\n (6):1–17,2017. \n \n [57] G.Pavlakos,D.Shan,I.Radosavovic,A.Kanazawa,D.Fouhey,and J.Malik. Reconstructing\n handsin 3 Dwithtransformers. In CVPR,2024. \n [58] N.Karaev,I.Rocco,B.Graham,N.Neverova,A.Vedaldi,and C.Rupprecht. Cotracker: Itis\n bettertotracktogether. ar Xivpreprintar Xiv:2307.07635,2023. \n \n [59] Q.-Y.Zhou,J.Park,and V.Koltun. Open 3 d: Amodernlibraryfor 3 ddataprocessing. ar Xiv\n preprintar Xiv:1801.09847,2018. \n [60] Y. Zhu, J. Wong, A. Mandlekar, R. Mart´ın-Mart´ın, A. Joshi, S. Nasiriany, and Y. Zhu. ro-\n bosuite: Amodularsimulationframeworkandbenchmarkforrobotlearning. ar Xivpreprint\n ar Xiv:2009.12293,2020. \n \n [61] T.Z.Zhao, V.Kumar, S.Levine, and C.Finn. Learningfine-grainedbimanualmanipulation\n withlow-costhardware. ar Xivpreprintar Xiv:2304.13705,2023. \n [62] S.Goel,G.Pavlakos,J.Rajasegaran,A.Kanazawa,and J.Malik. Humansin 4 D:Reconstruct-\n ingandtrackinghumanswithtransformers. In ICCV,2023. \n [63] Y.Xu,J.Zhang,Q.Zhang,and D.Tao. Vi TPose++: Visiontransformerforgenericbodypose\n estimation. IEEETransactionson Pattern Analysisand Machine Intelligence,2023.\n \n [64] S.Caron,Y.De Mont-Marin,R.Budhiraja,and S.H.Bang. Pink: Pythoninversekinematics\n basedon Pinocchio,2024. URLhttps://github.com/stephane-caron/pink. \n \n 12 \n \n "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n \n \n [65] Y.Qin,W.Yang,B.Huang,K.Van Wyk,H.Su,X.Wang,Y.-W.Chao,and D.Fox.Anyteleop:\n Ageneralvision-baseddexterousrobotarm-handteleoperationsystem. In Robotics: Science\n and Systems,2023. \n [66] R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear\n computational cost. Journal of the American Statistical Association, 107(500):1590–1598,\n 2012. \n [67] X.Cheng,J.Li,S.Yang,G.Yang,and X.Wang. Open-television: Teleoperationwithimmer-\n siveactivevisualfeedback. ar Xivpreprintar Xiv:2407.01512,2024. \n \n [68] T.Darcet,M.Oquab,J.Mairal,and P.Bojanowski. Visiontransformersneedregisters. ar Xiv\n preprintar Xiv:2309.16588,2023. \n [69] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,\n D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li,\n W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal,\n P. Labatut, A. Joulin, and P. Bojanowski. Dinov 2: Learning robust visual features without\n supervision,2023. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 13 \n \n "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n A Implementation Details \n \n A.1 Human Reconstruction From Videos \n Method. Forthe 3 Dhumanreconstruction,westartbytrackingthepersoninthevideoandgetting\n aninitialestimateoftheir 3 Dbodyposeusing 4 DHumans[62]. Thisbodyreconstructioncannot\n capturethehandposedetails(i.e.,thehandsareflat). Therefore,foreachdetectionofthepersonin\n thevideo, wedetectthetwohandsusing Vi TPose[63], andforeachhand, weapply Ha Me R[57]\n togetanestimateofthe 3 Dhandpose. However,thehandsreconstructedby Ha Me Rcanbeincon-\n sistent with the arms from the body reconstruction (e.g., different wrist orientation and location).\n Toaddressthis,weapplyanoptimizationrefinementtomakethebodyandthehandsconsistentin\n each frame, andencourage that the holistic bodyand hands motion is smooth overtime. This op-\n timizationissimilarto SLAHMR[55],withthedifferencethatbesidesthebodyposeandlocation\n ofthe SMPL+Hmodel[56],wealsooptimizethehandposes. Weinitializetheprocedureusingthe\n 3 Dbodyposeestimatefrom 4 DHumansandthe 3 Dhandposesfrom Ha Me R.Moreover, weuse\n the 2 Dprojectionofthe 3 Dhandspredictedby Ha Me Rtoconstraintheprojectionofthe 3 Dhand\n keypoints of the holistic model using a reprojection loss. Finally, we can jointly optimize all the\n parameters (body location, body pose, hand poses) over the duration of the video, as described in\n SLAHMR[55]. \n Our modified SLAHMR incorporates the SMPL-H model [56] to include hand poses in the hu-\n manmotionreconstruction. Weinitializehandposesineachframeusing 3 Dhandestimatesfrom\n Ha Me R [57]. The optimization process then jointly refines body locations, body poses, and hand\n posesoverthevideosequence. Thisjointoptimizationallowsforaccuratemodelingofhowhands\n interactwithobjects,whichiscrucialformanipulationtasks. \n Theoptimizationminimizestheerrorbetweenthe 2 Dprojectionsofthe 3 Djointsfromthe SMPL-H\n modelandthedetected 2 Djointlocationsfromthevideo. Weusestandardparametersandsettings\n asdescribedin SLAHMR[55],adaptingthemtoaccommodatethe SMPL-Hmodel. \n Inference Requirements. Themodelofhumanreconstructionweuseislargeandneedstoberun\n onacomputerwithsufficientlygoodcomputationspeed. Hereweprovidedetailsabouttheruntime\n performanceofthehumanreconstructionmodel.Weuseadesktopthatcomeswitha GPURTX 3090\n thathasthesizeofthememory 24 GB.Fora 10 secondsvideowithfps 30,itprocesses 10 minutes.\n A.2 Promptsof Using GPT 4 V \n \n In order to use GPT 4 V in OKAMI, we need GPT 4 V’s output to be in a typed format so that the\n restoftheprogramscanparsetheresult. Moreover,inorderforthepromptstobegeneralacrossa\n diversesetoftasks,ourpromptdoesnotleakanytaskinformationtothemodel. Herewedescribe\n thethreedifferentpromptsin OKAMIforusing GPT 4 V. \n Identify Task-relevant Objects. OKAMI usesthefollowingprompttoinvoke GPT 4 Vsothatit\n canidentifythetask-relevantobjectsfromaprovidedhumanvideo: \n Prompt: Youneedtoanalyzewhatthehumanisdoingintheimages,thentellme: 1. Allthe\n objectsinfrontscene(mostlyonthetable). Youshouldignorethebackgroundobjects. 2. The\n objectsofinterest. Theyshouldbeasubsetofyouranswertothefirstquestion. Theyarelikely\n theobjectsmanipulatedbyhumanornearhuman. Notethatthereareirrelevantobjectsinthe\n scene,suchasobjectsthatdoesnotmoveatall. Youshouldignoretheirelevantobjects.\n Youroutputformatis: \n \n The human is xxx. \n All objects are xxx. \n The objects of interest are: \n ‘‘‘json \n { \n \n 14 \n \n "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n \"objects\": [\"OBJECT 1\", \"OBJECT 2\", ...], \n } \n ‘‘‘ \n Ensuretheresponsecanbeparsedby Python‘json.loads’, e.g.: notrailingcommas, nosingle\n quotes, etc. You should output the names of objects of interest in a list [“OBJECT 1”, “OB-\n JECT 2”, ...] that can be easily parsed by Python. The name is a string, e.g., “apple”, “pen”,\n “keyboard”,etc. \n \n \n Identify Target Objects. OKAMI usesthefollowingprompttoidentifythetargetobjectofeach\n stepinthereferenceplan: \n \n Prompt:Thefollowingimagesshowsamanipulationmotion,wherethehumanismanipulating\n anobject. \n Yourtaskistodeterminewhichobjectisbeingmanipulatedintheimagesbelow. Youneedto\n choosefromthefollowingobjects: {alistoftask-relevantobjects}. \n Tips:themanipulatedobjectistheobjectthatthehumanisinteractingwith,suchaspickingup,\n moving,orpressing,anditisincontactwiththehuman’s{themajormovingarminthisstep}\n hand. \n \n Youroutputformatis: \n ‘‘‘json \n { \n \"manipulate_object_name\": \"MANIPULATE_OBJECT_NAME\", \n } \n ‘‘‘ \n Ensuretheresponsecanbeparsedby Python‘json.loads’, e.g.: notrailingcommas, nosingle\n quotes,etc. \n \n \n Identify Reference Objects. Hereisthepromptthatasks GPT 4 Vtoidentifythereferenceobject\n ofeachstepinthereferenceplan: \n \n Prompt:Thefollowingimagesshowsamanipulationmotion,wherethehumanismanipulating\n theobject{manipulate object name}. \n Please identify the reference object in the image below, which could be an object on which\n toplace{manipulate object name}, oranobjectthat{manipulate object name}isinteracting\n with. Notethattheremaynotnecessarilyhaveanreferenceobject, assometimeshumanmay\n just playing with the object itself, like throwing it, or spinning it around. You need to first\n identify whether there is a reference object. If so, you need to output the reference object’s\n namechosenfromthefollowingobjects: {alistoftask-relevantobjects}. \n Youroutputformatis: \n ‘‘‘json \n { \n \"reference_object_name\": \"REFERENCE_OBJECT_NAME\" or \"None\", \n } \n ‘‘‘ \n Ensuretheresponsecanbeparsedby Python‘json.loads’, e.g.: notrailingcommas, nosingle\n quotes,etc. \n \n \n 15 \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n \n A.3 Detailson Factorized Processfor Retargeting \n Body Motion Retarget. To retarget body motions from the SMPL-H representation to the hu-\n manoid, we extract the shoulder, elbow, and wrist poses from the SMPL-H models. We then use\n inversekinematicstosolvethebodyjointsonthehumanoid,ensuringtheyproducesimilarshoulder\n and elbow orientations and similar wrist poses. The inverse kinematics is implemented using an\n open-sourcedlibrary Pink[64]. The IKweightsweuseforshoulderorientation,elboworientation,\n wristorientation,andwristpositionare 0.04,0.04,0.08,and 1.0,respectively.\n Hand Pose Mapping. As we describe in the method section, we first retarget the hands from\n SMPL-Hmodelstothehumanoid’sdexteroushandsusingahybridimplementationofinversekine-\n maticsandanglemapping. Herearethedetailsofhowthismappingisperformed. Onceweobtain\n the SMPL-Hmodelsfromavideodemonstration,wecanobtainthelocationsof 3 Djointsfromthe\n handmeshmodelsfrom SMPL-H.Subsequently,wecancomputetherotatinganglesofeachjoint\n thatcorrespondtocertainhandposes. Thenweapplythecomputedjointanglestothehandmeshes\n of a canonical SMPL-H model, which is pre-defined to have the same size as the humanoid robot\n hardware. From this canonical SMPL-H model, we can get the 3 D keypoints of hand joints and\n useanexistingpackage,dex-retarget,anoff-the-shelfoptimizationpackagetodirectlycomputethe\n handjointanglesoftherobot[65]. \n Inverse Kinematics. Afterwarpingthearmtrajectory,weuseinversekinematicstocomputethe\n robot’s joint configurations. We assign weights of 1.0 to hand position and 0.08 to hand rotation,\n prioritizingaccuratehandplacementwhileallowingthearmstomaintainnaturalpostures.\n For retargeting human hand poses to the robot, we map the human hand joint angles to the corre-\n spondingjointsintherobot’shand. Thisenablestherobottoreplicatefine-grainedmanipulations\n demonstrated by the human, such as grasping and object interaction. Our implementation ensures\n that the retargeted motions are physically feasible for the robot and that overall execution appears\n naturalandeffectiveforthetaskathand. \n \n A.4 Additional Detailsof Plan Generation \n For temporal segmentation, we sample keypoints from the segmented objects in the first frame\n and track them across the video using Co Tracker [58]. We compute the average velocity of these\n keypointsateachframeandapplyanunsupervisedchangepointdetectionalgorithm[66]todetect\n significantchangesinmotion,identifyingkeyframesthatcorrespondtosubgoalstates.\n To determine contact between objects, we compute the relative spatial locations and distances be-\n tweenthepointcloudsofobjects. Ifthedistancebetweenobjectsfallsbelowapredefinedthreshold,\n we consider them to be in contact. For non-contact relations that are difficult to infer geometri-\n cally—suchasacupinapouringtask—weuse GPT 4 Vtopredictsemanticrelationsbasedonthe\n visualcontext. GPT 4 Vcaninferthatthecupistherecipientinapouringactionevenifthereisno\n directcontact. \n \n A.5 Trajectory Warping \n Here, we mathematically describe the process of trajectory warping. We denote the trajectory for\n robotasτrobotretargetedfromτSMPL inthegeneratedplan. Denotethestartingpointandendpoint\n ti:ti+1 \n ofτrobot asp ,p ,respectively. Notethatallpointsalongthetrajectoryarerepresentedin SE(3)\n start end \n space. \n Eachpointp ontheoriginalretargetdtrajectorycanbedescribedbythefollowingfunction:\n t \n p =p +(τrobot(t)−p ) (1) \n t start start \n wheret∈{t ,...,t },τrobot(t )=p ,τrobot(t )=p . \n i i+1 i start i+1 end \n When warping the trajectory, we either only needs to adapt the trajectory to the new target object\n location, or adapt the trajectory to the new locations of both the target and the reference objects,\n 16 "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n as described in Section 3.2. Without loss of generality, we denote the SE(3) transformation for\n the startingpoint is T , andthe SE(3)transformation forthe endpoint is T . Nowthe warped\n start end \n trajectorycanbedescribedbythefollowingfunction: \n p =T ·p +(τˆrobot(t)−T ·p ) (2) \n t start start start start \n whereτˆrobot(t)= τrobot(t)−pstart(T ·p −T ·p )+T ·p ,∀t∈{t ,...,t }.Inthisway,\n pend−pstart end end start start start start i i+1 \n wehaveτˆrobot(t )=T ·p ,τˆrobot(t )=T ·p . Notethatthistrajectorywarpingassumes\n i start start i+1 end end \n theendpointofatrajectoryisnotthesameasthestartingpoint,whichisacommonassumptionfor\n mostofthemanipulationbehaviors. \n B Additional Experimental Details \n B.1 Success Conditions \n Wedescribethesuccessconditionsweusetoevaluateifataskrolloutissuccessfulornot.\n • Sprinkle-salt: Thesaltbottlereachesapositionwherethesaltispouredoutintothe\n bowl. \n • Plush-toy-in-basket:Theplushtoyisputinsidethecontainer,withmorethan 50%\n ofthetoyinsidethecontainer. \n • Close-the-laptop: Thedisplayisloweredtowardsthebaseuntilthetwopartsmeet\n atthehinge(akathelaptopisclosed). \n • Close-the-drawer: Thedrawerispushedbacktothecontainingregion, eitherit’sa\n draweroralayerofacabinet. \n \n • Place-snacks-on-plate: The snack is placed on top of the plate, with more than\n 50%ofthesnackpackageontheplate. \n • Bagging: Thechipbagisputintotheshoppingbagwhichisinitiallyclosed.\n \n B.2 Implementationof Baseline \n We implement the baseline ORION [4] with minimal modifications to apply it to our humanoid\n setting.First,weestimatethepalmtrajectoryfrom SMPL-Htrajectoriesbyusingthecenterpointof\n thereconstructedfingersasthepalmpositionateachtimestep. Next,wewarpthepalmtrajectory\n basedonthetest-timeobjects’locations. Finally,weuseinversekinematicstosolvefortherobot’s\n bodyjoints,withthewarpedtrajectoryservingasthetargetpalmposition. \n \n B.3 Detailson Different Demonstrators \n \n Figure 6 shows the videos of three different human demonstrators performing\n Place-snacks-on-plate and Close-the-laptop tasks. We calculate the success\n ratesofimitatingdifferentvideos,andtheresultsareshownin Figure 4(b). \n B.4 Simulation Evaluation \n \n For easy reproducibility, we replicate two tasks, Sprinkle-salt and Close-the-drawer,\n insimulation(Figure 7). Weimplementthesetasksusingrobosuite[60],whichrecentlyprovided\n cross-embodimentsupport, includinghumanoidmanipulation. Weuse“GR 1 Fixed Lower Body”as\n therobotembodimentinthesetwotasks. \n Note that for the policy of each task, we use the same human video as the ones used in real robot\n experiments. Wecomparethreemethodsinsimulation: OKAMI(w/vision),OKAMI(w/ovision),\n and ORION.OKAMI(w/vision)thesamemethodweuseinourrealrobotexperiments. OKAMI\n (w/o vision) is the simplified version of OKAMI where we assume the model directly gets the\n \n 17 \n \n "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 6: Theinitialandendframesofvideosperformedbydifferenthumandemonstrators. Thefirstrowis\n Place-snacks-on-platetask,andthesecondrowis Close-the-laptoptask. \n \n ground-truth poses of objects. The evaluation results are shown in Table 1, where each reported\n numberisthesuccessrateaveragedover 50 rollouts. \n Wenoticethatthesimulationresultsaregenerallybetterthantherealrobotexperiments. Theper-\n formancedifferencecomesfromtheeasyphysicalinteractionbetweendexteroushandsandobjects\n comparedtotherealrobothardware. Also,OKAMIwithoutvisioncanachieveamuchhighersuc-\n cess rate than OKAMI with vision because the noise and uncertainty of perception are abstracted\n away. Specifically,alargeportionofuncertaintiescomefromthepartialobservationofobjectpoint\n clouds,andtheestimationoftheobjectlocationisofftheground-truthlocationsofobjects,whilethe\n successof OKAMIhighlydependsonthequalityoftrajectorywarping,whichisdependentonthe\n correctestimationofobjectlocations. Thissimulationresultalsoindicatesthattheperformanceof\n OKAMIisexpectedtoimproveifmorepowerfulvisionmodelswithhigheraccuracyareavailable.\n Method Sprinkle-salt Close-the-drawer \n OKAMI(w/vision) 82% 84% \n OKAMI(w/ovision) 100% 100% \n ORION 0% 10% \n Table 1: The average success rates (%) across different methods in two tasks, Sprinkle-salt and\n Close-the-drawer \n \n B.5 Visuomotor Policy Details \n \n Wechoose ACT[61]inourexperimentsforbehavioralcloning,analgorithmthathasbeenshown\n effective in learning humanoid manipulation policies [67]. Notably, we choose pretrained Di-\n no V 2 [68, 69] as the visual backbone of a policy. The policy takes a single RGB image and 26-\n dimensionjointpositionsasinputandoutputstheactionofthe 26-dimensionabsolutejointposition\n fortherobottoreach. In Table 2,weshowthehyperparametersusedforbehavioralcloning.\n \n 18 \n \n "
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n KLweight 10 \n chunksize 60 \n hiddendimension 512 \n batch size 45 \n feedforwarddimension 3200 \n epochs 25000 \n learning rate 5 e-5 \n temporalweighting 0.01 \n Table 2:Thehyperparametersusedin ACT. \n \n \n \n \n \n \n \n \n \n \n \n Close-the-drawer Sprinkle-salt \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 7:Thescreenshotsofthestartingandendingframesofthetwosimulationtasks,Close-the-drawer\n and Sprinkle-salt. \n \n \n \n \n \n \n 19 \n \n "
  }
]