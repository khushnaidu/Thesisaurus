[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n Ego Mimic: Scaling Imitation Learning via Egocentric Video \n \n Simar Kareer 1, Dhruv Patel 1∗, Ryan Punamiya 1∗, Pranay Mathur 1∗, Shuo Cheng 1\n Chen Wang 2, Judy Hoffman 1†, Danfei Xu 1† \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 1: Ego Mimic unlocks human embodiment data—egocentric videos paired with 3 D hand tracks—as a new scalable data source for\n imitation learning. We can capture this data anywhere, without a robot, by wearing a pair of Project Aria glasses while performing\n manipulation tasks with our own hands. Ego Mimic bridges kinematic, distributional, and appearance differences between human\n embodiment data (left) and traditional robot teleoperation data (right) to learn a unified policy. We find that human embodiment data\n boosts task performance by 34-228% over using robot data alone, and enables generalization to new objects or even scenes.\n Abstract—The scale and diversity of demonstration data To scale up data for robotics, there have been recent ad-\n required for imitation learning is a significant challenge. We vancesindatacollectionsystems.Forexample,ALOHA[1],\n present Ego Mimic,afull-stackframeworkwhichscalesmanip- \n [2] and GELLO [3] are intuitive leader-follower controls\n ulationviahumanembodimentdata,specificallyegocentrichu- \n for collecting teleoperated data. Other works have opted\n man videos paired with 3 D hand tracking. Ego Mimic achieves \n this through: (1) a system to capture human embodiment to develop hand-held grippers to collect data without a\n data using the ergonomic Project Aria glasses, (2) a low-cost robot [4]. Despite these advances, data collected via these\n bimanual manipulator that minimizes the kinematic gap to systemsstillrequirespecializedhardwareandactiveeffortin\n human data, (3) cross-domain data alignment techniques, and \n providingdemonstrations.Wehypothesizethatakeystepfor\n (4) an imitation learning architecture that co-trains on human \n achieving Internet-scalerobotdataispassivedatacollection.\n and robot data. Compared to prior works that only extract \n high-level intent from human videos, our approach treats Just as the Internet was not built for curating data to train\n human and robot data equally as embodied demonstration largevisionandlanguagemodels,anidealrobotdatasystem\n data and learns a unified policy from both data sources. should allow users to generate sensorimotor behavior data\n Ego Mimic achieves significant improvement on a diverse set \n without intending to do so. \n of long-horizon, single-arm and bimanual manipulation tasks \n Human videos, especially those captured from an egocen-\n over state-of-the-art imitation learning methods and enables \n generalization to entirely new scenes. Finally, we show a tric perspective, present an ideal source of data for passive\n favorable scaling trend for Ego Mimic, where adding 1 hour of data scalability. This data aligns closely with robot data,\n additionalhanddataissignificantlymorevaluablethan 1 hour as it provides an egocentric camera for vision, 3 D hand\n ofadditionalrobotdata.Videosandadditionalinformationcan \n tracking for actions, and onboard SLAM for localization.\n be found at https://egomimic.github.io/ \n The advent of consumer-grade devices capable of capturing\n I. INTRODUCTION such data, including Extended Reality (XR) devices and\n End-to-end imitation learning has shown remarkable per- camera-equipped “smart glasses”, opens up unprecedented\n formance in learning complex manipulation tasks, but it re- opportunities for passive data collection at scale. While\n mains brittle when facing new scenarios and tasks. Drawing recent works have begun to leverage human video data,\n on the recent success of Computer Vision and Natural Lan- their approaches are limited to extracting high-level intent\n guage Processing,wehypothesizethatforlearnedpoliciesto information from videos to build planners that guide low-\n achieve broad generalization, we must dramatically scale up level conditional policies [5], [6]. As a result, these systems\n the training data size. While these adjacent domains benefit remainconstrainedbytheperformanceoflow-levelpolicies,\n from Internet-sourceddata,roboticslackssuchanequivalent. which are typically trained solely on teleoperation data.\n Wearguethattotrulyscalerobotperformancewithhuman\n SK, DP, RP, PM, SC, JH, DX are with the Georgia Institute of data, we should not consider human videos as an auxiliary\n Technology and CW is with Stanford University. Email Correspondence: \n skareer@gatech.edu data source that requires separate handling. Instead, we\n *Denotesequalcontribution.†Denotesequaladvising. should exploit the inherent similarities between egocentric\n 4202 \n tc O \n 13 \n ]OR.sc[ \n 1 v 12242.0142:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n human data and robot data to treat them as equal parts in for instance RT 1 required 17 months of data collection and\n a continuous spectrum of embodied data sources. Learning 13 robots[13].Ourworkproposesalearningframeworkthat\n seamlessly from both data sources will require full-stack takes advantage of scalable human embodiment demonstra-\n innovation, from data collection systems that unify data tions, which has the potential to be larger and more diverse\n frombothsourcestoimitationlearningarchitecturesthatcan than any dataset consisting of robot demonstrations alone.\n enable such cross-embodied policy learning. \n To this end, our work treats human data as a first-class Learning from Video Demonstrations: To satisfy the data\n data source for robot manipulation. We believe our system requirements of pixel to action IL algorithms, many recent\n is a key step towards using passive data from wearable works leverage human data because it is highly scalable.\n smart glasses to train manipulation policies. We present Human data is used at different levels of abstraction, where\n Ego Mimic (Fig. 1), a framework to collect data and co-train some works use human videos from internet-scale datasets\n manipulation policies from both human egocentric videos to pretrain visual representations [15], [16], [17]. Other\n and teleoperated robot data consisting of: worksusehumanvideostomoreexplicitlyunderstandscene\n (i) A system to collect human data built on Project dynamics through point track prediction, intermediate state\n Aria glasses [7] that capture egocentric video, 3 D hand hallucination in pixel space, or affordance prediction [18],\n tracking, and device SLAM. This rich information allows us [19], [6], [20], [21]. And finally, recent works use hand tra-\n totransformhumanegocentricdataintoaformatcompatible jectorypredictionasaproxyforpredictingrobotactions[5].\n with robot imitation learning. While these approaches leverage hand data, they often have\n (ii) A capable yet low-cost bimanual robot that minimizes separate modules to process hand and robot data. Instead,\n the kinematic and camera-to-camera gap to human embodi- by fully leveraging the rich information provided by Aria\n ment data. In particular, we minimize the camera-to-camera glasses including on-board SLAM, our method is able to\n device gap (FOV, dynamic ranges, etc) between human and unify and treat human and robot data as equals and co-train\n robot data by using Project Aria glasses as the main robot from both data sources with a single end-to-end policy.\n sensor. \n (iii) To mitigate differences in data distributions, we nor- Data Collection Systems: Various methods have been\n malize and align action distributions between human and used to scale robot data. Low-cost devices such as the\n robots. Further, we minimize the appearance gap between Space Mouse offer sensitive and fine-grained teleoperation\n human arm and robot manipulator via visual masking. of robotic manipulators [22], [10], [23], [11], [24]. Further\n (iv)Aunifiedimitationlearningarchitecturethatco-trains works improve intuitive control through virtual reality sys-\n on hand and robot data with a common vision encoder and tems such as the VR headset [25], [26], [27], [28], [29]. Re-\n policy network. Despite distinct action spaces for human centsystemslike ALOHAand GELLOincreaseergonomics\n and robot, our model enforces a shared representation to for low-cost and fine-grained bimanual manipulation tasks\n enable performance scaling with human embodiment data, through a leader-follower teleoperation interface [1], [3]\n outperforming existing methods that treat human and robot or exoskeletons [30], [31]. Other works attempt to collect\n data separately. humanembodimentdatawithrichinformationlike 3 Daction\n We empirically evaluate Ego Mimic on three challenging tracking, but existing systems face tradeoffs. Those which\n long-horizon manipulation tasks in the real world: contin- leverage rich information are either not portable (e.g., static\n uous object-in-bowl, clothes folding, and grocery packing camera [32], [5], [33], [34]) or ergonomic (e.g., require a\n (Fig.5).Ourresultsdemonstratethat Ego Mimicsignificantly hand-heldgripper[4],[35]orbody-worncamera[36],[37]),\n enhancestaskperformanceacrossallscenarios,withrelative which prevent the passive scalability of the data collection\n improvements of up to 200%. Notably, we observe that system. Along these lines, our approach captures egocentric\n Ego Mimic exhibits generalization to objects and scenes videoand 3 Dhandtrackingdata,butviatheergonomicform\n encountered exclusively in human data. Finally, we analyze factorof Project Aria Glasses[7].Whileotherwearabledata\n thescalingpropertiesof Ego Mimic,andfoundlearningfrom collection methods like VR headsets capture hand positions\n an additional hour of hand data significantly outperforms toteleoperatearobot,oursystemdoesnotrequirearobotat\n training from an additional hour of robot data. all. This system has the potential to passively scale [38], as\n adoption of similar consumer-grade devices continue to rise.\n II. RELATEDWORKS \n Imitation Learning: Imitation Learning (IL) has been used Cross-embodiment Policy Learning: Advances in cross-\n to perform diverse and contact-rich manipulation tasks [8], embodiment learning show that large models trained on\n [9], [10]. Recent advancements in IL have led to the de- datasets with diverse robot embodiments are more general-\n velopment of pixel-to-action IL models, which directly map izable [39]. Some approaches aim to bridge the embodiment\n raw visual inputs to low-level robot control [1], [11]. These gapthroughobservationreprojection[40],actionabstractions\n visual IL models have demonstrated impressive reactive [41], and policies conditioned on embodiment, [42]. Recent\n policies [12], [5]. Scaling these models has displayed strong works view cross-embodiment learning as a domain adapta-\n generalization in works such as RT 1 and RT 2 [13], [14]. tion problem [43]. Our work argues that human data should\n However,thesemethodsremainlaborandresource-intensive, be treated as another embodiment in transfer learning."
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 2: Our human data system uses Aria glasses to capture Egocentric RGB and uses its side SLAM cameras to localize the device\n and track hands. The robot consists of two Viper X follower arms with Intel Real Sense D 405 wrist cameras, controlled by two Widow X\n leader arms. Our robot uses identical Aria glasses as the main vision sensor to help minimize the camera to camera gap.\n III. EGOMIMIC movements.Priorworksoftenrelyontable-mountedmanip-\n Ego Mimic is a full-stack framework that captures and ulators such as the Franka Emika Panda [46]. While these\n learns from both egocentric human embodiment data and systems are capable, they differ significantly from human\n robot data. We detail each component of our pipeline below, arms in terms of kinematics. Moreover, their substantial\n starting with our hardware setup for human and robot data weight andinertia necessitate slow, cautious movements due\n collection (Sec. III-A), followed by our methods for pro- to safety concerns, largely preventing them from perform-\n cessing and aligning the data from both sources (Sec. III- ing manipulation tasks at speeds comparable to humans.\n B), and finally our unified policy architecture (Sec. III-C). In response to these limitations, we have purpose-built a\n Our design choices throughout are motivated by making bimanual manipulator that is lightweight, agile, and cost-\n human embodiment data as suitable for robot learning as effective. Drawing inspiration from the ALOHA system [1],\n tele-operated robot data is. our robot setup comprises two 6-Do F Viper X 300 S arms\n with Intel Realsense D 405 wrist cameras, mounted in an\n A. Data Collection Systems and Hardware Design \n inverted configuration on a height-adjustable rig as the torso\n Aria glasses for egocentric demonstration collection. An (Fig 2), kinematically mimicking the upper body of a hu-\n ideal system for human data needs to capture rich infor- man.The Viper Xarmsareleanandrelativelysimilarinsize\n mation about the scene, while remaining passively scalable. to human arms, contributing to their enhanced agility. The\n Such a system should be wearable, ergonomic, capture a entire rig can be assembled for less than $1,000 excluding\n wide FOV, track hand positions, device pose, and more. the Viper Xarms(the BOMwillbemadeavailable).Wealso\n Ego Mimic fills this gap by building on top of the Project built a leader robot rig to collect teleoperation data, similar\n Aria glasses [7]. Aria glasses are head-worn devices for to ALOHA [1]. \n capturing multimodal egocentric data. The device assumes Further, as our method jointly learns visual policies from\n an ergonomic glasses form factor that weighs only 75 g, egocentric human and robot data, it is essential to align\n permitting long wearing time and passive data collection. the visual observation space. Thus in addition to alignment\n Our work leverages the front-facing wide-Fo V RGB camera through data post-processing (Sec. III-B), we directly match\n forvisualobservationandtwomono-colorscenecamerasfor the camera hardware by using a second pair of Aria glasses\n device pose and hand tracking (See Fig. 2 for sample data). as the main sensor for the robot, which we have mounted\n Inparticular,theside-facingscenecamerastrackhandposes directly to the top of the torso at a location similar to that\n even when they move out of the main RGB camera’s view, of human eyes (Fig. 2). This enables us to mitigate the\n significantly mitigating the challenges posed by humans’ observation domain gap associated with the camera devices,\n natural tendency to move their head and gaze ahead of their including FOVs, exposure levels, and dynamic ranges.\n hands during sequential manipulation tasks. \n B. Data Processing and Domain Alignment \n Further, there are large scale data collection efforts un- \n derway with Project Aria [44], [45], and the devices are To train unified policies from both human and robot data,\n made available broadly to the academic community through Ego Mimicbridgesthreekeyhuman-robotgaps:(1)unifying\n an active research partnership program. In the future, our action coordinate frames, (2) aligning action distributions,\n systemcanenableuserstoseamlesslymergedatatheycollect and (3) mitigating visual appearance gaps.\n with these large datasets. Ultimately, we present a system Raw data streams. We stream raw sensor data from the\n that enables passive yet feature-rich human data collection hardwaresetupasdescribedin Sec.III-A.Ariaglassesworn\n to help scale up robot manipulation. by the human and robot generate ego-centric RGB image\n Low-costbimanualmanipulator.Toeffectivelyutilizeego- streams. In addition, the robot generates two wrist camera\n centric human embodiment data, a robot manipulator should streams. For proprioception, we leverage the Aria Machine\n be capable of moving in ways that resemble human arm Perception Service (MPS) [47] to estimate 3 D poses of both"
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n Source Type Data \n Human DH Image Egocentricview \n Proprio 3 Dhandposes(Hp) \n Action Normalizedhandtracks(Hap) \n Robot DR Image Egocentric+wristviews \n Proprio EEFposes(Rp),Jointpositions(Rq) \n Action EEFactions(Rap),Jointactions(Raq) \n TABLE I: Comparison of human and robot data streams \n \n hands Hp ∈ SE(3) × SE(3). Robot proprioception data \n includes both its end effector poses Rp ∈ SE(3)×SE(3) \n and joint positions Rq ∈ R 2×7 (including the gripper jaw \n joint position). We in addition collect joint-space actions \n Raq ∈R 2×7 for teleoperated robot data. Fig.3:a)Actionnormalization:Theposedistributionsarediffer-\n Unifying human-robot data coordinate frames. Robot ent between hand and robot data, specifically in the y (left-right)\n dimension. We apply Gaussian normalization individually to the\n action and proprioception data typically use fixed reference \n hand and robot pose data before feeding them to the model. b)\n frames(e.g.,cameraorrobotbaseframe).However,egocen- \n Visual masking:Tohelpbridgetheappearancegapofhumanand\n tric hand data from moving cameras breaks this assumption. and the robot arm, we apply a black mask to the hand and robot\n To unify the reference frames for joint policy learning, we via SAM, then overlay a red line onto the image.\n transform both human hand and robot end effector trajecto- \n ries into camera-centered stable reference frames. Following \n the idea of predicting action chunks [11], [1], we aim to \n construct action chunks ap for both human hand and \n t:t+h \n robot end effector. To simplify the notation, we describe \n the single-arm case that generalizes to both arms. The raw \n trajectory is a sequence of 3 D poses [p Ft,p Ft+1,...p Ft+h], \n t t+1 t+h \n where F denotes the coordinate frame of the camera when \n i \n estimating p . F remains fixed for the robot but changes \n i i \n constantlyforhumanegocentricdata.Ourgoalistoconstruct \n ap bytransformingeachpositioninthetrajectoryintothe \n t:t+h \n observation camera frame F . This allows the policy to pre- Fig.4:Architectureofthejointhuman-robotpolicylearningframe-\n t \n work.Themodelprocessesnormalizedhandandrobotdatathrough\n dict actions without considering future camera movements. \n shared vision and ACT encoders, outputting pose predictions for\n For human data, we use the MPS visual-inertial SLAM to \n both human and robot data, and joint actions for robot data. The\n obtainthe Ariaglassespose T F W i ∈SE(3)intheworldframe frameworkusesmaskedimagestomitigatehuman-robotappearance\n and transform the action trajectory: gaps and incorporates wrist camera views for the robot.\n Hap =[(TW)−1 TWp Fi for i∈[t,t+1,...,t+h]] \n i Ft Fi i \n A sample trajectory is visualized in Fig. 2 (top-left). Robot empirically effective (Sec. IV-B), though we plan to explore\n data is transformed similarly using the fixed camera frame alternatives such as action quantization [13] in the future.\n estimated by hand-eye calibration. By creating a unified Bridging visual appearance gaps. Despite aligning sen-\n reference frame, we enable the policy to learn from action sor hardware for capturing robot and human data, there still\n supervisions regardless of whether they originate from hu- exists a large visual appearance gap between human hands\n man videos or teleoperated demonstrations. and robots. Previous works have acknowledged this gap\n Aligninghuman-robotposedistributions.Despitealign- and attempt to occlude or remove the manipulator in visual\n ing hand and robot data via hardware design and data observation[50],[51].Wefollowsimilarideasandmaskout\n processing, we still observe differences in the distributions both the hand and the robot via SAM [52] and overlay a red\n of hand and robot end effector poses in the demonstra- line to indicate end-effector directions (Fig. 3). The SAM\n tions collected. These discrepancies arise from biomechani- point prompts are generated by the robot end effector and\n cal differences, task execution variations, and measurement human hand poses transformed to image frames.\n precision disparities between human and robotic systems. \n C. Training Human-Robot Joint Policies \n Withoutmitigatingthisgap,thepolicytendstolearnseparate \n representationsforthetwodatasources[48],[49],preventing Existing approaches often opt for hierarchical architec-\n performance scaling with human data. To address this, we tures, where a high-level policy trained on human data\n apply Gaussian normalization individually to end effector conditionsalow-levelpolicyoutputtingrobotactions[5],[6].\n (hand)posesandactionsfromeachdatasource,asshownin However, this approach is inherently limited by the perfor-\n Fig. 3. Echoing [49], we found this simple technique to be manceofthelow-levelpolicy,whichdoesnotdirectlybenefit"
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 5: We evaluate Ego Mimic across three real world, long-horizon manipulation tasks. See Sec. IV-A for description.\n Algorithm 1 Joint Human-Robot Policy Learning TABLE II: Data collection overview for both Human(H) and\n Robot(R) data. We report both the number(#) of total task demon-\n Require: Human dataset D H , Robot dataset D R strations and the time(min) took to collect them.\n 1: Initialize shared transformer encoder f enc (·), pose de- \n coder fp(f (·)), and joint decoder fq(f (·)) \n enc enc Task H H H R R R \n 2: for iteration n=1,2,... do # min #/min # min #/min \n 3: // Human data Object-in-Bowl 1400 60 23 270 120 2 \n 4: Sample (I t ,p t ,ap t:t+h ) from D H Groceries 160 80 2 300 300 1 \n 5: Predict aˆp t:t+h from f p (f enc (I t ,p t )) Laundry 590 100 6 430 300 1\n 6: LH =MSE(aˆp ,ap ) \n p t:t+h t:t+h \n 7: // Robot data transform the visual and proprioceptive embeddings before\n 8: Sample (I t ,p t ,q t ,ap t:t+h ,aq t:t+h ) from D R passing to the policy transformer. The policy transformer\n 9: Predict aˆq t:t+h from f q (f enc (I t ,p t ,q t )) processesthesefeatures,andthetwooutputheadstransform\n 10: Predict aˆp t:t+h from f p (f enc (I t ,p t ,q t )) the transformer’s latent output into either pose or joint\n 11: LR =MSE(aˆq ,aq ) space predictions. The pose loss supervises both human and\n q t:t+h t:t+h \n 12: LR =MSE(aˆp ,ap ) robot data via Hap and Rap, whereas the joint action loss\n p t:t+h t:t+h \n 13: // Joint policy update only supervises robot data Raq. Since the two branches are\n 14: Update f enc ,fp,fq with LH p +LR p +LR q separated by only one linear layer, we effectively force the\n 15: end for model to learn joint representations for both domains. The\n algorithm is summarized in Alg. 1. Table I summarizes the\n data used for training. \n from large-scale human data. To address this limitation, we \n IV. EXPERIMENTS \n proposeasimplearchitecture(illustratedin Fig.4)thatlearns \n from unified data and promotes shared representation. Our Weaimtovalidatethreekeyhypotheses.H 1:Ego Mimicis\n model builds upon ACT [1], but the design is general and abletoleveragehumanembodimentdatatoboostin-domain\n can be applied to other transformer based imitation learning performance for complex manipulation tasks. H 2: Human\n algorithms. data helps Ego Mimic generalize to new objects and scenes.\n A critical challenge in this unified approach is the choice H 3: Given sufficient initial robot data, it is more valuable to\n of the robot action space. While the robot end-effector collect additional human data than additional robot data.\n poses are more semantically similar to human hand pose \n A. Experiment Setup \n than robot-joint positions, it is difficult to control our robot \n withend-effectorposesviaacartesian-basedcontroller(e.g., Tasks. We select a set of long-horizon real world tasks to\n differential IK) because the 6 Do F Viper X arms offer low evaluate our claims. Our tasks require precise alignment,\n solutionredundancy.Empirically,wefoundthatrobotsoften complex motions, and bimanual coordination (Fig. 5).\n encounter singularities or non-smooth solutions in a trajec- Continuous Object-in-Bowl:Therobotpicksasmallplush\n tory. Consequently, we opt for joint-space control (i.e., use toy (about 6 cm long), places it in a bowl, picks up the bowl\n predicted joint action aˆq to control the robot), while to dump the object onto the table, and repeats continuously\n t:t+h \n leveraging pose-space prediction to learn joint human-robot for 40 seconds. We randomly choose from a set of 3\n representation. Note that the need both pose- and joint- bowls and 5 toys which randomly positioned on the table\n spacepredictionsisspecifictoourrobothardware,andmore within a 45 cm x 60 cm range. The task stress-tests precise\n capable robots that better support end-effector space control manipulation, spatial generalization, and robustness in long-\n can eliminate the need for predicting joint-space actions. horizonexecution.Weaward Ptseachtimethetoyisplaced\n Specifically,allparametersinthepolicyaresharedbesides in a bowl, or the bowl is emptied. We perform 45 total\n the two shallow input and output heads. The input heads evaluation rollouts across 9 bowl-toy-position combinations."
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n TABLE III: Quantitative results for 3 real-world tasks. We report TABLEIV:Ablations-Weablateourmethodandreportfinaltask\n task success rates (%) and performance scores (pts) for all tasks performance on the Object-in-Bowl task.\n and bag grabbing rate for the Groceries tasks. \n Method Cotrained(Points) \n Method Bowl Laundry Groceries Ego Mimic 128 \n Pts Pts SR Pts SR Open Bag Ego Mimicw/o Line 112 \n Ego Mimicw/o Lineand Mask 95 \n ACT[1] 39 82 55% 82 22% 54% Ego Mimicw/o Action Norm 79 \n Mimicplay[5] 71 78 50% 53 8% 40% Ego Mimicw/o Hand Data 68 \n Ego Mimic(w/ohuman) 68 104 73% 92 28% 60% \n Ego Mimic 128 114 88% 110 30% 70% \n Laundry: A bimanual task that requires the robot to fold \n a t-shirt placed with random pose in a 90 cm × 60 cm range \n and a rotation range of ±30 deg. The robot must use both \n armstofoldtherightsidesleeve,theleftsidesleeve,thenthe \n whole shirt in half. We award Pts for each of these stages, \n and calculate Success Rate (SR) based as the percentage of \n runs where all stages were successful. We perform 40 total \n evaluation rollouts across 8 shirt-position combinations. \n Groceries: The robot fills a grocery bag with 3 packs of \n chips. It uses its left arm to grab the top side of the bag \n handle to create an opening, then uses the right arm to pick \n Fig.6:Wehighlight Ego Mimic’ssuccess,aswellasfailuremodes,\n thechippacksandplacesthemintothebag.Thetaskrequires forinstance(e)failuretocorrectlyalignwiththetoy,(f)failureto\n high-precision manipulation (picking up a deformable bag graspthebag’shandle,or(g)policyonlygrabs 1 sideoftheshirt.\n handle) and robustness in long-horizon rollout. We award Ego Mimicreducesthefrequencyofthesefailuremodes,improving\n success rates by 8-33% over the baselines.\n Pts for picking the handle and for each pack placed in the \n grocery bag. We report SR as the percentage of runs where \n all three packs were successfully placed in the bag, and 8-33% over ACT. Our largest improvement is on the Cont.\n Open Bag as the percentage of runs where the handle of Object-in-Bowltask,inwhichweyielda 228%improvement\n the bag was grasped, which is a difficult stage of this task. intaskscoreover ACT.Weobservethebaselinesoftenmiss\n We perform 50 evaluations across 10 bag positions. the toy or bowl by a few inches, which seems to indicate\n We detail the amount of data collected for each task in that our use of hand data helps the policy precisely reach\n Table II. While collecting robot data in particular, we make the toy. We show qualitative results in Fig. 6.\n sure to randomly perturb the robot’s position, which we To ensure this increase was due to leveraging hand data\n foundempiricallytoimproverobustness.Forhumandata,we rather than architectural changes, we compare to Ego Mimic\n note that while tasks like Continuous Object-in-Bowl were (0% human). We observe a 10-88% improvement in score\n particularly easy to scale, tasks like Groceries were slower and 2-15% improvement in success rate.\n because of resetting time. Ego Mimic enables generalization to new objects and\n Baselines. To evaluate that Ego Mimic can improve in- even scenes. We evaluate our method on two domain shifts:\n domain success rate by leveraging human data, we bench- attempting to fold shirts of an unseen color, and performing\n mark against ACT [1], a state of the art imitation learning the Cont. Object-in-Bowl task in an entirely different scene.\n algorithm. Further, we compare against Mimicplay [5], a re- Asshownin Fig.7,weobservethat ACTstrugglesonshirts\n cent state of the art method that learns planners from human of unseen colors (25% SR) whereas Ego Mimic fully retains\n data to guide low-level policies, to show that our unified its performance (85% SR). Surprisingly, by learning from\n architecture learns more effectively from human and robot humandatainanewscene(unseenbackgroundandlighting),\n data. For fair comparisons, we implement Mimicplay with Ego Mimicisabletogeneralizetothisnewenvironmentwith-\n the same Transformer backbone as our method, and we re- out any additional robot data, scoring 63 points. In contrast,\n moved goal conditioning because Ego Mimic is designed for Mimicplay, which had access to the same information but\n single-task policies. Since Ego Mimic contains architectural instead leverages a hierarchical framework for using hand\n changes to ACT, namely the simultaneous joint and pose dataonlyscored 4 points.Thissuggeststhatourarchitecture\n actionprediction,wealsobenchmarkagainst Ego Mimic(0% promotes joint hand-robot representation, whereas hierarchi-\n Human). This helps us conclude that improvements come cal architectures pose a generalization bottleneck.\n from leveraging human data rather than the architecture. Scaling human vs. robot data. To investigate the scaling\n effect of human and robot data sources on performance, we\n B. Results \n conducted additional data collection for the Cont. Object-in-\n Ego Mimic improves in-domain task performance.Across bowl task. As illustrated in Fig. 7, Ego Mimic trained on 2\n all tasks we observed a relative improvement in score of 34- hours of robot data and 1 hour of human data significantly\n 228%,andanimprovementinabsolutetasksuccessratefrom outperforms ACTtrainedon 3 hoursofrobotdata(128 vs 74"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n V. CONCLUSIONS \n We presented Ego Mimic, a framework to co-train ma-\n nipulation policies from human egocentric videos and tele-\n operated robot data. By leveraging Project Aria glasses, a\n low-costbimanualrobotsetup,cross-domainalignmenttech-\n niques,andaunifiedpolicylearningarchitecture,Ego Mimic\n improves over state-of-the-art baselines on three challenging\n real-world tasks and shows generalization to new scenes\n as well as favorable scaling properties. For future work,\n we plan to explore the possibility of generalizing to new\n robotembodimentsandentirelynewbehaviorsdemonstrated\n only in human data, such as folding pants instead of shirts.\n Overall, we believe our work opens up exciting new venues\n Fig. 7: Evaluation Results on Policy Generalization. (a) We of research on scaling robot data via passive data collection.\n evaluate the policy on the laundry task using unseen cloth colors \n andreportthesuccessrateforeachmethod.(b)Wetestthepolicy \n on the Object-in-Bowl task in unseen scenes. REFERENCES \n [1] T.Z.Zhao,V.Kumar,S.Levine,and C.Finn,“Learningfine-grained\n bimanual manipulation with low-cost hardware,” 2023. [Online].\n Available:https://arxiv.org/abs/2304.13705\n [2] A. . Team, J. Aldaco, T. Armstrong, R. Baruch, J. Bingham,\n S. Chan, K. Draper, D. Dwibedi, C. Finn, P. Florence, S. Goodrich,\n W. Gramlich, T. Hage, A. Herzog, J. Hoech, T. Nguyen, I. Storz,\n B. Tabanpour, L. Takayama, J. Tompson, A. Wahid, T. Wahrburg,\n S. Xu, S. Yaroshenko, K. Zakka, and T. Z. Zhao, “Aloha 2:\n An enhanced low-cost hardware for bimanual teleoperation,” 2024.\n [Online].Available:https://arxiv.org/abs/2405.02292\n [3] P.Wu,Y.Shentu,Z.Yi,X.Lin,and P.Abbeel,“Gello:Ageneral,low-\n cost, and intuitive teleoperation framework for robot manipulators,”\n 2024.[Online].Available:https://arxiv.org/abs/2309.13037\n [4] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng,\n R. Tedrake, and S. Song, “Universal manipulation interface: In-\n the-wild robot teaching without in-the-wild robots,” 2024. [Online].\n Available:https://arxiv.org/abs/2402.10329\n [5] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu,\n Y. Zhu, and A. Anandkumar, “Mimicplay: Long-horizon imitation\n learning by watching human play,” 2023. [Online]. Available:\n Fig. 8: Scaling robot vs. human data. Ego Mimic trained on 2 https://arxiv.org/abs/2302.12422\n hours robot data + 1 hour hand data (Blue) strongly outperforms [6] H. Bharadhwaj, A. Gupta, V. Kumar, and S. Tulsiani, “Towards\n ACT [1] trained on 3 hours of robot data (Orange). generalizablezero-shotmanipulationviatranslatinghumaninteraction\n plans,”2023.[Online].Available:https://arxiv.org/abs/2312.00775\n [7] J. Engel, K. Somasundaram, M. Goesele, A. Sun, A. Gamino,\n A. Turner, A. Talattof, A. Yuan, B. Souti, B. Meredith, C. Peng,\n points).Notably,onehourofhumandatayields 1400 demon- C.Sweeney,C.Wilson,D.Barnes,D.De Tone,D.Caruso,D.Valleroy,\n strations,comparedtoonly 135 demonstrationsfromanhour D.Ginjupalli,D.Frost,E.Miller,E.Mueggler,E.Oleinik,F.Zhang,\n G. Somasundaram, G. Solaira, H. Lanaras, H. Howard-Jenkins,\n of robot data. These results demonstrate Ego Mimic’s ability \n H.Tang,H.J.Kim,J.Rivera,J.Luo,J.Dong,J.Straub,K.Bailey,\n to effectively leverage the efficiency of human embodiment K. Eckenhoff, L. Ma, L. Pesqueira, M. Schwesinger, M. Monge,\n data collection, leading to a more pronounced scaling effect N.Yang,N.Charron,N.Raina,O.Parkhi,P.Borschowa,P.Moulon,\n P. Gupta, R. Mur-Artal, R. Pennington, S. Kulkarni, S. Miglani,\n that substantially boosts task performance beyond what is \n S. Gondi, S. Solanki, S. Diener, S. Cheng, S. Green, S. Saarinen,\n achievable with robot data alone. We note that Ego Mimic at S. Patra, T. Mourikis, T. Whelan, T. Singh, V. Balntas, V. Baiyya,\n 2 hours of robot data outperforms ACT at 2 hours of robot W. Dreewes, X. Pan, Y. Lou, Y. Zhao, Y. Mansour, Y. Zou, Z. Lv,\n Z.Wang,M.Yan,C.Ren,R.D.Nardi,and R.Newcombe,“Project\n data, so some improvement is attributed to architecture. \n aria: A new tool for egocentric multi-modal ai research,” 2023.\n [Online].Available:https://arxiv.org/abs/2308.13561\n Ablationstudies.Weablateourapproachtodemonstratethe \n [8] A.Paraschos,C.Daniel,J.R.Peters,and G.Neumann,“Probabilistic\n importance of each design decision on the Object-in-Bowl movementprimitives,”in Advancesin Neural Information Processing\n task (Table IV). First, removing action normalization results Systems, C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\n K.Weinberger,Eds.,vol.26. Curran Associates,Inc.,2013.\n in a 38% drop in task score. This highlights the importance \n [9] C.Finn,T.Yu,T.Zhang,P.Abbeel,and S.Levine,“One-shotvisual\n of action distribution alignment for co-training. Next, we imitation learning via meta-learning,” 2017. [Online]. Available:\n ablate away the visual techniques, specifically masking out https://arxiv.org/abs/1709.04905\n [10] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni,\n the hand and robot, as well as drawing the red overlay on \n L.Fei-Fei,S.Savarese,Y.Zhu,and R.Mart´ın-Mart´ın,“Whatmatters\n the image. Removing these components resulted in 13 and in learning from offline human demonstrations for robot manipula-\n 26% drops respectively. Finally, Ego Mimic trained without tion,”inar Xivpreprintar Xiv:2108.03298,2021.\n [11] C.Chi,Z.Xu,S.Feng,E.Cousineau,Y.Du,B.Burchfiel,R.Tedrake,\n any hand data, yields a large 47% drop, which highlights \n and S.Song,“Diffusionpolicy:Visuomotorpolicylearningviaaction\n how effective hand-robot co-training is on our stack. diffusion,”2024.[Online].Available:https://arxiv.org/abs/2303.04137"
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n [12] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and [29] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. Kitani,\n L. Pinto, “Visual imitation made easy,” 2020. [Online]. Available: C. Liu, and G. Shi, “Omnih 2 o: Universal and dexterous human-\n https://arxiv.org/abs/2008.04899 to-humanoid whole-body teleoperation and learning,” ar Xiv preprint\n [13] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, ar Xiv:2406.08858,2024.\n K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, [30] H.Fang,H.-S.Fang,Y.Wang,J.Ren,J.Chen,R.Zhang,W.Wang,\n B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, and C. Lu, “Airexo: Low-cost exoskeletons for learning whole-arm\n D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, manipulationinthewild,”ar Xivpreprintar Xiv:2309.14975,2023.\n U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, [31] S. Yang, M. Liu, Y. Qin, R. Ding, J. Li, X. Cheng, R. Yang, S. Yi,\n J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, and X.Wang,“Ace:Across-platformvisual-exoskeletonssystemfor\n G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, low-cost dexterous teleoperation,” ar Xiv preprint ar Xiv:2408.11805,\n C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, 2024. \n T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich, “Rt-1: Robotics [32] A.Sivakumar,K.Shaw,and D.Pathak,“Robotictelekinesis:Learning\n transformerforreal-worldcontrolatscale,”2023.[Online].Available: a robotic hand imitator by watching humans on youtube,” ar Xiv\n https://arxiv.org/abs/2212.06817 preprintar Xiv:2202.10448,2022. \n [14] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, [33] V.Jain,M.Attarian,N.J.Joshi,A.Wahid,D.Driess,Q.Vuong,P.R.\n K.Choromanski,T.Ding,D.Driess,A.Dubey,C.Finn,P.Florence, Sanketi,P.Sermanet,S.Welker,C.Chan,etal.,“Vid 2 robot:End-to-\n C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, endvideo-conditionedpolicylearningwithcross-attentiontransform-\n A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, ers,”ar Xivpreprintar Xiv:2403.12943,2024.\n D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, [34] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, “Humanplus:\n Y.Lu,H.Michalewski,I.Mordatch,K.Pertsch,K.Rao,K.Reymann, Humanoidshadowingandimitationfromhumans,”in Conferenceon\n M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, Robot Learning(Co RL),2024.\n R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, [35] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chin-\n P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, tala, and L. Pinto, “On bringing robots home,” ar Xiv preprint\n and B. Zitkovich, “Rt-2: Vision-language-action models transfer ar Xiv:2311.16098,2023.\n web knowledge to robotic control,” 2023. [Online]. Available: [36] C. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K.\n https://arxiv.org/abs/2307.15818 Liu, “Dexcap: Scalable and portable mocap data collection\n [15] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, “R 3 m: system for dexterous manipulation,” 2024. [Online]. Available:\n A universal visual representation for robot manipulation,” 2022. https://arxiv.org/abs/2403.07788\n [Online].Available:https://arxiv.org/abs/2203.12601 [37] G. Papagiannis, N. Di Palo, P. Vitiello, and E. Johns, “R+ x: Re-\n [16] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Dar- trieval and execution from everyday human videos,” ar Xiv preprint\n rell, “Real-world robot learning with masked visual pre-training,” in ar Xiv:2407.12957,2024.\n Conferenceon Robot Learning. PMLR,2023,pp.416–426. [38] K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik,\n [17] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and T.Afouras,K.Ashutosh,V.Baiyya,S.Bansal,B.Boote,etal.,“Ego-\n A. Zhang, “Vip: Towards universal visual reward and representa- exo 4 d: Understanding skilled human activity from first-and third-\n tionviavalue-implicitpre-training,”ar Xivpreprintar Xiv:2210.00030, personperspectives,”in Proceedingsofthe IEEE/CVFConferenceon\n 2022. Computer Visionand Pattern Recognition,2024,pp.19383–19400.\n [18] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha, and [39] E. Collaboration, A. O’Neill, A. Rehman, A. Gupta, A. Maddukuri,\n A. Garg, “Learning by watching: Physical imitation of manipulation A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar,\n skills from human videos,” 2021. [Online]. Available: https: A. Jain, A. Tung, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky,\n //arxiv.org/abs/2101.07241 A. Rai, A. Gupta, A. Wang, A. Kolobov, A. Singh, A. Garg,\n [19] C. Wen, X. Lin, J. So, K. Chen, Q. Dou, Y. Gao, and P. Abbeel, A. Kembhavi, A. Xie, A. Brohan, A. Raffin, A. Sharma, A. Yavary,\n “Any-point trajectory modeling for policy learning,” 2024. [Online]. A. Jain, A. Balakrishna, A. Wahid, B. Burgess-Limerick, B. Kim,\n Available:https://arxiv.org/abs/2401.00025 B. Scho¨lkopf, B. Wulfe, B. Ichter, C. Lu, C. Xu, C. Le, C. Finn,\n [20] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, “Track 2 act: C.Wang,C.Xu,C.Chi,C.Huang,C.Chan,C.Agia,C.Pan,C.Fu,\n Predicting point tracks from internet videos enables generalizable C.Devin,D.Xu,D.Morton,D.Driess,D.Chen,D.Pathak,D.Shah,\n robot manipulation,” 2024. [Online]. Available: https://arxiv.org/abs/ D. Bu¨chler, D. Jayaraman, D. Kalashnikov, D. Sadigh, E. Johns,\n 2405.01527 E. Foster, F. Liu, F. Ceola, F. Xia, F. Zhao, F. V. Frujeri, F. Stulp,\n [21] S.Bahl,R.Mendonca,L.Chen,U.Jain,and D.Pathak,“Affordances G.Zhou,G.S.Sukhatme,G.Salhotra,G.Yan,G.Feng,G.Schiavi,\n from human videos as a versatile representation for robotics,” 2023. G.Berseth,G.Kahn,G.Yang,G.Wang,H.Su,H.-S.Fang,H.Shi,\n [Online].Available:https://arxiv.org/abs/2304.08488 H. Bao, H. B. Amor, H. I. Christensen, H. Furuta, H. Bharadhwaj,\n H. Walke, H. Fang, H. Ha, I. Mordatch, I. Radosavovic, I. Leal,\n [22] A. Mandlekar, D. Xu, R. Mart´ın-Mart´ın, S. Savarese, and L. Fei- \n J. Liang, J. Abou-Chakra, J. Kim, J. Drake, J. Peters, J. Schneider,\n Fei, “Learning to generalize across long-horizon tasks from human \n J. Hsu, J. Vakil, J. Bohg, J. Bingham, J. Wu, J. Gao, J. Hu, J. Wu,\n demonstrations,”ar Xivpreprintar Xiv:2003.06085,2020. \n J. Wu, J. Sun, J. Luo, J. Gu, J. Tan, J. Oh, J. Wu, J. Lu, J. Yang,\n [23] V.Dhat,N.Walker,and M.Cakmak,“Using 3 dmicetocontrolrobot \n J. Malik, J. Silve´rio, J. Hejna, J. Booher, J. Tompson, J. Yang,\n manipulators,” Proceedings of the 2024 ACM/IEEE International \n J. Salvador, J. J. Lim, J. Han, K. Wang, K. Rao, K. Pertsch,\n Conference on Human-Robot Interaction, 2024. [Online]. Available: \n K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne,\n https://api.semanticscholar.org/Corpus ID:267322988 \n K.Oslund,K.Kawaharazuka,K.Black,K.Lin,K.Zhang,K.Ehsani,\n [24] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: Imitation learning \n K. Lekkala, K. Ellis, K. Rana, K. Srinivasan, K. Fang, K. P. Singh,\n for vision-based manipulation with object proposal priors,” 2023. \n K.-H.Zeng,K.Hatch,K.Hsu,L.Itti,L.Y.Chen,L.Pinto,L.Fei-Fei,\n [Online].Available:https://arxiv.org/abs/2210.11339 \n L. Tan, L. J. Fan, L. Ott, L. Lee, L. Weihs, M. Chen, M. Lepert,\n [25] S. P. Arunachalam, I. Gu¨zey, S. Chintala, and L. Pinto, “Holo-dex: M. Memmel, M. Tomizuka, M. Itkina, M. G. Castro, M. Spero,\n Teaching dexterity with immersive mixed reality,” in 2023 IEEE M. Du, M. Ahn, M. C. Yip, M. Zhang, M. Ding, M. Heo, M. K.\n International Conferenceon Roboticsand Automation(ICRA). IEEE, Srirama,M.Sharma,M.J.Kim,N.Kanazawa,N.Hansen,N.Heess,\n 2023,pp.5962–5969. N.J.Joshi,N.Suenderhauf,N.Liu,N.D.Palo,N.M.M.Shafiullah,\n [26] A. George, A. Bartsch, and A. B. Farimani, “Openvr: Teleoperation O. Mees, O. Kroemer, O. Bastani, P. R. Sanketi, P. T. Miller,\n for manipulation,” 2023. [Online]. Available: https://arxiv.org/abs/ P. Yin, P. Wohlhart, P. Xu, P. D. Fagan, P. Mitrano, P. Sermanet,\n 2305.09765 P. Abbeel, P. Sundaresan, Q. Chen, Q. Vuong, R. Rafailov, R. Tian,\n [27] I.A.Tsokalo,D.Kuss,I.Kharabet,F.H.P.Fitzek,and M.Reisslein, R. Doshi, R. Mart’in-Mart’in, R. Baijal, R. Scalise, R. Hendrix,\n “Remoterobotcontrolwithhuman-in-the-loopoverlongdistancesus- R. Lin, R. Qian, R. Zhang, R. Mendonca, R. Shah, R. Hoque,\n ingdigitaltwins,”in 2019 IEEEGlobal Communications Conference R. Julian, S. Bustamante, S. Kirmani, S. Levine, S. Lin, S. Moore,\n (GLOBECOM),2019,pp.1–6. S.Bahl,S.Dass,S.Sonawani,S.Tulsiani,S.Song,S.Xu,S.Haldar,\n [28] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, “Open-television: S. Karamcheti, S. Adebola, S. Guist, S. Nasiriany, S. Schaal,\n teleoperation with immersive active visual feedback,” ar Xiv preprint S.Welker,S.Tian,S.Ramamoorthy,S.Dasari,S.Belkhale,S.Park,\n ar Xiv:2407.01512,2024. S.Nair,S.Mirchandani,T.Osa,T.Gupta,T.Harada,T.Matsushima,"
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n T. Xiao, T. Kollar, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, \n T.Armstrong,T.Darrell,T.Chung,V.Jain,V.Kumar,V.Vanhoucke, \n W. Zhan, W. Zhou, W. Burgard, X. Chen, X. Chen, X. Wang, \n X. Zhu, X. Geng, X. Liu, X. Liangwei, X. Li, Y. Pang, Y. Lu, \n Y. J. Ma, Y. Kim, Y. Chebotar, Y. Zhou, Y. Zhu, Y. Wu, Y. Xu, \n Y. Wang, Y. Bisk, Y. Dou, Y. Cho, Y. Lee, Y. Cui, Y. Cao, Y.-H. \n Wu, Y. Tang, Y. Zhu, Y. Zhang, Y. Jiang, Y. Li, Y. Li, Y. Iwasawa, \n Y. Matsuo, Z. Ma, Z. Xu, Z. J. Cui, Z. Zhang, Z. Fu, and Z. Lin, \n “Open x-embodiment: Robotic learning datasets and rt-x models,” \n 2024.[Online].Available:https://arxiv.org/abs/2310.08864 \n [40] L. Y. Chen, K. Hari, K. Dharmarajan, C. Xu, Q. Vuong, \n and K. Goldberg, “Mirage: Cross-embodiment zero-shot policy \n transfer with cross-painting,” 2024. [Online]. Available: https: \n //arxiv.org/abs/2402.19249 \n [41] W. Huang, I. Mordatch, and D. Pathak, “One policy to control \n them all: Shared modular policies for agent-agnostic control,” 2020. \n [Online].Available:https://arxiv.org/abs/2007.04976 \n [42] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, \n D. Sadigh, and S. Levine, “Pushing the limits of cross-embodiment \n learningformanipulationandnavigation,”2024.[Online].Available: \n https://arxiv.org/abs/2402.19432 \n [43] J. Yang, D. Sadigh, and C. Finn, “Polybot: Training one policy \n acrossrobotswhileembracingvariability,”2023.[Online].Available: \n https://arxiv.org/abs/2307.03719 \n [44] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar, \n J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, \n I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, \n M.Xu,E.Z.Xu,C.Zhao,S.Bansal,D.Batra,V.Cartillier,S.Crane, \n T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, \n Q.Fu,A.Gebreselasie,C.Gonzalez,J.Hillis,X.Huang,Y.Huang, \n W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, \n Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, \n T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, \n K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, \n Y.Wang,X.Wu,T.Yagi,Z.Zhao,Y.Zhu,P.Arbelaez,D.Crandall, \n D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu, \n C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, \n H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, \n L. Torresani, M. Yan, and J. Malik, “Ego 4 d: Around the world \n in 3,000 hours of egocentric video,” 2022. [Online]. Available: \n https://arxiv.org/abs/2110.07058 \n [45] L.Ma,Y.Ye,F.Hong,V.Guzov,Y.Jiang,R.Postyeni,L.Pesqueira, \n A. Gamino, V. Baiyya, H. J. Kim, K. Bailey, D. S. Fosas, C. K. \n Liu, Z. Liu, J. Engel, R. D. Nardi, and R. Newcombe, “Nymeria: \n A massive collection of multimodal egocentric daily motion in the \n wild,”2024.[Online].Available:https://arxiv.org/abs/2406.09905 \n [46] S.Haddadin,S.Parusel,L.Johannsmeier,S.Golz,S.Gabl,F.Walch, \n M. Sabaghian, C. Ja¨hne, L. Hausperger, and S. Haddadin, “The \n franka emika robot: A reference platform for robotics research and \n education,”IEEERoboticsand Automation Magazine,vol.29,no.2, \n pp.46–64,2022. \n [47] Meta Research,“Basics—projectariadocs,”https://facebookresearch. \n github.io/projectariatools/docs/dataformats/mps/mpssummary, \n 2024,accessed:September 15,2024. \n [48] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, \n D. Sadigh, and S. Levine, “Pushing the limits of cross- \n embodimentlearningformanipulationandnavigation,”ar Xivpreprint \n ar Xiv:2402.19432,2024. \n [49] J. Hejna, C. Bhateja, Y. Jian, K. Pertsch, and D. Sadigh, “Re-mix: \n Optimizing data mixtures for large scale imitation learning,” ar Xiv \n preprintar Xiv:2408.14037,2024. \n [50] Y. Zhou, Y. Aytar, and K. Bousmalis, “Manipulator-independent \n representations for visual imitation,” 2021. [Online]. Available: \n https://arxiv.org/abs/2103.09016 \n [51] S. Bahl, A. Gupta, and D. Pathak, “Human-to-robot imitation in the \n wild,”2022.[Online].Available:https://arxiv.org/abs/2207.09450 \n [52] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, \n R.Ra¨dle,C.Rolland,L.Gustafson,E.Mintun,J.Pan,K.V.Alwala, \n N. Carion, C.-Y. Wu, R. Girshick, P. Dolla´r, and C. Feichtenhofer, \n “Sam 2: Segment anything in images and videos,” 2024. [Online]. \n Available:https://arxiv.org/abs/2408.00714 \n [53] L.Wang,X.Chen,J.Zhao,and K.He,“Scalingproprioceptive-visual \n learning with heterogeneous pre-trained transformers,” in Neurips, \n 2024. "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 9: Here we visualize hand and robot data from out dataset side by side with ground truth actions overlayed (purple). Note that the\n actions are of similar length, despite the hand traveling much faster than the robot.\n \n VI. APPENDIX which are used to prompt SAM 2 [52] to generate a mask of\n the robot arm. After obtaining the mask, we draw a red line\n A. Data Processing and Domain Alignment \n onthemaskedareafromthegrippertotheelbowinthe RGB\n Humans and teleoperated robots complete tasks at differ- \n image. For the human data, a similar process is followed,\n entspeeds.Toenablejointtrainingofhumanandrobotdata, \n where SAM 2 is prompted using the 3 D coordinates of the\n we must align these two sources of data temporally. Follow- \n human hand to generate a mask. A red line is then drawn\n ing Mimicplay [5], we “slow down” the human data, and \n along the hand’s contour, from the bottom right to top left\n we found empirically that a factor of 4 sufficiently aligned \n corner of the contour’s bounding box. \n both domains. Specifically, for robot data we construct joint \n During training, both the robot arm and human hand\n and pose based actions over a four second horizon but for \n are masked to align their visual representations. During\n humandataweusea 1 secondhorizon.Forbothdomains,our \n evaluation,SAM 2 isruninrealtimeonadesktoptomaskthe\n action chunk size is 100, meaning we construct 100 future \n robotarmandapplythesameredlineoverlay.Thisapproach\n actions spaced evenly over the horizon. This alignment is \n enablesbettervisualalignmentbetweentherobotandhuman\n independentofdatarecordingfrequencies,wherehumandata \n hand, facilitating more effective model generalization across\n is recorded at 30 hz and robot data is recorded at 50 hz. \n human and robotic tasks. \n To co-train on both human and robot data, we indi- \n vidually normalize the proprioception and actions for both B. Aria Machine Perception Services (MPS)\n embodiments (as shown in Fig. 3). Given proprioception We leveraged MPS to process human data from the Aria\n p t ∈ Rd where d depends on embodiment, we normalize glasses.Therawdatafrom Ariacontainstimestampedsensor\n by subtracting the dataset mean and dividing by standard information from the glasses, namely RGB camera, SLAM\n deviation cameras,IMU,eyetrackingcameras,microphone,andmore.\n norm(p t )=(p t −µ p )/σ p . The raw data is uploaded to the MPS server, where the\n cloud-hosted service estimates device pose via SLAM, a\n We perform the identical calculation to normalize actions \n semi-dense pointcloud of the environment, hand tracking\n a ∈Rd×100. \n t:t+h relative to the device frame, and even eye gaze. The MPS\n To bridge the appearance gap between human hand \n returns SLAM as a timestamped CSV of device poses in\n and robot arm, we visually mask each embodiment via \n world frame and hand tracking as a timestamped CSV of\n SAM 2[52],andoverlayaredlineonthesemaskstoenhance \n cartesian positions in the time-aligned device frame. These\n alignment (Fig. 3). For the robot, we first use forward \n hand positions are each in a distinct reference frame due to\n kinematics to compute the 3 D coordinates of key joints in \n head movements, so we project future actions to the current\n robot frame \n device coordinate frame (described in Sec. III-B). We use\n p R =FK(q )∈R 3×3, \n t t the undistorted Aria RGB camera data paired with the hand\n including the wrist, gripper and the forearm. These 3 D tracking and SLAM information to construct an hdf 5 file\n coordinates are then projected onto the image frame via compatible for training in robomimic [10].\n camera intrinsics (Ipixels) and extrinsics (Tcam) to obtain \n cam R C. Training Human-Robot Joint Policies \n 2 D keypoints in pixel space \n We depict our algorithm in detail in Fig. 10. At each\n ppixel =Ipixels Tcamp R ∈R 3×2, step we sample a batch of hand data as well as a batch of\n t cam R t "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 10: Detailed Architecture of Ego Mimic. \n TABLE V: Training details - Ego Mimic\n robot data, and pass each through our unified architecture. \n Ego Mimicperforms Z-scorenormalizationtohandandrobot \n proprioceptionandactionsindividually.Thenormalizedpro- Policy ACT \n Batch Size 128 \n prioception is passed through a linear layer to produce a \n Optimizer adamw \n proprioception token. Alongside the proprioception, the top learning rate(initial) 5 e-5\n down views fromhand androbot arepassed througha SAM Decayfactor 1 \n Scheduler Linear \n based masking module. These images, along with the robot \n Encoderlayers 4 \n wrist views are passed through a shared Resnet 18 visual Decoderlayers 7 \n encoder which produces visual tokens. Finally, we add an Hiddendim 512 \n Feedforwarddim 3200 \n additionalstyletokenzfromour CVAEencoderwhichisnot \n No.ofheads 8 \n depicted, but directly follows ACT [1]. All these tokens, are Data Augmentations Color Jitter\n passed through a transformer encoder decoder architecture. \n Thetransformerdecoder’shiddenoutputispassedthrougha \n lineardecoderdependingontheoutputtype,producingpose grasping action is supervised only via the robot joint predic-\n actions aˆp or joint based actions aˆj. tion loss L (Raˆj,Raj), where the gripper is represented as\n 1 \n For batches of robot data, we calculate another joint. \n L robot =L 1 (Raˆp,Rap)+L 1 (Raˆj,Raj)+KL D. Training Details \n and for hand data we have Welistthehyperparametersfor Ego Mimicin Table V.All\n models were trained for 120000 iterations with global batch\n L =L (Haˆp,Hap)+KL \n hand 1 \n size of 128 across 4 A 40 gpus, which takes about 24 hours.\n where KL is the CVAE latent regularizer as in ACT [1]. Our code is implemented in the robomimic framework [10].\n This yields L=L robot +L hand which we optimize at each More details in Table V\n step. \n E. Mimicplay Implementation \n We leverage the transformer’s flexible input sequence to \n account for differences in the number of visual observations For our implementation of Mimic Play [5], we closely\n based on the modality; specifically we have wrist images follow the original setup, training the high-level planner and\n in robot data but not hand data. When the wrist images are low-level control policy separately.\n present,weconcatenateadditionaltokenstoourtransformers First, we train a Res Net-18 based high-level encoder\n input sequence as in ACT [1]. In our experiments, we found using a Gaussian Mixture Model (GMM) to generate 3 D\n thatthisstrategywassufficienttoeffectivelyco-trainonboth trajectories,asdescribedintheoriginalwork.Thehigh-level\n hand and robot data, although we plan to experiment with encoder is trained on both human and robot data to predict\n more sophisticated cross-embodiment learning techniques 3 D trajectories. \n like HPTs [53]. Once the high-level encoder is trained, we extract the\n We note that the human data lacks information for the latent representation from the Res Net-18 encoder (i.e., the\n graspingaction,since Ariaonlyrecordshandpose.Thus,the high-levelplanner)anduseitasthestylevariablez,whichis"
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Fig. 11: Qualitative successes of Ego Mimic on each of our three tasks.\n TABLE VI: Training details - Mimicplay TABLE VII: Data recording and rollout rates for Human and\n Robot data. We “slow down” human data by 0.25 to account for\n differences in task execution speeds. \n High-level Resnet 18 \n learning rate(initial) 0.0001 \n Decayfactor 0.1 Type Human(Hz) Robot(Hz) \n batch size 50 \n GMMmodes 5 Recording 30 50 \n Low-level ACT \n Rollout(Inference) - 1 \n learning rate(initial) 5 e-5 \n Rollout(Control) - 25 \n Optimizer adamw \n Decayfactor 1 \n Scheduler Linear \n Aria camera which streams at 30 fps. \n passedtothetransformerencoder-decoder Fig.10.Thelow- \n level ACT policy is then trained solely on robot data with \n this additional input from the high level policy as guidance. \n F. Policy Rollout \n We rollout our policy with inference at 1 hz and control at \n 25 hz on a desktop with a an NVIDIA RTX 4090 GPU. The \n predicted action horizon is 4 seconds, with the first second \n of predicted actions executed in receding-horizon style. All \n the robot’s sensors update at 50 hz with the exception of the "
  }
]