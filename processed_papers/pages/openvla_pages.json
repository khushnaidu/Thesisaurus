[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n Open VLA: \n \n An Open-Source Vision-Language-Action Model \n \n Moo Jin Kim∗,1 Karl Pertsch∗,1,2 Siddharth Karamcheti∗,1,3 \n Ted Xiao 4 Ashwin Balakrishna 3 Suraj Nair 3 Rafael Rafailov 1 Ethan Foster 1 Grace Lam\n Pannag Sanketi 4 Quan Vuong 5,† Thomas Kollar 3 Benjamin Burchfiel 3 Russ Tedrake 3,6 Dorsa Sadigh 1\n Sergey Levine 2 Percy Liang 1 Chelsea Finn 1 \n https://openvla.github.io \n \n Large-Scale Robot Open VLA Closed-Loop \n Training Data Robot Control Policy \n Vision-Language-Action Model \n User: Wipe the table. \n Fine-tune VLM w/ Robot Actions: \n Open VLA: \n 970 k Robot [Δx, Δθ, ΔGrip] = … \n Llama 2 7 B \n Episodes \n Vi T \n Base VLM \n Multi-Robot Control & Efficient Fine-Tuning Fully \n Open-Source \n Data \n Weights \n Code \n \n Figure 1: Wepresent Open VLA,a 7 B-parameteropen-sourcevision-language-actionmodel(VLA),trained\n on 970 krobotepisodesfromthe Open X-Embodimentdataset[1]. Open VLAsetsanewstateoftheartfor\n generalistrobotmanipulationpolicies.Itsupportscontrollingmultiplerobotsoutoftheboxandcanbequickly\n adaptedtonewrobotdomainsviaparameter-efficientfine-tuning. The Open VLAcheckpointsand Py Torch\n trainingpipelinearefullyopen-sourceandmodelscanbedownloadedandfine-tunedfrom Hugging Face.\n Abstract: Largepoliciespretrainedonacombinationof Internet-scalevision-\n languagedataanddiverserobotdemonstrationshavethepotentialtochangehow\n weteachrobotsnewskills: ratherthantrainingnewbehaviorsfromscratch,wecan\n fine-tunesuchvision-language-action(VLA)modelstoobtainrobust,generalizable\n policiesforvisuomotorcontrol. Yet,widespreadadoptionof VLAsforrobotics\n hasbeenchallengingas 1)existing VLAsarelargelyclosedandinaccessibletothe\n public,and 2)priorworkfailstoexploremethodsforefficientlyfine-tuning VLAs\n fornewtasks,akeycomponentforadoption.Addressingthesechallenges,weintro-\n duce Open VLA,a 7 B-parameteropen-source VLAtrainedonadiversecollection\n of 970 kreal-worldrobotdemonstrations. Open VLAbuildsona Llama 2 language\n modelcombinedwithavisualencoderthatfusespretrainedfeaturesfrom DINOv 2\n and Sig LIP.Asaproductoftheaddeddatadiversityandnewmodelcomponents,\n Open VLAdemonstratesstrongresultsforgeneralistmanipulation,outperforming\n closedmodelssuchas RT-2-X(55 B)by 16.5%inabsolutetasksuccessrateacross\n 29 tasksandmultiplerobotembodiments,with 7 xfewerparameters. Wefurther\n showthatwecaneffectivelyfine-tune Open VLAfornewsettings,withespecially\n ∗:denotesequalcontribution \n Correspondenceto:moojink@stanford.edu, pertsch@berkeley.edu, skaramcheti@stanford.edu\n 1 Stanford University,2 UCBerkeley,3 Toyota Research Institute,4 Google Deepmind,5 Physical Intelligence,\n 6 MIT,†Workdoneinpartwhileat Google Deepmind \n 4202 \n pe S \n 5 \n ]OR.sc[ \n 3 v 64290.6042:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n stronggeneralizationresultsinmulti-taskenvironmentsinvolvingmultipleobjects\n andstronglanguagegroundingabilities,andoutperformexpressivefrom-scratch\n imitationlearningmethodssuchas Diffusion Policyby 20.4%Wealsoexplore\n compute efficiency; as a separate contribution, we show that Open VLA can be\n fine-tunedonconsumer GPUsviamodernlow-rankadaptationmethodsandserved\n efficientlyviaquantizationwithoutahittodownstreamsuccessrate. Finally,we\n releasemodelcheckpoints,fine-tuningnotebooks,andour Py Torchcodebasewith\n built-insupportfortraining VLAsatscaleon Open X-Embodimentdatasets.\n 1 Introduction \n Akeyweaknessoflearnedpoliciesforroboticmanipulationistheirinabilitytogeneralizebeyond\n theirtrainingdata: whileexistingpoliciestrainedforindividualskillsorlanguageinstructionshave\n thecapacitytoextrapolatebehaviorstonewinitialconditionssuchasobjectpositionsorlighting\n [2,3],theylackrobustnesstoscenedistractorsornovelobjects[4,5]andstruggletoexecuteunseen\n taskinstructions[6,7]. Yetbeyondrobotics,existingfoundationmodelsforvisionandlanguage\n suchas CLIP[8],Sig LIP[9],and Llama 2[10]arecapableofthesetypesofgeneralizationandmore,\n stemmingfromthepriorscapturedbytheir Internet-scalepretrainingdatasets. Whilereproducing\n thisscaleofpretrainingforroboticsisstillanopenchallenge—eventhelargestrobotmanipulation\n datasets[1,11]onlyhave 100 Kto 1 Mexamples–thisimbalancesuggestsanopportunity: using\n existing foundation models for vision and language as a core building block for training robotic\n policiesthatcangeneralizetoobjects,scenes,andtasksbeyondtheirtrainingdata.\n Towardsthisgoal,existingworkhasexploredintegratingpretrainedlanguageandvision-language\n modelsforroboticrepresentationlearning[12–14]andasacomponentinmodularsystemsfortask\n planningandexecution[15,16]. Morerecently,theyhavebeenusedfordirectlylearningvision-\n language-action models [VLAs; 1, 7, 17, 18] for control. VLAs provide a direct instantiation of\n usingpretrainedvision-and-languagefoundationmodelsforrobotics,directlyfine-tuningvisually-\n conditionedlanguagemodels(VLMs)suchas Pa LI[19,20]togeneraterobotcontrolactions. By\n building off of strong foundation models trained on Internet-scale data, VLAs such as RT-2 [7]\n demonstrateimpressiverobustnessresults,aswellasanabilitytogeneralizetonovelobjectsand\n tasks,settinganewstandardforgeneralistrobotpolicies. Yet,therearetwokeyreasonspreventing\n the widespread use of existing VLAs: 1) current models [1, 7, 17, 18] are closed, with limited\n visibilityintomodelarchitecture,trainingprocedures,anddatamixture,and 2)existingworksdo\n notprovidebestpracticesfordeployingandadapting VLAstonewrobots,environments,andtasks\n — especially on commodity hardware (e.g., consumer-grade GPUs). We argue that to develop a\n richfoundationforfutureresearchanddevelopment,roboticsneedsopen-source,generalist VLAs\n thatsupporteffectivefine-tuningandadaptation,akintotheexistingecosystemaroundopen-source\n languagemodels[21–24]. \n To this end, we introduce Open VLA, a 7 B-parameter open-source VLA that establishes a new\n state of the art for generalist robot manipulation policies.1 Open VLA consists of a pretrained\n visually-conditionedlanguagemodelbackbonethatcapturesvisualfeaturesatmultiplegranularities,\n fine-tuned on a large, diverse dataset of 970 k robot manipulation trajectories from the Open-X\n Embodiment[1]dataset—adatasetthatspansawiderangeofrobotembodiments,tasks,andscenes.\n Asaproductofincreaseddatadiversityandnewmodelcomponents,Open VLAoutperformsthe\n 55 B-parameter RT-2-X model [1, 7], the prior state-of-the-art VLA, by 16.5% absolute success\n rateacross 29 evaluationtasksonthe Widow Xand Google Robotembodiments. Weadditionally\n investigateefficientfine-tuningstrategiesfor VLAs,anewcontributionnotexploredinpriorwork,\n across 7 diversemanipulationtasksspanningbehaviorsfromobjectpick-and-placetocleaninga\n table. Wefindthatfine-tuned Open VLApoliciesclearlyoutperformfine-tunedpretrainedpolicies\n suchas Octo[5]. Comparedtofrom-scratchimitationlearningwithdiffusionpolicies[3],fine-tuned\n Open VLAshowssubstantialimprovementontasksinvolvinggroundinglanguagetobehaviorin\n 1 Open VLAusesmultiplepretrainedmodelcomponents:Sig LIP[9]and Dino V 2[25]visionencodersanda\n Llama 2[10]languagemodelbackbone.Forallthreemodels,weightsareopen,butnottheirtrainingdataor\n code.Wereleasetrainingdata,codeandmodelweightsforreproducing Open VLAontopofthesecomponents.\n 2 \n "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n multi-tasksettingswithmultipleobjects. Followingtheseresults,wearethefirsttodemonstratethe\n effectivenessofcompute-efficientfine-tuningmethodsleveraginglow-rankadaptation[Lo RA;26]\n andmodelquantization[27]tofacilitateadapting Open VLAmodelsonconsumer-grade GPUsinstead\n oflargeservernodeswithoutcompromisingperformance. Asafinalcontribution,weopen-source\n allmodels,deploymentandfine-tuningnotebooks,andthe Open VLAcodebasefortraining VLAs\n atscale, withthehopethattheseresourcesenablefutureworkexploringandadapting VLAsfor\n robotics. \n 2 Related Work \n Visually-Conditioned Language Models Visually-conditionedlanguagemodels(VLMs),which\n aretrainedon Internet-scaledatatogeneratenaturallanguagefrominputimage(s)andlanguage\n prompts,havebeenadoptedformyriadapplicationsfromvisualquestionanswering[28–31]toobject\n localization[32,33]. Oneofthekeyadvancesfuelingrecent VLMsaremodelarchitecturesthat\n bridgefeaturesfrompretrainedvisionencoders[8,9,25]withpretrainedlanguagemodels[10,23,34–\n 36],directlybuildingonadvancesinbothcomputervisionandnaturallanguagemodellingtocreate\n powerfulmultimodalmodels. Whileearlyworkexploredvariousarchitecturesforcross-attending\n betweenvisionandlanguagefeatures[37–41],newopen-source VLMs[20,42–44]haveconverged\n onasimpler“patch-as-token”approach,inwhichpatchfeaturesfrompretrainedvisualtransformers\n aretreatedastokens,andarethenprojectedintotheinputspaceofalanguagemodel. Thissimplicity\n makesiteasytorepurposeexistingtoolsfortraininglanguagemodelsatscalefor VLMtraining. We\n employthesetoolsinourworktoscale VLAtraining,andspecificallyuse VLMsfrom Karamcheti\n etal.[44]asourpretrainedbackbone,astheyaretrainedfrommulti-resolutionvisualfeatures,fusing\n low-levelspatialinformationfrom DINOv 2[25]withhigher-levelsemanticsfrom Sig LIP[9]toaid\n invisualgeneralization. \n Generalist Robot Policies Arecenttrendinroboticsworkstowardstrainingmulti-task“generalist”\n robotpolicies[2,6,45–49]onlargediverserobotdatasets[1,2,6,11,45,49–56],spanningmany\n differentrobotembodiments[1,5,53,57–66]. Notably,Octo[5]trainsageneralistpolicythatcan\n controlmultiplerobotsout-of-the-boxandallowsforflexiblefine-tuningtonewrobotsetups. A\n keydifferencebetweentheseapproachesand Open VLAisthemodelarchitecture. Priorworkslike\n Octotypicallycomposepretrainedcomponentssuchaslanguageembeddingsorvisualencoderswith\n additionalmodelcomponentsinitializedfromscratch[2,5,6],learningto“stitch”themtogether\n during the course of policy training. Unlike these works, Open VLA adopts a more end-to-end\n approach, directly fine-tuning VLMs to generate robot actions by treating them as tokens in the\n languagemodelvocabulary. Ourexperimentalevaluationshowsthatthissimpleyetscalablepipeline\n substantiallyboostsperformanceandgeneralizationabilityoverpriorgeneralistpolicies.\n Vision-Language-Action Models Anumberofworkshaveexploredtheuseof VLMsforrobotics,\n e.g.,forvisualstaterepresentations[12,13],objectdetection[67],high-levelplanning[16],andfor\n providingafeedbacksignal[68–71]. Othersintegrate VLMsdirectlyintoend-to-endvisuomotor\n manipulation policies [14, 15], but incorporate significant structure into the policy architecture\n or require calibrated cameras, which limits their applicability. A number of recent works have\n exploredsimilarrecipestooursanddirectlyfine-tunedlargepretrained VLMsforpredictingrobot\n actions[1,7,17,18,72–74]. Suchmodelsareoftenreferredtoasvision-language-actionmodels\n (VLAs), since they fuse robot control actions directly into VLM backbones. This has three key\n benefits: (1)itperformsalignmentofpretrainedvisionandlanguagecomponentsonalarge,Internet-\n scalevision-languagedataset,(2)theuseofagenericarchitecture,notcustom-madeforrobotcontrol,\n allowsustoleveragethescalableinfrastructureunderlyingmodern VLMtraining[75–77]andscale\n totrainingbillion-parameterpolicieswithminimalcodemodifications,and(3)itprovidesadirect\n pathwayforroboticstobenefitfromtherapidimprovementsin VLMs. Existingworkson VLAs\n eitherfocusontrainingandevaluatinginsinglerobotorsimulatedsetups[72–74,78]andthuslack\n generality,orareclosedanddonotsupportefficientfine-tuningtonewrobotsetups[1,7,17,18].\n Mostcloselyrelated,RT-2-X[1]trainsa 55 B-parameter VLApolicyonthe Open X-Embodiment\n datasetanddemonstratesstate-of-the-artgeneralistmanipulationpolicyperformance. However,our\n work differs from RT-2-X in multiple important aspects: (1) by combining a strong open VLM\n 3 \n "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n Open VLA \n Action De-Tokenizer \n Δx \n 3 Δθ \n Llama 2 7 B ΔGrip \n 7 D Robot \n Input Image Action \n 2 \n MLP Projector Llama Tokenizer \n “Put eggplant \n Dino V 2 Sig LIP \n in bowl” \n 1 \n Language Instruction \n “What should the robot do to {task}? A:” \n Figure 2:Open VLAmodelarchitecture.Givenanimageobservationandalanguageinstruction,themodel\n predicts 7-dimensionalrobotcontrolactions.Thearchitectureconsistsofthreekeycomponents:(1)avision\n encoderthatconcatenates Dino V 2[25]and Sig LIP[79]features,(2)aprojectorthatmapsvisualfeaturesto\n thelanguageembeddingspace,and(3)the LLMbackbone,a Llama 27 B-parameterlargelanguagemodel[10].\n backbonewitharicherrobotpretrainingdataset,Open VLAoutperforms RT-2-Xinourexperiments\n whilebeinganorderofmagnitudesmaller;(2)wethoroughlyinvestigatefine-tuningof Open VLA\n modelstonewtargetsetups,while RT-2-Xdoesnotinvestigatethefine-tuningsetting;(3)weare\n thefirsttodemonstratetheeffectivenessofmodernparameter-efficientfine-tuningandquantization\n approachesfor VLAs;and(4)Open VLAisthefirstgeneralist VLAthatisopen-sourceandthus\n supportsfutureresearchon VLAtraining,datamixtures,objectives,andinference.\n 3 The Open VLAModel \n Weintroducethe Open VLAmodel,a 7 B-parametervision-language-actionmodel(VLA)trained\n on 970 krobotdemonstrationsfromthe Open X-Embodimentdataset[1]. Therearemany,largely\n unexplored, questions around best practices for developing VLA models, e.g., what are the best\n modelbackbones,datasets,andhyperparameterstousefortraining. Below,wedetailourapproach\n fordeveloping Open VLAandsummarizeour keylearnings. Concretely, wefirst provideabrief\n overviewofmodern VLMs, whichformthebackboneof Open VLA(Section 3.1); thendescribe\n our basic training recipe and dataset (Section 3.2 and Section 3.3); discuss key design decisions\n (Section 3.4);andprovidedetailsoftheusedinfrastructurefortrainingandinference(Section 3.5).\n 3.1 Preliminaries: Vision-Language Models \n Thearchitectureofmostrecent VLMs[20,42–44]consistsofthreemainparts(see Fig.2): (1)a\n visualencoderthatmapsimageinputstoanumberof“imagepatchembeddings”,(2)aprojector\n that takes the output embeddings of the visual encoder and maps them into the input space of a\n languagemodel, and(3)alargelanguagemodel(LLM)backbone. During VLMtraining, the\n modelistrainedend-to-endwithanexttexttokenpredictionobjectiveonpairedorinterleavedvision\n andlanguagedatacuratedfromvarious Internetsources. \n Inthiswork,webuildonthe Prismatic-7 BVLM[44]. Prismaticfollowsthesamestandardarchitec-\n turedescribedabove,witha 600 M-parametervisualencoder,asmall 2-layer MLPprojector,anda\n 7 B-parameter Llama 2 languagemodelbackbone[10]. Notably,Prismaticusesatwo-partvisualen-\n coder,consistingofpretrained Sig LIP[79]and Dino V 2[25]models. Inputimagepatchesarepassed\n separatelythroughbothencodersandtheresultingfeaturevectorsareconcatenatedchannel-wise. In\n contrasttothemorecommonlyusedvisionencoderssuchas CLIP-[80]or Sig LIP-onlyencoders,\n theadditionof Dino V 2 featureshasbeenshowntobehelpfulforimprovedspatialreasoning[44],\n whichcanbeparticularlyhelpfulforrobotcontrol. \n Sig LIP,Dino V 2,and Llama 2 donotreleasedetailsabouttheirtrainingdata,whichlikelyconsistsof\n trillionsoftokensof Internet-sourcedimage-text,image-only,andtext-onlydatarespectively. The\n Prismatic VLMisfine-tunedontopofthesecomponentsusingthe LLa VA 1.5 datamixture[43],\n whichcontainsatotalofapproximately 1 Mimage-textandtext-onlydatasamplesfromopen-source\n datasets[29,42,81–83]. \n 4 "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n 3.2 Open VLATraining Procedure \n Totrain Open VLA,wefine-tuneapretrained Prismatic-7 BVLMbackboneforrobotactionprediction\n (see Fig.2). Weformulatetheactionpredictionproblemasa“vision-language”task,whereaninput\n observationimageandanaturallanguagetaskinstructionaremappedtoastringofpredictedrobot\n actions[7]. Toenablethe VLM’slanguagemodelbackbonetopredictrobotactions,werepresent\n theactionsintheoutputspaceofthe LLMbymappingcontinuousrobotactionstodiscretetokens\n usedbythelanguagemodel’stokenizer. Following Brohanetal.[7],wediscretizeeachdimensionof\n therobotactionsseparatelyintooneof 256 bins. Foreachactiondimension,wesetthebinwidth\n touniformlydividetheintervalbetweenthe 1 stand 99 thquantileoftheactionsinthetrainingdata.\n Usingquantilesinsteadofthemin-maxbounds Brohanetal.[7]usedallowsustoignoreoutlier\n actionsinthedatathatcouldotherwisedrasticallyexpandthediscretizationintervalandreducethe\n effectivegranularityofouractiondiscretization. \n Usingthisdiscretization,weobtain Ndiscreteintegers∈[0...255]foran N-dimensionalrobotac-\n tion. Unfortunately,thetokenizerusedby Open VLA’slanguagebackbone,the Llamatokenizer[10],\n only reserves 100 “special tokens” for tokens newly introduced during fine-tuning, which is too\n fewforthe 256 tokensofouractiondiscretization. Instead,weagainoptforsimplicityandfollow\n Brohanetal.[7]’sapproachbysimplyoverwritingthe 256 leastusedtokensinthe Llamatokenizer’s\n vocabulary(whichcorrespondstothelast 256 tokens)withouractiontokens. Oncetheactionsare\n processed into a sequence of tokens, Open VLA is trained with a standard next-token prediction\n objective, evaluating the cross-entropy loss on the predicted action tokens only. We discuss key\n designdecisionsforimplementingthistrainingprocedurein Section 3.4. Next,wedescribetherobot\n datasetweusefor Open VLAtraining. \n 3.3 Training Data \n The goal in constructing the Open VLA training dataset is to capture a large diversity of robot\n embodiments,scenes,andtasks. Thisenablesthefinalmodeltocontrolvariousrobotsoutofthe\n box and admits efficient fine-tuning to new robot setups. We leverage the Open X-Embodiment\n dataset[1](Open X)asabasetocurateourtrainingdataset. Thefull Open Xdataset,atthetimeof\n writing,consistsofmorethan 70 individualrobotdatasets,withmorethan 2 Mrobottrajectories,\n thatwerepooledintoacoherentandeasy-to-usedataformatinalargecommunityeffort. Tomake\n trainingonthisdatapractical,weapplymultiplestepsofdatacurationtotherawdataset.\n The goals of this curation are to ensure (1) a coherent input and output space across all training\n datasets,and(2)abalancedmixofembodiments,tasks,andscenesinthefinaltrainingmixture.2 To\n address(1),wefollow[1,5]andrestrictourtrainingdatasettocontainonlymanipulationdatasets\n withatleastone 3 rdpersoncameraandusesingle-armend-effectorcontrol. For(2),weleveragethe\n datamixtureweightsof Octo[5]foralldatasetsthatpassthefirstroundoffiltering.Octoheuristically\n down-weightsorremoveslessdiversedatasetsandup-weightsdatasetswithlargertaskandscene\n diversity;see Octo Model Teametal.[5]fordetails. \n Wealsoexperimentedwithincorporatingafewadditionaldatasetsintoourtrainingmixturethatwere\n addedtothe Open Xdatasetsincethereleaseof Octo,includingthe DROIDdataset[11],althoughata\n conservativemixtureweightof 10%. Inpractice,wefoundthattheactiontokenaccuracyon DROID\n remainedlowthroughouttraining,suggestingalargermixtureweightormodelmayberequiredtofit\n itsdiversityinthefuture. Tonotjeopardizethequalityofthefinalmodel,weremoved DROIDfrom\n thedatamixtureforthefinalthirdoftraining. Weprovideacompleteoverviewoftheuseddatasets\n andmixtureweightsin Appendix A. \n 3.4 Open VLADesign Decisions \n When developing the Open VLA model, we explored various design decisions in smaller-scale\n experiments before starting the final model training run. Concretely, we trained and evaluated\n Open VLAmodelson Bridge Data V 2[6]forourinitialexperiments,insteadoftrainingonthefull\n 2 Octo[5]demonstratedtrainingacrossdatasetswithheterogeneoussensoryinputs.Whileverypromising,\n weleaveaninvestigationof VLAtrainingacrossheterogeneoussensormodalitiesandactionspacestofuture\n work. \n 5 \n \n "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n Open X mixture, to increase iteration speed and reduce computational cost. We summarize key\n learningsfromtheseexplorationsbelow. \n VLM Backbone. Initially, we experimented with multiple VLM backbones. Apart from Pris-\n matic[44],wetestedfine-tuning IDEFICS-1[84]and LLa VA[85]forrobotactionprediction. We\n foundthat LLa VAand IDEFICS-1 performedcomparablyontaskswithonlyoneobjectinthescene,\n but LLa VAdemonstratedstrongerlanguagegroundingintasksthatinvolvedmultipleobjectsinthe\n sceneandrequiredthepolicytomanipulatethecorrectobject,i.e.,theobjectspecifiedinthelanguage\n instruction.Concretely,LLa VAimprovedupon IDEFICS-1 by 35%inabsolutesuccessrate,averaged\n acrossfivelanguagegroundingtasksina Bridge Data V 2 sinkenvironment. Thefine-tuned Prismatic\n VLMpolicyachievedfurtherimprovements,outperformingthe LLa VApolicybyroughly 10%in\n absolutesuccessrateacrossbothsimplesingle-objecttasksandmulti-object,languagegrounding\n tasks. Weattributethisperformancedeltatoimprovedspatialreasoningcapabilitiesaffordedbythe\n fused Sig LIP-Dino V 2 backbones(see Section 3.1). Inadditiontotheperformanceenhancements,\n Prismatic also provides a modular and easy-to-use codebase, so we ultimately chose it to be the\n backboneforthe Open VLAmodel. \n Image Resolution. The resolution of input images has significant impact on the computational\n requirementsof VLAtraining, sincehigher-resolutionimagesresultinmoreimagepatchtokens\n andthuslongercontextlengthsthatquadraticallyincreasetrainingcompute. Wecompared VLAs\n with 224×224 pxand 384×384 pxinputs,butfoundnoperformancedifferenceinourevaluations,\n while the latter takes 3 x longer to train. We thus opt for a resolution of 224 × 224 px for the\n final Open VLAmodel. Notethatonmany VLMbenchmarks,increasedresolutiondoesimprove\n performance[44,86,87],butwedidnotseethistrend(yet)for VLAs. \n Fine-Tuning Vision Encoder. Prior work on VLMs found that freezing vision encoders during\n VLMtrainingtypicallyleadstohigherperformance[44]. Intuitively,afrozenvisionencodermay\n betterpreservetherobustfeatureslearnedfromits Internet-scalepretraining. However,wefound\n fine-tuningthevisionencoderduring VLAtrainingtobecrucialforgood VLAperformance. We\n hypothesizethatthepretrainedvisionbackbonemaynotcapturesufficientfine-grainedspatialdetails\n aboutimportantpartsofthescenetoenablepreciseroboticcontrol. \n Training Epochs. Typical LLMor VLMtrainingrunscompleteatmostoneortwoepochsthrough\n their training dataset. In contrast, we found it important for VLA training to iterate through the\n trainingdatasetsignificantlymoretimes,withrealrobotperformancecontinuallyincreasinguntil\n trainingactiontokenaccuracysurpasses 95%. Ourfinaltrainingruncompletes 27 epochsthroughits\n trainingdataset. \n Learning Rate. Wesweptthelearningrateacrossmultipleordersofmagnitudefor VLAtraining,\n andachievedthebestresultsusingafixedlearningrateof 2 e-5(thesamelearningrateusedduring\n VLMpretraining[44]). Wedidnotfindlearningratewarmuptoprovidebenefits.\n 3.5 Infrastructurefor Trainingand Inference \n The final Open VLA model is trained on a cluster of 64 A 100 GPUs for 14 days, or a total of\n 21,500 A 100-hours,usingabatchsizeof 2048. Duringinference,Open VLArequires 15 GBof GPU\n memorywhenloadedinbfloat 16 precision(i.e.,withoutquantization)andrunsatapproximately\n 6 Hzonone NVIDIARTX 4090 GPU(withoutcompilation,speculativedecoding,orotherinference\n speed-uptricks). Wecanfurtherreducethememoryfootprintof Open VLAduringinferencevia\n quantization,withoutcompromisingperformanceinreal-worldroboticstasks,asshownin Section 5.4.\n Wereportinferencespeedonvariousconsumer-andserver-grade GPUsin Fig.6.Forconvenience,we\n implementaremote VLAinferenceservertoallowreal-timeremotestreamingofactionpredictions\n to the robot – removing the requirement of having access to a powerful local compute device to\n controltherobot. Wereleasethisremoteinferencesolutionaspartofouropen-sourcecoderelease\n (Section 4). \n 4 The Open VLACodebase \n Alongwithourmodel,wereleasethe Open VLAcodebase,amodular Py Torchcodebasefortraining\n VLA models (see https://openvla.github.io). It scales from fine-tuning VLAs on individ-\n ual GPUstotrainingbillion-parameter VLAsonmulti-node GPUclusters, andsupportsmodern\n 6 \n "
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n 87.0 85.0 90.0 \n 70.6 76.7 \n 60.0 \n 50.6 52.0 55.0 \n 38.836.3 40.0 \n 18.520.0 29.0 25.0 20.0 26.7 26.3 30.0 \n 8.0 7.5 10.0 \n 0.0 \n (Un d s is a e t p e ra n p c e b t a o a r r c a s k n , g c o r e b o s j u e ) n c d t s, (Unseen o r o ie b n je ta c t t i o p n o s s ) itions & (Unseen s h o a b p je e c s t ) sizes & ( & U n c s o e n e c n e p o t b s j e fr c o t m s, t in h s e t r I u n c te ti r o n n e s t) , (Ab s ili p ty e c to ifi e m p d r a o i n m n i p p la u t n ) la g t u e a o g b e j ect\n \n \n Put Yellow Corn Stack Blue Cup Put {Red Bottle, \n on Pink Plate Lift Eggplant Flip Pot Upright on Pink Cup Eggplant} into Pot\n Figure 3: Bridge Data V 2 Widow Xrobotevaluationtasksandresults. Weevaluate Open VLAandprior\n state-of-the-artgeneralistrobotpoliciesonacomprehensivesuiteoftaskscoveringseveralaxesofgeneralization,\n aswellastasksthatspecificallyassesslanguageconditioningability.Open VLAachieveshighestoverallperfor-\n manceandevenoutperformsclosed-sourcemodel RT-2-Xinallcategoriesexceptforsemanticgeneralization.\n Averagesuccessrates±Std Errarecomputedacross 170 totalrolloutsperapproach.See Table 4 fordetailed\n results. \n techniquesforlargetransformermodeltrainingsuchasautomaticmixedprecision(AMP,Py Torch\n [75]),Flash Attention[76],andfullyshardeddataparallelism(FSDP,Zhaoetal.[77]). Outofthe\n box, the Open VLAcodebasehasfullsupportfortrainingonthe Open Xdataset, integrateswith\n Hugging Face’s[21]Auto Modelclass, andsupports Lo RAfine-tuning[26]andquantizedmodel\n inference[27,88]. \n 5 Experiments \n Thegoalofourexperimentalevaluationsistotest Open VLA’sabilitytoserveasapowerfulmulti-\n robotcontrolpolicyoutofthebox,aswellasbeagoodinitializationforfine-tuningtonewrobot\n tasks. Concretely,weaimtoanswerthefollowingquestions: \n 1. Howdoes Open VLAcomparetopriorgeneralistrobotpolicies,whenevaluatingonmultiple\n robotsandvarioustypesofgeneralization? \n 2. Can Open VLAbeeffectivelyfine-tunedonanewrobotsetupandtask,andhowdoesit\n comparetostate-of-the-artdata-efficientimitationlearningapproaches?\n \n 3. Canweuseparameter-efficientfine-tuningandquantizationtoreducethecomputationalre-\n quirementsfortrainingandinferenceof Open VLAmodelsandmakethemmoreaccessible?\n Whataretheperformance-computetrade-offs? \n 5.1 Direct Evaluationson Multiple Robot Platforms \n Robot Setupsand Tasks. Weevaluate Open VLA’sperformance“out-of-the-box”ontworobot\n embodiments: the Widow X robot from the Bridge Data V 2 evaluations [6] (see Fig. 1, left) and\n the mobile manipulation robot from the RT-1 and RT-2 evaluations [2, 7] (“Google robot”; see\n Fig.1,middle). Bothplatformshavebeenextensivelyusedinpriorworksforevaluatinggeneralist\n robotpolicies[1,2,5,7]. Wedefineacomprehensivesetofevaluationtasksineachenvironment\n thatcoversvariousaxesofgeneralization,suchasvisual(unseenbackgrounds,distractorobjects,\n colors/appearancesofobjects);motion(unseenobjectpositions/orientations);physical(unseenobject\n sizes/shapes); and semantic (unseen target objects, instructions, and concepts from the Internet)\n generalization. Wealsoassesslanguageconditioningabilityinsceneswithmultipleobjects,testing\n whetherthepolicycanmanipulatethecorrecttargetobject,asspecifiedintheuser’sprompt. See\n bottomrowof Fig.3 and Fig.4 forexampletaskimagesinthe Bridge Data V 2 and Googlerobot\n evaluations,respectively. Overall,weevaluatedeachmethodin 170 rollouts(17 taskswith 10 trials\n each)for Bridge Data V 2 experimentsand 60 rollouts(12 taskswith 5 trialseach)for Googlerobot\n experiments. A detailed breakdown of all tasks and how they differ from the training data is in\n 7 \n \n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n Appendix B.Allevaluationsinthisandthefollowingsectionsareconductedas A/Bevaluations,\n usingthesametaskswiththesamesetsofinitialrobotandobjectstates,toensurefaircomparison.\n Comparisons. Wecompare Open VLA’sperformancetothreepriorgeneralistmanipulationpolicies:\n RT-1-X[1],RT-2-X[1],and Octo[5]. RT-1-X(35 Mparameters)and Octo(93 Mparameters)are\n transformerpoliciestrainedfromscratchonsubsetsofthe Open Xdataset;Octoisthestate-of-the-art\n model among open-source manipulation policies. RT-2-X (55 B parameters) is a state-of-the-art,\n closed-source VLAthatleverages Internet-pretrainedvisionandlanguagebackbones.\n Theresultsaresummarizedin Fig.3 for Bridge Data V 2 evaluationsand Fig.4 for Googlerobot\n evaluations(per-taskbreakdownin Appendix,Table 4 and Table 6). Wefindthatboth RT-1-Xand\n Octo struggle on the tested tasks, often failing to manipulate the correct object, especially when\n distractorsarepresent,andinsomecasescausingtherobottowaveitsarmaroundaimlessly. Note\n that our evaluations test even larger degrees of generalization than the evaluations performed in\n thosepriorworkstochallengethe Internet-pretrained VLAmodels. Thus,lowerperformanceof\n modelswithout Internetpretrainingisexpected. RT-2-Xclearlyoutperformsboth RT-1-Xand Octo,\n demonstratingthebenefitsoflarge,pretrained VLMsforrobotics. \n Notably,Open VLAperformscomparablyto RT- \n 2-X on Google robot evaluations and signifi- \n 78.3 85.0 88.0 82.982.9 \n cantlyoutperforms RT-2-Xon Bridge Data V 2 \n 72.0 \n evaluations despite being an order of magni- \n tude smaller (7 B vs. 55 B parameters). Qual- 44.0 \n 33.3 \n 26.7 32.0 34.3 \n itatively, we find that both RT-2-X and Open- \n VLA exhibit markedly more robust behaviors 14.3 \n thantheothertestedmodels,suchasapproach- \n ing the correct object when distractor objects (Tasks & conditions seen in (Unseen objects, tasks,\n training data) backgrounds, & concepts)\n arepresent,properlyorientingtherobot’send- \n effector to align with the orientation of the \n target object, and even recovering from mis- \n Move Coke Can \n takes such as insecurely grasping objects (see Pick Coke Can to Taylor Swift\n https://openvla.github.ioforqualitative Figure 4:Googlerobotevaluationresults.Weevaluate\n rolloutexamples). RT-2-Xachieveshigherper- generalistrobotpoliciesonin-distributionandout-of-\n formance in semantic generalization tasks, as distribution(OOD)tasksonthemobilemanipulatorused\n in RT-1 and RT-2 evaluations[2,7].Wefindthat Open-\n shown in Fig. 3, which is expected given that \n VLAand RT-2-Xattaincomparableperformanceand\n ituseslarger-scale Internetpretrainingdataand \n significantlyoutperform RT-1-Xand Octooverall.Aver-\n isco-fine-tunedwithbothrobotactiondataand agesuccessrates±Std Errarecomputedacross 60 total\n Internetpretrainingdatatobetterpreservethe rolloutsperapproach.See Table 6 fordetailedresults.\n pretraining knowledge, rather than being fine- \n tunedsolelyonrobotdata,like Open VLA.However,Open VLAperformscomparablyorbetterin\n all other task categories in both Bridge Data V 2 and Google robot evaluations. The performance\n differencecanbeattributedtoacombinationoffactors: wecuratedamuchlargertrainingdataset\n for Open VLAwith 970 ktrajectories(vs. 350 kfor RT-2-X);weperformedmorecarefulcleaningof\n thetrainingdatasetand,e.g.,filteredoutall-zeroactionsinthe Bridgedataset(see Appendix Cfora\n detaileddiscussion);and Open VLAusesafusedvisionencoderthatcombinespretrainedsemantic\n andspatialfeatures. See Appendix Dforablationanalysesofthesecomponents.\n 5.2 Data-Efficient Adaptationto New Robot Setups \n Whilepriorworksmainlyfocusedondirectlyevaluating VLAs“out-of-the-box”[1,7,16],effective\n fine-tuningof VLAmodelstonewtasksandrobotsetupsislargelyunexplored,yetiskeyfortheir\n widespreadadoption. Inthissection,weinvestigate Open VLA’sabilitytobequicklyadaptedtoa\n newreal-worldrobotsetup. (See Appendix Eforfine-tuningexperimentsinsimulation.)\n Robotsetupsandtasks. Wetestasimplefine-tuningrecipeforthe Open VLAmodel: fullfine-\n tuningofallmodelparameters,usingsmalldatasetswith 10–150 demonstrationsofatargettask(see\n Fig.5;weexploreparameter-efficientfine-tuningapproachesin Section 5.3). Wetest Open VLAin\n twosetups: Franka-Tabletop,astationary,table-mounted Franka Emika Panda 7-Do Frobotarm;\n and Franka-DROID,the Frankarobotarmsetupfromtherecentlyreleased DROIDdataset[11],\n 8 "
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n Franka-Tabletop Franka-DROID \n \n 43.437.141.535.2 63.8 66.7 53.5 33.3 46.7 60.0 93.3 80.0 13.3 53.3 83.3 63.3 26.7 70.0 93.3 19.427.830.6 16.7 69.4 27.822.2 66.769.4 77.8 16.725.0 91.7 44.450.0 35.0 26.7 38.3 21.7 58.3\n 0.0 \n \n \n \n Narrow Single-Instruction Tasks Diverse Multi-Instruction Tasks Visual Robustness\n Figure 5: Adapting to new robot setups. We evaluate the state-of-the-art Diffusion Policy trained from\n scratchonseven Franka Emika Pandatasks(10–150 demonstrationseach),aswellasgeneralistrobotpolicies\n Octoand Open VLAfine-tunedonthesamedata. Diffusion Policyexhibitsstrongperformanceonnarrow\n single-instructiontasks,while Octoand Open VLAperformbetterondiversefine-tuningtasksinvolvingmultiple\n instructionsanddistractorobjects. Overall,Open VLAachieveshighestaggregateperformanceacrossboth\n setups,suggestingthatitisaneffectivedefaultforlearningapolicyonadownstreamtask.Averagesuccessrates\n ±Std Errarecomputedacross 129 rolloutsperapproach(99 for Franka-Tabletoptasksand 30 for Franka-DROID\n tasks).See Table 7 fordetailedresults. \n mounted on a movable standing desk. The setups use 5 Hz and 15 Hz non-blocking controllers,\n respectively. Wechoose Frankarobotarmsasthetargetembodimentforourfine-tuningexperiments\n sincetheyarewidelyusedintherobotlearningcommunityandthusalikely“target”of Open VLA\n fine-tuning. Wetestonsetupswithdifferentcontrolfrequenciestotest Open VLA’sapplicabilitytoa\n rangeofusecases. \n Comparisons. Wecompareto Diffusion Policy[3],astate-of-the-artdata-efficientimitationlearning\n approach, trained from scratch. We also compare to Diffusion Policy (matched), a version of\n Diffusion Policythatmatchestheinputandoutputspecificationsof Open VLA.3 Additionally,we\n evaluate Octo[5]fine-tunedonthetargetdataset,sinceitiscurrentlythebestgeneralistpolicythat\n supportsfine-tuning (fine-tuningof RT-2-X isnotsupported throughits inference API).Wealso\n fine-tune Open VLAonthesametargetdataset,andtheresultingpolicyisdenotedby Open VLA.\n Finally, as an ablation experiment, we compare to Open VLA (scratch), where we directly fine-\n tune the underlying base Prismatic VLM on the target robot setup – rather than fine-tuning the\n Open X-pretrained Open VLAmodel–toassessthebenefitoflarge-scalerobotpretraining.\n Wepresenttheresultsin Fig.5(per-taskbreakdownin Appendix,Table 7). Wefindthatbothversions\n of Diffusion Policyarecompetitivewithoroutperformthegeneralistpolicies Octoand Open VLA\n onnarrowersingle-instructiontaskslike“Put Carrotin Bowl”and“Pour Corninto Pot”, butthe\n pretrainedgeneralistpoliciesperformbetterinmorediversefine-tuningtasksthatinvolvemultiple\n objectsinthesceneandrequirelanguageconditioning. Open Xpretrainingfor Octoand Open VLA\n enablesthemodelstobetteradapttothesemorediversetaskswherelanguagegroundingisimportant;\n weseeevidenceforthisinthelowerperformanceof Open VLA(scratch). \n Overall,wefindthat Open VLAachievesthehighestaverageperformance. Notably,mostpriorworks\n achievestrongperformanceonlyineithernarrowsingle-instructionordiversemulti-instructiontasks,\n resultinginwidelyvaryingsuccessrates. Open VLAistheonlyapproachthatachievesatleast 50%\n successrateacrossalltestedtasks, suggestingthatitcanbeastrongdefaultoptionforimitation\n learning tasks, particularly if they involve a diverse set of language instructions. For narrower\n but highly dexterous tasks, Diffusion Policy still shows smoother and more precise trajectories;\n incorporatingactionchunkingandtemporalsmoothing,asimplementedin Diffusion Policy,may\n help Open VLAattainthesamelevelofdexterityandmaybeapromisingdirectionforfuturework\n (see Section 6 foradetaileddiscussionofcurrentlimitations). \n 3 Thefull Diffusion Policyusesatwo-stepobservationhistorywithbothimagesandproprioceptivestate,and\n performsrecedinghorizoncontrolbypredictingachunkof T futureactionsandexecutingthefirst Xactionsin\n open-loopfashionbeforepredictingthenextchunk(for 15 Hzcontrol,weset T =16,X =8 likeinthe DROID\n priorwork[11];for 5 Hzcontrol,wereducethechunksizesto T = 8,X = 3). Itisalsotheonlymethod\n in Section 5.2 thatpredictsabsolute Cartesiancoordinatestocontroltherobot;allothermethodsuserelative\n positioncontrol.Diffusion Policy(matched)usesasingleimageasinput,hasnoproprioceptiveinformationand\n noobservationhistory,andpredictsasinglerelativepositioncontrolactionwithoutactionchunking.\n 9 "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n 5.3 Parameter-Efficient Fine-Tuning \n Thefullfine-tuningrunsof Open VLAintheprevioussectionused 8 A 100 GPUsfor 5-15 hoursper\n task(dependingonthedatasetsize)toachievehighperformance. Whilethisissubstantiallyless\n computethanwhatisrequiredfor VLApretraining,inthissectionweexploreevenmorecompute-\n andparameter-efficientfine-tuningapproachesandinvestigatetheireffectiveness.\n Concretely, we compare the follow- \n Table 1:Parameter-efficientfine-tuningevaluation.Lo RAfine-\n ingfine-tuningapproaches: fullfine- tuningachievesthebestperformance-computetrade-off,matching\n tuningupdatesallweightsduringfine- fullfine-tuningperformancewhiletrainingonly 1.4%ofthemodel\n tuning, as described in Section 5.2; parameters.Meansuccess±Std Errcomputedacross 33 rolloutsper\n lastlayeronlyfine-tunesonlythelast approachonselect Franka-Tabletoptasks(see Table 8 fordetails).\n ∗:Shardedacross 2 GPUswith FSDP[77]. \n layerof Open VLA’stransformerback- \n boneandthetokenembeddingmatrix; Strategy Success Rate Train Params(×106) VRAM(batch 16)\n frozen vision freezes the vision en- Full FT 69.7±7.2% 7,188.1 163.3 GB*\n coderbutfine-tunesallotherweights; Lastlayeronly 30.3±6.1% 465.1 51.4 GB\n sandwich fine-tuning unfreezes the Frozenvision 47.0±6.9% 6,760.4 156.2 GB*\n Sandwich 62.1±7.9% 914.2 64.0 GB \n visionencoder,tokenembeddingma- Lo RA,rank=32 68.2±7.5% 97.6 59.7 GB \n trix, and last layer; and Lo RA uses rank=64 68.2±7.8% 195.2 60.5 GB \n thepopularlow-rankadaptationtech- \n niqueof Huetal.[26]withmultiplerankvaluesr,appliedtoalllinearlayersofthemodel.\n Wereportfine-tuningsuccessratesacrossmultiple Franka-Tabletoptasks,aswellastrainingparame-\n tercountand GPUmemoryrequirements,in Table 1.4 Wefindthatonlyfine-tuningthenetwork’s\n lastlayerorfreezingthevisionencoderleadstopoorperformance,suggestingthatfurtheradaptation\n of the visual features to the target scene is crucial. In contrast, “sandwich fine-tuning” achieves\n betterperformancesinceitfine-tunesthevisionencoder,anditconsumesless GPUmemorysince\n it does not fine-tune the full LLM backbone. Lastly, Lo RA achieves the best trade-off between\n performanceandtrainingmemoryconsumption,outperforming“sandwichfine-tuning”andmatching\n fullfine-tuningperformancewhilefine-tuningonly 1.4%oftheparameters. Wefindthatthe Lo RA\n rankhasnegligibleeffectonpolicyperformanceandthusrecommendusingadefaultrankofr =32.\n With Lo RA,wecanfine-tune Open VLAonanewtaskwithin 10-15 hoursonasingle A 100 GPU–\n an 8 xreductionincomputecomparedtofullfine-tuning. \n 5.4 Memory-Efficient Inferencevia Quantization \n Precision Bridge Success VRAM \n bfloat 16 71.3±4.8% 16.8 GB \n int 8 58.1±5.1% 10.2 GB \n int 4 71.9±4.7% 7.0 GB \n N/A Table 2:Performancewithquantizedin- \n ference.4-bitquantizationmatchestheper-\n Figure 6: Open VLAinferencespeedforvarious GPUs.Both formanceofbfloat 16 inference(ourdefault\n bfloat 16 andint 4 quantizationachievehighthroughput,especially approach)whilereducingthe GPUmemory\n on GPUswith Ada Lovelacearchitecture(RTX 4090,H 100).Fur- footprintbymorethanhalf.Meansuccess\n therspeed-upsarepossiblewithmodern LLMinferenceframe- ±Std Errcomputedacross 8 representative\n workslike Tensor RT-LLM[89]. ♠: Modelshardedacrosstwo Bridge Data V 2 tasks[6]and 80 rolloutsper\n GPUstofit. approach(see Table 5 fordetails). \n Open VLA,a 7 B-parametermodel,consumesmorememoryatinferencetimethanprioropen-source\n generalistpoliciessuchas Octo,whichhas<100 Mparameters. Wefollowbest-practicesfrom LLM\n servingbysavingandloading Open VLAinbfloat 16 precisionforinference(ourdefaultapproach),\n whichcutsthememoryfootprintinhalf,allowingustoserve Open VLAon GPUswithonly 16 GB\n 4 In Section 5.3 and Section 5.4,weexperimentwithaversionofthe Open VLAmodelthatispretrainedwith\n asmallerrobotdatamixture(thesame Open Xdatasetmixtureas Octo)andhasaslightlysmallerarchitecture\n whichonlyusesa Sig LIP[79]visionbackboneinsteadofthefused Dino Sig LIPencoder. Wefindthatthis\n simplerarchitecturestillachievesstrongperformanceinbothfine-tuningtasksand“out-of-the-box”tasks.\n 10 "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n of GPUmemory. Inthissection,wetestwhetherwecanfurtherreducetherequiredmemoryfor\n policyinferenceandbroadenaccessibilityof VLApolicies,byusingmodernquantizationtechniques\n developedforserving LLMs[27,88]. Theseapproachesloadtheweightsofthenetworkatlower\n precision,therebytradingoffreducedmemoryrequirementsforpotentiallyreducedinferencespeed\n andaccuracy. \n Concretely,weinvestigateservingthe Open VLAmodelwith 8-bitand 4-bitprecisionon 8 represen-\n tative Bridge Data V 2 tasks. Wereportmemoryfootprintandrolloutperformancein Table 2. We\n alsoreportachievablecontrolfrequenciesonvariousconsumer-andserver-grade GPUsin Fig.6.\n Weobservethat 8-bitquantizationslowsdowninferenceacrossmost GPUs,duetotheoverheadof\n theaddedquantizationoperations. 4-bitinferenceachieveshigherthroughput,sincereduced GPU\n memorytransfercompensatesforthequantizationoverhead. \n As a result of the reduced inference speed, we observe a substantial performance decrease with\n 8-bitquantization: onthe A 5000 GPUweuseforourevaluations, wecanonlyrunthemodelat\n 1.2 Hz,whichsignificantlychangesthesystemdynamicscomparedtothetrainingdatasetforthe\n 5 Hznon-blockingcontrollerusedinthe Bridge Data V 2 tasks.5 Notably,4-bitquantizationresultsin\n similarperformanceasbfloat 16 half-precisioninferencedespiterequiringlessthanhalftheamount\n of GPUmemory. 4-bitquantizedmodelscanrunat 3 Hzonthe A 5000,thusmorecloselymatching\n thesystemdynamicsduringdatacollection. \n 6 Discussionand Limitations \n Inthiswork,wepresented Open VLA,astate-of-the-art,open-sourcevision-language-actionmodel\n thatobtainsstrongperformanceforcross-embodimentrobotcontrolout-of-the-box. Wealsodemon-\n stratedthat Open VLAcanbeeasilyadaptedtonewrobotsetupsviaparameter-efficientfine-tuning\n techniques. \n Thecurrent Open VLAmodelhasseverallimitations. First,itcurrentlyonlysupportssingle-image\n observations. Inreality,real-worldrobotsetupsareheterogeneous,withawiderangeofpossible\n sensoryinputs[5]. Expanding Open VLAtosupportmultipleimageandproprioceptiveinputsaswell\n asobservationhistoryisanimportantavenueforfuturework. Exploringtheuseof VLMspretrained\n oninterleavedimageandtextdatamayfacilitatesuchflexible-input VLAfine-tuning.\n Secondly,improvingtheinferencethroughputof Open VLAiscriticaltoenable VLAcontrolfor\n high-frequency control setups such as ALOHA [90], which runs at 50 Hz. This will also enable\n testing VLAsonmoredexterous,bi-manualmanipulationtasksthanwhatweinvestigatedinthis\n work. Exploringtheuseofactionchunkingoralternativeinference-timeoptimizationtechniques\n suchasspeculativedecoding[91]offerpotentialremedies. \n Additionally,thereisroomforfurtherperformanceimprovements. While Open VLAoutperforms\n prior generalist policies, it does not yet offer very high reliability on the tested tasks, typically\n achieving<90%successrate. \n Finally, due to compute limitations, many VLA design questions remain underexplored: What\n effectdoesthesizeofthebase VLMhaveon VLAperformance? Doesco-trainingonrobotaction\n predictiondataand Internet-scalevision-languagedatasubstantiallyimprove VLAperformance?\n Whatvisualfeaturesarebest-suitedfor VLAmodels? Wehopethatthereleaseofthe Open VLA\n modelandcodebasewillenablethecommunitytojointlyinvestigatethesequestions.\n Acknowledgments \n We are grateful to the Toyota Research Institute for providing significant funding and compute\n resourcesrequiredtocarryoutthisresearch. Wealsothankthe Stanford Centerfor Researchon\n Foundation Modelsforprovidingadditionalcomputeresourcesand Google Deep Mindforalpha\n accesstothe RT-2-XAPIforourevaluations. Weacknowledgeadditionalsupportfrom Volkswagen,\n Physical Intelligence,ONRgrants N 00014-22-1-2621 and N 00014-22-1-2293,the National Science\n Foundationthrough IIS-2246811,and DARPAANSR. \n 5 Weattributetheperformancelosstolowinferencespeed,sinceboth 8-bitand 4-bitquantizationachieve\n comparabletokenaccuracytobfloat 16 inferencewhenevaluatedofflineontrainingdata.See Appendix D.4 for\n supportingdetails. \n 11 \n \n "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n References \n [1] Open X-Embodiment Collaboration,A.Padalkar,A.Pooley,A.Jain,A.Bewley,A.Herzog,\n A.Irpan,A.Khazatsky,A.Rai,A.Singh,A.Brohan,A.Raffin,A.Wahid,B.Burgess-Limerick,\n B.Kim,B.Schölkopf,B.Ichter,C.Lu,C.Xu,C.Finn,C.Xu,C.Chi,C.Huang,C.Chan,\n C.Pan,C.Fu,C.Devin,D.Driess,D.Pathak,D.Shah,D.Büchler,D.Kalashnikov,D.Sadigh,\n E.Johns,F.Ceola,F.Xia,F.Stulp,G.Zhou,G.S.Sukhatme,G.Salhotra,G.Yan,G.Schiavi,\n H. Su, H.-S. Fang, H. Shi, H. B. Amor, H. I. Christensen, H. Furuta, H. Walke, H. Fang,\n I.Mordatch,I.Radosavovic,I.Leal,J.Liang,J.Kim,J.Schneider,J.Hsu,J.Bohg,J.Bingham,\n J.Wu,J.Wu,J.Luo,J.Gu,J.Tan,J.Oh,J.Malik,J.Tompson,J.Yang,J.J.Lim,J.Silvério,\n J.Han,K.Rao,K.Pertsch,K.Hausman,K.Go,K.Gopalakrishnan,K.Goldberg,K.Byrne,\n K.Oslund,K.Kawaharazuka,K.Zhang,K.Majd,K.Rana,K.Srinivasan,L.Y.Chen,L.Pinto,\n L.Tan,L.Ott,L.Lee,M.Tomizuka,M.Du,M.Ahn,M.Zhang,M.Ding,M.K.Srirama,\n M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf,\n N. D. Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer, P. R. Sanketi, P. Wohlhart, P. Xu,\n P. Sermanet, P. Sundaresan, Q. Vuong, R. Rafailov, R. Tian, R. Doshi, R. Martín-Martín,\n R.Mendonca,R.Shah,R.Hoque,R.Julian,S.Bustamante,S.Kirmani,S.Levine,S.Moore,\n S. Bahl, S. Dass, S. Song, S. Xu, S. Haldar, S. Adebola, S. Guist, S. Nasiriany, S. Schaal,\n S.Welker,S.Tian,S.Dasari,S.Belkhale,T.Osa,T.Harada,T.Matsushima,T.Xiao,T.Yu,\n T.Ding,T.Davchev,T.Z.Zhao,T.Armstrong,T.Darrell,V.Jain,V.Vanhoucke,W.Zhan,\n W.Zhou,W.Burgard,X.Chen,X.Wang,X.Zhu,X.Li,Y.Lu,Y.Chebotar,Y.Zhou,Y.Zhu,\n Y.Xu,Y.Wang,Y.Bisk,Y.Cho,Y.Lee,Y.Cui,Y.hua Wu,Y.Tang,Y.Zhu,Y.Li,Y.Iwasawa,\n Y.Matsuo,Z.Xu,and Z.J.Cui. Open X-Embodiment: Roboticlearningdatasetsand RT-X\n models. https://arxiv.org/abs/2310.08864,2023. \n [2] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-\n man, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi,\n R.Julian,D.Kalashnikov,Y.Kuang,I.Leal,K.-H.Lee,S.Levine,Y.Lu,U.Malla,D.Manju-\n nath,I.Mordatch,O.Nachum,C.Parada,J.Peralta,E.Perez,K.Pertsch,J.Quiambao,K.Rao,\n M.Ryoo,G.Salazar,P.Sanketi,K.Sayed,J.Singh,S.Sontakke,A.Stone,C.Tan,H.Tran,\n V.Vanhoucke,S.Vega,Q.Vuong,F.Xia,T.Xiao,P.Xu,S.Xu,T.Yu,and B.Zitkovich. Rt-1:\n Robotics transformer for real-world control at scale. In ar Xiv preprint ar Xiv:2212.06817,\n 2022. \n [3] C.Chi, S.Feng, Y.Du,Z.Xu,E.Cousineau,B.Burchfiel,and S.Song. Diffusionpolicy:\n Visuomotorpolicylearningviaactiondiffusion. In Proceedingsof Robotics: Scienceand\n Systems(RSS),2023. \n [4] A.Xie,L.Lee,T.Xiao,and C.Finn. Decomposingthegeneralizationgapinimitationlearning\n forvisualroboticmanipulation. ar Xivpreprintar Xiv:2307.03659,2023.\n [5] Octo Model Team,D.Ghosh,H.Walke,K.Pertsch,K.Black,O.Mees,S.Dasari,J.Hejna,\n C.Xu,J.Luo,T.Kreiman,Y.Tan,D.Sadigh,C.Finn,and S.Levine. Octo: Anopen-source\n generalistrobotpolicy. https://octo-models.github.io,2023. \n [6] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch,\n Q.Vuong,A.He,V.Myers,K.Fang,C.Finn,and S.Levine. Bridgedatav 2: Adatasetfor\n robotlearningatscale,2023. \n [7] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,\n A.Dubey,C.Finn,P.Florence,C.Fu,M.G.Arenas,K.Gopalakrishnan,K.Han,K.Hausman,\n A.Herzog,J.Hsu,B.Ichter,A.Irpan,N.Joshi,R.Julian,D.Kalashnikov,Y.Kuang,I.Leal,\n L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao,\n K.Reymann,M.Ryoo,G.Salazar,P.Sanketi,P.Sermanet,J.Singh,A.Singh,R.Soricut,\n H.Tran,V.Vanhoucke,Q.Vuong,A.Wahid,S.Welker,P.Wohlhart,J.Wu,F.Xia,T.Xiao,\n P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-2: Vision-language-action models transfer web\n knowledgetoroboticcontrol. Inar Xivpreprintar Xiv:2307.15818,2023. \n \n 12 \n \n "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n \n \n [8] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell,\n P.Mishkin,J.Clark,G.Krueger,and I.Sutskever. Learningtransferablevisualmodelsfrom\n natural language supervision. In International Conference on Machine Learning (ICML),\n volume 139,pages 8748–8763,2021. \n [9] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-\n training. In International Conferenceon Computer Vision(ICCV),2023. \n \n [10] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,\n P.Bhargava,S.Bhosale,etal. Llama 2: Openfoundationandfine-tunedchatmodels. ar Xiv\n preprintar Xiv:2307.09288,2023. \n [11] A.Khazatsky, K.Pertsch, S.Nair, A.Balakrishna, S.Dasari, S.Karamcheti, S.Nasiriany,\n M.K.Srirama,L.Y.Chen,K.Ellis,P.D.Fagan,J.Hejna,M.Itkina,M.Lepert,Y.J.Ma,\n P.T.Miller,J.Wu,S.Belkhale,S.Dass,H.Ha,A.Jain,A.Lee,Y.Lee,M.Memmel,S.Park,\n I.Radosavovic,K.Wang,A.Zhan,K.Black,C.Chi,K.B.Hatch,S.Lin,J.Lu,J.Mercat,\n A.Rehman,P.R.Sanketi,A.Sharma,C.Simpson,Q.Vuong,H.R.Walke,B.Wulfe,T.Xiao,\n J.H.Yang,A.Yavary,T.Z.Zhao,C.Agia,R.Baijal,M.G.Castro,D.Chen,Q.Chen,T.Chung,\n J.Drake,E.P.Foster,J.Gao,D.A.Herrera,M.Heo,K.Hsu,J.Hu,D.Jackson,C.Le,Y.Li,\n K.Lin,R.Lin,Z.Ma,A.Maddukuri,S.Mirchandani,D.Morton,T.Nguyen,A.O’Neill,\n R.Scalise,D.Seale,V.Son,S.Tian,E.Tran,A.E.Wang,Y.Wu,A.Xie,J.Yang,P.Yin,\n Y.Zhang,O.Bastani,G.Berseth,J.Bohg,K.Goldberg,A.Gupta,A.Gupta,D.Jayaraman,\n J.J.Lim,J.Malik,R.Martín-Martín,S.Ramamoorthy,D.Sadigh,S.Song,J.Wu,M.C.Yip,\n Y.Zhu,T.Kollar,S.Levine,and C.Finn. Droid: Alarge-scalein-the-wildrobotmanipulation\n dataset. 2024. \n [12] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,and A.Gupta. R 3 m: Auniversalvisualrepresenta-\n tionforrobotmanipulation. In Co RL,2022. \n [13] S.Karamcheti,S.Nair,A.S.Chen,T.Kollar,C.Finn,D.Sadigh,and P.Liang. Language-\n driven representation learning for robotics. Ar Xiv, abs/2302.12766, 2023. URL https:\n //api.semanticscholar.org/Corpus ID:257205716. \n \n [14] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic\n manipulation. In Conferenceonrobotlearning,pages 894–906.PMLR,2022.\n [15] A.Stone,T.Xiao,Y.Lu,K.Gopalakrishnan,K.-H.Lee,Q.Vuong,P.Wohlhart,B.Zitkovich,\n F. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language\n models. ar Xivpreprintar Xiv:2303.00905,2023. \n \n [16] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,\n Q.Vuong,T.Yu,etal. Palm-e: Anembodiedmultimodallanguagemodel. ar Xivpreprint\n ar Xiv:2303.03378,2023. \n [17] A. S. et al. Introducing rfm-1: Giving robots human-like reason-\n ing capabilities, 2024. URL https://covariant.ai/insights/ \n introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/.\n \n [18] Wayve.Lingo-2:Drivingwithnaturallanguage.2024.URLhttps://wayve.ai/thinking/\n lingo-2-driving-with-language/. \n [19] X.Chen,X.Wang,S.Changpinyo,A.J.Piergiovanni,P.Padlewski,D.M.Salz,S.Goodman,\n A.Grycner,B.Mustafa,L.Beyer,A.Kolesnikov,J.Puigcerver,N.Ding,K.Rong,H.Akbari,\n G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K.\n Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali:\n Ajointly-scaledmultilinguallanguage-imagemodel. Ar Xiv, abs/2209.06794, 2022. URL\n https://api.semanticscholar.org/Corpus ID:252222320. \n \n 13 \n \n "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n [20] X.Chen,X.Wang,L.Beyer,A.Kolesnikov,J.Wu,P.Voigtlaender,B.Mustafa,S.Goodman,\n I.M.Alabdulmohsin, P.Padlewski, D.M.Salz, X.Xiong, D.Vlasic, F.Pavetic, K.Rong,\n T.Yu,D.Keysers,X.-Q.Zhai,and R.Soricut. Pa LI-3 visionlanguagemodels: Smaller,faster,\n stronger. ar Xivpreprintar Xiv:2310.09199,2023. \n [21] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,\n M.Funtowicz,J.Davison,S.Shleifer,and... Transformers: State-of-the-artnaturallanguage\n processing. In Proceedingsofthe 6 th International Conferenceon Learning Representations,\n 2020. URLhttps://arxiv.org/abs/1910.03771. \n [22] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière,\n N.Goyal,E.Hambro,F.Azhar,etal. Llama: Openandefficientfoundationlanguagemodels.\n ar Xivpreprintar Xiv:2302.13971,2023. \n \n [23] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bressand,\n G.Lengyel,G.Lample,L.Saulnier,etal. Mistral 7 b. ar Xivpreprintar Xiv:2310.06825,2023.\n [24] G.Team,T.Mesnard,C.Hardin,R.Dadashi,S.Bhupatiraju,S.Pathak,L.Sifre,M.Rivière,\n M.S.Kale,J.Love,etal. Gemma: Openmodelsbasedongeminiresearchandtechnology.\n ar Xivpreprintar Xiv:2403.08295,2024. \n [25] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,\n D.Haziza, F.Massa, A.El-Nouby, etal. Dinov 2: Learningrobustvisualfeatureswithout\n supervision. ar Xivpreprintar Xiv:2304.07193,2023. \n \n [26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:\n Low-rankadaptationoflargelanguagemodels. ar Xivpreprintar Xiv:2106.09685,2021.\n [27] T.Dettmers, A.Pagnoni, A.Holtzman, and L.Zettlemoyer. Qlora: Efficientfinetuningof\n quantizedllms. Advancesin Neural Information Processing Systems,36,2024.\n \n [28] Y.Goyal,T.Khot,D.Summers-Stay,D.Batra,and D.Parikh. Makingthe Vin VQAmatter:\n Elevatingtheroleofimageunderstandinginvisualquestionanswering. In Computer Vision\n and Pattern Recognition(CVPR),2017. \n [29] D.A.Hudsonand C.D.Manning. GQA:Anewdatasetforreal-worldvisualreasoningand\n compositional question answering. In Computer Vision and Pattern Recognition (CVPR),\n 2019. \n \n [30] A.Singh,V.Natarajan,M.Shah,Y.Jiang,X.Chen,D.Batra,D.Parikh,and M.Rohrbach.\n Towards VQAmodelsthatcanread. In Computer Visionand Pattern Recognition(CVPR),\n 2019. \n [31] J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz,\n B.White,S.White,and T.Yeh. Viz Wiz: nearlyreal-timeanswerstovisualquestions. In User\n Interface Softwareand Technology(UIST),pages 333–342,2010. \n [32] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Refer It Game: Referring to objects\n in photographs of natural scenes. In Empirical Methods in Natural Language Processing\n (EMNLP),pages 787–798,2014. \n \n [33] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring\n expressions. In European Conferenceon Computer Vision(ECCV),2016. \n [34] T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S.\n Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua,\n A.Botev,A.Castro-Ros,A.Slone,A.Héliou,A.Tacchetti,A.Bulanova,A.Paterson,B.Tsai,\n B. Shahriari, C. L. Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid,\n E.Buchatskaya, E.Ni, E.Noland, G.Yan, G.Tucker, G.-C.Muraru, G.Rozhdestvenskiy,\n \n 14 \n \n "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n H.Michalewski,I.Tenney,I.Grishchenko,J.Austin,J.Keeling,J.Labanowski,J.-B.Lespiau,\n J.Stanway,J.Brennan,J.Chen,J.Ferret,J.Chiu,J.Mao-Jones,K.Lee,K.Yu,K.Millican,\n L.L.Sjoesund,L.Lee,L.Dixon,M.Reid,M.Mikuła,M.Wirth,M.Sharman,N.Chinaev,\n N.Thain,O.Bachem,O.Chang,O.Wahltinez,P.Bailey,P.Michel,P.Yotov,R.Chaabouni,\n R.Comanescu,R.Jana,R.Anil,R.Mc Ilroy,R.Liu,R.Mullins,S.L.Smith,S.Borgeaud,\n S.Girgin,S.Douglas,S.Pandya,S.Shakeri,S.De,T.Klimenko,T.Hennigan,V.Feinberg,\n W.Stokowiec,Y.hui Chen,Z.Ahmed,Z.Gong,T.Warkentin,L.Peran,M.Giang,C.Farabet,\n O.Vinyals,J.Dean,K.Kavukcuoglu,D.Hassabis,Z.Ghahramani,D.Eck,J.Barral,F.Pereira,\n E.Collins,A.Joulin,N.Fiedel,E.Senter,A.Andreev,and K.Kenealy. Gemma: Openmodels\n basedongeminiresearchandtechnology. ar Xivpreprintar Xiv:2403.08295,2024.\n [35] Y.Li,S.Bubeck,R.Eldan,A.D.Giorno,S.Gunasekar,and Y.T.Lee. Textbooksareallyou\n needii: phi-1.5 technicalreport. ar Xivpreprintar Xiv:2309.05463,2023.\n [36] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,etal. Qwen\n technicalreport. ar Xivpreprintar Xiv:2309.16609,2023. \n \n [37] J.Li,D.Li,C.Xiong,and S.C.H.Hoi. BLIP:Bootstrappinglanguage-imagepre-training\n forunifiedvision-languageunderstandingandgeneration. In International Conferenceon\n Machine Learning(ICML),2022. \n [38] J.Li,D.Li,S.Savarese,and S.C.H.Hoi. BLIP-2:Bootstrappinglanguage-imagepre-training\n with frozen image encoders and large language models. In International Conference on\n Machine Learning(ICML),2023. \n [39] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.A.Li,P.Fung,and S.C.H.Hoi.\n Instruct BLIP:Towardsgeneral-purposevision-languagemodelswithinstructiontuning. ar Xiv\n preprintar Xiv:2305.06500,2023. \n \n [40] H.H.Tanand M.Bansal. LXMERT:Learningcross-modalityencoderrepresentationsfrom\n transformers. In Empirical Methodsin Natural Language Processing(EMNLP),2019.\n [41] H.Laurençon,L.Saulnier,L.Tronchon,S.Bekman,A.Singh,A.Lozhkov,T.Wang,S.Karam-\n cheti,A.M.Rush,D.Kiela,M.Cord,and V.Sanh. OBELICS:Anopenweb-scalefiltered\n datasetofinterleavedimage-textdocuments. In Neural Information Processing Systems Track\n on Datasetsand Benchmarks(Neur IPSDatasetsand Benchmarks),2023. \n \n [42] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In Advances in Neural\n Information Processing Systems(Neur IPS),2023. \n [43] H.Liu,C.Li,Y.Li,and Y.J.Lee. Improvedbaselineswithvisualinstructiontuning. ar Xiv\n preprintar Xiv:2310.03744,2023. \n \n [44] S.Karamcheti,S.Nair,A.Balakrishna,P.Liang,T.Kollar,and D.Sadigh. Prismaticvlms:\n Investigating the design space of visually-conditioned language models. ar Xiv preprint\n ar Xiv:2402.07865,2024. \n [45] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,\n M. Kalakrishnan, V. Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for\n vision-basedroboticmanipulation. ar Xivpreprintar Xiv:1806.10293,2018.\n \n [46] D.Kalashnkov,J.Varley,Y.Chebotar,B.Swanson,R.Jonschkowski,C.Finn,S.Levine,and\n K.Hausman. Mt-opt: Continuousmulti-taskroboticreinforcementlearningatscale. ar Xiv,\n 2021. \n [47] F.Ebert,Y.Yang,K.Schmeckpeper,B.Bucher,G.Georgakis,K.Daniilidis,C.Finn,and\n S.Levine. Bridgedata: Boostinggeneralizationofroboticskillswithcross-domaindatasets.\n ar Xivpreprintar Xiv:2109.13396,2021. \n \n 15 \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n \n [48] K. Ehsani, T. Gupta, R. Hendrix, J. Salvador, L. Weihs, K.-H. Zeng, K. P. Singh, Y. Kim,\n W.Han,A.Herrasti,etal. Imitatingshortestpathsinsimulationenableseffectivenavigation\n andmanipulationintherealworld. ar Xivpreprintar Xiv:2312.02976,2023.\n [49] H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tulsiani, and V. Kumar. Roboagent:\n Generalizationandefficiencyinrobotmanipulationviasemanticaugmentationsandaction\n chunking. ar Xivpreprintar Xiv:2309.01918,2023. \n \n [50] L.Pintoand A.Gupta. Supersizingself-supervision: Learningtograspfrom 50 ktriesand\n 700 robothours. In 2016 IEEEinternationalconferenceonroboticsandautomation(ICRA),\n pages 3406–3413.IEEE,2016. \n [51] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta,\n E. Orbay, et al. Roboturk: A crowdsourcing platform for robotic skill learning through\n imitation. In Conferenceon Robot Learning,pages 879–893.PMLR,2018. \n [52] A. Gupta, A. Murali, D. P. Gandhi, and L. Pinto. Robot learning in homes: Improving\n generalizationandreducingdatasetbias. Advancesinneuralinformationprocessingsystems,\n 31,2018. \n \n [53] S.Dasari,F.Ebert,S.Tian,S.Nair,B.Bucher,K.Schmeckpeper,S.Singh,S.Levine,and\n C.Finn. Robonet: Large-scalemulti-robotlearning. Co RL,2019. \n [54] S. Cabi, S. G. Colmenarejo, A. Novikov, K. Konyushkova, S. Reed, R. Jeong, K. Zolna,\n Y.Aytar,D.Budden,M.Vecerik,O.Sushkov,D.Barker,J.Scholz,M.Denil,N.de Freitas,\n and Z.Wang. Scalingdata-drivenroboticswithrewardsketchingandbatchreinforcement\n learning. RSS,2019. \n \n [55] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn.\n Bc-z: Zero-shottaskgeneralizationwithroboticimitationlearning. In Conferenceon Robot\n Learning,pages 991–1002.PMLR,2022. \n [56] H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and C. Lu. Rh 20 t: A\n comprehensive robotic dataset for learning diverse skills in one-shot. Towards Generalist\n Robots: Learning Paradigmsfor Scalable Skill Acquisition@Co RL 2023,3:5,2023.\n \n [57] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural net-\n workpoliciesformulti-taskandmulti-robottransfer. In Proceedingsof IEEEInternational\n Conferenceon Roboticsand Automation,2017. \n [58] E.S.Hu,K.Huang,O.Rybkin,and D.Jayaraman. Knowthyself: Transferablevisualcontrol\n policiesthroughrobot-awareness. In International Conferenceon Learning Representations,\n 2022. \n \n [59] J. H. Yang, D. Sadigh, and C. Finn. Polybot: Training one policy across robots while\n embracing variability. In 7 th Annual Conference on Robot Learning, 2023. URL https:\n //openreview.net/forum?id=HEIRj 51 lc S. \n [60] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-maron,M.Giménez,\n Y.Sulsky,J.Kay,J.T.Springenberg,T.Eccles,J.Bruce,A.Razavi,A.Edwards,N.Heess,\n Y.Chen,R.Hadsell,O.Vinyals,M.Bordbar,and N.de Freitas.Ageneralistagent.Transactions\n on Machine Learning Research,2022. ISSN 2835-8856. \n [61] G.Salhotra,I.-C.A.Liu,and G.Sukhatme. Bridgingactionspacemismatchinlearningfrom\n demonstrations. ar Xivpreprintar Xiv:2304.03833,2023. \n \n [62] I.Radosavovic, B.Shi, L.Fu, K.Goldberg, T.Darrell, and J.Malik. Robotlearningwith\n sensorimotorpre-training. In Conferenceon Robot Learning,2023. \n \n 16 \n \n "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n [63] D.Shah,A.Sridhar,A.Bhorkar,N.Hirose,and S.Levine. Gnm: Ageneralnavigationmodel\n to drive any robot. In 2023 IEEE International Conference on Robotics and Automation\n (ICRA),pages 7226–7233.IEEE,2023. \n [64] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou,\n A.Gupta,A.Raju,etal. Robocat: Aself-improvingfoundationagentforroboticmanipulation.\n ar Xivpreprintar Xiv:2306.11706,2023. \n [65] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,and S.Levine. Vi NT:A\n foundationmodelforvisualnavigation. In 7 th Annual Conferenceon Robot Learning,2023.\n URLhttps://arxiv.org/abs/2306.14846. \n \n [66] J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong, C. Finn, D. Sadigh, and S. Levine.\n Pushing the limits of cross-embodiment learning for manipulation and navigation. ar Xiv\n preprintar Xiv:2402.19432,2024. \n [67] S.Y.Gadre,M.Wortsman,G.Ilharco,L.Schmidt,and S.Song. Cowsonpasture: Baselines\n and benchmarks for language-driven zero-shot object navigation. In Proceedings of the\n IEEE/CVFConferenceon Computer Visionand Pattern Recognition, pages 23171–23181,\n 2023. \n \n [68] Y.Du, K.Konyushkova, M.Denil, A.Raju, J.Landon, F.Hill, N.de Freitas, and S.Cabi.\n Vision-languagemodelsassuccessdetectors. ar Xivpreprintar Xiv:2303.07280,2023.\n [69] Y.J.Ma,V.Kumar,A.Zhang,O.Bastani,and D.Jayaraman. Liv: Language-imagerepresen-\n tationsandrewardsforroboticcontrol. In International Conferenceon Machine Learning,\n pages 23301–23320.PMLR,2023. \n [70] X.Zhang,Y.Ding,S.Amiri,H.Yang,A.Kaminski,C.Esselink,and S.Zhang. Grounding\n classicaltaskplannersviavision-languagemodels. ar Xivpreprintar Xiv:2304.08587,2023.\n \n [71] S. Sontakke, J. Zhang, S. Arnold, K. Pertsch, E. Bıyık, D. Sadigh, C. Finn, and L. Itti.\n Roboclip:Onedemonstrationisenoughtolearnrobotpolicies.Advancesin Neural Information\n Processing Systems,36,2024. \n [72] J.Huang,S.Yong,X.Ma,X.Linghu,P.Li,Y.Wang,Q.Li,S.-C.Zhu,B.Jia,and S.Huang.\n Anembodiedgeneralistagentin 3 dworld. In Proceedingsofthe International Conferenceon\n Machine Learning(ICML),2024. \n \n [73] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu,\n et al. Vision-language foundation models as effective robot imitators. ar Xiv preprint\n ar Xiv:2311.01378,2023. \n [74] H.Zhen,X.Qiu,P.Chen,J.Yang,X.Yan,Y.Du,Y.Hong,and C.Gan. 3 d-vla: 3 dvision-\n language-actiongenerativeworldmodel. ar Xivpreprintar Xiv:2403.09631,2024.\n \n [75] Py Torch. Automatic mixed precision. URL https://pytorch.org/docs/stable/amp.\n html. \n [76] T.Dao. Flashattention-2: Fasterattentionwithbetterparallelismandworkpartitioning. ar Xiv\n preprintar Xiv:2307.08691,2023. \n \n [77] Y.Zhao,A.Gu,R.Varma,L.Luo,C.-C.Huang,M.Xu,L.Wright,H.Shojanazeri,M.Ott,\n S. Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. ar Xiv\n preprintar Xiv:2304.11277,2023. \n [78] N.Dorka, C.Huang, T.Welschehold, and W.Burgard. Whatmattersinemployingvision\n languagemodelsfortokenizingactionsinrobotcontrol? In First Workshopon Vision-Language\n Modelsfor Navigationand Manipulationat ICRA 2024. \n \n 17 \n \n "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n \n \n [79] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-\n training. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n pages 11975–11986,2023. \n [80] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell,\n P.Mishkin,J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervi-\n sion. In Internationalconferenceonmachinelearning,pages 8748–8763.PMLR,2021.\n [81] P.Sharma,N.Ding,S.Goodman,and R.Soricut.Conceptualcaptions:Acleaned,hypernymed,\n image alt-text dataset for automatic image captioning. In Proceedings of the 56 th Annual\n Meetingofthe Associationfor Computational Linguistics(Volume 1: Long Papers),pages\n 2556–2565,2018. \n \n [82] C.Schuhmann,R.Vencu,R.Beaumont,R.Kaczmarczyk,C.Mullis,A.Katta,T.Coombes,\n J.Jitsev,and A.Komatsuzaki.Laion-400 m:Opendatasetofclip-filtered 400 millionimage-text\n pairs. ar Xivpreprintar Xiv:2111.02114,2021. \n [83] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: a dataset for image captioning\n withreadingcomprehension. In Computer Vision–ECCV 2020: 16 th European Conference,\n Glasgow,UK,August 23–28,2020,Proceedings,Part II 16,pages 742–758.Springer,2020.\n \n [84] H.Face. Introducingidefics: Anopenreproductionofstate-of-the-artvisuallangagemodel.\n Hugging Face Blog,2024. \n [85] H.Liu,C.Li,Q.Wu,and Y.J.Lee. Visualinstructiontuning. Advancesinneuralinformation\n processingsystems,36,2024. \n \n [86] B.Mc Kinzie,Z.Gan,J.-P.Fauconnier,S.Dodge,B.Zhang,P.Dufter,D.Shah,X.Du,F.Peng,\n F.Weers,etal. Mm 1: Methods,analysis&insightsfrommultimodalllmpre-training. ar Xiv\n preprintar Xiv:2403.09611,2024. \n [87] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and\n S.Han. Vila: Onpre-trainingforvisuallanguagemodels. ar Xivpreprintar Xiv:2312.07533,\n 2023. \n [88] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt 3. int 8 (): 8-bit matrix multi-\n plicationfortransformersatscale. Advancesin Neural Information Processing Systems,35:\n 30318–30332,2022. \n \n [89] NVIDIA. Tensorrt-llm. URLhttps://github.com/NVIDIA/Tensor RT-LLM.\n [90] T.Z.Zhao,V.Kumar,S.Levine,and C.Finn. Learningfine-grainedbimanualmanipulation\n withlow-costhardware. ar Xivpreprintar Xiv:2304.13705,2023. \n \n [91] Y.Leviathan,M.Kalman,and Y.Matias. Fastinferencefromtransformersviaspeculative\n decoding. In International Conferenceon Machine Learning,pages 19274–19286.PMLR,\n 2023. \n [92] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-\n man,A.Herzog,J.Hsu,etal. Rt-1: Roboticstransformerforreal-worldcontrolatscale. ar Xiv\n preprintar Xiv:2212.06817,2022. \n \n [93] E.Rosete-Beas,O.Mees,G.Kalweit,J.Boedecker,and W.Burgard. Latentplansfortask\n agnostic offline reinforcement learning. In Proceedings of the 6 th Conference on Robot\n Learning(Co RL),2022. \n [94] O.Mees,J.Borja-Diaz,and W.Burgard. Groundinglanguagewithvisualaffordancesover\n unstructured data. In Proceedings of the IEEE International Conference on Robotics and\n Automation(ICRA),London,UK,2023. \n \n 18 \n \n "
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n \n \n [95] S.Dass,J.Yapeter,J.Zhang,J.Zhang,K.Pertsch,S.Nikolaidis,and J.J.Lim. CLVRjaco\n playdataset,2023. URLhttps://github.com/clvrai/clvr_jaco_play_dataset.\n [96] J.Luo,C.Xu,X.Geng,G.Feng,K.Fang,L.Tan,S.Schaal,and S.Levine. Multi-stagecable\n routingthroughhierarchicalimitationlearning. ar Xivpreprintar Xiv:2307.08927,2023.\n \n [97] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta,\n E.Orbay,S.Savarese,and L.Fei-Fei. Robo Turk: Acrowdsourcingplatformforroboticskill\n learningthroughimitation. Co RR,abs/1811.02790,2018. URLhttp://arxiv.org/abs/\n 1811.02790. \n [98] Y.Zhu,A.Joshi,P.Stone,and Y.Zhu. Viola: Imitationlearningforvision-basedmanipulation\n withobjectproposalpriors,2023. \n \n [99] L. Y. Chen, S. Adebola, and K. Goldberg. Berkeley UR 5 demonstration dataset. https:\n //sites.google.com/view/berkeley-ur 5/home. \n [100] G.Zhou,V.Dean,M.K.Srirama,A.Rajeswaran,J.Pari,K.Hatch,A.Jain,T.Yu,P.Abbeel,\n L.Pinto,C.Finn,and A.Gupta. Trainoffline,testonline: Arealrobotlearningbenchmark,\n 2023. \n \n [101] C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,and P.Florence.\n Interactivelanguage: Talkingtorobotsinrealtime. IEEERoboticsand Automation Letters,\n 2023. \n [102] S.Belkhale,Y.Cui,and D.Sadigh. Hydra: Hybridrobotactionsforimitationlearning. arxiv,\n 2023. \n \n [103] Y.Zhu,P.Stone,and Y.Zhu. Bottom-upskilldiscoveryfromunsegmenteddemonstrationsfor\n long-horizonrobotmanipulation. IEEERoboticsand Automation Letters,7(2):4126–4133,\n 2022. \n [104] Z. J. Cui, Y. Wang, N. M. M. Shafiullah, and L. Pinto. From play to policy: Conditional\n behaviorgenerationfromuncuratedrobotdata. ar Xivpreprintar Xiv:2210.10047,2022.\n \n [105] M.Heo,Y.Lee,D.Lee,and J.J.Lim. Furniturebench: Reproduciblereal-worldbenchmark\n forlong-horizoncomplexmanipulation. In Robotics: Scienceand Systems,2023.\n [106] G.Yan,K.Wu,and X.Wang. ucsdkitchens Dataset. August 2023. \n \n [107] S.Nasiriany,T.Gao,A.Mandlekar,and Y.Zhu. Learningandretrievalfrompriordatafor\n skill-basedimitationlearning. In Conferenceon Robot Learning(Co RL),2022.\n \n [108] H.Liu,S.Nasiriany,L.Zhang,Z.Bao,and Y.Zhu. Robotlearningonthejob: Human-in-\n the-loopautonomyandlearningduringdeployment. In Robotics: Scienceand Systems(RSS),\n 2023. \n [109] G.Quere,A.Hagengruber,M.Iskandar,S.Bustamante,D.Leidner,F.Stulp,and J.Vogel.\n Shared Control Templatesfor Assistive Robotics. In 2020 IEEEInternational Conferenceon\n Roboticsand Automation(ICRA),page 7,Paris,France,2020. \n [110] S. Saxena, M. Sharma, and O. Kroemer. Multi-resolution sensing for real-time control\n with vision-language models. In 7 th Annual Conference on Robot Learning, 2023. URL\n https://openreview.net/forum?id=Wu Bv 9-IGDUA. \n \n [111] R.Shah,R.Martín-Martín,and Y.Zhu. MUTEX:Learningunifiedpoliciesfrommultimodal\n task specifications. In 7 th Annual Conference on Robot Learning, 2023. URL https:\n //openreview.net/forum?id=Pwqiqaa Ez J. \n \n 19 \n \n "
  },
  {
    "page_num": 20,
    "text": " \n \n \n \n \n \n [112] X.Zhu,R.Tian,C.Xu,M.Ding,W.Zhan,and M.Tomizuka. Fanucmanipulation: Adataset\n forlearning-basedmanipulationwithfanucmate 200 idrobot. 2023. \n [113] R.Mendonca,S.Bahl,and D.Pathak. Structuredworldmodelsfromhumanvideos. Co RL,\n 2023. \n \n [114] J.Luo, C.Xu, F.Liu, L. Tan, Z. Lin, J.Wu, P. Abbeel, and S. Levine. Fmb: a functional\n manipulationbenchmarkforgeneralizableroboticlearning. ar Xivpreprintar Xiv:2401.08553,\n 2024. \n [115] N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chintala, and L. Pinto. On\n bringingrobotshome,2023. \n [116] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone. Libero: Benchmarking\n knowledgetransferforlifelongrobotlearning. Advancesin Neural Information Processing\n Systems,36,2024. \n \n [117] V.Sanh,L.Debut,J.Chaumond,and T.Wolf. Distilbert,adistilledversionofbert: smaller,\n faster,cheaperandlighter. ar Xivpreprintar Xiv:1910.01108,2019. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 20 \n \n "
  },
  {
    "page_num": 21,
    "text": " \n \n \n \n \n \n A Data Mixture Details \n We list our used data mixture in Table 3. The mixture mostly follows [5], with a few additional\n datasets. \n \n Open VLATraining Dataset Mixture \n Fractal[92] 12.7% \n Kuka[45] 12.7% \n Bridge[6,47] 13.3% \n Taco Play[93,94] 3.0% \n Jaco Play[95] 0.4% \n Berkeley Cable Routing[96] 0.2% \n Roboturk[97] 2.3% \n Viola[98] 0.9% \n Berkeley Autolab UR 5[99] 1.2% \n Toto[100] 2.0% \n Language Table[101] 4.4% \n Stanford Hydra Dataset[102] 4.4% \n Austin Buds Dataset[103] 0.2% \n NYUFranka Play Dataset[104] 0.8% \n Furniture Bench Dataset[105] 2.4% \n UCSDKitchen Dataset[106] <0.1% \n Austin Sailor Dataset[107] 2.2% \n Austin Sirius Dataset[108] 1.7% \n DLREDANShared Control[109] <0.1% \n IAMLab CMUPickup Insert[110] 0.9% \n UTAustin Mutex[111] 2.2% \n Berkeley Fanuc Manipulation[112] 0.7% \n CMUStretch[113] 0.2% \n BC-Z[55] 7.5% \n FMBDataset[114] 7.1% \n Dobb E[115] 1.4% \n DROID[11] 10.0%6 \n Table 3:Open VLAtrainingdatamixtureusingdatasetsfromthe Open X-Embodimentdataset[1],following[5]\n withafewadditions. \n B Evaluation Tasksand Detailed Results \n Inthissection,weprovidemoredetailsonthe Bridge Data V 2 Widow Xand Googlerobotevaluations\n discussedin Section 5.1,aswellasthe Franka-Tabletopand Franka-DROIDfine-tuningevaluations\n discussedin Section 5.2. \n B.1 Bridge Data V 2 Widow XEvaluation Details \n Herewefocusspecificallyon Bridge Data V 2 evaluationsdiscussedin Section 5.1.\n B.1.1 Bridge Data V 2 Evaluation Tasks \n Asdescribedin Section 5.1,weevaluateeachgeneralistrobotmanipulationpolicyon 17 taskswith\n 10 trialseach. Inthissection,weprovidedetailsonthetaskcategoriesandindividualtasks.\n In total, we evaluate on 5 visual generalization tasks, 2 motion generalization tasks, 3 physical\n generalizationtasks,4 semanticgeneralizationtasks,and 3 languagegroundingtasks. Notethatall\n tasksweevaluateonintroducesomeformofdistributionshiftsinceweareunabletoprocurethe\n exactobjectsusedintheoriginaldataset(otherdistributionshiftsnaturallyariseaswereproducea\n real-worldtestenvironmentoriginallyconstructedatadifferentlocation;see Appendix B.1.2 fora\n detaileddiscussiononsuchdistributionshifts). All 17 tasksaredepictedin Fig.7. Eachrolloutis\n 6 Weremove DROIDforthelastthirdoftrainingduetoslowlearningprogress(see Section 3.3)andre-\n distributeitsmixtureweightsacrossallotherdatasets. \n 21 \n \n "
  },
  {
    "page_num": 22,
    "text": " \n \n \n \n \n \n Put Eggplant into Pot (Easy Version) Put Eggplant into Pot\n \n \n \n Put Cup from Counter into Sink Put Eggplant into Pot (w/ Clutter)\n \n \n \n Put Yellow Corn on Pink Plate \n \n \n \n Lift Eggplant Put Carrot on Plate (w/ Height Change)\n \n \n \n Put Carrot on Plate Flip Pot Upright \n \n \n Lift AAA Battery \n \n \n \n Move Skull into Drying Rack Lift White Tape \n \n \n \n Take Purple Grapes out of Pot Stack Blue Cup on Pink Cup\n \n \n \n Put Eggplant into Pot Put Red Bottle into Pot \n \n \n \n Lift Cheese Lift Red Chili Pepper \n \n \n \n Put Blue Cup on Plate Put Pink Cup on Plate \n .ne G \n lausi V \n .ne G \n noito M \n .ne G \n lacisyh P \n .ne G \n citname S \n gnidnuor G \n egaugna L \n Figure 7: Bridge Data V 2 Widow Xrobotevaluationtasks. Weevaluateeverygeneralistrobotpolicyon 4\n typesout-of-distribution(OOD)generalizationtasks: visual,motion,physical,andsemantic(asdefinedin\n Section 5.1).Everypairofimagesshowsthestartstateandanexampleendstateaftertherobotcompletesthe\n task.Wealsorigorouslyassesslanguagegroundinginthe 3 tasksshowninthebottom 3 rows,bychangingthe\n promptwhilefixingtheinitialstateandtestingwhetherthepolicycanapproachthecorrecttargetobject.\n 22 "
  },
  {
    "page_num": 23,
    "text": " \n \n \n \n \n \n markedasafailure(0)orsuccess(1). Insomemoredifficulttasks,werecordpartialsuccesses(0.5);\n wedescribetheconditionsforpartialcreditinthetaskdescriptionsbelow. \n Belowwedescribeeachofthe 17 tasks,intheordershownin Fig.7: \n 1. Put Eggplantinto Pot(Easy Version): Therobot’sgoalistopickuptheeggplantand\n dropitintothepot. Thisisavisualgeneralizationtaskbecauseweuseahandcraftedpaper\n potthathasadifferentappearancethanthepotusedintheoriginal Bridge Data V 2 training\n dataset (since we are unable to procure the original pot). Unlike all 16 other tasks, for\n thisparticulartaskweinitializetherobot’send-effectordirectlyabovetheeggplantbefore\n rollingoutthepolicy;hence,wecallthisthe“Easy Version”ofthe“Put Eggplantinto Pot”\n task. \n 2. Put Eggplantinto Pot: Thisisthesametaskasdescribedabove,exceptthattherobot’s\n end-effector is not initialized directly above the eggplant. Instead, we initialize it in a\n positionthatisfixedacrossallrollouts,whichmeansthattherobotmusthorizontallyreach\n for the eggplant first before manipulating it. (Note: The same applies to all other tasks\n describedbelow.) Thisisavisualgeneralizationtaskforthesamereasonasabove.\n 3. Put Cupfrom Counterinto Sink: Therobot’sgoalistopickupthepinkcupfromeither\n thekitchencountertopordryingrackandplaceitintothesinkontheright. Thisisavisual\n generalizationtaskbecauseweuseapinkcupratherthanabluecup(abluecupisusedin\n theoriginal Bridge Data V 2 dataset,butwefindthatnoneofthemethodsweevaluateisable\n tomanipulateitreliably–mostlikelybecausethecolorofthecupblendsinwiththecolor\n ofthesink). \n 4. Put Eggplantinto Pot(w/Clutter): Thisisthesametaskasthe“Put Eggplantinto Pot”\n task,exceptthatitismoredifficultduetothepresenceofseveraldistractorobjects. Itisa\n visualgeneralizationtaskforthesamereasondiscussedinthenormal“Put Eggplantinto\n Pot”task,andevenmoresogivenunseendistractorsinthescene. Partialcredit(0.5 outof\n 1)isrewardedwhentherobotmovestowardsthecorrecttargetobject. \n 5. Put Yellow Cornon Pink Plate: Therobot’sgoalistopickuptheyellowcornandplace\n it on the pink plate. This is a visual generalization task due to the presence of unseen\n distractorobjectsinthescene,suchasagreendinosauronthecountertopinthebacksection\n of the sink. Partial credit (0.5 out of 1) is rewarded when the robot moves towards the\n correcttargetobject. \n 6. Lift Eggplant:Therobot’sgoalistograspandlifttheeggplantintotheair. Thisisamotion\n generalizationtaskbecausetheeggplantisinitializedinunseenpositionsand/ororienta-\n tions,andtherobotisforcedtomovebeyonditstrainingdistributionofpositionsand/or\n orientationsandoftenperformlong-rangereachinginordertocompletethetask. (Note:\n Long-rangereachingisnotdemonstratedinthisenvironmentintheoriginal Bridge Data V 2\n demonstrations;see Appendix B.1.2 fordetails.) Wefindthatthistask,thoughseemingly\n simple,isdeceptivelychallengingformanypolicies. Partialcredit(0.5 outof 1)isrewarded\n whentherobotmakescontactwiththeeggplant. \n 7. Put Carroton Plate(w/Height Change):Therobot’sgoalistopickupthecarrotandplace\n itontheyellowplate. Thisisamotiongeneralizationtaskbecausetheplateiselevated\n fromitsusualpositionatthebottomofthesink, andtherobotmustadjustitstrajectory\n tocorrectlyplacethecarrotontheelevatedplatform(withoutknockingdowntheplatein\n theprocess). Partialcredit(0.5 outof 1)isrewardedwhentherobotgraspsthecarrotand\n touchestheplatewithit. \n 8. Put Carroton Plate: Thisisthesametaskasabove,exceptthattheplateisatitsnormal\n position(atthebottomofthesinkordryingrack). Weconsiderthisaphysicalgeneraliza-\n tiontaskbecausethecarrothasadifferentsizeandshapethantheoneusedintheoriginal\n Bridge Data V 2 dataset,whichisshorterandnarrower.(Notethatthepreviousversionofthis\n tasklistedabovewouldalsotechnicallybeaphysicalgeneralizationtasksinceitinvolves\n thesamecarrot,butwelistitunderthe“motiongeneralization”categorysincethatisthe\n focusthere.) \n 23 \n "
  },
  {
    "page_num": 24,
    "text": " \n \n \n \n \n \n 9. Flip Pot Upright: Therobot’sgoalistomanipulatethepotsuchthatitisorientedupright\n inthesinkattheendoftheepisode. Thisisaphysicalgeneralizationtaskbecausethis\n pothasadifferentsizeandshapethantheoneusedintheoriginal Bridge Data V 2 training\n demonstrations(thepotweuseiswiderandshorter). \n 10. Lift AAABattery:Therobot’sgoalissimplytograspthe AAAbatteryandliftitupintothe\n air. Thisisconsideredaphysicalgeneralizationtaskbecausethebatteryismuchsmaller\n andthinnerthantargetobjectsseeninthe Bridge Data V 2 trainingdemonstrationsinthis\n environment;see Appendix B.1.2 fordetails. (Notethatthistargetobjectdoesnotexistin\n theoriginal Bridge Data V 2 demonstrationsinthisenvironment,sothisisalsoaninstanceof\n “semanticgeneralization”,butweclassifyitsolelyas“physicalgeneralization”sincethatis\n themainfocushere). \n 11. Move Skullinto Drying Rack: Therobot’sgoalistograsptheskullwinduptoyanddrop\n itintotheyellowdryingrackintheleftpartofthesink. Thisisasemanticgeneralization\n tasksincetheskullisanunseentargetobject(doesnotappearinthe Bridge Data V 2 training\n demonstrations). \n 12. Lift White Tape: Therobot’sgoalistograspandliftthewhiterolloftapeintotheair.\n Thisisasemanticgeneralizationtasksincethewhitetaperollisanunseentargetobject\n (doesnotappearinthe Bridge Data V 2 trainingdemonstrations). (Notethatthistaskmay\n alsobeconsideredas“physicalgeneralization”becauseofitsshapebeingdifferentthanthe\n objectsseeninthetrainingdemonstrationsinthisenvironment;mostpoliciesstruggleto\n graspobjectswiththisringstructure,andtheyoftenmovetherobot’send-effectordirectly\n intothecenterregion.) \n 13. Take Purple Grapesoutof Pot: Therobot’sgoalistograspthepurplegrapeslyinginside\n thesteelpotandremoveitfromthepot(byliftingitoutand/ordroppingitanywhereoutside\n thepot).Thisisasemanticgeneralizationtaskbecauseitisanunseenlanguageinstruction;\n therobothasneverseenthistaskintheoriginal Bridge Data V 2 trainingdataset.\n 14. Stack Blue Cupon Pink Cup:Therobot’sgoalistograspthebluecupandplaceitsecurely\n on top of the pink cup. This is a semantic generalization task because it is an unseen\n languageinstruction;therobothasneverseenthistaskinthisenvironmentintheoriginal\n Bridge Data V 2 trainingdataset. Partialcredit(0.5 outof 1)isrewardedwhentherobot\n graspsthebluecupandtouchesthepinkcupwiththebluecup. \n 15. Put{Eggplant,Red Bottle}into Pot: Thisisalanguagegroundingtask. Therobot’sgoal\n istoputthespecifiedtargetobjectintothepot. Boththeeggplantandredbottlearepresent\n inthescene. Weconductpairedevaluations: forthesameinitialstate,wepromptthepolicy\n totargettheeggplantinoneepisode,andthentheredbottleinthenextepisode. Wetest\n eachmethod 5 timeswiththeeggplantand 5 timeswiththeredbottle,usingthesameset\n of 5 initialstatesforbothtargetobjects. Partialcredit(0.5 outof 1)isrewardedwhenthe\n robotmovestowardsthecorrecttargetobject. \n 16. Lift{Cheese,Red Chili Pepper}: Thisisalanguagegroundingtask. Therobot’sgoalis\n tograspandliftthespecifiedtargetobject. Weconductpairedevaluationsasdescribedin\n thetaskabove. Partialcredit(0.5 outof 1)isrewardedwhentherobotmovestowardsthe\n correcttargetobject. \n 17. Put {Blue Cup, Pink Cup} on Plate: This is a language grounding task. The robot’s\n goalistograspthespecifiedtargetobjectandplaceitontotheplate. Weconductpaired\n evaluationsasdescribedinotherlanguagegroundingtasks. Partialcredit(0.5 outof 1)is\n rewardedwhentherobotmovestowardsthecorrecttargetobject. \n B.1.2 Comparing Evaluation Tasksto Original Bridge Data V 2 Training Data \n Weconductourevaluationsinasinkenvironmentusedintheoriginal Bridge Data V 2 dataset[6].\n Wereproducetheenvironmenttomatchtheoriginalenvironmentinthe Bridge Data V 2 datasetwith\n roughapproximationsfortherobot’slocationrelativetothesink,aswellasthecamera’splacement\n \n 24 \n \n "
  },
  {
    "page_num": 25,
    "text": " \n \n \n \n \n \n relativetothescene. Giventhelackofprecisemeasurementsofthesepositionsintheoriginaldataset,\n weareunabletoreproducetheexactenvironmentsetup,andnaturaldistributionshiftsarisedueto\n slightlydifferentrobot,sink,andcameraplacements. Inaddition,sinceweevaluaterobotpolicies\n in a different location than where the training demonstrations were collected from, other natural\n distributionshiftsarise. Forexample, thelightingconditionsandbackground(e.g., visibleareas\n behindthesink)areinevitablydifferentthanwhatwasseeninthetrainingdataset. Furthermore,we\n areunabletoprocuretheexactsetofobjectsusedintheoriginal Bridge Data V 2 dataset,sothereare\n distributionshiftsbetweentheobjectsusedattraintimeandthoseusedattesttime.\n Despiteallthesechallenges,wefindthatcertaingeneralistpolicies,suchas Open VLAand RT-2-X,\n can still generalize and perform various tasks fairly reliably “out-of-the-box”. Other generalist\n policies,suchas RT-1-Xand Octo,canalsocompletesometasks,thoughtheystrugglewhentested\n withmoredifficultgeneralizationtasksinour Bridge Data V 2 evaluationsuite.\n The original Bridge Data V 2 dataset includes demonstrations of the following seven tasks in this\n specificsinkenvironment: “Flip Pot Upright”,“Put Carroton Plate”,“Put Cupfrom Counter(or\n Drying Rack)into Sink”,“Put Eggplantinto Pot”,“Put Knifeon Cutting Board”,“Put Spoonin Pot”,\n and“Turn Lever Verticalto Front”. See Fig.8 forsamplesimagesofallthesetasksfromtheoriginal\n dataset. Notethatalltrainingdemonstrationscollectedinthisenvironmentareinitializedsuchthat\n therobot’send-effectorispositioneddirectlyabovethetargetobjectinthebeginningoftheepisode.\n (However,thisisnotthecaseacrossallenvironmentsinthe Bridge Data V 2 dataset;insomeother\n environments,therobotisinitializedfartherawayfromthetargetobject,soitmusthorizontallyreach\n fortheobjectfirstbeforemanipulatingit.) \n \n Flip Pot Upright Put Carrot on Plate Put Cup from Counter into Sink\n \n \n Turn Lever Vertical to Front\n \n \n \n Put Eggplant into Pot Put Knife on Cutting Board Put Spoon in Pot \n \n \n \n \n \n Figure 8:Original Bridge Data V 2 sinkenvironmenttasks.Imagesfromsampledemonstrationsinthesink\n environmentfromtheoriginal Bridge Data V 2 datasetrevealthatalldemonstrationsinthisenvironmentwere\n initializedsuchthattherobot’send-effectorwaspositionedimmediatelyabovethetargetobject.Notethatthese\n initialstatesaredifferentfromtheinitialstatesweuseinour Bridge Data V 2 evaluationtasksshownin Fig.7.\n Inourevaluations,wealwaysinitializetherobot’send-effectortoafixedlocationabovethesink,ratherthan\n positioningitdirectlyabovethetargetobject(exceptforonetask:“Put Eggplantinto Pot(Easy Version)”).\n Inour Bridge Data V 2 evaluationsuite,onlyonetask–“Put Eggplantinto Pot(Easy Version”)–is\n initializedwiththerobot’send-effectorhoveringdirectlyoverthetargetobject;inall 16 othertasks,\n theend-effectorisinitializedatafixedlocationabovethesinksuchthattherobotmusthorizontally\n reach towards the object. This initial condition, in combination with the distribution shifts we\n introduceinthevarioustypesof OODgeneralizationinourevaluationsuite,challengesthegeneralist\n policiesandrequiresahighdegreeofrobustnessinordertocompletethetaskssuccessfully. Hence,\n thesuccessratesforpolicieslike RT-1-Xand Octoarelowerthanwhatisreportedinpriorworks.\n However,wefindthatotherpoliciessuchas RT-2-Xand Open VLAstillachieverelativelystrong\n performancedespiteallthesedistributionshiftsandchallenges. \n \n 25 \n \n "
  },
  {
    "page_num": 26,
    "text": " \n \n \n \n \n \n B.1.3 Detailed Bridge Data V 2 Evaluation Results \n See Table 4 forthefull Bridge Data V 2 Widow Xevaluationresults. Thenumberofsuccessesforeach\n method,outof 10 trials,islistedforeachof 17 tasks. Open VLAachievesstrongestperformancein\n themajorityofthetasksandhasthehighestaggregatesuccessrateamongthegeneralistpolicies.\n RT-2-Xalsoshowsgoodperformance,outperforming RT-1-Xand Octo,thoughitdoesnotperform\n aswellas Open VLA.RT-1-Xand Octogenerallyexperiencedifficultyinthesegeneralizationtasks.\n \n Table 4:Detailed Bridge Data V 2 Widow Xevaluationresults.Wereportperformanceonthefullevaluation\n suiteof 17 tasks(discussedin Section 5.1),includingvisual/motion/physical/semanticgeneralizationtasksand\n languagegroundingtasks.Notethatpartialsuccess(scoreof 0.5)ispossibleforsometasks;see Appendix B.1.1\n fordetails. Wefindthat Open VLAperformsbestinmosttasksandachieveshighestperformanceoverall,\n followedby RT-2-X.Ontheotherhand,RT-1-Xand Octostruggleintheevaluations,onlygetting 0–2 successes\n inseveraltasks.See Fig.7 forillustrationsofalltasks. \n RT-1-X Octo RT-2-X Open VLA(ours) \n Category Task #Trials \n #Successes #Successes #Successes #Successes\n Visualgen Put Eggplantinto Pot(Easy Version) 10 1 5 7 10 \n Visualgen Put Eggplantinto Pot 10 0 1 5 10 \n Visualgen Put Cupfrom Counterinto Sink 10 1 1 0 7 \n Visualgen Put Eggplantinto Pot(w/Clutter) 10 1 3.5 6 7.5 \n Visualgen Put Yellow Cornon Pink Plate 10 1 4 8 9 \n Motiongen Lift Eggplant 10 3 0.5 6.5 7.5 \n Motiongen Put Carroton Plate(w/Height Change) 10 2 1 4.5 4.5 \n Physicalgen Put Carroton Plate 10 1 0 1 8 \n Physicalgen Flip Pot Upright 10 2 6 5 8 \n Physicalgen Lift AAABattery 10 0 0 2 7 \n Semanticgen Move Skullinto Drying Rack 10 1 0 5 5 \n Semanticgen Lift White Tape 10 3 0 0 1 \n Semanticgen Take Purple Grapesoutof Pot 10 6 0 5 4 \n Semanticgen Stack Blue Cupon Pink Cup 10 0.5 0 5.5 4.5 \n Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 2.5 4 8.5 7.5 \n Languagegrounding Lift{Cheese,Red Chili Pepper} 10 1.5 2.5 8.5 10 \n Languagegrounding Put{Blue Cup,Pink Cup}on Plate 10 5 5.5 8.5 9.5 \n Mean Success Rate 18.5±2.7% 20.0±2.6% 50.6±3.5% 70.6±3.2%\n Additionally,in Table 5,weprovidethefullevaluationresultsforthequantizedinferenceexperi-\n mentsthatweresummarizedin Table 2. Fortheseevaluations,wetestpolicieson 8 representative\n Bridge Data V 2 tasksspanningalltaskcategoriesinthefullevaluationsuite. \n Table 5:Fullquantizedinferenceresults.Herewepresentthedetailedversionoftheresultsshownin Table 2.\n bfloat 16 int 8 int 4 \n Category Task #Trials \n #Successes #Successes #Successes \n Visualgen Put Eggplantinto Pot(Easy Version) 10 9 7 9 \n Visualgen Put Eggplantinto Pot 10 7 7 7 \n Visualgen Put Cupfrom Counterinto Sink 10 5 3 7 \n Motiongen Lift Eggplant 10 6 4 7.5 \n Physicalgen Put Carroton Plate 10 6 5 7 \n Physicalgen Lift AAABattery 10 7 5 3 \n Semanticgen Take Purple Grapesoutof Pot 10 8 8 9 \n Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 9 7.5 8 \n Mean Success Rate 71.3±4.8% 58.1±5.1% 71.9±4.7%\n B.2 Google Robot Evaluation Details \n Inthissection,weprovidemoredetailsonthe Googlerobotevaluationsintroducedin Section 5.1.\n B.2.1 Google Robot Evaluation Tasks \n Onthe Googlerobot,weevaluateeachgeneralistrobotpolicyon 12 taskswith 5 rolloutseach,fora\n totalof 60 rollouts. Thefirstfivetaskstestonin-distributionconditions,andthelastseventaskstest\n onmoredifficultout-of-distribution(OOD)conditions. Alltasksaredepictedin Fig.9. Eachrollout\n ismarkedasafailure(0)orsuccess(1). \n Wedescribethe 12 tasksbelow: \n 1. Pick Coke Can(in-distribution): Therobotispositionedinfrontofaplatformwithacan\n of Cokeontopofit. Therobot’sgoalistograspandliftthe Cokecan. \n 26 "
  },
  {
    "page_num": 27,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 9: Googlerobotevaluationtasks. Weevaluateeverygeneralistrobotpolicyonin-distributiontasks\n andout-of-distribution(OOD)generalizationtasks. OODtasksinvolveunseenbackgrounds,targetobjects,\n instructions/objectrelations,andsemanticconcepts(e.g.,photosfromthe Internetthatdonotappearinrobot\n actiondata). \n \n 2. Move Applenear Green Can(in-distribution):Therobotispositionedinfrontofaplatform\n withanappleandagreensodacanontopofit. Therobot’sgoalistograsptheappleand\n moveitnexttothegreencan. \n 3. Move Blue Chip Bagnear Apple(in-distribution): Therobotispositionedinfrontofa\n platformwithabluebagofchipsandanappleontopofit. Therobot’sgoalistograspthe\n bluebagofchipsandmoveitclosetotheapple. \n \n 4. Place Coke Can Upright(in-distribution): Therobotispositionedinfrontofaplatform\n withacanof Cokeontopofit,andthecanisorientedhorizontallyonitsside. Therobot’s\n goalistograspthe Cokecanandorientittobeinaverticalposition. \n 5. Open Middle Drawer(in-distribution): Therobotispositionedinfrontofasetofthree\n drawers. Therobot’sgoalistograspthemiddledrawerhandleandpullthedraweropen.\n 6. Move Orangenear Brown Chip Bag(OOD):Therobotispositionedinfrontofaplatform\n withabrownbagofchipsandanorangeontopofit. Atableclothwithblueskyandwhite\n cloudpatternscoverstheplatformunderneaththeobjects. Therobot’sgoalistograspthe\n orangeandbringitnexttothebagofchips. Thistaskis OODbecausetheorangeisan\n unseenobjectrelativetothetrainingdataset,andthetableclothisanunseenbackground.7\n \n 7 See Appendixof Brohanetal.[7]foradetailedlistof OODconditionsin Googlerobotevaluations.\n \n 27 \n \n "
  },
  {
    "page_num": 28,
    "text": " \n \n \n \n \n \n 7. Pick Pepsi Can(OOD):Therobotispositionedinfrontofaplatformwithacanof Pepsi\n ontopofit. Atableclothwithbrightyellow/brownpatternscoverstheplatformunderneath\n thecan. Therobot’sgoalistograspandliftthecan. Thistaskis OODbecausethe Pepsi\n canisanunseenobject,andthetableclothisanunseenbackground. \n 8. Pick Banana(OOD):Therobotispositionedinfrontofaplatformwithanapple,acan\n of Coke,andabanana. Therobot’sgoalistograspandliftthebanana. Thistaskis OOD\n becausethebananaisanunseentargetobject. \n 9. Pick Green Cup(OOD):Therobotispositionedinfrontofaplatformwithabanana,acan\n of Pepsi,andagreencup. Therobot’sgoalistograspandliftthegreencup. Thistaskis\n OODbecauseallobjectsinthesceneareunseeninthetrainingdata. \n 10. Place Appleon Plate(OOD):Therobotispositionedinfrontofaplatformwithaplateand\n anapple. Therobot’sgoalistograsptheappleandmoveitontotheplate. Thistaskis OOD\n becauseitisanovelinstructiondescribinganunseenobjectrelation:trainingdemonstrations\n onlycovermovingtheappleneartheplate,ratherthanplacingitontopof theplate.\n \n 11. Place Bananain Pan(OOD):Therobotispositionedinfrontofaplatformwithapanand\n abanana. Therobot’sgoalistograspthebananaandmoveitintothepan. Thistaskis OOD\n becausethebananaisanunseentargetobject,anditisanovelinstructiondescribingan\n unseenobjectrelation,asexplainedintheprevioustask. \n 12. Move Coke Canto Taylor Swift(OOD):Therobotispositionedinfrontofaplatformwith\n acanof Cokeandphotosofthreedifferentcelebrities,including Taylor Swift. Therobot’s\n goalistograspthecanandmoveittothephotoof Taylor Swift. Thistaskis OODbecause\n thephotosofthecelebritiesareunseenintherobotinteractiondata. \n B.2.2 Detailed Google Robot Evaluation Results \n \n \n Table 6:Detailed Googlerobotevaluationresults.Wereportfullevaluationresultsfor Googlerobotevaluations\n discussedin Section 5.1. Eachgeneralistpolicyisevaluatedwith 60 rolloutsacross 12 tasks,coveringboth\n in-distributionandout-of-distribution(OOD)testingconditions.Inthebottomrow,wereportmeansuccessrate\n ±Std Errforeachpolicy.Open VLAand RT-2-Xbothsignificantlyoutperform RT-1-Xand Octooverall(we\n boldthemeansuccessrateforbothduetooverlappingerrorbars).See Fig.9 forillustrationsofalltasks.\n RT-1-X Octo RT-2-X Open VLA(ours) \n Category Task #Trials \n #Successes #Successes #Successes #Successes\n In-distribution Pick Coke Can 5 5 1 5 5 \n In-distribution Move Applenear Green Can 5 3 3 3 5 \n In-distribution Move Blue Chip Bagnear Apple 5 0 3 4 5 \n In-distribution Place Coke Can Upright 5 0 0 4 4 \n In-distribution Open Middle Drawer 5 0 4 2 3 \n OOD Move Orangenear Brown Chip Bag 5 1 2 5 5 \n OOD Pick Pepsi Can 5 3 0 5 4 \n OOD Pick Banana 5 5 3 5 5 \n OOD Pick Green Cup 5 1 0 5 5 \n OOD Place Appleon Plate 5 0 0 4 4 \n OOD Place Bananain Pan 5 0 0 2 4 \n OOD Move Coke Cannear Taylor Swift 5 2 0 3 2 \n Mean Success Rate 33.3±6.1% 26.7±5.8% 78.3±5.4% 85.0±4.6%\n Fullresultsforthe Googlerobotevaluationsareshownin Table 6. Overall,wefindthat RT-1-Xand\n Octoexperiencedifficultyontheevaluationtasks;theyareoftenunabletoachieveasinglesuccess\n out of five trials in several tasks. On the other hand, RT-2-X and Open VLA demonstrate strong\n performance, completing every task at least two times out of five trials; these two VLA policies\n performcomparablywitheachotheronthisparticularevaluationsuite. \n B.3 Data-Efficient Adaptation Experiment Details \n Inthissection,weprovidemoredetailsonthedata-efficientadaptationexperimentsdiscussedin\n Section 5.2,whereweinvestigatetheeffectivenessoffine-tuned Open VLApoliciesonnewrobot\n setupssuchas Franka-Tabletopand Franka-DROID. \n 28 "
  },
  {
    "page_num": 29,
    "text": " \n \n \n \n \n \n B.3.1 Franka-Tabletopand Franka-DROIDTasks \n Wecollect 10–150 demonstrationsofeachofseventasks. Thefirstsixtaskscorrespondtoarobot\n setupwhichwedenoteas“Franka-Tabletop”(Franka Emika Pandarobotmountedontopofatable),\n andthefinaltaskcorrespondstoarobotsetupwhichwecall“Franka-DROID”. \n Inthe Franka-Tabletopsetup,thefirstthreeofsixtaskscorrespondtosingle-instructiontasksand\n arenarrow,whilethelastthreetaskscorrespondtomulti-instructiontasksinwhichmultipleobjects\n arepresentinthesceneandtherobotmustmanipulatethecorrectonedependingonthelanguage\n instruction. \n \n \n In-Distribution Out-of-Distribution \n \n \n Put Carrot in Bowl \n \n \n \n \n Pour Corn into Pot \n \n \n \n \n Flip Pot Upright \n \n \n \n \n Move <object> onto Plate \n \n \n \n \n \n Knock <object> Over \n \n \n \n \n Cover <object> with Towel \n sksa T \n noitcurtsn I-elgni S \n worra N \n sksa T \n noitcurtsn I-itlu M \n esrevi D \n Figure 10: Franka-Tabletopfine-tuningtasks. Franka-Tabletoptasksusedinthedata-efficientadaptation\n experimentsin Section 5.2 anddescribedindetailin Fig.10 aredepictedabove. Thefirstthreeofsixtasks,\n showninthetopthreerows,onlyinvolveasingleinstruction,whilethelastthreetasksinthebottomthree\n rowsinvolvemultipleobjectsandinstructions(theinstructionsspecifythetargetobjectortargetlocation).\n Thefirstcolumnshowssampleinitialstatesmatchingthetrainingdatadistribution,whilethesecondcolumn\n showsout-of-distribution(OOD)initialstates(e.g.,unseenbackgrounds,targetobjects,distractors,andobject\n positions/orientations).Everypolicyin Section 5.2 isevaluatedwith 10–12 rolloutsonin-distributiontasksand\n 5–6 rolloutson OODtasks. \n 29 "
  },
  {
    "page_num": 30,
    "text": " \n \n \n \n \n \n Belowwedescribeeachofthesix Franka-Tabletoptasksshownin Fig.10: \n 1. Put Carrotin Bowl(single-instruction): Therobot’sgoalistograspthecarrotandplaceit\n intothebowl. Wecollect 50 demonstrationsofthistaskforthetrainingdataset,randomly\n placingthecarrotandthebowlatdifferentlocationsonthetableineveryepisode.Thecarrot\n isalwaysinitializedontheleftsideofthebowl. Duringevaluation,eachtrialisrecordedas\n asuccess(1)orfailure(0);thereisnopartialcredit. \n 2. Pour Corninto Pot(single-instruction): Therobot’sgoalistograsptheredbowl,move\n towards the steel pot, and pour the contents (a yellow corn) into the pot. We collect 50\n demonstrationsofthistaskforthetrainingdataset,randomlyplacingthebowlandthepotat\n differentlocationsonthetableineveryepisode. Thebowlisalwaysinitializedontheright\n sideofthepot. Duringevaluation,eachtrialisrecordedasasuccess(1)orfailure(0);there\n isnopartialcredit. \n 3. Flip Pot Upright (single-instruction): The robot’s goal is to grasp the steel pot (which\n is initially oriented vertically), rotate it to be in the upright position, and place it back\n onto the table. We collect only 10 demonstrations of this task for the training dataset,\n randomly placing the steel pot at various locations within a small section of the table.\n Duringevaluation,eachtrialisrecordedasasuccess(1),failure(0),orpartialsuccess(0.5).\n Partialsuccessesincludegraspingthepotbutnotorientingitupright,orknockingitoverto\n theuprightpositionbutnotcarefullyguidingit. Therobotmustreleasethepotattheendof\n theepisodeforfullcredit. \n 4. Move <object> onto Plate (multi-instruction): The robot’s goal is to grasp one out of\n threeobjects(dependingonthetargetspecifiedinthelanguageinstruction)andplaceit\n ontheplateontherightsideofthetable. Wecollect 150 demonstrationsofthistaskfor\n thetrainingdataset,randomlyplacingdifferentcombinationsofthreeobjectsonthetable\n andselectingoneasthetarget. Theplateisalwaysinitializedontherightsideofthetable.\n Duringevaluation,eachtrialisrecordedasasuccess(1),failure(0),orpartialsuccess(0.5).\n Partialsuccessisrecordedwhenthefirstobjectthattherobotmakescontactwithisthe\n correcttargetobject(i.e.,theobjectspecifiedinthelanguageinstruction),buttherobotdoes\n notcompletethetask. \n 5. Knock<object>Over(multi-instruction): Therobot’sgoalistoapproachoneoutofthree\n objects (depending on the target specified in the language instruction) and push it until\n itfallsover. Wecollect 70 demonstrationsofthistaskforthetrainingdataset,randomly\n placingdifferentcombinationsofthreeobjectsonthetableandselectingoneasthetarget.\n Duringevaluation,eachtrialisrecordedasasuccess(1),failure(0),orpartialsuccess(0.5).\n Partialsuccessisrecordedwhenthefirstobjectthattherobotmakescontactwithisthe\n correcttargetobject(i.e.,theobjectspecifiedinthelanguageinstruction),buttherobotdoes\n notcompletethetask. \n 6. Cover<object>with Towel(multi-instruction): Therobot’sgoalistograspthebluetowel\n andplaceitononeoutofthreeobjects(dependingonthetargetspecifiedinthelanguage\n instruction). Wecollect 45 demonstrationsofthistaskforthetrainingdataset,randomly\n placingdifferentcombinationsofthreeobjectsonthetable. Duringevaluation,eachtrial\n isrecordedasasuccess(1),failure(0),orpartialsuccess(0.5). Partialsuccessisrecorded\n whenthefirstobjectthattherobottoucheswiththetowelisthecorrecttargetobject(i.e.,\n theobjectspecifiedinthelanguageinstruction),buttherobotdoesnotcompletethetask\n (e.g.,itdropsthetowelontothetableinsteadofontopofthetargetobject). Fullcreditis\n givenwhenanypartofthetowelisrestingoverthetopsurfaceofthetargetobject,i.e.,the\n objectdoesnotneedtobefullycovered. \n Forevery Franka-Tabletoptask,weevaluateeachmethodwith 10–12 in-distributiontrialsand 5–6\n OOD generalization trials. The in-distribution and OOD test conditions are depicted in Fig. 10\n (secondcolumn). \n Wedescribethe OODtestconditionsforeachofthesixtasksbelow: \n 30 \n \n "
  },
  {
    "page_num": 31,
    "text": " \n \n \n \n \n \n 1. Put Carrotin Bowl(OOD):Aneggplant(unseenobject)replacesthecarrot.\n 2. Pour Corninto Pot(OOD):Anunseenbrowntableclothcoversthetabletop. \n 3. Flip Pot Upright(OOD):Anunseenwhitetableclothcoversthetabletop \n 4. Move<object>onto Plate(OOD):Asetofthreeunseenobjectsareplacedonthetable.\n \n 5. Knock<object>Over(OOD):Twounseendistractorobjects(redplasticcupandbrown\n box)arepositionedbehindthesetofthreeseenobjects. \n 6. Cover<object>with Towel(OOD):Thethreeobjectsonthetableareplacedupside-down\n andatunseenpositions. \n Finally,inthe Franka-DROIDenvironment,weexperimentwithonetaskandvariantsofit: Wipe\n Table(see Fig.11). Inthistask,therobot’sgoalistograbthebrushandsweepallthreesmallbrown\n objectsintothedustpan. Wecollect 70 demonstrationsforthistaskforthetrainingdataset,varying\n thepositionsofalltheobjects. \n \n \n \n \n \n \n \n \n \n \n Figure 11: Franka-DROIDfine-tuningtask. The“Wipe Table”taskshownhereisthefinaltaskusedin\n thedata-efficientadaptationexperimentsin Section 5.2. Theleftimageshowstheinitialconditionsforan\n in-distributiontrial.Therightimageshowsanout-of-distributiontrialinwhichunseendistractorobjectsare\n presentonthetable.Tofullycompletethetask,therobotmustgrabthebrushandsweepallthreeobjectsinto\n thedustpan. \n Attesttime,weevaluateonin-distributionconditionsmatchingthetrainingdata(Fig.11,left),as\n wellasout-of-distribution(OOD)conditionsinwhichdistractorobjectsarealsopresentinthescene\n onthetable(Fig.11,right). Sincetherearevariouspossibleoutcomesforeachtrial,wedefinea\n scoringrubricasfollows: Themaximumscoreforeachtrialis 2 points. Thepolicyreceivesthefull\n 2 pointsiftherobotsweepsallthreeobjectsintothedustpan. Itreceives 1 pointforsuccessfully\n sweepingoneortwoobjectsintothedustpan. Otherwise, itreceives 0 points. Weevaluateeach\n policywith 18 in-distributiontrialsand 12 OODtrials,soeachpolicyreceivesanaggregatescoreout\n of 60 points. \n B.3.2 Detailed Franka-Tabletopand Franka-DROIDEvaluation Results \n Fullevaluationresultsforboth Franka-Tabletopand Franka-DROIDevaluationsareshownin Table 7.\n We evaluate the methods discussed in Section 5.2. We find that Diffusion Policy demonstrates\n strongperformanceonthesingle-instruction Franka-Tabletoptasks(e.g.,“Put Carrotin Bowl”and\n “Pour Cornin Pot”),outperformingothermethods. However,Open VLAand Octoachievehigher\n performance in the more diverse multi-instruction tasks (“Move <object> onto Plate”, “Knock\n <object>Over”,and“Cover<object>with Towel”). Inthe Franka-DROIDenvironment,Open VLA\n obtainsbestresults. Overall,wefindthat Open VLAachievesthehighestaverageperformanceacross\n bothtasks. \n Additionally,in Table 8,weshowthedetailedversionoftheparameter-efficientfine-tuningexperiment\n resultssummarizedin Table 1. Intheseexperiments,weusearepresentativesubsetoftwo Franka-\n Tabletoptasks,withbothin-distributionand OODvariants: onenarrowsingle-instructiontask(“Put\n Carrotin Bowl”)andonediversemulti-instructiontask(“Move<object>onto Plate”). Weusethe\n samenumberoftrainingdemonstrationsusedin Section 5.2(50 and 150,respectively),whichis\n delineatedin Appendix B.3.1. \n 31 \n \n "
  },
  {
    "page_num": 32,
    "text": " \n \n \n \n \n \n Table 7:Detaileddata-efficientadaptationexperimentresults.Herewepresentthefullbreakdownofresults\n summarizedin Fig.5.Wereporttheperformanceof Diffusion Policytrainedfromscratchonnewrobottasks,\n aswellasgeneralistpoliciesfine-tunedonthesamedata. Eachpolicyistestedagainstbothin-distribution\n andout-of-distribution(OOD)generalizationconditions(see Fig.10 for Franka-Tabletoptasksand Fig.11 for\n Franka-DROIDtasks).Wefindthatnosinglepolicyperformsbestonalltasks:Diffusion Policyachieveshigh\n successratesonsingle-instructiontasks,while Open VLAand Octoperformswellondiversemulti-instruction\n tasks.Intermsofaggregateperformance,however,Open VLAobtainsthehighestaveragesuccessrateacross\n bothenvironments. \n Diffusion Policy Open VLA Open VLA \n #trials Diffusion Policy Octo \n (matched) (scratch) (ours) \n Franka-Tabletop(5 Hz) “Put Carrotin Bowl”(in-distribution) 10 90.0% 80.0% 40.0% 70.0% 70.0%\n “Put Carrotin Bowl”(OOD) 5 20.0% 0.0% 20.0% 0.0% 40.0% \n “Pour Corninto Pot”(in-distribution) 10 100.0% 90.0% 0.0% 10.0% 50.0%\n “Pour Corninto Pot”(OOD) 5 80.0% 60.0% 0.0% 20.0% 60.0% \n “Flip Pot Upright”(in-distribution) 10 100.0% 85.0% 40.0% 85.0% 100.0%\n “Flip Pot Upright”(OOD) 5 50.0% 20.0% 0.0% 40.0% 80.0% \n “Move<object>onto Plate”(in-distribution) 12 25.0% 25.0% 41.7% 8.3% 75.0%\n “Move<object>onto Plate”(OOD) 6 8.3% 33.3% 8.3% 33.3% 58.3% \n “Knock<object>Over”(in-distribution) 12 33.3% 25.0% 83.3% 75.0% 75.0%\n “Knock<object>Over”(OOD) 6 16.7% 16.7% 33.3% 58.3% 83.3% \n “Cover<object>with Towel”(in-distribution) 12 16.7% 20.8% 91.7% 41.7% 50.0%\n “Cover<object>with Towel”(OOD) 6 16.7% 33.3% 91.7% 50.0% 50.0%\n Average 48.5±4.9% 43.4±4.7% 43.4±4.4% 43.4±4.6% 67.2±4.0%\n Franka-DROID(15 Hz) “Wipe Table”(in-distribution) 18 50.0% 27.8% 52.8% 25.0% 55.6%\n “Wipe Table”+Distractors(OOD) 12 12.5% 25.0% 16.7% 16.7% 62.5%\n Average 35.0±8.0% 26.7±7.5% 38.3±8.5% 21.7±6.6% 58.3±7.2%\n Table 8: Detailedparameter-efficientfine-tuningexperimentresults. Herewepresentthedetailedtask\n performanceresultssummarizedin Table 1. \n #trials Full FT Lastlayeronly Frozenvision Sandwich Lo RA,r=32 Lo RA,r=64\n Franka-Tabletop(5 Hz) “Put Carrotin Bowl”(in-distribution) 10 90.0 40.0 40.0 90.0 60.0 90.0\n “Put Carrotin Bowl”(OOD) 5 40.0 0.0 40.0 0.0 60.0 40.0 \n “Move<object>onto Plate”(in-distribution) 12 79.2 33.3 50.0 75.0 75.0 62.5\n “Move<object>onto Plate”(OOD) 6 41.7 33.3 58.3 41.7 75.0 66.7 \n Average 69.7±7.2% 30.3±6.1% 47.0±6.9% 62.1±7.9% 68.2±7.5% 68.2±7.8%\n C RT-2-Xvs. Open VLAin Bridge Data V 2 Evaluations \n Inthissection,weprovideadditionaldetailson RT-2-Xvs. Open VLAcomparisonsin Bridge Data V 2\n evaluationsdiscussedin Section 5.1. Asdiscussedpreviously,Open VLAispretrainedonalarger\n subsetof Open Xdatathan RT-2-Xandusesafused Sig LIP-Dino V 2 visionbackboneratherthana\n singlevisualencoder. However,inadditiontothesefactors,webelievethat Open VLA’ssignificant\n improvementupon RT-2-Xspecificallyin Bridge Data V 2 evaluations(asshownin Fig.3)alsostems\n frommorecarefulpreprocessingofthe Bridgedataset. \n During the development of the Open VLA model, we discovered that the original version of the\n Bridge Data V 2 datasetcontainedmanytransitionswithall-zero(no-op)actions. Forinstance,in\n everydemonstration,anall-zeroactionwasrecordedastheground-truthactioninthefirsttimestep.\n Consequently, training a highly expressive VLA model on the original dataset without any data\n preprocessingledtoapolicythatfrequentlypredictedall-zeroactionsandfrozeduringevaluations.\n Therefore, we simply filtered out the first transition in every demonstration when training the\n Open VLAmodel,andthiswassufficientformitigatingthefreezingbehaviorinmostcases.\n However, the RT-2-X model was trained without such data preprocessing, so it often suffers the\n aforementionedfreezingbehaviorifdeployedoutoftheboxwithoutmodifyingthemodelquerying\n procedure–whichseverelydeterioratesrolloutperformance. Sincethisisaproprietarymodelthat\n isinfeasibleforustore-train(e.g.,withourpreprocessedversionofthe Bridge Data V 2 dataset),\n wemitigatedthisissuebysimplyqueryingthesecond-most-likelyactionfromthemodel,sincethe\n first-most-likelyactionwasoftenallzeroswhilethesecond-most-likelyactionwasnot.(Notethatthis\n isthesameworkaroundthatwasappliedbythedevelopersofthe RT-2-Xmodelfor Bridge Data V 2\n evaluationsreportedinthe Open X-Embodimentexperiments[1].) Thisworkaroundledtomuch\n stronger RT-2-X performance on Bridge Data V 2 evaluations – though we believe that it is still\n suboptimalcomparedtore-trainingthemodelonthepreprocessedversionofthedataset.\n Wealsotriedtodynamicallyquery RT-2-X,i.e.,byfirstsamplingthefirst-most-likelyactionand\n thensamplingthesecond-most-likelyactionifthefirstonewasallzeros. However,weempirically\n 32 "
  },
  {
    "page_num": 33,
    "text": " \n \n \n \n \n \n foundthatdynamicqueryingledtoworseperformancethansimplyqueryingthesecond-most-likely\n actionatalltimes. Wehypothesizethatthisisduetoachangeintherobot’sdynamicsthatarises\n fromdynamicquerying: pausinginthemiddleofatrajectorytore-querythemodelleadstoslight\n interruptionsintherobot’smovementduetonon-negliblelatencyinthequeryingpipeline,andthis\n leadstosubtleperformancedegradation. Therefore, wereporttheperformanceof RT-2-Xwhen\n alwaysqueryingthesecond-most-likelyaction,asdoneinthe Open X-Embodimentproject[1].\n D Additional Experimentsand Ablations \n In this section, we conduct several additional experiments to analyze the effects of individual\n componentsofthe Open VLAmodelarchitectureandtrainingscheme,aswellasprovidequantitative\n evidenceforclaimsmadeinearliersectionsofthiswork. Weaimtoanswerthefollowingquestions:\n 1. Howimportantis Open Xtrainingandhowdoesitimpact Open VLA’sperformance(Ap-\n pendix D.1)? \n \n 2. Whateffectdoesusingafused Sig LIP-Dino V 2 visionencoderhaveon Open VLA’sperfor-\n mance,comparedtousinga Sig LIP-onlyvisionencoder(Appendix D.2)? \n 3. Isitbettertofine-tuneorfreezethevisionencoderin Open VLA(Appendix D.3)?\n 4. Howdothequantizedinferenceresultsdiscussedin Section 5.3 changewhenpolicyperfor-\n manceisdisentangledfrommodelinferencespeed(Appendix D.4)? \n \n Wediscusstheexperimentalsetupandresultsaddressingeachoftheabovequestionssequentiallyin\n thefollowingsections. \n D.1 Open XTraining Data Ablation Experiments \n Asdiscussedin Section 3.3,Open VLAistrainedonalargedatasetofrobotembodiments,scenes,and\n tasksfromthe Open X-Embodimentdataset[1](Open X).Inthissection,weablatethe Open Xmixture\n andtraina VLApolicysolelyononerobotdataset,toassesstheimpactof Open Xtrainingonpolicy\n performance.Notethatwehavealreadyobservedthenegativeeffectofablating Open Xtraininginthe\n fine-tuningregime,asdiscussedin Section 5.2(see Open VLA(Scratch)),butwediscussadditional\n experimentsonanotherrobotembodimentinthissectiontoprovidemoresupportingevidence.\n Experimentalsetupandtasks. Wecomparetheoriginal Open VLAmodelwith Open VLA-Bridge,\n which is produced by taking the same pretrained VLM as Open VLA (Prismatic VLM [44]) and\n fine-tuningitsolelyon Bridge Data V 2[6]ratherthantheentire Open Xtrainingmixturediscussed\n in Appendix A.Weevaluate Open VLAand Open VLA-Bridgeonasubsetof 8 representativetasks\n fromthe Bridge Data V 2 Widow Xrobotevaluationsuitediscussedin Appendix B.1.1. Thetasksare\n listedin Table 9. \n Results. Results for the Open X training mixture ablation are shown in Table 9. By comparing\n Open VLAwith Open VLA-Bridge,weseethatperformancedropsdrastically(reductionof 30 percent\n inabsolutesuccessrate),whichdemonstratestheimportanceof Open Xpretrainingonfinalpolicy\n performance.Althoughthelanguagegroundingperformanceisnotimpacted,weobserveperformance\n reductionacrossallgeneralizationcategories. Thisresultsuggeststhatthelargediversityofscenes,\n objects,andtasksinthe Open Xtrainingmixtureisessentialforunlockingimprovedgeneralization\n capabilitiesinthe Open VLAmodel. \n D.2 Dualvs. Single Vision Encoder Experiments \n The Open VLAmodelarchitectureconsistsofafusedvisionbackbonethatcombinesthe Sig LIP[9]\n and Dino V 2[25]encoders. Inthissection,weablatethe Dino V 2 componenttoassesstheimportance\n ofusingadualvisionencoder. \n Experimentalsetupandtasks. Weinstantiateamodel, Open VLA-Bridge-Sig LIP,whichisa\n versionof Open VLAthatistrainedonlyon Bridge Data V 2 andconsistsofonlythe Sig LIPencoder\n as the vision backbone. We compare this model with the Open VLA-Bridge model discussed in\n the previous section (Appendix D.1), which shares the same model architecture as the original\n Open VLAmodelandisonlytrainedon Bridgerobotdata. Therefore,theonlydifferencebetween\n Open VLA-Bridge-Sig LIPand Open VLA-Bridgeisthattheformeromitsthe Dino V 2 encoderinthe\n 33 \n \n "
  },
  {
    "page_num": 34,
    "text": " \n \n \n \n \n \n Table 9:Bridge Data V 2 Widow Xablationexperimentresults.Weevaluatevariousmethodsonasubsetof\n 8 representativetaskstoassesstheimportanceofdifferentcomponentsofthe Open VLAmodelarchitecture\n andtrainingscheme.Open VLA-Bridgeisaversionof Open VLAwithout Open Xtraining(itistrainedonlyon\n Bridge Data V 2),and Open VLA-Bridge-Sig LIPadditionallyablatesthefusedvisionbackbonebyremoving\n the Dino V 2 encoder(itsvisionbackboneonlyconsistsofthe Sig LIPencoder).Weobservethatboth Open X\n trainingandthefusedvisionencoderimprovepolicyperformance,thoughtheformerhasamuchgreatereffect\n thanthelatter. \n Open VLA Open VLA-Bridge Open VLA-Bridge-Sig LIP\n Category Task #Trials \n #Successes #Successes #Successes \n Visualgen Put Eggplantinto Pot(Easy Version) 10 10 8 8 \n Visualgen Put Eggplantinto Pot 10 10 2 3 \n Visualgen Put Cupfrom Counterinto Sink 10 7 4 2 \n Motiongen Lift Eggplant 10 7.5 5.5 6.5 \n Physicalgen Put Carroton Plate 10 8 4 1 \n Physicalgen Lift AAABattery 10 7 2 2 \n Semanticgen Take Purple Grapesoutof Pot 10 4 3 3 \n Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 7.5 8 7 \n Mean Success Rate 76.3±4.8% 45.6±5.6% 40.6±5.5% \n visionbackbone. Weevaluatethesemodelsonthesamesubsetof 8 Bridgetasksdescribedinthe\n previoussection. \n Results. Resultsforthedualvisionencoderablationareshownin Table 9. Thedropinperformance\n from Open VLA-Bridgeto Open VLA-Bridge-Sig LIPimpliesthatadditionallyincludingthe Dino V 2\n encoderinthevisionbackboneimprovespolicyperformance. However,the 5 percentreductionin\n performancehereisnotassignificantasthe 30 percentdropinperformanceobservedfromablating\n Open Xtraining. Thelow-levelspatialfeaturesrepresentedin Dino V 2 appeartoaidgeneralizationin\n onlysomecases. \n D.3 Fine-Tunedvs. Frozen Vision Encoder Experiments \n Asdiscussedin Section 3.4,priorworkon VLMsobservedhigherperformancefromfreezingthe\n visionencoderthanfine-tuningitsparameters[44].However,whentraining Open VLA,wefine-tuned\n all 7 Bparametersinthemodel,includingthe Sig LIP-Dino V 2 visionbackbone,aswediscovered\n earlyonduringdevelopmentthatfine-tuningthevisionencoderledtohigher-performing VLAs—a\n findingwhichheldacrossvariouspretrained VLMsandmodelarchitectures. Wediscussdetailsof\n suchfindingsbelow. \n Experimentalsetupandtasks. Inthissection, wereporttheperformanceoftwo VLApolicies\n producedbyfine-tuningtwodifferentpretrainedmodelsfromthe Prismatic VLMs[44]repositoryon\n Bridge Data V 2. Thetwopretrainedmodelsarenamed Sig LIPVi T-SO 224 pxand LLa Vav 1.57 B\n (Reproduction);see Karamchetietal.[44]fordetailsontheirarchitecturesandtrainingmixtures.\n We evaluate both policies on various Bridge tasks shown in Table 10. Note that the evaluation\n configurationsheredifferfrompreviouslydiscussed Bridgeevaluations,sotheresultsarenotdirectly\n comparabletoresultsinothersimilarexperiments. \n Results. Resultsforthefine-tunedvs. frozenvisionencoderexperimentsareshownin Table 10. We\n findthatforboth VLAstested,fine-tuningthevisionencoderleadstosignificantlyhighersuccess\n ratesacrossvarioustasks. Qualitatively,insomecases,deployingthefrozenvisionencoderpolicies\n leadstounstablerobotbehaviorsthatareclearlysuboptimal. Consequently,wedecidedearlyon\n duringdevelopmenttonotconductfurtherexperimentationwithfrozenvisionencoders.\n D.4 Additional Quantized Inference Experiments: Disentangling Policy Performanceand\n Model Inference Speed \n In Section 5.3, we evaluated Open VLA with different levels of precision at inference time: half\n precision (bfloat 16), 8-bit quantization, and 4-bit quantization. 8-bit quantization led to lower\n Bridge Data V 2 performance relative to the other two approaches, and we hypothesized that the\n reductioninperformancewascausedbylowermodelinferencespeedfromtheoperationsusedin\n 8-bitquantization. Inthissection,weconductexperimentstoassesstheveracityofthisclaim.\n Specifically,weevaluate Open VLAagainwiththethreedifferentlevelsofprecisionlistedabove,\n but now with blocking control. In otherwords, each actionis fullyexecuted onthe robotbefore\n thenextoneispredictedbythepolicyandexecutedbythecontroller. Thisschemecontrolssystem\n 34 "
  },
  {
    "page_num": 35,
    "text": " \n \n \n \n \n \n Table 10:Fine-tunedvs.frozenvisionencoderexperimentresults.Weevaluatetheperformanceoffine-tuning\n (“Fine-Tuned”)vs. freezingthevisionencoder(“Frozen Vision”)intwo VLApoliciesbuiltontopoftwo\n differentpretrained VLMsfromthe Prismatic VLMs[44]repository.Bridge Data V 2 Widow Xtasksshownhere\n areperformedinthesamesinkenvironmentusedforother Bridgeexperimentsinthiswork(however,theinitial\n environmentconfigurationsherediffer,astheseevaluationswereconductedatanearlierstageintheproject).\n Wefindthatfine-tuningthevisionencoderiscrucialtoobtaingoodpolicyperformance.Certainfrozenvision\n encoderevaluationswerediscontinuedduetoverypoor(near-zero)performanceandunstablerobotbehaviors.\n Amongtheevaluationswherebothfrozenvisionandfine-tunedapproachesaretested,fine-tuningthevision\n encoderleadsto 80.0%averagesuccessversus 46.7%averagesuccessfromleavingitfrozen.\n Sig LIPVi T-SO 224 px LLa Vav 1.57 B(Reproduction)\n Frozen Vision Fine-Tuned Frozen Vision Fine-Tuned\n Task #Trials \n #Successes #Successes #Successes #Successes\n Put Eggplantinto Pot 10 7 10 5 9 \n Put Cornon Plate 10 10 9 0 9 \n Mean Success Rate 85 95 25 90 \n Put{Eggplant,Red Bottle}into Pot 4 2 4 – 3 \n Put{Blue Cup,Pink Cup}on Plate 4 0 0 – 0 \n Lift{Cheese,Red Chili Pepper} 4 0 3 – 2 \n Put{Strawberry,Lime}into Pot 4 1 0 – 3 \n Move{Sushi,Grapes} 4 3 4 – 3 \n Mean Success Rate 30 55 – 55 \n dynamicsacrossmethodswithvaryingamountsoflatencyandthusallowsustotestthequalityof\n apolicy’sactionpredictions,independentofitspredictionspeed. Effectively,theprecisionlevels\n thathavehigherthroughput–bfloat 16 and 4-bitquantization–areforcedtorunslowertomatchthe\n dynamicsobservedwhendeploying Open VLAwith 8-bitprecision.Therefore,weexpect Open VLA’s\n performancewith 8-bitprecisiontomatchtheperformanceofbfloat 16 and 4-bitprecisionunder\n blockingcontrol. \n Experimentalsetupandtasks. Wereporttheperformanceof Open VLAwithblockingcontrol\n andquantizedinferenceonthesamesubsetof 8 Bridge Data V 2 tasksusedin Appendix D.1 and\n Appendix D.2. \n Results. Quantizedinferenceexperimentresultswithblockingcontrolareshownin Table 11. Unlike\n in Table 2, where 8-bit quantization led to the worst rollout performance due to low inference\n speed,hereweobservethat 8-bitquantizationperformscomparablytobfloat 16 precisionand 4-bit\n quantizationgiventhatweevaluatewithblockingcontroltoremovetheinfluenceofvaryinginference\n speedsontaskperformance. Thisconfirmsourhypothesisabouttheeffectofinferencespeedon 8-bit\n quantizationperformanceinpreviousexperiments(whenusingnon-blockingcontrol). Wealsosee\n nosubstantialperformancedegradationwhenusingthelowestprecision,4-bit,asalsoobservedin\n Section 5.3. \n Table 11: Quantizedinferenceexperimentresultswithblockingcontrol. Wereportthesuccessrateand\n standarderrorof Open VLAonvarious Bridge Data V 2 Widow Xtaskswithbfloat 16 precision(thedefault\n approach),8-bitquantization(int 8),and 4-bitquantization(int 4)atinferencetime.Allaveragesuccessrates\n haveoverlappingerrorbars,whichsuggeststhatallmethodsperformcomparably.\n bfloat 16 int 8 int 4 \n Category Task #Trials \n #Successes #Successes #Successes \n Visualgen Put Eggplantinto Pot(Easy Version) 10 10 10 10 \n Visualgen Put Eggplantinto Pot 10 9 10 10 \n Visualgen Put Cupfrom Counterinto Sink 10 5 5 3 \n Motiongen Lift Eggplant 10 8 7 7.5 \n Physicalgen Put Carroton Plate 10 10 10 10 \n Physicalgen Lift AAABattery 10 3 6 4 \n Semanticgen Take Purple Grapesoutof Pot 10 2 2 2 \n Languagegrounding Put{Eggplant,Red Bottle}into Pot 10 9 9.5 8.5 \n Mean Success Rate 70.0±5.1% 74.4±4.9% 68.8±5.2%\n 35 "
  },
  {
    "page_num": 36,
    "text": " \n \n \n \n \n \n E LIBEROSimulation Experiments \n Ourpreviousdiscussionsin Section 5.2 and Section 5.3 focusedonadapting Open VLAtonovel\n real-worldrobotsetupsandtasks.Thissectionexploresadapting Open VLAtosimulatedrobotsetups\n andtasks,specificallyutilizingthe LIBERObenchmark[116]. Ourexperimentationinsimulation\n offerstwokeyadvantages: \n 1. Demonstration of versatility: We show that Open VLA, despite having been pretrained\n exclusivelyonreal-worldrobotdata,caneffectivelyadapttosimulateddomains,overcoming\n potentialdisparitiesbetweenreal-worldandsimulatedenvironmentsanddynamics.\n 2. Enhancedaccessibilityandreproducibility:Integrationof Open VLAintoapubliclyavailable\n simulationplatformmakesourmodelmoreaccessibletootherresearchers,especiallythose\n whomaynothaveaccesstorobotichardware. Additionally,simulatedexperimentsaremore\n easilyreproducedthantheirreal-worldcounterparts. \n \n Wediscusstheexperimentalsetupin Appendix E.1 andtheresultsin Appendix E.2. Wereleasethe\n materialsrequiredtoreproducetheexperimentsalongwiththe Open VLAcodebase.\n E.1 LIBEROSimulation Experimental Setup \n Simulationsetupandtasks. The LIBERObenchmark[116]consistsoffourtasksuitesdesigned\n forstudyinglifelonglearninginroboticmanipulation,andtheoriginalpaperthereforeinvestigates\n bothforwardandbackwardtransfertoavarietyoftasks. Inourexperiments,wefocussolelyon\n supervisedfine-tuningonthetargettasksuite,measuringtheperformanceofvariouspoliciestrained\n viabehavioralcloningonsuccessfuldemonstrationsofthetasks. \n Weperformexperimentswiththefollowingfourtasksuites,whicheachcontain 10 taskswith 50\n human-teleoperateddemonstrationseach: \n • LIBERO-Spatial consists of the same set of objects but different layouts, and tests the\n model’sunderstandingofspatialrelationships. \n \n • LIBERO-Object consists of the same scene layouts but different objects, and tests the\n model’sunderstandingofobjecttypes. \n • LIBERO-Goalconsistsofthesameobjectsandlayoutsbutdifferenttaskgoals,andtests\n themodel’sknowledgeofdifferenttask-orientedbehaviors. \n • LIBERO-Long (also called LIBERO-10) consists of long-horizon tasks with diverse\n objects,layouts,andtasks. \n \n Wemakethefollowingmodificationstoeachofthetrainingdatasetsabove: \n 1. To accommodate methods requiring higher-resolution images (such as 256×256 px or\n 224×224 px),weregeneratealldemonstrationsatanincreasedresolutionof 256×256 px.\n Originally, thedatasetprovidedbythebenchmarkconsistsof 128×128 pximages. We\n find that simply upscaling these images to 256 × 256 px results in poor image quality.\n Therefore,wechoosetobeginwithhigher-resolutionimages,whichcanbedownscaled\n asnecessary,ensuringhigherimagequalityacrossvariousresolutionrequirements. These\n higher-resolutionimageswereobtainedbysteppingthroughthesimulationenvironments\n with the actions stored in the provided human-collected demonstrations and saving the\n imagesrenderedbythesimulator. \n 2. Wefilteroutall“no-op”actionsfromthedataset,i.e.,actionsthathavenear-zeromagnitude\n inthetranslationandrotationcomponentsanddonotchangethestateoftherobot’sgripper.\n Wefindthatthissimpledatacleaningstepiscrucialforhighlyexpressivesingle-steppolicies\n suchas Open VLA,whichotherwiselearntoimitatetheseno-opactionsandconsequently\n freezeindefinitelyatcertainstatesduringevaluation. \n 3. Werotateallthird-personimagesatbothtrainandtesttimeby 180 degreesbecausewe\n observethatthe LIBEROenvironmentsreturnimagesthatareupsidedownonourhardware.\n \n 36 \n \n "
  },
  {
    "page_num": 37,
    "text": " \n \n \n \n \n \n 4. Sincewetrainpoliciesviaimitationlearning,whichexpectsdemonstrationstobesuccessful,\n wereplayalldemonstrationsinthecorrespondingsimulationenvironmentsandfilteroutthe\n demonstrationsthatfailtocompletethetask(asdeterminedbytheenvironments’success\n criteria). As a result, we remove 68 of 500 LIBERO-Spatial demonstrations, 46 of 500\n LIBERO-Objectdemonstrations,72 of 500 LIBERO-Goaldemonstrations,and 121 of 500\n LIBERO-Longdemonstratinos. \n 5. Forallmethodsinourcomparisons,weonlyutilizethestaticthird-personcameraimages;\n wedonotusethewristcameraimagesthatareadditionallyprovidedintheoriginaldatasets.\n Thisisforsakeofhavingfaircomparisons, as Open VLA’svisualinputsonlyconsistof\n third-personcameraimages. \n Comparisons. Themethodsthatwecompareinclude Diffusion Policy 8 [3]trainedfromscratch,\n Octo[5]fine-tunedonthetargetdataset,and Open VLAfine-tunedonthetargetdatasetvia Lo RA\n (r =32)asdescribedin Section 5.3. Eachpolicyistrainedindependentlyoneachofthetasksuites\n above(ratherthantrainingasinglepolicyonallfoursuitescombined). Allpoliciesaretrainedwith\n thesamesetofdemonstrations,soallmethodsbenefitfromthedatacleaningstepsdescribedabove.\n Evaluationdetails. Toensurelowervarianceintheexperimentalresults,allmethodsareevaluated\n across 500 trialsforeachtasksuite,andthereportedperformanceistheaveragesuccessrateover\n threerandomseeds(resultingin 1500 totaltrialsperstatistic). Althoughwemodifythetraining\n datasets,asdescribedearlier,wedonotchangethetestenvironmentsbutratherusethesameinitial\n environmentconfigurationsprovidedbytheoriginal LIBERObenchmark. \n E.2 LIBEROSimulation Experimental Results \n Wepresentthe LIBEROexperimentalresultsin Table 12. Importantly,weobservethat Open VLAcan\n beeffectivelyadaptedtotasksinthe LIBEROsimulationenvironments,asitobtainshighestaverage\n successrateandrankamongthetestedmethods. However,wefindthattheoverallmarginbetween\n Open VLA and the other methods are tighter here than in the real-world fine-tuning experiments\n discussed in Section 5.2. We attribute this to the fact that Open VLA was pretrained with purely\n real-worldrobotdataandnosimulationdata,whichsuggeststhatfine-tuningthemodelonsimulated\n robottasksmaynotbeaseffectiveasfine-tuningitonreal-worldtasksduetothedomaingapbetween\n simulatedandreal-worldenvironmentsanddynamics. Weseeevidenceforthisnotionintheresults\n obtained by Octo – another policy pretrained on large amounts of real-world robot data – which\n alsoonlyachievesasmallboostinoverallperformancerelativetoasimple,strongbaselinesuchas\n Diffusion Policytrainedfromscratch. Weexpectincreasedgainsinperformanceforthepretrained\n andfine-tunedmethodsifsimulationdataisaddedtothepretrainingdatamixture.\n Table 12:LIBEROsimulationbenchmarkresults.Wereportthesuccessrate(SR)andstandarderrorofeach\n methodforthefourtasksuitesinthe LIBERObenchmark,averagedoverthreerandomseedswith 500 trials\n each.Inaddition,weshowtherankingofeachmethodwithineachtasksuite,wherearankof 1 indicatesthe\n strongestmethodinthesuiteandarankof 3 indicatestheweakestmethod.(Theaveragerankingisimportantto\n notesinceitinformswhichmethodmaybemostsuitabletouseasadefaultforavarietyoftasks;itismore\n informativethantheaveragesuccessrate,whichisnotnormalizedbyindividualtasksuitedifficulty.)Overall,\n wefindthatfine-tuned Open VLAachieveshighestaveragesuccessrateandrank,followedbyfine-tuned Octo\n andthen Diffusion Policytrainedfromscratch. \n LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average\n SR(↑) Rank(↓) SR(↑) Rank(↓) SR(↑) Rank(↓) SR(↑) Rank(↓) SR(↑) Rank(↓)\n Diffusion Policyfromscratch 78.3±1.1% 3 92.5±0.7% 1 68.3±1.2% 3 50.5±1.3% 3 72.4±0.7% 2.5\n Octofine-tuned 78.9±1.0% 2 85.7±0.9% 3 84.6±0.9% 1 51.1±1.3% 2 75.1±0.6% 2\n Open VLAfine-tuned(ours) 84.7±0.9% 1 88.4±0.8% 2 79.2±1.0% 2 53.7±1.3% 1 76.5±0.6% 1.5\n \n \n 8 Weusetheimplementationof Diffusion Policythatisdescribedinthe DROIDdatasetpaper[11],which\n conditionsactiongenerationon Distil BERT[117]languageembeddingsofthetasklabel.\n \n 37 \n \n "
  }
]