[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n \n \n Robo GSim: A Real 2 Sim 2 Real Robotic Gaussian Splatting Simulator \n \n \n Xinhai Li 1*, Jialin Li 2*, Ziheng Zhang 3†, Rui Zhang 4, Fan Jia 3, Tiancai Wang 3,\n Haoqiang Fan 3, Kuo-Kun Tseng 1‡, Ruiping Wang 2‡ \n 1 Harbin Instituteof Technology,Shenzhen \n 2 Instituteof Computing Technology,Chinese Academyof Sciences \n 3 MEGVIITechnology 4 Zhejiang University \n \n Novel View Synthesis Novel Scene Synthesis \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Closed-Loop Evaluation Novel Object Synthesis \n \n Figure 1. Robo GSimisanefficient,low-costinteractiveplatformwithhigh-fidelityrendering. Itachievesdemonstrationsynthesiswith\n novel scenes, novel objects, and novel views, facilitating data scaling for policy learning. Additionally, it can perform the closed-loop\n simulationforsafe,fairandrealisticevaluationondifferentpolicymodels. \n Abstract tal Twins Builder,Scene Composer,and Interactive Engine.\n It can synthesize the simulated data with novel views, ob-\n Efficientacquisitionofreal-worldembodieddatahasbeen jects, trajectories, and scenes. Robo GSim also provides\n increasinglycritical. However, large-scaledemonstrations an online, reproducible, and safe evaluation for different\n captured by remote operation tend to take extremely high manipulation policies. The real 2 sim and sim 2 real trans-\n costsandfailtoscaleupthedatasizeinanefficientman- ferexperimentsshowahighconsistencyinthetextureand\n ner. Samplingtheepisodesunderasimulatedenvironment physics. We compared the test results of Robo GSim data\n is a promising way for large-scale collection while exist- andrealrobotdataonboth Robo GSimandrealrobotplat-\n ingsimulatorsfailtohigh-fidelitymodelingontextureand forms. The experimental results show that the Robo GSim\n physics. To address these limitations, we introduce the data model can achieve zero-shot performance on the real\n Robo GSim, a real 2 sim 2 real robotic simulator, powered by robot, with results comparable to real robot data. Addi-\n 3 DGaussian Splattingandthephysicsengine. Robo GSim tionally, in experiments with novel perspectives and novel\n mainlyincludesfourparts: Gaussian Reconstructor, Digi- scenes, the Robo GSim data model performed even better\n on the real robot than the real robot data model. This not\n *Equal contribution only helps reduce the sim 2 real gap but also addresses the\n †Project leader limitationsofrealrobotdatacollection,suchasitssingle-\n ‡Corresponding authors \n 5202 \n gu A \n 3 \n ]OR.sc[ \n 2 v 93811.1142:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n source and high cost. We hope Robo GSim serves as a ulator that unifies the demonstration synthesis and closed-\n closed-loopsimulatorforfaircomparisononpolicylearn- loopevaluation. Robo GSimcangeneraterealisticmanipu-\n ing. More information can be found on our project page lateddemonstrationswithnovelscenes,views,andobjects\n https://robogsim.github.io/. forpolicylearning. Itcanalsoperformclosed-loopevalua-\n tionfordifferentpolicynetworks,ensuringfaircomparison\n underarealisticenvironment. Inconclusion,ourcorecon-\n 1.Introduction tributionscanbeconcludedas: \n • Realistic 3 DGS-Based Simulator:Wedevelopa 3 DGS-\n Collecting large-scale manipulated data is of great impor- \n based simulator that reconstructs scenes and objects\n tance for efficient policy learning. Some methods propose \n with realistic textures from multi-view RGB videos.\n tocapturethedemonstrationsaswellastheactionsthrough \n Robo GSimisoptimizedforsomechallengingconditions\n theremoteoperation[11,37,39].Whilesuchoperationrel- \n likeweaktextures,lowlight,andreflectivesurfaces.\n atively improves the collection efficiency, it tends to bring \n • Digital Twin System:Weintroducethelayoutalignment\n extremelylargecostswiththeincreasingdatasize.Tosolve \n moduleinthesystem. Withthelayout-aligned Isaac Sim,\n this problem, some works [14, 34] attempt to generate the \n Robo GSim maps the physical interactions between ob-\n synthetic data under the simulated environment, which is \n jectsandroboticarmsfrom Real 2 Simspaces. \n further used to learn the manipulation policy. However, \n • Synthesizer and Evaluator: Robo GSim can synthe-\n those Sim 2 Real approaches suffer from the large domain \n size the realistic manipulated demonstrations with novel\n gapbetweensimulatedandreal-worldenvironments,mak- \n scenes,views,andobjectsforpolicylearning. Itcanalso\n ingthelearnedpolicyinvalid. \n work as the Evaluator to perform model evaluation in a\n Recently, some works introduce the Real 2 Sim 2 Real \n physics-consistent manner.The experiment results show\n (R 2 S 2 R) paradigm for robotic learning [3, 21]. The core \n thatourgenerateddatacanachievethesameperformance\n insight is to perform realistic reconstruction via radiance \n as real robot data, which in a way solves the sim 2 real\n field methods, such as Ne RF [25] and 3 D Gaussian Splat- \n gap problem. At the same time, in experiments with\n ting (3 DGS) [15], and insert learned representations into \n novel scenes and novel perspectives, our generated data\n thesimulator. Amongthosemethods,thetypicalapproach, \n is more effective than real robot data, even with the 2 D\n Robo-GS[21],presentsa Real 2 Simpipelineandintroduces \n Aug method. Our evaluator also partially verifying the\n a hybrid representation to generate digital assets enabling \n performanceoftherealrobotmodelinaclosed-loop.\n high-fidelity simulation. However, it lacks the demonstra- \n tion synthesis on novel scenes, views, and objects, as well \n 2.Related Work \n asverificationaspolicylearningdata. Moreover,itfailsto \n performclosed-loopevaluationfordifferentpoliciesdueto \n 2.1.Sim 2 Realin Robotics \n themisalignmentbetweenthelatentrepresentation,simula- \n tion,andreal-worldspaces. The Real 2 Sim 2 Real approach fundamentally seeks to ad-\n In this paper, we develop a Real 2 Sim 2 Real simulator, dressthe Sim 2 Realgap,whichremainsapersistentobstacle\n called Robo GSim,forbothhigh-fidelitydemonstrationsyn- inthetransformationfromsimulationtorealworld[8,27].\n thesis and physics-consistent closed-loop evaluation. It In order to bridge the Sim 2 Real gap as much as possible,\n mainly includes four parts: Gaussian Reconstructor, Digi- manyfeature-richsimulatorshaveemergedinrecentyears,\n tal Twins Builder,Scene Composerand Interactive Engine. including [7, 23, 28, 35, 38]. To this end, various datasets\n Giventhemulti-view RGBimagesequencesand MDH [6] andbenchmarkshavealsobeenproposedforeffectivepol-\n parameters of the robotic arm, Gaussian Reconstructor is icylearning[12,13,16,26].\n built upon 3 DGS [43] and reconstructs the scene and ob- Previous Sim 2 Real methods can be broadly classified\n jects.Then,the Digital Twins Builderperformsthemeshre- intothreecategories: domainrandomization,domainadap-\n constructionandcreatesadigitaltwinin Isaac Sim. In Dig- tation, and learning with disturbances [40]. Domain ran-\n ital Twins Builder, we propose the layout alignment mod- domizationmethodsaredesignedtoexpandtheoperational\n ule to align the space between the simulation, real-world, envelope of a robot in a simulator by introducing random-\n and GS representation. After that, the Scene Composer ness. Thesimulationenvironmentshouldbecapableofmi-\n combinesthescene,roboticarmandobjectsinsimulation, grationoftheaforementionedcapabilitiesinreal-worldset-\n and renders the images from new perspective. Finally, in tings[1,10,14,34]. Domainadaptationapproachesaimto\n the Interactive Engine,Robo GSimworksasthe Synthesizer unifythefeaturespaceofsimulatedandrealenvironments,\n and Evaluatortoperformsthedemonstrationsynthesisand facilitatingthetrainingandmigrationwithintheunifiedfea-\n closed-looppolicyevaluation. ture space [2, 19, 41]. The objective of learning methods\n Robo GSim brings many advantages compared to exist- introducethedisturbancesintothesimulatedenvironment,\n ing(Real 2)Sim 2 Realframeworks. Itisthefirstneuralsim- inwhichthepolicyofrobotsislearned. Itdevelopstheca-"
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n pacitytooperateeffectivelyintherealworldwithnoiseand the manipulated data in simulation using VR/Xbox equip-\n unpredictability[5,36]. mentoftherealworld. \n 2.2.3 DGaussian Splattingin Robotics 3.2.Gaussian Reconstructor \n \n Asasignificantadvancementinthefieldof 3 Dreconstruc- We employ the 3 DGS method to reconstruct static scenes,\n tion, 3 DGS [15] represents the scene as a large set of ex- followed by point cloud segmentation of the robotic arm’s\n plicit Gaussianpointsandcombinesitwithefficientraster- joints. Subsequently, we utilize the MDH dynamic model\n izationtoachievehigh-fidelityreal-timerendering,extend- tocontrolthepointcloudscorrespondingtoeachjoint, fa-\n ingthecapabilitiesof Ne RF[25]. cilitatingthedynamicrenderingoftheroboticarm.\n More recently, a number of studies have explored the 3 D Gaussian Splatting (3 DGS) [15] employs a set of\n use of 3 DGS to perform manipulation tasks within em- multi-view images as input to achieve high-fidelity scene\n bodied simulators and the real world. For example, Mani- reconstruction. 3 DGSrepresentsthesceneasasetof Gaus-\n Gaussian [22] introduces a dynamic GS framework along- sians and utilizes a differentiable rasterization rendering\n sidea Gaussianworldmodel,whichrespectivelyrepresents methodtoenablereal-timerendering.\n Gaussianpointsimplicitlyandparameterizesthemtomodel Specifically, for a scene G = {g }N represented by\n i i=1 \n and predict future states and actions. Similarly, Gaussian- N Gaussians, each Gaussian can be represented as g =\n i \n Grasper[42]utilizes RGB-Dimagesasinputsandembeds (µ ,Σ ,o ,c ). Here, µ ∈ R 3, Σ ∈ R 3×3, o ∈ R and\n i i i i \n semanticandgeometricfeaturesinto 3 DGSthroughfeature c ∈ SH(4) denote the mean, covariance matrix, opacity\n distillation and geometric reconstruction, thereby enabling andcolorfactor, representedbysphericalharmoniccoeffi-\n language-guidedgrasping operations. Toeffectively trans- cients,respectively.\n fer the knowledge learned in simulation to the real world Duringtherenderingprocess,thefinalcolorvalue C of\n and reduce the Sim 2 Real gap, recent works [18, 21, 29] thepixelcanbeobtainedthrougharenderingmethod,sim-\n basedon 3 DGShaveappeared.Amongthem,themostsim- ilar to alpha-blending [15]. It utilizes a sequence of N or-\n ilartooursare Robo-GS[21]and Splat Sim[29]. Robo-GS dered Gaussians that overlap with the pixel. Such process\n achieves manipulable robotic arm reconstruction by bind- canbeexpressedasfollows:\n ing Gaussian points, grids, and pixels, with a primary fo- \n cusonhigh-fidelity Real 2 Simtransfer;however,itprovides i−1 \n (cid:88) (cid:89) \n limited discussion on the Sim 2 Real phase. Splat Sim re- C = c i α i (1−α j ) (1)\n constructs both the robotic arm and objects in the scene i∈N j=1 \n andsimultaneouslyverifiesthefeasibilityofthemethodfor \n 1 \n Sim 2 Realtasks. However, itlacksdiscussionsongenerat- α =o ·exp( δ⊤Σ−1δ ) (2)\n ingdigitaltwinassetsoftheobjects, whicharecriticalfor i i 2 i 2 D i \n achievingaccuratemanipulation. where α is the opacity of the i-th Gaussian. δ ∈ R 2 de-\n i i \n notes the offset between 2 D Gaussian center and current\n 3.Methods pixel. Σ ∈R 2×2 representsthe 2 Dcovariancematrix.\n 2 D \n Modified Denavit-Hartenberg (MDH) [6] convention is\n 3.1.Overall Architecture \n aparameterizedmodeltodescribethekinematicchainofa\n As shown in Fig. 2, Robo GSim mainly includes four manipulator. Each joint and link in the kinematic chain is\n parts: Gaussian Reconstructor, Digital Twins Builder, characterized by a set of parameters. In MDH, a transfor-\n Scene Composer,and Interactive Engine. Givenmulti-view mation matrix can be constructed for each link, achieving\n images and MDH parameters of the robotic arm, Gaus- anaccuraterepresentationofthemanipulator’sposeateach\n sian Reconstructor (Sec. 3.2) reconstructs scenes and ob- stageofmotion. Letx ,y ,z denotethecoordinatesofthe\n i i i \n jectsusing 3 DGS,segmentstheroboticarm,andbuildsan origin for the i-th joint. For a manipulator, the i-th joint\n MDH kinematic drive graph structure to enable accurate configurationcanberepresentedas:\n motion modeling of the robotic arm. Digital Twin Builder \n (Sec.3.3)involvesmeshreconstructionofthesceneandob- Θ={β ,a ,d ,θ } (3) \n i i i i \n jects. Throughlayoutalignment,theassetdataflowcanbe \n interconnected,facilitatingthesubsequentevaluationin In- where β represents the twist angle, which is the rotation\n i \n teractive Engine. Scene Composer (Sec. 3.4) achieves the aroundthex-axisfromthe(i−1)-thjointtothei-thjoint.\n synthesis of novel objects, scenes, and views. Interactive a denotesthelinklength,measuringthedistancealongthe\n i \n Engine (Sec. 3.5) synthesizes novel view/scene/object im- x-axis from z to z . d is the link offset, indicating the\n i−1 i i \n agesforpolicylearning. Itcanalsoevaluatethepolicynet- displacementalongthez-axisfromx tox .θ represents\n i−1 i i \n works in a closed-loop manner. Moreover, we can collect thejointangle,rotationaroundthez-axisfromx tox .\n i−1 i "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 2. Overviewofthe Robo GSim Pipeline:(1)Inputs:multi-view RGBimagesequencesand MDHparametersoftheroboticarm.\n (2)Gaussian Reconstructor: reconstructthesceneandobjectsusing 3 DGS,segmenttheroboticarmandbuildan MDHkinematicdrive\n graphstructureforaccuratearmmotionmodeling. (3)Digital Twins Builder: performmeshreconstructionofboththesceneandobjects,\n thencreateadigitaltwinin Isaac Sim, ensuringhighfidelityinsimulation. (4)Scene Composer: combinetheroboticarmandobjects\n inthesimulation, identifyoptimaltestviewpointsusingtracking, andrenderimagesfromnewperspectives. (5)Interactive Engine: (i)\n Thesynthesizedimageswithnovelscenes/views/objectsareusedforpolicylearning.(ii)Policynetworkscanbeevaluatedinaclose-loop\n manner.(iii)Theembodieddatacanbecollectedbythe VR/Xboxequipment. \n Thetransformationmatrixforeachlink T ,using MDH turntable and extract matching features with GIM [33] to\n i \n parameters,canbewrittenas: address issues such as lack of texture and reflections. We\n thenintegratethe COLMAPpipeline[32]toobtaintheini-\n   \n cosθ −sinθ cosβ sinθ sinβ a cosθ \n i i i i i i i tial SFMpointcloud,whichissubsequentlyusedforrecon-\n T i =    sin 0 θ i cos s θ in i c β o i sβ i −co c s o θ s i β s i inβ i a i s d in i θ i   (4) s o t n ru t c h t e io w n e b b y , w 3 D e G in S it . ia M lly or e e m ov p e l r o , y fo g r en n e o r v a e ti l ve ob 3 j D ect r s ec a o v n a s il t a ru b c le -\n 0 0 0 1 \n tionmethods[17,20]toprocure 3 Dgaussiansandtextured\n meshesoftheobjects. Subsequently,weutilizethemethod\n Bysequentiallymultiplyingthesetransformationmatrices, \n in Gaussian Editor[4]thatappliesthediffusionmodel[31]\n wecanobtainthefinaltransformationmatrixfromthebase \n tofacilitateobjectreconstructionin 3 DGS. \n to the end effector. We segment each joint and then treat \n all Gaussianpointswithinajointasapointmass. Wefur- \n thermoveall Gaussianpointswithinajointaccordingto T , Layout Alignment:Asshownin Fig.2,sincewefollowthe\n i \n achievingkinematic-driven Controlofthe Gaussianpoints. localcoordinatesystemoftheroboticarm,theworldcoor-\n dinates and Isaac Sim are axis-aligned. We first measure\n 3.3.Digital Twins Builder \n thereal-worldscenetoalignthesizeoftheimportedtable\n Digitaltwinsshouldnotonlymapreal-worldassetsbutalso scene in Isaac Sim. In the GS scene, a downward-facing\n involve coordinate alignment. Through Real 2 Sim layout camera is placed 1.6 meters above the base joint to render\n alignmentand Sim 2 GSsparsekeypointalignment, wecan asegmentationmap. Forcoordinatealignment,weplacea\n digitize the real world, enabling the flow of digital assets downward-facingcamera 1.6 metersabovethebasejointin\n between the real, simulated, and GS representation. This Isaac Sim. Bycomparingtherenderedscenefromthe BEV,\n facilitates the conversion of digital assets in all directions, frontandsideviewsegmentation,withtheviewsfrom Isaac\n achievingcomprehensiveassetflooding. Sim,weadjusttheshifttoachievelayoutalignment.\n 3 D Assets Generation: We employ two methods to gen- \n erate 3 D object assets. For real-world objects, we cap- Sim 2 GS Alignment: Given the MDH-based transforma-\n ture high-quality multi-view images of the objects using a tion matrices Tgs and simulated transformation matrices\n i "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n Tsim,thereexistsatransformationmatrix Tsim suchthat: Σ′ =R ΣR⊤ (14) \n i gs(i) norm norm \n Tsim =Tsim·Ti (5) Object Editing: The transformation here can extend the\n gs(i) i gs transformation from the scene editing mentioned above.\n Tocomputetheaveragetransformationmatrix Tsim,weuse However,thedifferenceisthatthetargetobject’scoordinate\n gs \n theweightedsumandapplynormalization: centerisgivenby Eq.7. Thecoordinatetransformationfor\n its Gaussianpointscanberepresented: \n (cid:80)6 w ·Tsim \n T g s s im = (cid:13) (cid:13)(cid:80) i 6 =1 w i ·T g s s im i (cid:13) (cid:13) (6) µ′ =R(µ−µ 0 )+µ 0 +t (15)\n (cid:13) i=1 i gsi(cid:13) \n 3.5.Interactive Engine \n wherew istheweightofeachjoint. \n i \n For the target object Tsimin Isaac Sim, we can trans- Ourinteractiveenginecanworkas: Synthesizerand Evalu-\n obj \n ator. As Synthesizer,itproduceslargevolumesofdatawith\n form it into the GS coordinate system using the following \n low-cost for downstream policy learning. As Evaluator, it\n formula: \n Tgs =Tgs ·Tsim (7) canperformsafe,real-time,andreproducibleevaluation.\n obj sim obj \n Synthesizer:Weusetheenginetogeneratenumeroustrain-\n Camera Localization: Totransformthereal-worldcoordi- \n ing trajectories, including robotic arm movements and tar-\n natesystemintothe GScoordinate, weapplythelocaliza- \n get object trajectories. These trajectories drive the GS to\n tion approach from GS-SLAM [24]. For a pre-trained GS \n generate massive and photorealistic simulated datasets for\n model,G = {g }N ,wefrozetheattributesof 3 DGSand \n i i=1 policylearning. Thisdiversedataincludesnovelviewren-\n optimizetheexternalcameraparameters TW. \n C derings,scenecombinations,andobjectreplacements.\n In camera localization, only the current camera pose is \n Evaluator: For trained models, testing directly on phys-\n optimized without updates to the map representation. For \n ical devices may pose safety risks or incur high costs for\n monocular cases, we minimize the following photometric \n reproduction. Therefore,weconvertthepredictedtrajecto-\n residual: \n riesinto GS-renderedresultstoefficientlyandrapidlyeval-\n L pho = \n (cid:13) \n (cid:13)I(G,T C \n W)−I¯(cid:13) \n (cid:13) 1 , (8) uate the model’s prediction quality. Specifically, the Isaac\n where I(G,TW) represents rendering Gaussians G from Sim [28] outputs an initial state of the target object and\n C \n TW,and I¯istheobservedimage. robotic arm, and GS renders according to the status. The\n C \n rendered images are then fed to the policy to predict the\n 3.4.Scene Composer next frame’s action. The predicted action is passed to the\n Scene Editing: To merge the point cloud into the robotic simulation for kinematic inverse parsing, collision detec-\n armscene,thetransformation T[R|t]ofthemarkedpointis tion,andotherphysicalinteractions. Then,Isaacsimsends\n firstcalculated. Thenthecoordinatesofthepointcloudin theparsedsix-axisrelativeposetothe GSrenderer, which\n the new scene are projected into the arm coordinate based thensendstherenderedresultasfeedbacktothepolicynet-\n onthetransformation. Expandingthe 3 DCovarianceΣin work. Thisservesasvisualfeedbackforpredictingthenext\n 3 DGSintoscalesandrotationquaternionqby: action,andtheprocessiteratesuntilthetaskisfinished.\n Σ=qss Tq T (9) 4.Experiments \n Theratiorofthetransformationcanbeisolatedandex- Sincethereisnobenchmarksavailablefor Real 2 Sim 2 Real,\n tractedasanindependentcomponent: we construct the following four groups of proxy exper-\n iments to comprehensively evaluate the performance of\n (cid:113) \n r = (RRT) (10) Robo GSim under simulation and real-world. We use UR 5\n (0,0) \n robot arm for all experiments. The robot arm rendering is\n wecanfurtheruseittonormalizetherotationmatrix R: partiallybuiltuponthecodebaseof Robo-GS[21].\n Real 2 Sim Novel Pose Synthesisverifieswhethertherobot\n R \n R norm = r (11) arm pose captured in the real world can be effectively uti-\n lizedtoachieveprecisecontrolinthesimulator.\n Thescaleattributesofthe Gaussianpointsisadjusted: Sim 2 Real Trajectory Replaycheckswhetherthetrajecto-\n riescollectedinthesimulatorcanbeaccuratelyreproduced\n s=s+log(r) (12) \n bythereal-worldrobotarm. \n Applythe Transformation T to Gaussianpointcoordinates Robo GSim as Synthesizer demonstrates the ability of\n Robo GSim to generate high-fidelity demonstrations with\n µ′ =Rµ+t (13) novelscenes,views,andobjects,aligningwithrealworld."
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n Grasp Suc. Place Suc. \n TV NV(MD) NS TV NV NS \n Real 100% 30% 40% 90% 0% 20% \n Real+2 DAUG 80% 100% 60% 80% 0% 60% \n Robo GSim 100% 70% 100% 90% 0% 90% \n Table 1.Performanceonring-tosstaskinrealworld.TVdenotestestview,NVisnovelviewand NSisthenovelscene.MDmeansminor\n deviation. \n \n Robo GSimas Evaluatorshowsthat Robo GSimcaneffec- 4.3.Robo GSimas Synthesizer \n tivelyperformclosed-loopevaluationforpolicynetworks. \n In this section, we use the vision-language-action (VLA)\n model to validate the effectiveness of synthetic data by\n Method Grasp Suc. Place Suc. \n Robo GSim. Weusethe LLAMA 3-8 B[9]asthe LLMand\n Real-to-Real 100% 90% \n CLIP [30] as the vision encoder. Two-layer MLP is used\n Real-to-Robo GSim 100% 30% \n as the projection network. The VLA model is trained on\n Sim-to-Real 80% 0% \n 8 x A 100(80 GB)for 1 epoch.Thetrainingprocessisdivided\n Robo GSim-to-Real 100% 90% \n into three stages: (1) Pre-training with only the connector\n Table 2. Cross validation between real world and simulation. enabled,usingthe LAION-558 Kdataset. (2)Trainingwith\n “Sim-to-Real”meansthattestingthe VLAmodeltrainedwiththe LLM unfrozen using the LLa VA 665 K dataset. (3) Super-\n Isaacsimdataontherealworldrobotarm. vised Finetuning(SFT)withroboticimage-actiondataand\n the CLIPweightisfrozen. \n By using the real machine distribution to guide the\n 4.1.Real 2 Sim Novel Pose Synthesis \n Robo GSimdistribution,weaimtoimprovethemodel’ssuc-\n Theobjectiveofthenovelposesynthesisistovalidatethe cess rate. We perform the experiments on a challenging\n performance of Real 2 Sim reconstruction, with a particular ring-toss task (see Fig. 7), which is divided into two sub-\n focusontheaccuracyoftheroboticarm’smovementsand tasks: picking up the ring and tossing it onto the target.\n the fidelity of the image texture. The static scene is re- The accuracy requirement for the Z-axis when picking up\n constructed using the initial pose of the robotic arm from the ring is within 5 mm. For real data, 1,000 samples are\n thefirstframeof GT.Thetrajectorycollectedfromthereal collected manually. For a fair comparison, we used 1,000\n roboticarmisusedasthedrivingforce,andweemploythe synthetic samples generated by Robo GSim. During test-\n kinematic control for novel pose rendering. As shown in ing, each model was tested 10 times, with three attempts\n Fig. 3, the results demonstrate that our reconstruction ac- allowed per trial for grasping. If all three attempts failed,\n curatelycapturesboththetextureandthephysicaldynam- thetrialwasmarkedasunsuccessful.\n icsoftheroboticarm,highlightingthefidelityachievedby As shown in Tab. 1, We compared three models: one\n Robo GSim. Tocomparewiththevideosequencedrivenby trained with real machine data, one trained with real ma-\n therealrobotunderthenewviewpoint,Robo GSimachieves chinedataplus 2 DAUG,andonetrainedwith Robo GSim.\n a 31.3 PSNRand 0.79 SSIMrenderingresult,whileensur- Thecomparisonwasmadeintermsoftestview,novelview,\n ingreal-timerenderingwith 10 FPS. andnovelscene. Theresultsshowthatinthetestview,the\n generated data from Robo GSim can achieve zero-shot ca-\n 4.2.Sim 2 Real Trajectory Replay \n pability, withperformancecomparabletotherealmachine\n To verify whether the trajectories from Issac Sim can per- data(bothat 90%).Inthenovelscenecase,Robo GSimper-\n fectly align with the real machine and Robo GSim, we de- formed much better than real machine data, reaching 90%\n signedanexperimentwherethetrajectoryiscollectedusing compared to the 60% of real machine data. In the novel\n Issac Sim,andthenthetrajectoryisusedtodrive GStoren- view experiment, Robo GSim also had less bias compared\n dera Coke-graspingscene,whilethesametrajectoryisused to the real machine data model. We also compared the\n todrivetherealmachinetograspa Cokecan. Asshownin effect of adding 2 D AUG to the real machine data. Af-\n Fig.4, thecomparisonreveals astrongalignment between ter adding AUG, the performance inthe test view dropped\n thesimulatedpolicyandtheactualphysicalbehaviorofthe (90% → 80%), but in the novel scene, it improved (40%\n roboticarm,highlightingtheeffectivenessofthe Sim 2 Real → 60%). However, in the novel view, the bias increased.\n transfer in our system. These results suggest that our sim- Pure 2 DAUGlacksspatialawareness,anditsperformance\n ulationcanreliablymodelreal-worlddynamics,facilitating isfarworsethanthatof Robo GSim,whichhasspatialintel-\n successfulpolicytransferfromsimulationtotherealworld. ligence. a Itshouldbenotedthatmanualcollectiontakesa"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n Real \n \n \n \n Robo GSim \n \n \n Depth \n \n \n \n Diff \n \n \n Dr. Robot \n \n \n PSNR:31.4 SSIM:0.79 FPS:10 \n \n Figure 3. Real 2 Sim Novel Pose Synthesis: ”Real”representsthecaptureoftherealroboticarmfromanewviewpoint. ”Robo GSim”\n showstherenderingofthenovelposefromthenewviewpointdrivenbytherealrecordedtrajectory. ”Depth”showstherenderingdepth\n by GS.”Diff”isthedifferencecalculatedbetweenthe Realandtherendered RGBimages. Wecomputethepixeldistanceofthesame\n pointbetweenthe Realand Robo GSim,whichis 7.37. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 4. Sim 2 Real Trajectory Replay: The”Sim”rowdisplaysthevideosequencecollectedfrom Isaac Sim. ”Real”representsthe\n demonstrationdrivenbythetrajectoryinsimulation.”Robo GSim”isthe GSrenderingresultdrivenbythesametrajectory.”Diff”indicates\n thedifferencesbetween Realandtherenderedresults. \n \n totalof 40 hourswhile Robo GSimonlyrequires 4 hoursfor doorenvironments.Thehigh-fidelitymulti-viewrenderings\n synthesis. It is promising to further scale up the data size demonstratethat Robo GSimenablestherobotarmtooper-\n ofsynthesisforfurtherperformanceimprovements. Fig.7 ateseamlesslyacrossdiversescenes.\n shows the visualization of some success and failure cases. \n 4.4.Robo GSimas Evaluator \n Moreover, we also illustrate some more qualitative analy- \n sis for novel scene synthesis. As shown in Fig. 5, we dis- Realisticclosed-loopevaluationiscrucialforvalidationand\n playtheresultsofthephysicalmigrationofthe UR 5 robot comparisonofpolicynetworks. Inthissection, wemainly\n armtonewscenes,includingafactory,ashelf,andtwoout- explore the effectiveness of using Robo GSim as an Evalu-\n \n \n \n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 5. Novel Scene Synthesis: Weshowtheresultsofthephysicalmigrationoftherobotarmtonewscenes,includingafactory,a\n shelf,andtwooutdoorenvironments.Thehigh-fidelitymulti-viewrenderingsdemonstratethat Robo GSimenablestherobotarmtooperate\n seamlesslyacrossdiversescenes. \n \n \n \n \n \n \n \n \n Figure 6. Robo GSimas Synthesizer: Renderingofthesametrainingsetinnovelviewandnovelscenes.\n \n Method L 1 ↓ PSNR↑ tialalignmenttoenables 3 Dassertflow. Withnovelview-\n 3 DGS 0.01381 34.19939 point, object, trajectory and scene, our Robo GSim engine\n 2 DGS 0.01798 32.36417 can generate high-fidelity synthesized data. Additionally,\n PGSR 0.01925 31.72386 due to our precise spatial alignment, Robo GSim can serve\n Mip Ne RF 360 0.02618 23.51348 as evaluator that allows real-time online policy evaluation.\n Despiteitsgreatprogress,thecurrentversionof Robo GSim\n Table 3.3 DGSachievesbetterstaticreconstructionthan 2 DGS has several limitations. It can only simulate rigid objects\n andthelightingforsynthesizedobjectsisnotyetfullyuni-\n fied with the robotic arm. Moreover, generating geometri-\n ator. It aims to show its high consistency with real-world \n cally consistent object meshes remains challenging, which\n inference. Given the well-trained VLA model, we deploy \n isoftenkeytocompletingcomplexmanipulationtasks. In\n itforbothreal-worldrobotsand Robo GSimsimulation. As \n thenearfuture,wewillexploremoreadvancedmeshextrac-\n shownin Fig.9, ourclosed-loopsimulator Robo GSimcan \n tionmethods,furtherexpandthetaskcategoriesandestab-\n reproduceresultssimilartothosefromtherealworld. For \n lish the benchmarks to comprehensively evaluate the per-\n similarbadcases,our Robo GSimcanavoidtheissuesexist- \n formanceacrossdiversescenarios. \n ingintherealworld,likeviolationsandcollisions. There- \n fore,ourevaluatorprovidesafair,safe,andefficientevalu- \n ationplatformforpolicy. \n 6.Acknowledgements \n 5.Conclusionand Discussion \n Inthispaper,webuilta Real 2 Sim 2 Realsimulator,basedon Theworkwassupportedbythe National Scienceand Tech-\n 3 DGS.Wealsointroducethedigitaltwinsystemwithspa- nology Major Projectof China(2023 ZD 0121300)."
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n \n \n \n \n kci P \n \n \n \n \n \n \n \n \n \n \n ecal P \n Success \n Fail \n Success \n Fail \n \n Figure 7. Robo GSimas Synthesizer: Thefirsttworowsshowrealrobotvideoscapturedfromthetestviewpoint,illustratingsuccessful\n andfailedcasesofthe VLAmodelonthe Picktask.Thelasttworowsdisplayrealrobotvideoscapturedfromthetestviewpoint,showing\n successfulandfailedcasesofthe VLAmodelonthe Placetask. \n \n Real \n \n \n \n \n Robo GSim \n \n \n \n \n Real Real \n \n \n Hit someone Break gripper \n in real world in real world \n Robo GSim Robo GSim \n \n \n \n \n Figure 8. Robo GSimas Evaluator: Thefirsttworows,labeled”Real”and”Robo GSim”,showthefootagecapturedfromtherealrobot\n and Robo GSim,respectively. Theyarebothdrivenbythetrajectorygeneratedbythesame VLAnetwork. Inthethirdrow,theleftside\n showsthereal-worldinferencewheretherobotarmexceedsitsoperationallimits,resultinginamanualshutdown. Therightsideshows\n aninstancewhereawrongdecisionfromthe VLAnetwork,causestheroboticarmtocollidewiththetable. Thefourthrowpresentsthe\n simulationresultsfrom Robo GSim,whichcanavoiddangerouscollisions. \n \n \n \n "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n sus_mi SG \n \n \n liaf_mi SG \n \n \n sus_mi SG \n \n \n \n liaf_mi SG \n \n \n sus_lae R \n \n \n liaf_lae R \n \n \n sus_lae R \n \n \n \n liaf_lae R \n \n \n liaf_mi S \n \n \n Figure 9. Expon Realrobot: Thefirstrow,GSim sus,representssuccessfulcasesof Robo GSimdataunderthetestview. Thesecond\n row, GSim fail, represents failure cases of Robo GSim data under the novel view. The third row, GSim sus, represents successful\n casesof Robo GSimdataunderthenovelscene. Thefourthrow,GSim fail,representsfailurecasesof Robo GSimdataunderthenovel\n scene.Thefifthrow,Real sus,representssuccessfulcasesofreal-worlddataunderthetestview.Thesixthrow,Real fail,represents\n successful cases of real-world data under the novel view. The seventh row, Real sus, represents successful cases of real-world data\n underthenovelscene. Theeighthrow, Real fail, representsfailurecasesofreal-worlddataunderthenovelscene. Theninthrow,\n Sim fail,representsfailurecasesof Isaac Simdataunderthetestview. \n \n References Moran,Steven Bohez,Fereshteh Sadeghi,Bojan Vujatovic,\n and Nicolas Heess. Nerf 2 real: Sim 2 realtransferofvision-\n [1] Open AI: Marcin Andrychowicz, Bowen Baker, Maciek \n guidedbipedalmotionskillsusingneuralradiancefields. In\n Chociej, Rafal Jozefowicz, Bob Mc Grew, Jakub Pachocki, \n 2023 IEEE International Conference on Robotics and Au-\n Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, \n tomation(ICRA),pages 9362–9369,2023. 2\n etal. Learningdexterousin-handmanipulation. The Inter- \n [4] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xi-\n national Journalof Robotics Research,39(1):3–20,2020. 2 \n aofeng Yang,Yikai Wang,Zhongang Cai,Lei Yang,Huaping\n [2] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Liu,and Guosheng Lin. Gaussianeditor: Swiftandcontrol-\n Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel- lable 3 deditingwithgaussiansplatting. In Proceedingsof\n level domain adaptation with generative adversarial net- the IEEE/CVFConferenceon Computer Visionand Pattern\n works. In Proceedingsofthe IEEEconferenceoncomputer Recognition(CVPR),pages 21476–21485,2024. 4\n visionandpatternrecognition,pages 3722–3731,2017. 2 \n [5] Cheng Chi,Zhenjia Xu,Siyuan Feng,Eric Cousineau,Yilun\n [3] Arunkumar Byravan, Jan Humplik, Leonard Hasenclever, Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song.\n Arthur Brussee, Francesco Nori, Tuomas Haarnoja, Ben Diffusionpolicy: Visuomotorpolicylearningviaactiondif-"
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n fusion. The International Journal of Robotics Research, [19] Mingsheng Long,Yue Cao,Jianmin Wang,and Michael Jor-\n 2024. 3 dan.Learningtransferablefeatureswithdeepadaptationnet-\n [6] Peter ICorke. Asimpleandsystematicapproachtoassign- works. In International conference on machine learning,\n ing denavit–hartenberg parameters. IEEE transactions on pages 97–105.PMLR,2015. 2\n robotics,23(3):590–594,2007. 2,3 [20] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\n [7] Erwin Coumans and Yunfei Bai. Pybullet, a python mod- Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\n uleforphysicssimulationforgames,roboticsandmachine Marc Habermann,Christian Theobalt,etal. Wonder 3 d:Sin-\n learning. http://pybullet.org,2016–2021. 2 gleimageto 3 dusingcross-domaindiffusion.ar Xivpreprint\n [8] Konstantinos Dimitropoulos, Ioannis Hatzilygeroudis, and ar Xiv:2310.15008,2023. 4\n Konstantinos Chatzilygeroudis. Abriefsurveyofsim 2 real [21] Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng\n methods for robot learning. In International Conference Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen\n on Roboticsin Alpe-Adria Danube Region,pages 133–140. Feng,Lu Shi,etal. Robo-gs: Aphysicsconsistentspatial-\n Springer,2022. 2 temporalmodelforroboticarmwithhybridrepresentation.\n [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- ar Xivpreprintar Xiv:2408.14873,2024. 2,3,5\n hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil [22] Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Ji-\n Mathur,Alan Schelten,Amy Yang,Angela Fan,etal. The wen Lu,and Yansong Tang. Manigaussian: Dynamicgaus-\n llama 3 herd ofmodels. ar Xiv preprint ar Xiv:2407.21783, sian splatting for multi-task robotic manipulation. In Eu-\n 2024. 6 ropean Conference on Computer Vision, pages 349–366.\n [10] Ioannis Exarchos, Yifeng Jiang, Wenhao Yu, and C Karen Springer,2025. 3\n Liu. Policy transfer via kinematic domain randomization \n [23] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,\n andadaptation. In 2021 IEEEInternational Conferenceon \n Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,\n Roboticsand Automation(ICRA),pages 45–51.IEEE,2021. \n Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel\n 2 \n State. Isaac gym: High performance GPU based physics\n [11] Zipeng Fu, Tony Z. Zhao, and Chelsea Finn. Mobile \n simulationforrobotlearning. In Thirty-fifth Conferenceon\n ALOHA: Learning bimanual mobile manipulation using \n Neural Information Processing Systems Datasetsand Bench-\n low-cost whole-body teleoperation. In 8 th Annual Confer- \n marks Track(Round 2),2021. 2 \n enceon Robot Learning,2024. 2 \n [24] Hidenobu Matsuki, Riku Murai, Paul H.J. Kelly, and An-\n [12] Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao \n drew J.Davison. Gaussiansplattingslam. In Proceedingsof\n Dong, and He Wang. Partmanip: Learning cross-category \n the IEEE/CVFConferenceon Computer Visionand Pattern\n generalizablepartmanipulationpolicyfrompointcloudob- \n Recognition(CVPR),pages 18039–18048,2024. 5\n servations. ar Xivpreprintar Xiv:2303.16958,2023. 2 \n [25] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\n [13] Minho Heo, Youngwoon Lee, Doohyun Lee, and Joseph J. \n Jonathan T.Barron,Ravi Ramamoorthi,and Ren Ng. Nerf:\n Lim. Furniturebench: Reproducible real-world benchmark \n representingscenesasneuralradiancefieldsforviewsynthe-\n for long-horizon complex manipulation. In Robotics: Sci- \n sis. Commun.ACM,65(1):99–106,2021. 2,3\n enceand Systems,2023. 2 \n [26] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita\n [14] Johann Huber, Franc¸ois He´le´non, Hippolyte Watrelot, \n Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yun-\n Fa¨ız Ben Amar, and Ste´phane Doncieux. Domain ran- \n rong Guo,Hammad Mazhar,Ajay Mandlekar,Buck Babich,\n domizationforsim 2 realtransferofautomaticallygenerated \n Gavriel State, Marco Hutter, and Animesh Garg. Orbit: A\n graspingdatasets.In 2024 IEEEInternational Conferenceon \n unified simulation framework for interactive robot learning\n Roboticsand Automation(ICRA),pages 4112–4118.IEEE, \n environments. IEEERoboticsand Automation Letters,8(6):\n 2024. 2 \n 3740–3747,2023. 2 \n [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler, \n [27] Jean-Baptiste Mouretand Konstantinos Chatzilygeroudis.20\n and George Drettakis. 3 d gaussian splatting for real-time \n radiancefieldrendering.ACMTransactionson Graphics,42 yearsofrealitygap:afewthoughtsaboutsimulatorsinevo-\n lutionary robotics. In Proceedings of the genetic and evo-\n (4),2023. 2,3 \n lutionarycomputationconferencecompanion, pages 1121–\n [16] Vikash Kumar,Rutav Shah,Gaoyue Zhou,Vincent Moens, \n 1124,2017. 2 \n Vittorio Caggiano, Abhishek Gupta, and Aravind Ra- \n jeswaran. Robohive: Aunifiedframeworkforrobotlearn- [28] NVIDIA. Isaacsim. https://developer.nvidia.\n ing. In Thirty-seventh Conference on Neural Information com/isaac/sim,2024. Software. 2,5\n Processing Systems Datasetsand Benchmarks Track,2023. [29] Mohammad Nomaan Qureshi,Sparsh Garg,Francisco Yan-\n 2 dun, David Held, George Kantor, and Abhishesh Sil-\n [17] Xinhai Li, Huaibin Wang, and Kuo-Kun Tseng. Gaus- wal. Splatsim: Zero-shot sim 2 real transfer of rgb manip-\n siandiffusion: 3 dgaussiansplattingfordenoisingdiffusion ulation policies using gaussian splatting. ar Xiv preprint\n probabilistic models with structured noise. ar Xiv preprint ar Xiv:2409.10161,2024. 3\n ar Xiv:2311.11221,2023. 4 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n [18] Ruoshi Liu, Alper Canberk, Shuran Song, and Carl Von- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n drick. Differentiable robot rendering. In 8 th Annual Con- Amanda Askell,Pamela Mishkin,Jack Clark,etal.Learning\n ferenceon Robot Learning,2024. 3 transferable visual models from natural language supervi-"
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n sion.In Internationalconferenceonmachinelearning,pages grasping. IEEE Robotics and Automation Letters, 9(9):\n 8748–8763.PMLR,2021. 6 7827–7834,2024. 3 \n [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, [43] Licheng Zhong,Hong-Xing Yu,Jiajun Wu,and Yunzhu Li.\n Patrick Esser, and Bjo¨rn Ommer. High-resolution image Reconstructionandsimulationofelasticobjectswithspring-\n synthesis with latent diffusion models. In Proceedings of mass 3 d gaussians. In European Conference on Computer\n the IEEE/CVF conference on computer vision and pattern Vision,pages 407–423.Springer,2025. 2\n recognition,pages 10684–10695,2022. 4 \n [32] Johannes LSchonbergerand Jan-Michael Frahm. Structure- \n from-motion revisited. In Proceedings of the IEEE con- \n ference on computer vision and pattern recognition, pages \n 4104–4113,2016. 4 \n [33] Xuelun Shen,Zhipeng Cai,Wei Yin,Matthias Mu¨ller,Zijun \n Li,Kaixuan Wang,Xiaozhi Chen,and Cheng Wang. Gim: \n Learninggeneralizableimagematcherfrominternetvideos. \n In The Twelfth International Conferenceon Learning Repre- \n sentations,2024. 4 \n [34] Josh Tobin,Rachel Fong,Alex Ray,Jonas Schneider,Woj- \n ciech Zaremba, and Pieter Abbeel. Domainrandomization \n fortransferringdeepneuralnetworksfromsimulationtothe \n real world. In 2017 IEEE/RSJ international conference on \n intelligent robots and systems (IROS), pages 23–30. IEEE, \n 2017. 2 \n [35] Emanuel Todorov,Tom Erez,and Yuval Tassa. Mujoco: A \n physicsengineformodel-basedcontrol. In 2012 IEEE/RSJ \n International Conferenceon Intelligent Robotsand Systems, \n pages 5026–5033.IEEE,2012. 2 \n [36] Jingkang Wang,Yang Liu,and Bo Li. Reinforcementlearn- \n ingwithperturbedrewards.In Proceedingsofthe AAAIcon- \n ferenceonartificialintelligence,pages 6202–6209,2020. 3 \n [37] David Whitney, Eric Rosen, Daniel Ullman, Elizabeth \n Phillips, and Stefanie Tellex. Rosreality: Avirtualreality \n frameworkusingconsumer-gradehardwareforros-enabled \n robots. In 2018 IEEE/RSJInternational Conferenceon In- \n telligent Robotsand Systems(IROS),pages 1–9,2018. 2 \n [38] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao \n Zhu,Fangchen Liu,Minghua Liu,Hanxiao Jiang,Yifu Yuan, \n He Wang,Li Yi,Angel X.Chang,Leonidas J.Guibas,and \n Hao Su. SAPIEN:Asimulatedpart-basedinteractiveenvi- \n ronment. In The IEEEConferenceon Computer Visionand \n Pattern Recognition(CVPR),2020. 2 \n [39] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea \n Finn. Learning Fine-Grained Bimanual Manipulation with \n Low-Cost Hardware. In Proceedings of Robotics: Science \n and Systems,Daegu,Republicof Korea,2023. 2 \n [40] Wenshuai Zhao, Jorge Pen˜a Queralta, and Tomi Wester- \n lund. Sim-to-realtransferindeepreinforcementlearningfor \n robotics:asurvey.In 2020 IEEESymposium Serieson Com- \n putational Intelligence(SSCI),pages 737–744,2020. 2 \n [41] Liming Zheng, Wenxuan Ma, Yinghao Cai, Tao Lu, and \n Shuo Wang. Gpdan:Graspposedomainadaptationnetwork \n for sim-to-real 6-dof object grasping. IEEE Robotics and \n Automation Letters,8(8):4585–4592,2023. 2 \n [42] Yuhang Zheng,Xiangyu Chen,Yupeng Zheng,Songen Gu, \n Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zeng- \n mao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen \n Chen,Xiaoxiao Long,and Meiqing Wang.Gaussiangrasper: \n 3 dlanguagegaussiansplattingforopen-vocabularyrobotic "
  }
]