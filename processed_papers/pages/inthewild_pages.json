[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n Learning Generalizable Robotic Reward Functions \n \n from “In-The-Wild” Human Videos \n \n \n Annie S. Chen, Suraj Nair, Chelsea Finn \n Stanford University \n \n \n Abstract—We are motivated by the goal of generalist robots ”In-the-wild” Human Videos Robot Videos\n that can complete a wide range of tasks across many en- Many Environments, Many Tasks One Environment, Few Tasks\n vironments. Critical to this is the robot’s ability to acquire \n some metric of task success or reward, which is necessary for \n reinforcement learning, planning, or knowing when to ask for \n help. For a general-purpose robot operating in the real world, \n this reward function must also be able to generalize broadly \n across environments, tasks, and objects, while depending only \n on on-board sensor observations (e.g. RGB images). While deep \n learning on large and diverse datasets has shown promise as a \n DVD Reward \n pathtowardssuchgeneralizationincomputervisionandnatural Function \n language,collectinghighqualitydatasetsofroboticinteractionat \n Training \n scaleremainsanopenchallenge.Incontrast,“in-the-wild”videos \n ofhumans(e.g.You Tube)containanextensivecollectionofpeople Unseen Environment Human Demo Testing\n doing interesting tasks across a diverse range of settings. In this Unseen Task\n work, we propose a simple approach, Domain-agnostic Video \n Discriminator (DVD), that learns multitask reward functions \n DVD Reward \n by training a discriminator to classify whether two videos are \n Function \n performing the same task, and can generalize by virtue of \n learning from a small amount of robot data with a broad dataset \n of human videos. We find that by leveraging diverse human \n datasets, this reward function (a) can generalize zero shot to Task Completion\n unseen environments, (b) generalize zero shot to unseen tasks, \n and (c) can be combined with visual model predictive control to \n solve robotic manipulation tasks on a real Widow X 200 robot in \n an unseen environment from a single human demo. \n I. INTRODUCTION \n Despite recent progress in robotic learning on tasks ranging Figure 1: Reward Learning and Planning from In-The-Wild\n from grasping [24] to in-hand manipulation [31], the long- Human Videos. During training (top), the agent learns a reward\n function from a small set of robot videos in one environment, and\n standing goal of the “generalist robot” that can complete many \n a large set of in-the-wild human videos spanning many tasks and\n tasks across environments and objects has remained out of \n environments. At test time (bottom), the learned reward function is\n reach. While there are numerous challenges to overcome in conditioned upon a task specification (a human video of the desired\n achieving this goal, one critical aspect of learning general task),andproducesarewardfunctionwhichtherobotcanusetoplan\n purpose robotic policies is the ability to learn general purpose actionsorlearnapolicy.Byvirtueoftrainingondiversehumandata,\n this reward function generalizes to unseen environments and tasks.\n reward functions. Such reward functions are necessary for the \n robottodetermineitsownproficiencyatthespecifiedtaskfrom \n its on-board sensor observations (e.g. RGB camera images). robots at a large scale remains challenging for a number of\n Moreover, unless these reward functions can generalize across reasons,suchasneedingtobalancedataqualitywithscalability,\n varying environments and tasks, an agent cannot hope to use and maintaining safety without relying heavily on human\n them to learn generalizable multi-task policies. supervision and resets. Alternatively, You Tube and similar\n While prior works in computer vision and NLP [11, 12, 4] sources contain enormous amounts of “in-the-wild” visual data\n haveshownnotablegeneralizationvialargeanddiversedatasets, of humans interacting in diverse environments. Robots that can\n translating these successes to robotic learning has remained learn reward functions from such data have the potential to be\n challenging, partially due to the dearth of broad, high-quality able to generalize broadly due to the breadth of experience in\n robotic interaction data. Motivated by this, a number of recent this widely available data source.\n works have taken important steps towards the collection of Of course, using such “in-the-wild” data of humans for\n largeanddiversedatasetsofroboticinteraction[28,22,10,53] robotic learning comes with a myriad of challenges. First, such\n and have shown some promise in enabling generalization [10]. data often will have tremendous domain shift from the robot’s\n At the same time, collecting such interaction data on real observationspace,inboththemorphologyoftheagentandthe\n 1202 \n ra M \n 13 \n ]OR.sc[ \n 1 v 71861.3012:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n visualappearanceofthescene(e.g.see Figure 1).Furthermore, which study single task problems in a single environment, the\n the human’s action space in these “in-the-wild” videos is often focusofthisworkisinlearninggeneralizablemulti-taskreward\n quite different from the robot’s action space, and as a result functions for visual robotic manipulation that can produce\n there may not always be a clear mapping between human and rewards for different tasks by conditioning on a single video\n robot behavior. Lastly, in practice these videos will often be of a human completing the task.\n low quality, noisy, and may have an extremely diverse set of \n B. Robotic Learning from Human Videos \n viewpoints or backgrounds. Critically however, this data is \n plentiful and already exists, and is easily accessible through A number of works have studied learning robotic behavior\n websites like You Tube or in pre-collected academic datasets fromhumanvideos.Oneapproachistoexplicitlyperformsome\n like the Something-Something data set [21], allowing them form of object or hand tracking in human videos, which can\n to be incorporated into the robot learning process with little then be translated into a sequence of robot actions or motion\n additional supervision cost or collection overhead. primitives for task execution [26, 52, 30, 25, 36]. Unlike these\n Given the above challenges, how might one actually learn works,whichhand-designthemappingfromahumansequence\n reward functions from these videos? The key idea behind our to robot behaviors, we aim to learn the functional similarity\n approach is to train a classifier to predict whether two videos between human and robot videos through data.\n are completing the same task or not. By leveraging the activity More recently, a range of techniques have been proposed for\n labels that come with many human video datasets, along with end-to-end learning from human videos. One such approach\n a modest amount of robot demos, this model can capture the is to learn to translate human demos or goals to the robot\n functional similarity between videos from drastically different perspective directly through pixel based translation with paired\n visual domains. This approach, which we call a Domain- [27, 44] or unpaired [47] data. Other works attempt to infer\n agnostic Video Discriminator (DVD), is simple and therefore actions, rewards, or state-values of human videos and use them\n can be readily scaled to large and diverse datasets, including for learning predictive models [40] or RL [14, 39]. Learning\n heterogeneousdatasetswithbothpeopleandrobotsandwithout keypoint [51, 8] or object/task centric representations from\n any dependence on a close one-to-one mapping between the videos [42, 38, 34] is another promising strategy to learning\n robot and human data. Once trained, DVD conditions on a rewards and representations between domains. Simulation has\n human video as a demonstration, and the robot’s behavior alsobeenleveragedassupervisiontolearnsuchrepresentations\n as the other video, and outputs a score which is an effective [32] or to produce human data with domain randomization [3].\n measure of task success or reward. Finally,meta-learning[54]andsubtaskdiscovery[41,20]have\n The core contribution of this work is a simple technique alsobeenexploredastechniquesforacquiringrobotrewardsor\n for learning multi-task reward functions from a mix of robot demos from human videos. In contrast to the majority of these\n and in-the-wild human videos, which measures the functional works, which usually study a small set of human videos in a\n similarity between the robot’s behavior and that of a human similar domain as the robot, we explicitly focus on leveraging\n demonstrator. We find that this method is able to handle the “in-the-wild” human videos, specifically large and diverse sets\n diversity of human videos found in the Something-Something- of crowd-sourced videos from the real world from an existing\n V 2 [21] dataset, and can be used in conjunction with visual dataset, which contains many different individuals, viewpoints,\n model predictive control (VMPC) to solve tasks. Most notably, backgrounds, objects, and tasks.\n we find that by training on diverse human videos (even from Our approach is certainly not the first to study using such\n unrelated tasks), our learned reward function is able to more in-the-wildhumanvideos.Worksthathaveusedobjecttrackers\n effectively generalize to unseen environments and unseen tasks [52], simulation [32], and sub-task discovery [20] have also\n than when only using robot data, yielding a 15-20% absolute been applied on in-the-wild video datasets like You Cook [9],\n improvement in downstream task success. Lastly, we evaluate Something-Something [21], and Activity Net [15]. Learning\n ourmethodonareal Widow X 200 robot,andfindthatitenables from such videos has also shown promise for navigation\n generalizationtoanunseentaskinanunseenenvironmentgiven problems [6]. Most related to this work is Concept 2 Robot\n only a single human demonstration video. [43], which learns robotic reward functions using videos from\n the Something-Something dataset [21] by using a pretrained\n II. RELATEDWORK \n video classifier. Unlike Concept 2 Robot, our method learns a\n A. Reward Learning \n reward function that is conditioned on a human video demo,\n The problem of learning reward functions from demonstra- andthuscanbeusedtogeneralizetonewtasks.Furthermore,in\n tions of tasks, also known as inverse reinforcement learning Section IV-D, we empirically find that our proposed approach\n or inverse optimal control [1], has a rich literature of prior providesarewardthatgeneralizestounseenenvironmentswith\n work [35, 58, 50, 17, 18]. A number of recent works have much greater success than the Concept 2 Robot classifier.\n generalized this setting beyond full demonstrations to the case \n C. Robotic Learning from Large Datasets \n wherehumansprovideonlydesiredoutcomesorgoals[19,45]. \n Furthermore, both techniques have been shown to be effective Much like our work, a number of prior works have studied\n for learning manipulation tasks on real robots in challenging how learning from broad datasets can enhance generalization\n high dimensional settings [17, 45, 57]. Unlike these works, in robot learning [16, 33, 56, 13, 22, 24, 10, 5]. These works"
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n DVD DVD DVD \n Label = 1 Label = 0 Label = 1 \n \n “Closing Something” = “Pushing Something Away” ≠ “Pushing Something Away” =\n “Closing Something” “Closing Something” “Pushing Something Away” \n Figure 2: Training DVD. DVD is trained to predict if two videos are completing the same task or not. By leveraging task labels from\n in-the-wild human video datasets and a small number of robot demos, DVD is trained compare a video of a human to that of a robot (left,\n middle) and to compare pairs of human videos which may have significant visual differences, but may still be doing the same task (right).\n By training on these visually diverse examples, DVD is forced to learn the functional similarity between the videos.\n havelargelystudiedtheproblemofcollectinglargeanddiverse for the task of “move two objects apart”, the reward depends\n robotic datasets in scalable ways [28, 22, 10, 53, 7] as well not only on the current state, but on how close together the\n as techniques for learning general purpose policies from this objectswereinitially.Weinsteadassumethattherewardattime\n style of data in an offline [13, 5] or online [33, 29, 24] fashion. t is only dependent on the last H <T timesteps, specifically\n While our motivation of achieving generalization by learning states s . Our goal then is to learn a parametric model\n t−H:t \n from diverse data heavily overlaps with the above works, our which estimates the underlying reward function for each task,\n approach fundamentally differs in that it aims to sidestep the conditioned on a task-specifying video. That is, given (1) a\n challenges associated with collecting diverse robotic data by sequence of H states s and (2) a video demonstration\n 1:H \n instead leveraging existing human data sources. d =s∗ of variable length for each task T , we aim to learn\n i 1:tdi i \n arewardfunction R (s ,d )thatapproximates R (s )for\n θ 1:H i i 1:H \n III. LEARNINGGENERALIZABLEREWARDFUNCTIONS each i. Such a non-Markovian reward can then be optimized\n WITHDOMAIN-AGNOSTICVIDEODISCRIMINATORS \n using a number of strategies, ranging from open-loop planners\n In this section, we describe our problem setting and intro- to policies with memory or frame stacking.\n duce Domain-agnostic Video Discriminators (DVD), a simple For training the reward function R , we assume access to a\n θ \n approachforlearningrewardfunctionsthatleveragein-the-wild dataset Dh = {Dh }N of videos of humans doing N < K\n Ti i=1 \n human videos to generalize to unseen environments and tasks. tasks{T }N .Therearenovisualconstraintsontheviewpoints,\n i i=1 \n backgrounds or quality of this dataset, and the dataset does not\n A. Problem Statement needtobebalancedbytask.Wearealsogivenalimiteddataset\n In our problem setting, we consider a robot that aims to Dr = {D T r i }M i=1 of videos of robot doing M tasks {T i }M i=1\n complete K tasks{T i }K i=1 ,eachofwhichhassomeunderlying where {T i }M i=1 ⊂ {T i }N i=1 , and so M ≤ N. Both datasets\n task reward function R . As a result, for any given task i, are partitioned by task. Since human data is widely available,\n i \n our robotic agent operates in a fixed horizon Markov decision we have many more human video demonstrations than robot\n process (MDP) Mr, consisting of the tuple (S,Ar,pr,R ,T) video demonstrations per task and often many more tasks\n i i \n where S isthestatespace(inourcase RGBimages),Ar isthe that have human videos but not robot videos, in which case\n robot’sactionspace,pr(s |s ,ar)istherobotenvironment’s M << N. Importantly, the reward is inferred only through\n t+1 t t \n stochastic dynamics, R indicates the reward for task T , and visual observations and does not assume any access to actions\n i i \n T is the episode horizon. Additionally, for each task T , we or low dimensional states from either the human or robot data,\n i \n consider a human operating in an MDP Mh, consisting of the and we do not make any assumptions on the visual similarity\n i \n tuple(S,Ah,ph,R ,T)where Ah isthehuman’sactionspace between the human and robot data. As a result, there can be a\n i \n and ph(s |s ,ah) is the human environment’s stochastic large domain shift between the two datasets.\n t+1 t t \n dynamics. Note that the human and robot MDPs for task i During evaluation, the robot is tasked with inferring the\n share a state space S, reward function R , and horizon T, but reward R based on a new demo d specifying a task T .\n i θ i i \n may have different action spaces and transition dynamics. The goal is for this reward to be effective for solving a task\n Weassumethatthetaskrewardfunctions R areunobserved, T . Furthermore, we aim to learn R in a way such that it\n i i θ \n andneedtobeinferredthroughavideoofthetask.Notethatfor can generalize to unseen tasks T (cid:54)∈ {T }N given a task\n new i i=1 \n many tasks these rewards will not be Markovian–for example demonstration d . \n new "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n Video 1: Moving sthup Video 2: Moving sthup \n 20-40 x 120 x 120 20-40 x 120 x 120 \n (D x W x H) (D x W x H) \n \n \n \n \n 3 D Conv 1 32 Pretrained \n 3 D Conv 2 Video Encoder \n 64 \n 3 D Conv 3 3 x \n 128 3 x 3 D Conv 4 \n 256 \n 3 x 3 D Conv 4 \n 256 \n Average \n features \n Concatenate \n embeddings \n \n 6 CF 215 7 CF 652 8 CF 821 9 CF 46 01 CF 23 11 CF 2 xamtfo S ytiralimi S eroc S\n Algorithm 1 DOMAIN-AGNOSTICVIDEODISCRIMINATOR(DVD)\n 1: //Training DVD \n 2: Require:Dh humandemonstrationdatafor N tasks{Tn}\n 3: Require:Dr robotdemonstrationdatafor M tasks{Tm}⊆{Tn}\n 4: Require:Pre-trainedvideoencoderfenc \n 5: Randomlyinitializeθ \n 6: whiletrainingdo \n 7: Sampleanchorvideodi∈Dh∪Dr \n 8: Samplepositivevideod(cid:48) \n i \n ∈{D \n T \n h \n i \n }∪{D \n T \n r \n i \n }\\di \n 9: Samplenegativevideodj ∈{D \n T \n h \n j \n }∪{D \n T \n r \n j \n }∀j(cid:54)=i \n 3 D C 32 onv 1 1 1 1 0 : : //P U la p n d n a i t n e g R C θ on w di i t t i h on d e i d ,d o (cid:48) i n ,d V j id a e c o co D rd e i m ng o to Eq.1\n 3 D Conv 2 12: Require:Trainedrewardfunction R θ &videopredictionmodelp φ\n 64 13: Require:Humanvideodemodi fortask Ti \n 3 x 3 D Conv 3 14: fortrials 1,...,ndo \n 3 x 3 D 1 C 2 o 8 nv 4 1 1 6 5 : : S S t a e m p p a le ∗ 1: { H a 1 1 w : : G H hi } ch & m g a e x t im pr i e z d es ic R tio θ n ( s s˜ { g 1: s˜ H g 1: , H d } i) ∼{p φ (s 0,ag 1:H )}\n 256 \n 3 x 3 D Conv 4 \n 256 \n the robot videos and associate it with actions in human videos.\n Average \n features \n C. DVD Implementation \n We implement our reward function R as \n θ \n R (d ,d )=f (f (d ),f (d );θ) (2) \n θ i j sim enc i enc j \n where h = f is a pretrained video encoder enc and f (h ,h ;θ) is a fully connected neural network\n sim i j \n parametrized by θ trained to predict if video encodings h\n i \n and h are completing the same task. Specifically, we encode\n j \n Figure 3: DVD Architecture. We use the same video encoder \n each video using a neural network video encoder f into\n architecture as [43]. For each 3 D convolution layer, the number of enc \n a latent space, and then train f as a binary classifier\n filters is denoted, and all kernels are 3×3×3 except for the first, sim \n which is 3×5×5. All conv layers have stride 1 in the temporal trained according to Equation 1. See Figure 3 for the detailed\n dimension, and conv layers 1, 3, 6, 9 and 11 have stride 2 in the architecture.f ispretrainedontheentire Sth Sth V 2 dataset\n enc \n spatial dimensions, the others having stride 1. All conv layers are and fixed during training (as in [43]), while f is randomly\n sim \n followed by a Batch Norm 3 D layer and all layers except the last FC \n initialized. While the training dataset contains many more\n are followed by a Re LU activation. \n human videos than robotvideos, we sample the batches sothat\n B. Domain-Agnostic Video Discriminators they are roughly balanced between robot and human videos;\n specifically, each of (d ,d(cid:48),d ) are selected to be a robot\n i i j \n How exactly do we go about learning R θ ? Our key idea demonstration with 0.5 probability.\n is to learn R that captures functional similarity by training \n θ \n a classifier which takes as input two videos d from T and D. Using DVD for Task Execution\n i i \n d j from T j and predicts if i=j. Both videos can come from Once we’ve trained the reward function R θ , how do we use\n either Dh or Dr, and labels can be acquired since we know ittoselectactionsthatwillsuccessfullycompleteatask?While\n which demos d i correspond to which tasks T i (See Figure 2). in principle, this reward function can be combined with either\n To train R θ , we sample batches of videos (d i ,d(cid:48) i ,d j ) from model-free or model based reinforcement learning approaches,\n Dh∪Dr, where d i and d(cid:48) i are both labelled as completing the we choose to use visual model predictive control (VMPC)\n same task T i , and d j is completing a different task T j . The [49, 16, 13, 23], which uses a learned visual dynamics model\n output of R θ represents a “similarity score” that indicates how to plan a sequence of actions. We condition R θ on a human\n similar task-wise the two input videos are. More formally, R θ demonstration video d i of the desired task T i and then use the\n is trained to minimize the following objective, which is the predicted similarity as a reward for optimizing actions with a\n average cross-entropy loss over video pairs in the distribution learned visual dynamics model (See Figure 4).\n of the training data: Concretely, we first train an action-conditioned video pre-\n diction model p (s |s ,a ) using the SV 2 P model\n J(θ)=E [log(R (d ,d(cid:48)))+log(1−R (d ,d ))]. (1) φ t+1:t+H t t:t+H \n Dh∪Dr θ i i θ i j [2]. We then uses the cross-entropy method (CEM) [37] with\n Since in-the-wild human videos are so diverse and visually this dynamics model p to choose actions that maximize\n φ \n different from the robot environment, a large challenge lies similarity with the given demonstration. More specifically, for\n in bridging the domain gap between the range of human each iteration of CEM, for an input image s , we sample G\n t \n video environments and the robot environment. In optimizing action trajectories of length H and roll out G corresponding\n Equation 1, R must learn to identify functional behavior in predictedtrajectories{s }g usingp .Wethenfeedeach\n θ t+1:t+H φ "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n Human Demo: Close the drawer \n Final Task Execution \n \n \n \n \n \n DVD à0.5 DVD à0.1 DVD à0.9 \n \n \n \n \n \n Sampled Future Trajectories \n \n Visual \n Dynamics \n Model \n Sampled Actions Current State \n Figure 4: Planning with DVD. To use DVD to select actions, we perform visual model predictive control (VMPC) with a learned visual\n dynamics model. Specifically, we sample many action sequences from an action distribution and feed each through our visual dynamics\n model to get many “imagined” future trajectories. For each trajectory, we feed the predicted visual sequence into DVD along with the human\n provideddemonstrationvideo,whichspecifiesthetask.DVDscoreseachtrajectorybyitsfunctionalsimilaritytothehumandemovideo,and\n steps the highest scored action sequence in the environment to complete the task.\n predicted trajectory and demonstration d i into R θ , resulting SS S iimm im Suu uillmaa la ttui t ioo iloanntn i o EE Ennn n vv Ev ssnsvs WWWW iididdo iood wwwo XXXw 222 X 0002 0000 EEE 0 nnn v Evvssnsvs\n in G similarity scores corresponding to the task-similarity \n between d and each predicted image trajectory. The action \n trajectoryc \n i \n orrespondingtotheimagesequencewiththehighest \n T \n T \n Tr \n r \n ar \n a \n ai i Tn \n n \n inr a E \n E \n Ein nnnv\n v \n v Env \n predicted probability is then executed to complete the task. Train Env Train Env Rearranged\n TTr Traarianini n EE Ennnvvv TTr Traariainnin EE Ennnvvv RR Reeeaaarrrrraraannngggeeeddd\n The full algorithm with all stages is described in Algorithm 1. \n Te Tsets Et n Evnv\n IV. EXPERIMENTS Tes Tt e Esntv Env\n In our experiments, we aim to study how effectively our Test Env 1 Test Env 2 Test Env 3\n method DVD can leverage diverse human data, and to what Te TTs eet s st Et E n En vn v 1 v 1 1 TTTeeesssttt E EEnnnvvv 222 TTTeeessstt t EE Ennnvvv 33 3\n extent doing so enables generalization to unseen environments Figure 5: Environment Domains. We consider various simulated\n tabletop environments that have a drawer, a faucet, a coffee cup, and\n and tasks. Concretely, we study the following questions: \n a coffee machine, as well as a real robot environment with a tissue\n 1) By leveraging human videos is DVD able to more box, stuffed animal, and either a file cabinet or a toy kitchen set. In\n effectively generalize to new environments? the simulation experiments, half of the robot demonstrations that are\n 2) By leveraging human videos is DVD able to more used for training come from the train env and the other half from the\n rearranged train env. \n effectively generalize to new tasks? \n 3) Does DVD enable robots to generalize from a single environment generalization, each of which is progressively\n humandemonstrationmoreeffectivelythanpriorwork? more difficult, shown in Figure 5. These include an original\n 4) Can DVD infer rewards from a human video on a real variant (Train Env), from which we have task demos, as well\n robot? as a variant with changed colors (Test Env 1), changed colors\n In the following sections, we first describe our experimental and viewpoint (Test Env 2), and changed colors, viewpoint,\n setup and then investigate the above questions. For videos and object arrangement (Test Env 3).\n please see https://sites.google.com/view/dvd-human-videos. b) Tasks: Weevaluateourmethodonthreetargettasksin\n simulation, specifically, (1) closing an open drawer, (2) turning\n A. Simulated Experimental Set-Up \n thefaucetright,and(3)pushingthecupawayfromthecamera\n a) Environments: For our first 3 experimental questions, to the coffee machine. Each task is specified by an unseen\n we utilize a Mu Jo Co [48] simulated tabletop environment in-the-wild human video completing the task (See Figure 6).\n adapted from Meta-World [55] that consists of a Sawyer robot c) Training Data: For human demonstration data, we\n arminteractingwithadrawer,afaucet,andacoffeecup/coffee use the Something-Something-V 2 dataset [21], which contains\n machine. We use 4 variants of this environment to study 220,837 total videos and 174 total classes, each with humans"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n Move faucet to the right in Test Env 3 \n \n High-ranked \n Human video \n Trajectories: \n Score = .88 \n Low-ranked \n Trajectory: \n Score < 0.01 \n Figure 6: Example Rankings During Planning. Examplesofpredicted trajectories thatarerankedhighand lowforthe taskofmovingthe\n faucet to the right in the test env 3 with the similarity scores that were outputted by DVD. DVD associates high functional similarity with\n trajectories that complete the same task as specified in the human video and low scores to trajectories that do not, despite the large visual\n domain shift between the given videos and the simulation environments. \n performing a different basic action with a wide variety of \n different objects in various environments. Depending on the \n experiment, we choose videos from up to 15 different human \n tasks for training DVD, where each task has from 853-3170 \n videos (See Appendix for details). For our simulated robot \n demonstration data, we assume 120 video demonstrations of \n 3 tasks in the training environment only (See Figure 5). We \n ablate the number of robot demos needed in Section IV-F. \n B. Experiment 1: Environment Generalization \n In our first experiment, we aim to study how varying the \n amount of human data used for training impacts the reward \n function’s ability to generalize across environments. To do so, \n we train DVD on robot videos of the 3 target tasks from the \n training environment, as well as varying amounts of human \n data,andmeasuretaskperformanceacrossunseenenvironments. \n One of our core hypotheses is that the use of diverse human \n data can improve the reward function’s ability to generalize \n to new environments. To test this hypothesis, we compare Figure 7: Effectof Human Dataon Environment Generalization.\n training DVD on only the robot videos (Robot Only), to We compare DVD’s performance on seen and unseen environments\n when trained on only robot videos compared to varying number\n training DVD on a mix of the robot videos and human videos \n of human videos. We see that training with human videos provides\n from K tasks (Robot + K Human Tasks). Note that the \n significantlyimprovedperformanceoveronlytrainingonrobotvideos,\n first 3 human tasks included are for the same 3 target tasks and that DVD is generally robust to the number of different human\n in the robot videos, and thus K > 3 implies using human video tasks used. Each bar shows the average success rate over all\n videos for completely unrelated tasks to the target tasks. To 3 target tasks, computed over 3 seeds of 100 trials, with error bars\n denoting standard error. \n evaluate the learned reward functions, we report the success \n rate from running visual MPC with respect to the inferred generally robust to the number of human tasks included, even\n reward, where we train the visual dynamics model on data ifthesetasksareunrelated tothetargettasks.Evenwhenusing\n that is autonomously collected in the test environment. (See 9 completely unrelated tasks, performance greatly exceeds not\n Appendix A for details). All methods infer the reward from using any humanvideos. Qualitatively, in Figure 6, we observe\n a single human video. However, to provide an even stronger that DVD gives high similarity scores to trajectories that are\n comparison,wealsoevaluatethe Robot Only DVDmodelwith completing the task specified by the human video demo and\n a robot demo at test time Robot Only (Robot Demo), since low scores to trajectories that have less relevant behavior.\n this model has only been trained with robot data. \n C. Experiment 2: Task Generalization \n In Figure 7, we report the success rate using each reward \n function, computed over 3 randomized sets of 100 trials. In our second experiment, we study how including human\n Our first key observation is that training with human videos data for training affects the reward function’s ability to\n significantly improves environment generalization performance generalize to new tasks. In this case, we do not train on any\n over using only robot videos (20% on average), even when (human or robot) data from the target tasks, and instead train\n the robot only comparison gets the privileged information of DVD on robot videos of the 3 different tasks from the training\n a robot demonstration. Additionally, we observe that DVD is environment, namely (1) opening the drawer, (2) moving"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n something from right to left, (3) not moving any objects, as \n well as varying amounts of human data. To again test how \n human videos affect generalization, we compare the same \n methods as in the previous experiment. Since we are testing \n taskgeneralization,allevaluationisinthetrainingenvironment. \n In the bottom section of Table I, we report the success rate \n using DVD with varying amounts of human data, computed \n over 3 randomizedsetsof 100 trials.Similartotheconclusions \n of the environment generalization experiment, first we find \n that training with human videos significantly improves task \n generalization performance over using only robot videos (by \n roughly 10%onaverage),evenwiththerobotonlycomparison \n conditioned on a robot demonstration. Given a human video \n demonstration, Robot Only does well at closing the drawer, \n but is completely unable to move the faucet to the right, \n suggesting that it is by default moving to the same area of \n the environment and is unable to actually distinguish tasks. \n This is unsurprising considering the reward function is not Figure 8: Environment Generalization Prior Work Comparison.\n trained on any human videos. Second, we observe that on Compared to Concept 2 Robot, the most relevant work leveraging “in-\n average, including human videos for 6 unrelated human tasks the-wild” human videos, as well as a demo-conditioned behavioral\n cloningpolicyandarandompolicy,DVDperformssignificantlybetter\n can significantly improve performance, leading to more than \n across all environments, and over 20% better on average. Each bar\n a 20% gap over just training with robot videos, suggesting \n shows the average success rate over all 3 target tasks, computed over\n that training with human videos from more unrelated tasks is 3 seeds of 100 trials, with error bars denoting standard error.\n particularly helpful for task generalization. \n V 2 dataset,thereisnonaturalmethodfortestinggeneralization\n D. Experiment 3: Prior Work Comparison to an unseen task specified by a human video. We see that\n DVD outperforms both other baselines by over 30%.\n Inthisexperiment,westudyhoweffective DVDiscompared \n First, DVD’s significant improvement over Concept 2 Robot\n toothertechniquesforlearningfromin-the-wildhumanvideos. \n The most related work is Concept 2 Robot [43], which uses a suggests that in learning reward functions which address the\n human-domain robot gap, using some robot data, even in small\n pretrained 174-way video classifier on only the Sth Sth V 2 \n quantities,isimportantforgoodperformance.Second,methods\n dataset (no robot videos) as a reward. Since this method is \n like demo-conditioned behavior cloning likely require many\n not naturally conducive to one-shot imitation from a video \n morerobotdemonstrationstolearngoodpolicies,aspriorwork\n demonstration, during planning we follow the method used in \n in demo-conditioned behavior cloning often use on the order\n theoriginalpaperandtaketheclassificationscoreforthetarget \n of thousands of demonstrations [46]. DVD on the other hand,\n task from the predicted robot video as the reward (instead of \n uses the demos only to learn a reward function and offloads\n conditioningonahumanvideo).Unliketheopen-looptrajectory \n the behavior learning to visual MPC. Lastly, when examining\n generator used in the original paper, we use the same visual \n the performance of demo-conditioned behavioral cloning on\n MPCapproachforselectingactionsforafaircomparisonofthe \n each individual task, we see the policy learns to ignore the\n learned reward function; we expect the relative performance of \n conditioning demo and mimics one trajectory for one of the\n the reward functions to be agnostic to this choice. In addition, \n we also compare to a demo-conditioned behavioral cloning targettasks,doingwellforonlythattaskbutcompletelyfailing\n at other tasks, suggesting that the policy struggles to infer the\n method,similartowhathasbeenusedinpriorwork[54,3,46]. \n task from the visually diverse human videos.\n Wetrainthisapproachusingbehaviorcloningonthe 120 robot \n demonstrations and their actions for 3 tasks conditioned on a \n E. Experiment 4: Real Robot Efficacy \n video demo of the task from either a robot or a human. See \n Appendix A for more details on this comparison. We also To answer our last experimental question, we study how\n include a comparison to a random policy. DVD with human data enables better environment and task\n In Figure 8 we compare DVD with 6 human videos to these generalization on a real Widow X 200 robot. We consider a\n prior methods on the environment generalization experiment similar setup as described in Sections IV-B and IV-C, where\n presented in Section IV-B. Across all environments, DVD DVDisnowtrainedon 80 robotdemosfromeachof 2 training\n performs significantly better than all three comparisons on the tasksinatrainingenvironmentandhumanvideos.Thenduring\n targettasks,and 20%betteronaveragethanthebest-performing testing, DVD is used as reward for visual MPC in an unseen\n othermethod.In Table I,wemakethesamecomparison,nowon environment, performing both a seen and unseen task.\n theexperimentoftaskgeneralizationpresentedin Section IV-C. Specifically,inourrealrobotsetup,thetrainingenvironment\n Since Concept 2 Robot is not demo-conditioned and is already consists of a file cabinet, and in the testing environment, it is\n trained on all 174 possible human video tasks in the Sth Sth replaced with a toy kitchen set (See Figure 5). The training"
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n Method Close drawer Move faucet to right Push cup away from the camera Average\n \n Random 20.00 (3.00) 9.00 (1.73) 32.33 (8.08) 20.44 (2.78) \n Behavioral Cloning Policy 0.00 (0.00) 45.33 (38.84) 1.00 (0.00) 15.44 (12.95)\n Concept 2 Robot 174-way classifier n/a n/a n/a n/a \n DVD, Robot Only (Human Demo) 67.33 (4.51) 1.00 (1.00) 29.67 (0.58) 32.67 (1.53)\n DVD, Robot Only (Robot Demo) 29.33 (14.99) 23.67 (1.53) 28.33 (0.58) 27.11 (5.23)\n DVD, Robot + 3 Human Tasks 66.33 (6.03) 19.33 (0.58) 40.00 (6.93) 41.89 (3.10)\n DVD, Robot + 6 Human Tasks 59.00 (5.29) 17.00 (7.94) 56.33 (11.06) 44.11 (1.39)\n DVD, Robot + 9 Human Tasks 57.67 (0.58) 52.67 (1.15) 55.00 (5.57) 55.11 (2.04)\n DVD, Robot + 12 Human Tasks 31.67 (9.02) 49.00 (6.24) 57.33 (2.08) 46.00 (2.60)\n Table I: Task generalization results in the original environment. DVD trained with human videos performs significantly better on average\n than with only robot videos, a baseline behavioral cloning policy, and random. We report the average success rate for all 3 target tasks,\n computed over 3 seeds of 100 trials, as well as the standard deviation in parentheses.\n Test Env + \n Method (Out of 20 Trials) Test Env Unseen Task \n 80 \n Random 5 5 \n Concept 2 Robot 174-way classifier 4 n/a 70 \n DVD, Robot Only (Human Demo) 5 6 \n 60 \n DVD, Robot Only (Robot Demo) 5 8 \n DVD, Robot + 2 Human Tasks 7 7 50 \n DVD, Robot + 6 Human Tasks 13 14 \n DVD, Robot + 9 Human Tasks 9 11 40 \n DVD, Robot + 12 Human Tasks 10 9 \n 30 \n Table II: Env and task generalization results on a real robot. \n We report successes out of 20 trials on a Widow X 200 in an unseen 20 \n environment on two different tasks, one on closing a toy kitchen \n door and another on moving a tissue box to the left. On both, DVD 10 \n performs significantly better when trained with human videos than \n 0 \n with only robot demonstrations. Train env Test env 1 Test env 2 Test env 3 Average\n Environment \n tasks are “Closing Something” and “Pushing something left to \n right” and the test tasks are “Closing Something” (seen) and \n “Pushing something right to left” (unseen). \n We compare DVD with varying amounts of human data to \n only robot data and baselines in Table II, where we report \n the success rate out of 20 trials when used with visual MPC \n conditioned on a human demo of the task. DVD trained with \n humanvideoshasabouttwicethesuccessratewhenleveraging \n the diverse human dataset than when relying only on robot \n videos.Inparticular,DVDtrainedwith 6 tasksworthofhuman \n videos succeeds over 65-70% of the time whereas robot only \n succeeds at most 40%. We also observe that in general using \n human videos from unrelated tasks improves over only using \n humanvideosforthetrainingtasks.Finally,weseequalitatively \n in Figure 14 in Appendix C that DVD captures the functional \n task being specified, in this case closing the door. \n F. Ablation on Amount of Robot Data for Training \n In our previous simulation experiments, we use 120 robot \n demonstrations per task. While this is a manageable number \n of robot demonstrations, it would be better to rely on fewer \n demonstrations. Hence, we ablate on the number of robot \n demonstrations used during training and evaluate environment \n generalization. In Figure 9, we see that the performance of \n DVD decreases by only a small margin when using as few \n as 20 robot demonstrations per task. This suggests that by \n leveraging the diversity in the human data, DVD can perform \n well even with very little robot data. \n )001 \n fo \n tuo( \n sesseccu S \n DVD With Varying Amounts of Robot Data\n Robot (20) + 6 Human Tasks\n Robot (40) + 6 Human Tasks\n Robot (120) + 6 Human Tasks\n Figure 9: Ablation on Amount of Robot Data Used for Training.\n While using 120 robot demonstrations per task slightly benefits\n performanceoverusingonly 20 or 40,DVDstillperformscomparably\n with fewer robot demos. \n V. LIMITATIONSANDFUTUREWORK \n Wepresentedanapproach,domain-agnosticvideodiscrimina-\n tor (DVD), that leverages the diversity of “in-the-wild” human\n videos to learn generalizable robotic reward functions. Our\n experiments find that training with a large, diverse dataset of\n human videos can significantly improve the reward function’s\n ability to generalize to unseen tasks and environments, and\n can be combined with visual MPC to solve tasks.\n There are multiple limitations and directions for future work.\n First, our method focuses only on learning reward functions\n that generalize and does not learn a generalizable policy or\n visual dynamics model directly. This is a necessary next step\n to achieve agents that broadly generalize and is an exciting\n direction for future work. Second, while limited in quantity,\n our work assumes access to some robot demonstrations and\n task labels for these demos and for all of the human videos.\n Techniques that can sidestep the need for this supervision\n would further enhance the scalability of DVD. Lastly, so far\n we have only tested DVD on coarse tasks that don’t require\n fine-grained manipulation. Designing more powerful visual\n models and testing DVD with them on harder, more precise\n tasks is another exciting direction for future work."
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n ACKNOWLEDGMENTS multi-robot learning. In Conference on Robot Learning,\n 2019. \n The authors would like to thank Ashvin Nair as well as \n [11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\n members of the IRIS lab for valuable discussions. This work \n L. Fei-Fei. Image Net: A Large-Scale Hierarchical Image\n was supported in part by Schmidt Futures, by ONR grant \n Database. In CVPR 09, 2009. \n N 00014-20-1-2675, and by an NSF GRFP. Chelsea Finn is a \n [12] Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina\n CIFAR Fellow in the Learning in Machines & Brains program. \n Toutanova. BERT: Pre-training of deep bidirectional\n REFERENCES transformers for language understanding. In Conference\n [1] Pieter Abbeel and Andrew Y. Ng. In Proceedings of of the North American Chapter of the Association for\n the Twenty-First International Conference on Machine Computational Linguistics: Human Language Technolo-\n Learning, ICML ’04, page 1, 2004. gies (NAACL-HLT), Minneapolis, Minnesota, June 2019.\n [2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Association for Computational Linguistics.\n Roy H. Campbell, and Sergey Levine. Stochastic varia- [13] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie\n tional video prediction. In International Conference on Xie, Alex Lee, and Sergey Levine. Visual foresight:\n Learning Representations, 2018. Model-baseddeepreinforcementlearningforvision-based\n [3] Alessandro Bonardi, Stephen James, and Andrew J robotic control. ar Xiv:1812.00568, 2018.\n Davison. Learning one-shot imitation from humans [14] Ashley DEdwardsand Charles LIsbell. Perceptualvalues\n without humans. IEEE Robotics and Automation Letters, fromobservation. ar Xivpreprintar Xiv:1905.07861,2019.\n 2020. [15] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia\n [4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie and Juan Carlos Niebles. Activitynet: A large-scale\n Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- video benchmark for human activity understanding. In\n lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Proceedings of the IEEE Conference on Computer Vision\n Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, and Pattern Recognition, pages 961–970, 2015.\n Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. [16] Chelsea Finnand Sergey Levine. Deepvisualforesightfor\n Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, planning robot motion. In IEEE International Conference\n Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, on Robotics and Automation (ICRA), 2017.\n Benjamin Chess, Jack Clark, Christopher Berner, Sam [17] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided\n Mc Candlish, Alec Radford, Ilya Sutskever, and Dario cost learning: Deep inverse optimal control via policy\n Amodei. Language models are few-shot learners. optimization. In International conference on machine\n ar Xiv:2005.14165, 2020. learning, pages 49–58. PMLR, 2016. \n [5] Serkan Cabi, Sergio Gómez Colmenarejo, Alexander [18] Justin Fu, Katie Luo, and Sergey Levine. Learning robust\n Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, rewards with adverserial inverse reinforcement learning.\n Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, In International Conference on Learning Representations,\n et al. Scaling data-driven robotics with reward sketching 2018. \n and batch reinforcement learning. ar Xiv:1909.12200, [19] Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and\n 2019. Sergey Levine. Variational inverse control with events:\n [6] Matthew Chang, Arjun Gupta, and Saurabh Gupta. Se- A general framework for data-driven reward definition.\n mantic visual navigation by watching youtube videos. In In Advances in Neural Information Processing Systems,\n Neur IPS, 2020. 2018. \n [7] Annie S. Chen, Hyun Ji Nam, Suraj Nair, and Chelsea [20] W. Goo and S. Niekum. One-shot learning of multi-step\n Finn.Batchexplorationwithexamplesforscalablerobotic tasksfromobservationviaactivitylocalizationinauxiliary\n reinforcement learning. IEEE Robotics and Automation video. In 2019 International Conference on Robotics and\n Letters, 2021. Automation (ICRA), 2019. \n [8] Neha Das, Sarah Bechtle, Todor Davchev, Dinesh Jayara- [21] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\n man, Akshara Rai, and Franziska Meier. Model-based ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\n inversereinforcementlearningfromvisualdemonstrations, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\n 2021. Mueller-Freitag, et al. The\" something something\" video\n [9] Pradipto Das,Chenliang Xu,Richard FDoell,and Jason J databaseforlearningandevaluatingvisualcommonsense.\n Corso. A thousand frames in just a few words: Lingual In Proceedings of the IEEE International Conference on\n description of videos through latent topics and sparse Computer Vision, pages 5842–5850, 2017.\n object stitching. In Proceedings of the IEEE conference [22] Abhinav Gupta,Adithyavairavan Murali,Dhiraj Prakashc-\n on computer vision and pattern recognition, pages 2634– hand Gandhi, and Lerrel Pinto. Robot learning in homes:\n 2641, 2013. Improving generalization and reducing dataset bias. In\n [10] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Advances in Neural Information Processing Systems,\n Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, 2018. \n Sergey Levine, and Chelsea Finn. Robonet: Large-scale [23] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben"
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n Villegas, David Ha, Honglak Lee, and James Davidson. [35] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.\n Learning latent dynamics for planning from pixels. In Zinkevich. Maximummarginplanning. In Proceedingsof\n International Conference on Machine Learning, pages the 23 rd International Conference on Machine Learning,\n 2555–2565. PMLR, 2019. ICML ’06, page 729–736, 2006. \n [24] Dmitry Kalashnikov,Alex Irpan,Peter Pastor,Julian Ibarz, [36] Jonas Rothfuss, Fabio Ferreira, Eren Erdal Aksoy, You\n Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Zhou, and Tamim Asfour. Deep episodic memory:\n Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Encoding, recalling, and predicting episodic experiences\n Scalable deep reinforcement learning for vision-based forrobotactionexecution. IEEERoboticsand Automation\n robotic manipulation. In Conference on Robot Learning, Letters, 3(4):4007–4014, 2018.\n pages 651–673. PMLR, 2018. [37] Reuven Y Rubinstein and Dirk P Kroese. The cross-\n [25] Jangwon Lee and Michael S Ryoo. Learning robot activ- entropy method: a unified approach to combinatorial op-\n ities from first-person human videos using convolutional timization,Monte-Carlosimulationandmachinelearning.\n future regression. In Proceedings of the IEEE Conference Springer Science & Business Media, 2013.\n on Computer Vision and Pattern Recognition Workshops, [38] Rosario Scalise, Jesse Thomason, Yonatan Bisk, and\n pages 1–2, 2017. Siddhartha Srinivasa. Improving robot success detection\n [26] Kyuhwa Lee, Yanyu Su, Tae-Kyun Kim, and Yiannis using static object data. In Proceedings of the 2019\n Demiris. A syntactic approach to robot imitation learning IEEE/RSJ International Conference on Intelligent Robots\n using probabilistic activity grammars. Robotics and and Systems, 2019. \n Autonomous Systems, 61(12):1323–1334, 2013. [39] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis,\n [27] Yu Xuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Sergey Levine, and Chelsea Finn. Reinforcement learn-\n Levine. Imitation from observation: Learning to imitate ing with videos: Combining offline observations with\n behaviors from raw video via context translation. In interaction. In Co RL, 2020.\n 2018 IEEE International Conference on Robotics and [40] Karl Schmeckpeper, Annie Xie, Oleh Rybkin, Stephen\n Automation (ICRA), pages 1118–1125. IEEE, 2018. Tian, Kostas Daniilidis, Sergey Levine, and Chelsea\n [28] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Finn. Learning predictive models from observation and\n Booher, Max Spero, Albert Tung, Julian Gao, John interaction. In ECCV, 2020.\n Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, [41] Pierre Sermanet, Kelvin Xu, and Sergey Levine. Un-\n and Li Fei-Fei. Roboturk: A crowdsourcing platform for supervised perceptual rewards for imitation learning.\n robotic skill learning through imitation. In Conference Proceedings of Robotics: Science and Systems (RSS),\n on Robot Learning, 2018. 2017. \n [29] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar [42] Pierre Sermanet,Corey Lynch,Yevgen Chebotar,Jasmine\n Bahl,Steven Lin,and Sergey Levine.Visualreinforcement Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-\n learning with imagined goals. In Advances in Neural contrastivenetworks:Self-supervisedlearningfromvideo.\n Information Processing Systems, 2018. Proceedings of International Conference in Robotics and\n [30] Anh Nguyen, Dimitrios Kanoulas, Luca Muratore, Dar- Automation (ICRA), 2018.\n win G Caldwell, and Nikos G Tsagarakis. Translating [43] Lin Shao,Toki Migimatsu,Qiang Zhang,Karen Yang,and\n videos to commands for robotic manipulation with deep Jeannette Bohg. Concept 2 robot: Learning manipulation\n recurrent neural networks. In 2018 IEEE International concepts from instructions and human demonstrations.\n Conference on Robotics and Automation (ICRA), pages In Proceedings of Robotics: Science and Systems (RSS),\n 3782–3788. IEEE, 2018. 2020. \n [31] Open AI, Marcin Andrychowicz, Bowen Baker, Maciek [44] P. Sharma, Deepak Pathak, and Abhinav Gupta. Third-\n Chociej,Rafal Jozefowicz,Bob Mc Grew,Jakub Pachocki, personvisualimitationlearningviadecoupledhierarchical\n Arthur Petron, Matthias Plappert, Glenn Powell, Alex controller. In Neur IPS, 2019.\n Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter [45] Avi Singh, Larry Yang, Chelsea Finn, and Sergey Levine.\n Welinder, Lilian Weng, and Wojciech Zaremba. Learning End-to-endroboticreinforcementlearningwithoutreward\n dexterous in-hand manipulation, 2019. engineering. In Proceedings of Robotics: Science and\n [32] Vladimír Petrík, Makarand Tapaswi, Ivan Laptev, and Systems, Freiburgim Breisgau, Germany, June 2019.\n Josef Sivic. Learning object manipulation skills via [46] Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler,\n approximate state estimation from real videos, 2020. Murtaza Dalal, Sergey Levinev, Mohi Khansari, and\n [33] Lerrel Pinto and Abhinav Gupta. Supersizing self- Chelsea Finn. Scalable multi-task imitation learning with\n supervision: Learning to grasp from 50 k tries and 700 autonomous improvement. In 2020 IEEE International\n robothours. In IEEEinternationalconferenceonrobotics Conference on Robotics and Automation (ICRA), pages\n and automation (ICRA), 2016. 2167–2173. IEEE, 2020. \n [34] Sören Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, [47] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter\n and Pierre Sermanet. Online object representations with Abbeel, and Sergey Levine. AVID: Learning Multi-Stage\n contrastive learning, 2019. Tasks via Pixel-Level Translation of Human Videos. In"
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n Proceedings of Robotics: Science and Systems, Corvalis, \n Oregon, USA, July 2020. \n [48] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics \n engine for model-based control. In 2012 IEEE/RSJ Inter- \n national Conference on Intelligent Robots and Systems, \n 2012. \n [49] Manuel Watter, Jost Tobias Springenberg, Joschka \n Boedecker, and Martin Riedmiller. Embed to control: \n a locally linear latent dynamics model for control from \n raw images. In Proceedings of the 28 th International \n Conference on Neural Information Processing Systems- \n Volume 2, pages 2746–2754, 2015. \n [50] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. \n Maximum entropy deep inverse reinforcement learning, \n 2016. \n [51] Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga \n Bharadhwaj,Samarth Sinha,and Animesh Garg. Learning \n by watching: Physical imitation of manipulation skills \n from human videos, 2021. \n [52] Yezhou Yang, Yi Li, Cornelia Fermüller, and Yiannis \n Aloimonos. Robot learning manipulation action plans by \n \"watching\" unconstrained videos from the world wide \n web. In AAAI, pages 3686–3693, 2015. \n [53] Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav \n Gupta, Pieter Abbeel, and Lerrel Pinto. Visual imitation \n made easy. In Co RL, 2020. \n [54] Tianhe Yu,Chelsea Finn,Sudeep Dasari,Annie Xie,Tian- \n hao Zhang, Pieter Abbeel, and Sergey Levine. One-shot \n imitation from observing humans via domain-adaptive \n meta-learning. In Proceedings of Robotics: Science and \n Systems, Pittsburgh, Pennsylvania, June 2018. \n [55] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, \n Karol Hausman, Chelsea Finn, and Sergey Levine. Meta- \n world: A benchmark and evaluation for multi-task and \n meta reinforcement learning. In Conference on Robot \n Learning, 2020. \n [56] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, \n Alberto Rodriguez, and Thomas Funkhouser. Learn- \n ing synergies between pushing and grasping with self- \n supervised deep reinforcement learning. Proceedings of \n the IEEE International Conference on Intelligent Robots \n and Systems (IROS), 2018. \n [57] Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, \n Kristian Hartikainen, Avi Singh, Vikash Kumar, and \n Sergey Levine. The ingredients of real world robotic \n reinforcement learning. In International Conference on \n Learning Representations, 2020. \n [58] Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and \n Anind K. Dey. Maximum entropy inverse reinforcement \n learning. In Proc. AAAI, pages 1433–1438, 2008. \n \n \n \n \n \n "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n APPENDIX \n \n A. Training Details \n a) Dataset Details: Depending on the experiment, we \n choose videos from the following 15 different human tasks in \n the Something-Something-V 2 dataset for training DVD, where \n eachtaskhasfrom 853-3170 trainingvideos:1)Closingsth,2) \n Moving sth away from camera, 3) Moving sth towards camera, \n 4)Openingsth,5)Pushingsthlefttoright,6)Pushingsthright \n to left, 7) Poking sth so lightly it doesn’t move, 8) Moving \n sth down, 9) Moving sth up, 10) Pulling sth from left to right, Figure 10: SV 2 P Architecture. We use video prediction models\n 11) Pulling sth from right to left, 12) Pushing sth with sth, trained via SV 2 P with the reward from DVD in order to complete\n 13) Moving sth closer to sth, 14) Plugging sth into sth, and tasksspecifiedbyagivenhumanvideo.Figuretakenfromtheoriginal\n 15) Pushing sth so that it slightly moves. We used these tasks paper [2]. \n because they are appropriate for a single-arm setting and cover thetrainingclip.Additionally,duringtraining,eachinputvideo\n a diverse range of various actions. The first seven of those is first randomly rotated between -15 and 15 degrees, scaled so\n tasks were chosen as they are relevant to tasks possible in the thattheheighthassize 120,andthenrandomlycroppedtohave\n simulation environments, but the other tasks were not chosen size 120×120×3. At planning time, the video demonstration\n for any particular reason, i.e. could have been replaced by a is spliced so that it is between 30 and 40 frames, rescaled to\n different set of 8 other appropriate tasks. For an experiment, have a height of 120 pixels, and then center-cropped to have\n if human videos for a task are used, we use all of the human size 120×120×3. The demo-conditioned behavioral cloning\n videos available in the Sth Sth V 2 training set for that task to baseline uses the same hyperparameters and training details\n train DVD. except that it uses weight decay 0.0001. \n For the simulation experiments, DVD is also trained on c) SV 2 P Training: To evaluate DVD’s performance on\n 120 robot video demonstrations of 3 tasks, half of which are potentially unseen tasks with a given human video, we\n collected in the original training environment and the other employ visual model predictive control with SV 2 P visual\n half in the rearranged training environment. These videos are prediction models trained on datasets autonomously collected\n collected via model predictive control with random shooting, a in each environment. SV 2 P learns an action-conditioned video\n groundtruthvideo-predictionmodel,andaground-truthshaped predictionmodelbysamplingalatentvariableandsubsequently\n reward particular to each task. For the real robot experiments generating an image prediction with that sample. We use the\n on the Widow X 200, in addition to varying amounts of human same architecture, which is shown in Figure 10, and default\n videos, DVD is trained on 80 robot video demonstrations of 2 hyperparameters as the original paper [2].\n tasks, which are collected in the original training environment. For each of the four simulation environments in which we\n Thesedemonstrationsarecollectedviaahard-codedscriptwith evaluate DVD, we collect 10,000 random episodes, each with\n uniform noise between -0.02 and 0.02 added to each action. 60 totalframes,oftheagentinteractinginthatenvironmentand\n To evaluate DVD’s training progress, we use a validation set train SV 2 P for 200,000 epochs on all of the data. The models\n consisting of all of the human videos available in the Sth Sth are trained to predict the next fifteen frames given an input of\n V 2 validationsetforthechosentasksaswellas 48 robotvideo five frames. To evaluate DVD in the robot test environment,\n demonstrations for the same 3 tasks with robot demos in the we train SV 2 P for 160,000 epochs on 58,500 frames worth of\n training set,withhalfofthesecomingfromtheoriginaltraining autonomously collected robot interaction in the original train\n environment and the other half from the rearranged. For the environment,andthenwefinetunethemodelforanother 60,000\n Widow X 200 experiments,weadd 8 robotvideodemonstrations epochson 21,000 framesworthofautonomouslycollecteddata\n for each of the 2 tasks into the validation set. inthetestenvironmentthathasthetoykitchendoor.Collecting\n b) Hyperparameters: For DVD, the similarity discrimi- this data on the Widow X 200 took a total of roughly 75 hours\n nator is trained with a learning rate of 0.01 using stochastic but was entirely autonomous.\n gradient descent (SGD) with momentum 0.9 and weight decay d) Additional DVD Details: Here we expand on some of\n 0.00001. We use a batch size of 24, where each element of the DVDimplementationdetailstoucheduponin Section III-C,\n the batch consists of a triplet with two videos having the same particularly the way that batches are sampled during training.\n task label and the third having a different label. Each version Each batch consists of triplets (d ,d(cid:48),d ), where d is labeled\n i i j j \n of DVD in the experiments is trained for 120 epochs, where as a different task as d and d(cid:48), which are labeled as the same\n i i \n one epoch consists of 200 optimizer steps. For each epoch, task.Ineachtriplet,d israndomlysampledwith 0.5 probability\n i \n the video clips fed into DVD for training are sequences of of being a robot demonstration. Then, if d is from a task with\n i \n consecutive frames with random length between 20 and 40 only human data, d(cid:48) will be chosen from the remaining human\n i \n frames taken from the original video. If the original video has data for that task; otherwise it is chosen to be a robot video\n fewer than the randomly selected amount of frames, the last from that task with 0.5 probability. Finally, d is randomly\n j \n frame is repeated to achieve the desired number of frames for sampled repeatedly (usually just once) with 0.5 probability of"
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n being a robot demonstration until a video with a different task tasks: 1) Closing the drawer, which is defined as the last frame\n label from d and d(cid:48) is sampled. in the 60-frame trajectory having the drawer pushed in to be\n i i \n e) Comparisons: For the Concept 2 Robot comparison, less than 0.05, where it starts open at 0.07, 2) Turning the\n we use the same 174-way classifier that the paper used and faucet to the right more than 0.01 distance, where it starts at 0,\n do not alter it. For the demo-conditioned behavioral cloning and 3) Moves cup to be less than 0.07 distance to the coffee\n comparison,weuseamethodsimilarto[3],[46],and[54].We machine, where the cup starts out at least 0.1 away. We run\n train a model that takes in as input the concatenated encodings 100 trials for 3 different seeds for each task for every method\n of a conditioning video and the image state from one of the in all experiments.\n robot demonstrations in the training set and outputs an action c) Real Robot Experiments: On the Widow X 200, for all\n that aims to lead the agent from the given image state to experiments,ineachtrialweplan 1 trajectoryoflength 10.For\n completing the same task as shown in the conditioning demo. this trajectory, we run 2 iterations of the cross-entropy method\n Duringtraining,themodelistrainedonbatchesof(conditioning (CEM), sampling 100 action sequences and refitting to the\n video,robotdemonstration)pairs,wheretheconditioningvideo top 20 repeatedly. We then choose one of the top 5 predicted\n is randomly taken from the combined human and robot dataset trajectories with the highest functional similarity score given\n andarobotdemonstrationwiththesametasklabelisrandomly by DVD to execute in the environment. We evaluate on the\n chosen.Becausetherearemanymorehumanvideosthanrobot following two target tasks: 1) Closing the toy kitchen door,\n demonstrations, the conditioning video is chosen to be a robot where a success is recorded for any trial where the robot arm\n demonstration with 50% probability, which is analogous to completely closes the door, and 2) Pushing the tissue box to\n the balancing of batches used in DVD. Note that this method the left, where the robot arm must clearly push the tissue box\n cannot naturally use human videos from tasks for which there left of its original starting position. We run 20 trials for each\n are no robot demonstrations. task for each method. \n The behavior cloning model uses the same pretrained \n C. Additional Experimental Results \n video encoder as DVD to encode the conditioning demo \n as well as a pretrained Res Net 18 for the image state. The In our simulation environment generalization experiments,\n resulting features are concatenated and passed into an MLP we evaluate on the three tasks of 1) Closing the drawer, 2)\n that takes an input of size [1512] and has fully connected Turning the faucet to the right, and 3) Pushing the cup away\n layers [512,256,128,64,32,a], where each layer except the from the camera. In Section IV, we reported the average\n last is followed by a Re LU activation and a corresponds to the performance across all the three tasks. In Figure 11, we\n numberofactiondimensions.Themodelistrainedtominimize presenttheindividualtaskresultsfor DVDtrainedwithvarying\n mean squared error between the output action and the true amounts of human data. The conclusions of these experiments\n action. are the same as those in Section IV, in that leveraging diverse\n human videos in DVD allows for more effective generalization\n B. Experimental Details \n across new environments rather than relying only on robot\n a) Domains: For the simulation domains, we use a videos. \n Mujoco simulation built off the Meta-World environments In Figure 12, we present results on the individual tasks\n [55]. In simulation, the state space is the space of RGB image across all four environments for DVD trained with 6 tasks\n observations with size [180, 120, 3]. We use a continuous worth of human videos compared with our three comparisons:\n action space over the linear and angular velocity of the robot’s Concept 2 Robot [43], a demo-conditioned behavior cloning\n gripper and a discrete action space over the gripper open/close policy, and a random policy. On average over all of the\n action, for a total of five dimensions. For the robot domain, we environments, DVD performs over 40% better on the drawer\n considerareal Widow X 200 robotinteractingwithafilecabinet, task and 30% better on the faucet task than the next best\n a tissue box, a stuffed animal, and a toy kitchen set. The state performing method. It also performs reasonably on the cup\n space is the space of RGB image observations with size [120, task;Concept 2 Robotjustperformsparticularlywellonthattask\n 120, 3], and the action space consists of the continuous linear since it often chooses to push the cup away no matter which\n velocity of the robot’s gripper in the x and z directions as well task is specified. The behavioral cloning policy has somewhat\n as the gripper’s y-position, for a total of three dimensions. erratic behavior, mimicking the trajectory for one of the target\n b) Simulation Experiments: For all environment and task tasks in each environment and doing well on that task but not\n generalization experiments, in each trial we plan 3 trajectories ontheothertasks.Hence,weseethatboth Concept 2 Robotand\n of length 20. For each trajectory, we sample 100 action the behavioral cloning policy are not able to provide effective\n sequencesuniformlyrandomlyandrandomlychooseoneofthe multi-task reward signals for each environment.\n top 5 predictedtrajectorieswiththehighestfunctionalsimilarity Additionally, in Figure 13, we show the accuracy curves\n score given by DVD to execute in the environment. For on the training and validation sets while training DVD. We\n Concept 2 Robot, we take one of the top 5 predicted trajectories see unsurprisingly that the model trained only on three\n with the highest classification score for the specified task, and tasks of robot demonstrations (Robot Only) has the highest\n for the behavioral cloning policy, we simply take the predicted validation accuracy at 99%. However, while adding human\n action at each state. We evaluate on the following three target videos significantly increases the difficulty of the optimization,"
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 12:Environment Generalization Prior Work Comparison.\n Wecompare DVD’sperformanceto Concept 2 Robot,themostrelevant\n work, a demo-conditioned behavioral cloning policy, and a random\n policy. On average across environments, DVD performs around or\n over 30% better than the next-best performing method on two of the\n three tasks. Each bar shows the average success rate over all 3 target\n tasks, computed over 3 seeds of 100 trials, with error bars denoting\n standard error. \n themodelsremaingenerallyrobust,with DVDtrainedonrobot\n Figure 11:Effectof Human Dataon Environment Generalization. \n We compare DVD’s performance on seen and unseen environments data and 12 tasks worth of human videos still obtaining 89%\n when trained on only robot videos compared to varying number of validation accuracy. We find in our experiments in Section IV\n humanvideos.Acrossallthreetasks,weseethattrainingwithhuman thatthistrade-offindiscriminatoraccuracyfromaddinghuman\n videosprovidessignificantlyimprovedperformanceoveronlytraining \n videos to the training set results in much greater ability to\n on robot videos, and that DVD is generally robust to the number of \n generalize to unseen environments and tasks.\n differenthumanvideotasksused.Eachbarshowstheaveragesuccess \n rate over all 3 target tasks, computed over 3 seeds of 100 trials, with Finally, in Figure 14, we include examples on the real\n error bars denoting standard error. Widowx 200 of predicted trajectories and their similarity scores\n with a human video demonstration given by DVD. We see that"
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 13: Accuracy Curves During DVD Training. We plot both training and validation accuracies over the course of training DVD for\n 150 epochs with varying amounts of human data. The accuracies gradually decrease as more human videos are added, but we find that this\n trade-off is worthwhile for greater generalization capabilities. \n \n \n \n \n \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n Closing the toy kitchen door in Test Env 1\n \n High-ranked \n Human video \n Trajectories: \n Score = 0.79 \n \n Low-ranked \n Trajectory: \n Score = 0.43 \n \n Figure 14: Rankings on the real robot. Examples of predicted trajectories on the Widow X 200 that are ranked high and low for the task of\n closing an unseen toy kitchen door. DVD gives the predicted trajectory where the door is closed a high similarity score and the predicted\n trajectory where the door stays open a low similarity score. \n \n DVD highly ranks trajectories that are completing the same \n task as demonstrated in the given human video. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n "
  }
]