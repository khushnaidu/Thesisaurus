[
  {
    "page_num": 1,
    "text": " \n \n \n https://robotics-transformer 2.github.io\n 2023-8-1 \n \n RT-2: Vision-Language-Action Models Transfer \n \n Web Knowledge to Robotic Control \n \n \n Anthony Brohan,Noah Brown,Justice Carbajal,Yevgen Chebotar,Xi Chen,Krzysztof Choromanski,\n Tianli Ding,Danny Driess,Avinava Dubey,Chelsea Finn,Pete Florence,Chuyuan Fu, \n Montse Gonzalez Arenas,Keerthana Gopalakrishnan,Kehang Han,Karol Hausman,Alexander Herzog,\n Jasmine Hsu,Brian Ichter,Alex Irpan,Nikhil Joshi,Ryan Julian,Dmitry Kalashnikov,Yuheng Kuang,\n Isabel Leal,Lisa Lee,Tsang-Wei Edward Lee,Sergey Levine,Yao Lu,Henryk Michalewski,Igor Mordatch,\n Karl Pertsch,Kanishka Rao,Krista Reymann,Michael Ryoo,Grecia Salazar,Pannag Sanketi,\n Pierre Sermanet,Jaspiar Singh,Anikait Singh,Radu Soricut,Huong Tran,Vincent Vanhoucke,Quan Vuong,\n Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Jialin Wu,Fei Xia,Ted Xiao,Peng Xu,Sichun Xu,Tianhe Yu,\n and Brianna Zitkovich \n Google Deep Mind.Authorslistedinalphabeticalorder,withcontributionslistedin Appendix A.\n \n \n Westudyhowvision-languagemodelstrainedon Internet-scaledatacanbeincorporateddirectlyinto\n end-to-endroboticcontroltoboostgeneralizationandenableemergentsemanticreasoning. Ourgoalis\n toenableasingleend-to-endtrainedmodeltobothlearntomaprobotobservationstoactionsandenjoy\n thebenefitsoflarge-scalepretrainingonlanguageandvision-languagedatafromtheweb. Tothisend,\n weproposetoco-fine-tunestate-of-the-artvision-languagemodelsonbothrobotictrajectorydataand\n \n Internet-scalevision-languagetasks,suchasvisualquestionanswering. Incontrasttootherapproaches,\n weproposeasimple,generalrecipetoachievethisgoal: inordertofitbothnaturallanguageresponses\n androboticactionsintothesameformat,weexpresstheactionsastexttokensandincorporatethem\n directly into the training set of the model in the same way as natural language tokens. We refer to\n suchcategoryofmodelsasvision-language-actionmodels(VLA)andinstantiateanexampleofsuch\n amodel,whichwecall RT-2. Ourextensiveevaluation(6 kevaluationtrials)showsthatourapproach\n leadstoperformantroboticpoliciesandenables RT-2 toobtainarangeofemergentcapabilitiesfrom\n Internet-scaletraining. Thisincludessignificantlyimprovedgeneralizationtonovelobjects,theability\n tointerpretcommandsnotpresentintherobottrainingdata(suchasplacinganobjectontoaparticular\n numberoricon),andtheabilitytoperformrudimentaryreasoninginresponsetousercommands(such\n \n aspickingupthesmallestorlargestobject,ortheoneclosesttoanotherobject). Wefurthershowthat\n incorporatingchainofthoughtreasoningallows RT-2 toperformmulti-stagesemanticreasoning,for\n examplefiguringoutwhichobjecttopickupforuseasanimprovisedhammer(arock),orwhichtype\n ofdrinkisbestsuitedforsomeonewhoistired(anenergydrink). \n \n \n 1. Introduction \n \n High-capacity models pretrained on broad web-scale datasets provide an effective and powerful\n platformforawiderangeofdownstreamtasks: largelanguagemodelscanenablenotonlyfluenttext\n generation(Aniletal.,2023;Brohanetal.,2022;Open AI,2023)butemergentproblem-solving(Cobbe\n et al., 2021; Lewkowycz et al., 2022; Polu et al., 2022) and creative generation of prose (Brown\n et al., 2020; Open AI, 2023) and code (Chen et al., 2021), while vision-language models enable\n open-vocabulary visual recognition (Kirillov et al., 2023; Minderer et al., 2022; Radford et al., 2021)\n andcanevenmakecomplexinferencesaboutobject-agentinteractionsinimages(Alayracetal.,2022;\n Chenetal.,2023 a,b;Driessetal.,2023;Haoetal.,2022;Huangetal.,2023;Wangetal.,2022). Such\n semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously\n useful for generalist robots that must perform a variety of tasks in real-world environments. However,\n \n \n Correspondingauthor(s):chebotar@google.com,tianheyu@google.com,karolhausman@google.com\n ¬© 2023 Google Deep Mind.Allrightsreserved \n 3202 \n lu J \n 82 \n ]OR.sc[ \n 1 v 81851.7032:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n Internet-Scale VQA + Robot Action Data Vision-Language-Action Models for Robot Control Closed-Loop\n Robot Control \n Q: What is happening Q: What should the robot RT-2 \n in the image? do to <task>? A: ‚Ä¶ Large Language Model \n A: 311 423 170 55 244 \n A grey donkey walks \n down the street. \n Put the strawberry\n Q: Que puis-je faire avec Vi T into the correct bowl\n ces objets? \n A: 3455 1144 189 25673 \n Faire cuire un g√¢teau. \n ŒîT = [0.1, -0.2, 0] \n Q: What should the robot A: 132 114 128 5 25 156 De-Tokenize ŒîR = [10‚àò , 25‚àò , -7‚àò ] Pick the nearly falling bag\n do to <task>? Robot Action \n A: 132 114 128 5 25 156 \n Œî Translation = [0.1, -0.2, 0] \n ŒîRotation = [10‚àò , 25 ‚àò, -7 ‚àò ] Co-Fine-Tune Deploy \n Pick object that is different\n Figure 1|RT-2 overview:werepresentrobotactionsasanotherlanguage,whichcanbecastintotexttokensand\n trainedtogetherwith Internet-scalevision-languagedatasets. Duringinference,thetexttokensarede-tokenized\n into robot actions, enabling closed loop control. This allows us to leverage the backbone and pretraining\n of vision-language models in learning robotic policies, transferring some of their generalization, semantic\n understanding,andreasoningtoroboticcontrol. Wedemonstrateexamplesof RT-2 executionontheproject\n website: robotics-transformer 2.github.io. \n it is unclear how robots should acquire such capabilities. While a brute force approach might entail\n collectingmillionsofroboticinteractiontrials,themostcapablelanguageandvision-languagemodels\n are trained on billions of tokens and images from the web (Alayrac et al., 2022; Chen et al., 2023 a,b;\n Huang et al., 2023) ‚Äì an amount unlikely to be matched with robot data in the near future. On the\n other hand, directly applying such models to robotic tasks is also difficult: such models reason about\n semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such\n as Cartesian end-effector commands. While a number of recent works have sought to incorporate\n language models (LLMs) and vision-language models (VLMs) into robotics (Ahn et al., 2022; Driess\n etal.,2023;Vempralaetal.,2023),suchmethodsgenerallyaddressonlythe‚Äúhigherlevel‚Äùaspectsof\n robotic planning, essentially taking the role of a state machine that interprets commands and parses\n them into individual primitives (such as picking and placing objects), which are then executed by\n separate low-level controllers that themselves do not benefit from the rich semantic knowledge of\n Internet-scale models during training. Therefore, in this paper we ask: can large pretrained vision-\n language models be integrated directly into low-level robotic control to boost generalization and\n enable emergent semantic reasoning? \n To this end, we explore an approach that is both simple and surprisingly effective: we directly\n train vision-language models designed for open-vocabulary visual question answering and visual\n dialogue to output low-level robot actions, along with solving other Internet-scale vision-language\n tasks. Although such models are typically trained to produce natural language tokens, we can train\n them on robotic trajectories by tokenizing the actions into text tokens and creating ‚Äúmultimodal\n sentences‚Äù(Driessetal.,2023)that‚Äúrespond‚Äùtoroboticinstructionspairedwithcameraobservations\n by producing corresponding actions. In this way, vision-language models can be directly trained to\n actasinstructionfollowingroboticpolicies. Thissimpleapproachisincontrastwithprioralternatives\n for incorporating VLMs into robot policies (Shridhar et al., 2022 a) or designing new vision-language-\n action architectures from scratch (Reed et al., 2022): instead, pre-existing vision-language models,\n with already-amortized significant compute investment, are trained without any new parameters to\n output text-encoded actions. We refer to this category of models as vision-language-action (VLA)\n models. We instantiate VLA models by building on the protocol proposed for RT-1 (Brohan et al.,\n 2022), using a similar dataset, but expanding the model to use a large vision-language backbone.\n Hence we refer to our model as RT-2 (Robotics Transformer 2). We provide an overview in Figure 1.\n 2 \n "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n We observe that robotic policies derived from such vision-language models exhibit a range of\n remarkable capabilities, combining the physical motions learned from the robot data with the ability\n to interpret images and text learned from web data into a single model. Besides the expected benefit\n of dramatically improving generalization to novel objects and semantically varied instructions, we\n observe a number of emergent capabilities. While the model‚Äôs physical skills are still limited to the\n distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in\n new ways by interpreting images and language commands using knowledge gleaned from the web.\n Some example highlights are shown in Figure 2. The model is able to re-purpose pick and place\n skills learned from robot data to place objects near semantically indicated locations, such as specific\n numbersoricons,despitethosecuesnotbeingpresentintherobotdata. Themodelcanalsointerpret\n relations between objects to determine which object to pick and where to place it, despite no such\n relations being provided in the robot demonstrations. Furthermore, if we augment the command\n with chain of thought prompting, the model is able to make even more complex semantic inferences,\n such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type\n of drink is best suited for someone who is tired (an energy drink). \n \n Our main contribution is RT-2, a family of models derived from fine-tuning large vision-language\n models trained on web-scale data to directly act as generalizable and semantically aware robotic\n policies. Our experiments investigate models with up to 55 B parameters trained on Internet data\n and instruction-annotated robotic trajectories from previous work (Brohan et al., 2022). Over the\n courseof 6 kroboticevaluations,weshowthat RT-2 enablesignificantimprovementstogeneralization\n over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from\n web-scale vision-language pretraining. \n \n \n 2. Related Work \n \n Vision-language models. There are several categories of Vision-Language Models (VLMs) (Gan et al.,\n 2022), with perhaps two most relevant: (1) representation-learning models, e.g. CLIP (Radford\n et al., 2021), which learn common embeddings for both modalities, and (2) visual language models\n of the form {vision,text} ‚Üí {text} which learn to take vision and language as input and provide\n free-form text. Both categories have been used to provide pretraining for a wide variety of applied\n to downstream applications such as object classification (Radford et al., 2021), detection (Gu et al.,\n 2021), and segmentation (Ghiasi et al., 2021). In this work, we focus on the latter category (Alayrac\n et al., 2022; Chen et al., 2023 a,b; Driess et al., 2023; Hao et al., 2022; Li et al., 2023, 2019; Lu\n et al., 2019). These models are generally trained on many different tasks, such as image captioning,\n vision-question answering (VQA), and general language tasks on multiple datasets at the same time.\n While prior works study VLMs for a wide range of problems and settings including in robotics, our\n focus is on how the capabilities of VLMs can be extended to robotics closed-loop control by endowing\n themwiththeabilitytopredictrobotactions,thusleveragingtheknowledgealreadypresentin VLMs\n to enable new levels of generalization. \n \n Generalization in robot learning. Developing robotic controllers that can broadly succeed in a\n variety of scenarios is a long-standing goal in robotics research (Kaelbling, 2020; Smith and Coles,\n 1973). A promising approach for enabling generalization in robotic manipulation is by learning from\n large anddiverse datasets(Dasari etal.,2019;Levineetal.,2018; Pintoand Gupta,2016). Bydoing\n so, prior methods have demonstrated how robots can generalize to novel object instances (Finn and\n Levine,2017;Levineetal.,2018;Mahleretal.,2017;Pintoand Gupta,2016;Youngetal.,2021),to\n tasks involving novel combinations of objects and skills (Dasari and Gupta, 2021; Finn et al., 2017;\n James et al., 2018; Jang et al., 2021; Yu et al., 2018), to new goals or language instructions (Jang\n et al., 2021; Jiang et al., 2022; Liu et al., 2022; Mees et al., 2022; Nair et al., 2022 a; Pong et al.,\n \n \n 3 \n \n \n "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n 2019), to tasks with novel semantic object categories (Shridhar et al., 2021; Stone et al., 2023), and\n tounseenenvironments(Cuietal.,2022;Duetal.,2023 a;Hansenetal.,2020). Unlikemostofthese\n prior works, we aim to develop and study a single model that can generalize to unseen conditions\n along all of these axes. A key ingredient of our approach is to leverage pre-trained models that have\n been exposed to data that is much broader than the data seen by the robot.\n Pre-training for robotic manipulation. Pre-training has a long history in robotic learning. Most\n works focus on pre-trained visual representations that can be used to initialize the encoder of the\n robot‚Äôs camera observations, either via supervised Image Net classification (Shah and Kumar, 2021),\n data augmentation (Kostrikov et al., 2020; Laskin et al., 2020 a,b; Pari et al., 2021) or objectives\n that are tailored towards robotic control (Karamcheti et al., 2023; Ma et al., 2022; Majumdar et al.,\n 2023 b; Nair et al., 2022 b; Xiao et al., 2022 b). Other works have incorporated pre-trained language\n \n models, often either as an instruction encoder (Brohan et al., 2022; Hill et al., 2020; Jang et al.,\n 2021; Jiang et al., 2022; Lynch and Sermanet, 2020; Nair et al., 2022 a; Shridhar et al., 2022 b) or\n for high-level planning (Ahn et al., 2022; Driess et al., 2023; Huang et al., 2022; Mu et al., 2023;\n Singh et al., 2023; Wu et al., 2023). Rather than using pre-training vision models or pre-trained\n language models, we specifically consider the use of pre-trained vision-language models (VLMs),\n which provide rich, grounded knowledge about the world. Prior works have studied the use of VLMs\n for robotics (Driess et al., 2023; Du et al., 2023 b; Gadre et al., 2022; Karamcheti et al., 2023; Shah\n et al., 2023; Shridhar et al., 2021; Stone et al., 2023), and form part of the inspiration for this\n work. Thesepriorapproaches use VLMs forvisualstaterepresentations(Karamchetietal.,2023), for\n identifyingobjects(Gadreetal.,2022;Stoneetal.,2023),forhigh-levelplanning(Driessetal.,2023),\n or for providing supervision or success detection (Du et al., 2023 b; Ma et al., 2023; Sumers et al.,\n 2023; Xiao et al., 2022 a; Zhang et al., 2023). While CLIPort (Shridhar et al., 2021) and MOO (Stone\n et al., 2023) integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both\n incorporate significant structure into the policy that limits their applicability. Notably, our work does\n notrelyonarestricted 2 Dactionspaceanddoesnotrequireacalibratedcamera. Moreover,acritical\n distinction is that, unlike these works, we leverage VLMs that generate language, and the unified\n output space of our formulation enables model weights to be entirely shared across language and\n action tasks, without introducing action-only model layer components. \n \n \n 3. Vision-Language-Action Models \n \n In this section, we present our model family and the design choices for enabling training VLMs to\n directly perform closed-loop robot control. First, we describe the general architecture of our models\n and how they can be derived from models that are commonly used for vision-language tasks. Then,\n we introduce the recipe and challenges of fine-tuning large VLMs that are pre-trained on web-scale\n data to directly output robot actions, becoming VLA models. Finally, we describe how to make these\n modelspracticalforrobottasks,addressingchallengeswithmodelsizeandinferencespeedtoenable\n real-time control. \n \n \n 3.1. Pre-Trained Vision-Language Models \n \n The vision-language models (Chen et al., 2023 a; Driess et al., 2023) that we build on in this work\n take as input one or more images and produce a sequence of tokens, which conventionally represents\n natural language text. Such models can perform a wide range of visual interpretation and reasoning\n tasks, from inferring the composition of an image to answering questions about individual objects\n and their relations to other objects (Alayrac et al., 2022; Chen et al., 2023 a; Driess et al., 2023;\n Huang et al., 2023). Representing the knowledge necessary to perform such a wide range of tasks\n \n \n 4 \n \n \n "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n requires large models and web-scale datasets. In this work, we adapt two previously proposed VLMs\n to act as VLA models: Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023). We will refer\n to vision-language-action versions of these models as RT-2-Pa LI-X and RT-2-Pa LM-E. We leverage\n instantiations of these models that range in size from billions to tens of billions of parameters. We\n provide a detailed description of the architecture of these two models in Appendix D.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 2 | RT-2 is able to generalize to a variety of real-world situations that require reasoning, symbol\n understanding,andhumanrecognition. Westudythesechallengingscenariosindetailin Section 4.\n \n \n 3.2. Robot-Action Fine-tuning \n \n To enable vision-language models to control a robot, they must be trained to output actions. We\n take a direct approach to this problem, representing actions as tokens in the model‚Äôs output, which\n are treated in the same way as language tokens. We base our action encoding on the discretization\n proposed by Brohan et al. (2022) for the RT-1 model. The action space consists of 6-Do F positional\n and rotational displacement of the robot end-effector, as well as the level of extension of the robot\n gripper and a special discrete command for terminating the episode, which should be triggered by\n the policy to signal successful completion. The continuous dimensions (all dimensions except for\n the discrete termination command) are discretized into 256 bins uniformly. Thus, the robot action\n can be represented using ordinals of the discrete bins as 8 integer numbers. In order to use these\n discretized actions to finetune a vision-language into a vision-language-action model, we need to\n associate tokens from the model‚Äôs existing tokenization with the discrete action bins. This requires\n \n \n 5 \n \n \n "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n reserving 256 tokens to serve as action tokens. Which tokens to choose depends on the particular\n tokenization used by each VLM, which we discuss later in this section. In order to define a target\n for VLM fine-tuning we convert the action vector into a single string by simply concatenating action\n tokens for each dimension with a space character: \n ‚Äúterminate Œîpos Œîpos Œîpos Œîrot Œîrot Œîrot gripper_extension‚Äù. \n ùë• ùë¶ ùëß ùë• ùë¶ ùëß \n A possible instantiation of such a target could be: ‚Äú1 128 91 241 5 101 127‚Äù. The two VLMs that\n we finetune in our experiments, Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023), use\n different tokenizations. For Pa LI-X, integers up to 1000 each have a unique token, so we simply\n associate the action bins to the token representing the corresponding integer. For the Pa LM-E model,\n which does not provide this convenient representation of numbers, we simply overwrite the 256 least\n frequently used tokens to represent the action vocabulary. It is worth noting that training VLMs to\n override existing tokens with action tokens is a form of symbol tuning (Wei et al., 2023), which has\n been shown to work well for VLMs in prior work. \n \n Taking the action representation described above, we convert our robot data to be suitable for\n VLM model fine-tuning, where our inputs include robot camera image and textual task description\n (usingstandard VQAformat‚ÄúQ:whatactionshouldtherobottaketo[taskinstruction]? A:‚Äù),andour\n output is formatted as a string of numbers/least frequently used tokens representing a robot action.\n Co-Fine-Tuning. As we will show in our experiments, a key technical detail of the training recipe\n that improves robot performance is co-fine-tuning robotics data with the original web data instead of\n na√Øvefinetuningonrobotdataonly. Wenoticethatco-fine-tuningleadstomoregeneralizablepolicies\n sincethepoliciesareexposedtobothabstractvisualconceptsfromwebscaledataandlowlevelrobot\n actions during fine-tuning, instead of just robot actions. During co-fine-tuning we balance the ratios\n of robot and web data in each training batch by increasing the sampling weight on the robot dataset.\n \n Output Constraint. One important distinction between RT-2 and standard VLMs is that RT-2\n is required to output valid action tokens for execution on the real robot. Thus, to ensure that RT-2\n outputs valid action tokens during decoding, we constrain its output vocabulary via only sampling\n valid action tokens when the model is prompted with a robot-action task, whereas the model is still\n allowed to output the full range of natural language tokens on standard vision-language tasks.\n \n \n 3.3. Real-Time Inference \n The size of modern VLMs can reach tens or hundreds of billions of parameters (Chen et al., 2023 a;\n Driess et al., 2023). The largest model trained in this work uses 55 B parameters. It is infeasible to\n directlyrunsuchmodelsonthestandarddesktop-stylemachinesoron-robot GPUscommonlyusedfor\n real-time robot control. To the best of our knowledge, our model is the largest ever, by over an order\n \n ofmagnitude,usedfordirectclosed-looproboticcontrol,andthereforerequiresanewsetofsolutions\n to enable efficient real-time inference. We develop a protocol that allows us to run RT-2 models on\n robots by deploying them in a multi-TPU cloud service and querying this service over the network.\n Withthissolution,wecanachieveasuitablefrequencyofcontrolandalsoservemultiplerobotsusing\n the same cloud service. The largest model we evaluated, the 55 B parameter RT-2-Pa LI-X-55 B model,\n can run at a frequency of 1-3 Hz. The smaller version of that model, consisting of 5 B parameters, can\n run at a frequency of around 5 Hz. \n \n 4. Experiments \n \n Our experiments focus on real-world generalization and emergent capabilities of RT-2 and aim to\n answer the following questions: \n \n \n 6 \n \n \n "
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n 1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects,\n backgrounds, and environments? \n 2. Can we observe and measure any emergent capabilities of RT-2? \n 3. How does the generalization vary with parameter count and other design decisions?\n 4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models?\n \n We evaluate our approach and several baselines with about 6,000 evaluation trajectories in a variety\n of conditions, which we describe in the following sections. Unless specified otherwise, we use a\n 7 Do F mobile manipulator with the action space described in Sec. 3.2. We also demonstrate examples\n of RT-2 execution on the project website: robotics-transformer 2.github.io. We train two\n specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-Pa LI-X is built from 5 B and\n 55 B Pa LI-X (Chen et al., 2023 a), and (2) RT-2-Pa LM-E is built from 12 B Pa LM-E (Driess et al., 2023).\n For training, we leverage the original web scale data from Chen et al. (2023 a) and Driess et al.\n (2023), which consists of visual question answering, captioning, and unstructured interwoven image\n and text examples. We combine it with the robot demonstration data from Brohan et al. (2022),\n which was collected with 13 robots over 17 months in an office kitchen environment. Each robot\n \n demonstration trajectory is annotated with a natural language instruction that describes the task\n performed,consistingofaverbdescribingtheskill(e.g.,‚Äúpick‚Äù,‚Äùopen‚Äù,‚Äúplaceinto‚Äù)andoneormore\n nouns describing the objects manipulated (e.g., ‚Äú7 up can‚Äù, ‚Äúdrawer‚Äù, ‚Äúnapkin‚Äù) (see Appendix B for\n moredetailsontheuseddatasets). Forall RT-2 trainingrunsweadoptthehyperparametersfromthe\n original Pa LI-X (Chen et al., 2023 a) and Pa LM-E (Driess et al., 2023) papers, including learning rate\n schedules and regularizations. More training details can be found in Appendix E.\n Baselines. We compare our method to multiple state-of-the-art baselines that challenge different\n aspects of our method. All of the baselines use the exact same robotic data. To compare against a\n state-of-the-art policy, we use RT-1 (Brohan et al., 2022), a 35 M parameter transformer-based model.\n Tocompareagainststate-of-the-artpretrainedrepresentations,weuse VC-1(Majumdaretal.,2023 a)\n and R 3 M (Nair et al., 2022 b), with policies implemented by training an RT-1 backbone to take their\n \n representationsasinput. Tocompareagainstotherarchitecturesforusing VLMs,weuse MOO(Stone\n et al., 2023), which uses a VLM to create an additional image channel for a semantic map, which is\n then fed into an RT-1 backbone. More information is provided in Appendix C.\n \n 4.1. How does RT-2 perform on seen tasks and more importantly, generalize over new objects,\n backgrounds, and environments? \n \n \n \n \n \n (a) Unseen Objects (b) Unseen Backgrounds (c) Unseen Environments \n Figure 3 | Examplegeneralizationscenariosusedforevaluationin Figures 4 and 6 band Tables 4 and 6.\n \n To evaluate in-distribution performance as well as generalization capabilities, we compare the\n RT-2-Pa LI-X and RT-2-Pa LM-E models to the four baselines listed in the previous sections. For the\n seen tasks category, we use the same suite of seen instructions as in RT-1 (Brohan et al., 2022), which\n include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for\n placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for\n pickingoutofandplacingobjectsintodrawers. Note,however,thatthese‚Äúin-distribution‚Äùevaluations\n still vary the placement of objects and factors such as time of day and robot position, requiring the\n skills to generalize to realistic variability in the environment. \n \n \n 7 \n \n \n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n Figure 3 showsexamplegeneralizationevaluations,whicharesplitintounseencategories(objects,\n backgroundsandenvironments),andareadditionallysplitintoeasyandhardcases. Forunseenobjects,\n hard cases include harder-to-grasp and more unique objects (such as toys). For unseen backgrounds,\n hardcasesincludemorevariedbackgroundsandnovelobjects. Lastly,forunseenenvironments,hard\n cases correspond to a more visually distinct office desk environment with monitors and accessories,\n whiletheeasierenvironmentisakitchensink. Theseevaluationsconsistsofover 280 tasksthatfocus\n primarily on pick and placing skills in many diverse scenarios. The list of instructions for unseen\n categories is specified in Appendix F.2. \n \n \n \n \n \n \n \n \n \n \n Figure 4 | Overallperformanceoftwoinstantiationsof RT-2 andbaselinesacrossseentrainingtasksaswellas\n unseenevaluationsmeasuringgeneralizationtonovelobjects,novelbackgrounds,andnovelenvironments.\n Appendix Table 4 detailsthefullresults. \n \n The evaluation results are shown in Figure 4 and Appendix Table 4. The performance on seen\n tasks is similar between the RT-2 models and RT-1, with other baselines attaining a lower success\n rate. The difference between the RT-2 models and the baseline is most pronounced in the various\n generalization experiments, suggesting that the strength of vision-language-action models lies in\n transferring more generalizable visual and semantic concepts from their Internet-scale pretraining\n data. Here, on average, both instantiations of RT-2 perform similarly, resulting in ‚àº2 x improvement\n over the next two baselines, RT-1 and MOO, and ‚àº6 x better than the other baselines. The Pa LM-E\n version of RT-2 seems to perform better than the RT-2-Pa LI-X in harder versions of generalization\n scenarios while under-performing on easier ones, resulting in a similar average performance.\n \n Open Source Language Table Benchmark. To provide an additional point of comparison using\n open-source baselines and environments, we leverage the open-source Language-Table simulation\n environmentfrom Lynchetal.(2022). Weco-fine-tuneasmaller Pa LI 3 Bmodelonseveralprediction\n tasks, including in-domain VQA tasks, for the Language-Table dataset, and evaluate the resulting\n policy in simulation. For the action prediction task, we discretize and encode actions as text in the\n format‚ÄúX Y‚Äù,where Xand Yrangebetween{-10,-9,...,+9,+10},andrepresentdelta 2 Dcartesian\n setpointsoftheendeffector. Duetoitsreducedsize,theresultingmodelcanruninferenceatasimilar\n rate(5 Hz)astheotherbaselines. Theresultsofthisexperimentarepresentedin Table 1. Weobserve\n a significant performance boost when using our model compared to the baselines, indicating that the\n VLM-based pre-training together with the expressiveness of the large Pa LI model can be beneficial in\n other scenarios, in this case, simulation with a different robot. We also show qualitative real-world\n out-of-distribution behaviors behaviors in Figure 5, demonstrating novel pushing tasks and targeting\n objects not before seen in this environment. More details about the Language Table experiments can\n be found in Appendix B and D. \n \n \n 4.2. Can we observe and measure any emergent capabilities of RT-2? \n Inadditiontoevaluatingthegeneralizationcapabilitiesofvision-language-actionmodels,wealsoaim\n to evaluate the degree to which such models can enable new capabilities beyond those demonstrated\n \n \n 8 \n \n \n "
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n Model Language-Table \n BC-Zero(Jangetal.,2021) 72¬±3 \n RT-1(Brohanetal.,2022) 74¬±13 \n LAVA(Lynchetal.,2022) 77¬±4 \n RT-2-Pa LI-3 B(ours) 90¬±10 \n Figure 5 | Real-world out-of-distribution behaviors in the Table 1 | Performance on the simulated\n Language Tableenvironment. Identical RT-2-Pa LI-3 Bmodel Language-Table tasks (Lynch and Ser-\n checkpointisusedasin Tab.1. manet,2020). \n \n intherobotdatabytransferringknowledgefromtheweb. Werefertosuchcapabilitiesasemergent,in\n the sense that they emerge by transferring Internet-scale pretraining. We do not expect such transfer\n to enable new robotic motions, but we do expect semantic and visual concepts, including relations\n andnouns,totransfereffectively, evenincaseswherethoseconceptswerenotseenintherobotdata.\n \n Qualitative Evaluations. First, we experiment with our RT-2-Pa LI-X model to determine various\n emergent capabilities transferred from vision-language concepts. We demonstrate some examples of\n such interactions in Figure 2. We find through our explorations that RT-2 inherits novel capabilities\n in terms of semantic understanding and basic reasoning in the context of the scene. For example\n accomplishing the task ‚Äúput strawberry into the correct bowl‚Äù requires a nuanced understanding of\n not only what a strawberry and bowl are, but also reasoning in the context the scene to know the\n strawberry should go with the like fruits. For the task ‚Äúpick up the bag about to fall off the table,‚Äù\n RT-2 demonstrates physical understanding to disambiguate between two bags and recognize the\n precariously placed object. All the interactions tested in these scenarios have never been seen in the\n robot data, which points to the transfer of semantic knowledge from vision-language data.\n Quantitative Evaluations. Toquantifytheseemergentcapabilities,wetakethetoptwobaselines\n fromthepreviousevaluations,RT-1 and VC-1,andcomparethemagainstourtwomodels: RT-2-Pa LI-X\n and RT-2-Pa LM-E. To reduce the variance of these experiment, we evaluate all of the methods using\n the A/B testing framework (Fisher, 1936), where all four models are evaluated one after another in\n the exact same conditions. \n \n We‚Äô split the emergent capabilities of RT-2 into three categories covering axes of reasoning and\n semantic understanding (with examples of each shown in Appendix Figure 8). The first we term\n symbol understanding, which explicitly tests whether the RT-2 policy transfers semantic knowledge\n from vision-language pretraining that was not present in any of the robot data. Example instructions\n in this category are ‚Äúmove apple to 3‚Äù or ‚Äúpush coke can on top of heart‚Äù. The second category we\n termreasoning,whichdemonstratestheabilitytoapplyvariousaspectsofreasoningoftheunderlying\n VLMtocontroltasks. Thesetasksrequirevisualreasoning(‚Äúmovetheappletocupwithsamecolor‚Äù),\n math (‚Äúmove X near the sum of two plus one‚Äù), and multilingual understanding (‚Äúmueve la manzana\n al vaso verde‚Äù). We refer to the last category as human recognition tasks, which include tasks such as\n ‚Äúmove the coke can to the person with glasses‚Äù, to demonstrate human-centric understanding and\n recognition. The full list of instructions used for this evaluation is specified in Appendix F.2.\n \n Wepresenttheresultsofthisexperimentin Figure 6 awithallthenumericalresultsin Appendix H.2.\n We observe that our VLA models significantly outperform the baselines across all categories, with\n our best RT-2-Pa LI-X model achieving more than 3 x average success rate over the next best baseline\n (RT-1). Wealsonotethatwhilethelarger Pa LI-X-basedmodelresultsinbettersymbolunderstanding,\n reasoning and person recognition performance on average, the smaller Pa LM-E-based model has\n an edge on tasks that involve math reasoning. We attribute this interesting result to the different\n pre-trainingmixtureusedin Pa LM-E,whichresultsinamodelthatismorecapableatmathcalculation\n than the mostly visually pre-trained Pa LI-X. \n \n \n \n 9 \n \n \n "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n \n \n \n \n \n \n \n \n \n \n (a)Performancecomparisononvariousemergentskillevalu-(b)Ablationsof RT-2-Pa LI-Xshowcasingtheimpactofparam-\n ations(Figure 8)between RT-2 andtwobaselines. etercountandtrainingstrategyongeneralization.\n \n Figure 6 | Quantitativeperformanceof RT-2 across(6 a)emergentskillsand(6 b)sizeandtrainingablations.\n Appendix Tables 5 and 6 detailthefullnumericalresults. \n 4.3. How does the generalization vary with parameter count and other design decisions?\n \n For this comparison, we use RT-2-Pa LI-X model because of its flexibility in terms of the model size\n (duetothenatureof Pa LM-E,RT-2-Pa LM-Eisrestrictedtoonlycertainsizesof Pa LMand Vi Tmodels).\n In particular, we compare two different model sizes, 5 B and 55 B, as well as three different training\n routines: training a model from scratch, without using any weights from the VLM pre-training;\n fine-tuning a pre-trained model using robot action data only; and co-fine-tuning (co-training with\n fine-tuning), the primary method used in this work where we use both the original VLM training\n data as well as robotic data for VLM fine-tuning. Since we are mostly interested in the generalization\n aspects of these models, we remove the seen tasks evaluation from this set of experiments.\n \n The results of the ablations are presented in Figure 6 b and Appendix Table 6. First, we observe\n that training a very large model from scratch results in a very poor performance even for the 5 B\n model. Given this result, we decide to skip the evaluation of an even bigger 55 B Pa LI-X model when\n trained from scratch. Second, we notice that co-fine-tuning a model (regardless of its size) results in\n a better generalization performance than simply fine-tuning it with robotic data. We attribute this to\n the fact that keeping the original data around the fine-tuning part of training, allows the model to\n notforgetitspreviousconceptslearnedduringthe VLMtraining. Lastly,somewhatunsurprisingly,we\n notice that the increased size of the model results in a better generalization performance.\n \n \n 4.4. Can RT-2 exhibit signs of chain-of-thought reasoning similarly to vision-language models?\n Inspiredbythechain-of-thoughtpromptingmethodin LLMs(Weietal.,2022),wefine-tuneavariant\n of RT-2 with Pa LM-E for just a few hundred gradient steps to increase its capability of utilizing\n language and actions jointly with the hope that it will elicit a more sophisticated reasoning behavior.\n We augment the data to include an additional ‚ÄúPlan‚Äù step, which describes the purpose of the action\n that the robot is about to take in natural language first, which is then followed by the actual action\n tokens, e.g. ‚ÄúInstruction: I‚Äôm hungry. Plan: pick rxbar chocolate. Action: 1 128 124 136 121 158 111\n 255.‚Äù This data augmentation scheme acts as a bridge between VQA datasets (visual reasoning) and\n manipulation datasets (generating actions). \n \n We qualitatively observe that RT-2 with chain-of-thought reasoning is able to answer more so-\n phisticated commands due to the fact that it is given a place to plan its actions in natural language\n first. This is a promising direction that provides some initial evidence that using LLMs or VLMs as\n planners(Ahnetal.,2022;Driessetal.,2023)canbecombinedwithlow-levelpoliciesinasingle VLA\n model. Rollouts of RT-2 with chain-of-thought reasoning are shown in Figure 7 and in Appendix I.\n \n \n 10 \n \n \n "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n Prompt: Prompt: Prompt: Prompt: \n Given <img> Instruction: Given <img> Instruction: Given <img> Instruction: Given <img> Instruction:\n Bring me a drink. Move all the objects Pick the object that is Move the green objects\n Prediction: together. different from all other together. \n Plan: pick 7 up can. Prediction: objects Prediction: \n Action: 1 143 129 123 145 Plan: move green can near Prediction: Plan: move green can near\n 114 115 127 green rice chip bag. Plan: pick rxbar green rice chip bag.\n Action: 1 128 126 127 135 chocolate. Action: 1 128 Action: 1 130 129 121 131\n 123 119 127 129 125 131 125 128 127 127 128 127 \n \n \n \n \n \n Prompt: \n Given <img> I need to \n hammer a nail, what \n object from the scene \n might be useful? \n Prediction: \n Rocks. Action: 1 129 138 \n 122 132 135 106 127 \n Figure 7 | Rolloutsof RT-2 withchain-of-thoughtreasoning,where RT-2 generatesbothaplanandanaction.\n 5. Limitations \n Even though RT-2 exhibits promising generalization properties, there are multiple limitations of this\n approach. First,althoughweshowthatincludingweb-scalepretrainingvia VLMsboostsgeneralization\n over semantic and visual concepts, the robot does not acquire any ability to perform new motions\n by virtue of including this additional experience. The model‚Äôs physical skills are still limited to the\n distribution of skills seen in the robot data (see Appendix G), but it learns to deploy those skills in\n new ways. We believe this is a result of the dataset not being varied enough along the axes of skills.\n An exciting direction for future work is to study how new skills could be acquired through new data\n collection paradigms such as videos of humans. \n Second, although we showed we could run large VLA models in real time, the computation cost\n of these models is high, and as these methods are applied to settings that demand high-frequency\n control,real-timeinferencemaybecomeamajorbottleneck. Anexcitingdirectionforfutureresearch\n is to explore quantization and distillation techniques that might enable such models to run at higher\n ratesoronlower-costhardware. Thisisalsoconnectedtoanothercurrentlimitationinthatthereare\n onlyasmallnumberofgenerallyavailable VLMmodelsthatcanbeusedtocreate RT-2. Wehopethat\n more open-sourced models will become available (e.g. https://llava-vl.github.io/) and the\n proprietary ones will open up their fine-tuning APIs, which is a sufficient requirement to build VLA\n models. \n \n \n 6. Conclusions \n \n In this paper, we described how vision-language-action (VLA) models could be trained by combining\n vision-language model (VLM) pretraining with robotic data. We then presented two instantiations of\n VLAs based on Pa LM-E and Pa LI-X, which we call RT-2-Pa LM-E and RT-2-Pa LI-X. These models are co-\n fine-tuned with robotic trajectory data to output robot actions, which are represented as text tokens.\n We showed that our approach results in very performant robotic policies and, more importantly,\n leads to a significantly better generalization performance and emergent capabilities inherited from\n \n \n 11 \n \n \n "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n web-scale vision-language pretraining. We believe that this simple and general approach shows a\n promise of robotics directly benefiting from better vision-language models, which puts the field of\n robot learning in a strategic position to further improve with advancements in other fields.\n \n Acknowledgments \n \n Wewouldliketoacknowledge Fred Alcober,Jodi Lynn Andres,Carolina Parada,Joseph Dabis,Rochelle\n Dela Cruz, Jessica Gomez, Gavin Gonzalez, John Guilyard, Tomas Jackson, Jie Tan, Scott Lehrer, Dee\n M, Utsav Malla, Sarah Nguyen, Jane Park, Emily Perez, Elio Prado, Jornell Quiambao, Clayton Tan,\n \n Jodexty Therlonge, Eleanor Tomlinson, Wenxuan Zhou, and the greater Google Deep Mind team for\n their feedback and contributions. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 12 \n \n \n "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n References \n M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,K.Gopalakrishnan,K.Hausman,\n \n A.Herzog,etal. Doas Ican,notas Isay: Groundinglanguageinroboticaffordances. ar Xivpreprint\n ar Xiv:2204.01691, 2022. \n J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\n M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. ar Xiv preprint\n ar Xiv:2204.14198, 2022. \n \n R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin,A.Passos,S.Shakeri,E.Taropa,P.Bailey,Z.Chen,\n et al. Palm 2 technical report. ar Xiv preprint ar Xiv:2305.10403, 2023. \n \n A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\n A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. ar Xiv preprint\n ar Xiv:2212.06817, 2022. \n \n T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\n processing systems, 33:1877‚Äì1901, 2020. \n \n D.Cer,Y.Yang,S.Kong,N.Hua,N.Limtiaco,R.S.John,N.Constant,M.Guajardo-Cespedes,S.Yuan,\n C. Tar, Y. Sung, B. Strope, and R. Kurzweil. Universal sentence encoder. Co RR, abs/1803.11175,\n 2018. URL http://arxiv.org/abs/1803.11175. \n M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\n N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. ar Xiv preprint\n ar Xiv:2107.03374, 2021. \n \n X.Chen,J.Djolonga,P.Padlewski,B.Mustafa,S.Changpinyo,J.Wu,C.R.Ruiz,S.Goodman,X.Wang,\n Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu, M. Joshi,\n B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters,\n G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu,\n K. Rong, A. Kolesnikov, M. Seyedhosseini, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali-x:\n On scaling up a multilingual vision and language model, 2023 a. \n \n X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner,\n B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue,\n A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner,\n A.Angelova,X.Zhai,N.Houlsby,and R.Soricut. Pali: Ajointly-scaledmultilinguallanguage-image\n model, 2023 b. \n \n K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,\n R. Nakano, et al. Training verifiers to solve math word problems. ar Xiv preprint ar Xiv:2110.14168,\n 2021. \n \n Z.J.Cui,Y.Wang,N.Muhammad,L.Pinto,etal. Fromplaytopolicy: Conditionalbehaviorgeneration\n from uncurated robot data. ar Xiv preprint ar Xiv:2210.10047, 2022. \n \n S. Dasari and A. Gupta. Transformers for one-shot visual imitation. In Conference on Robot Learning,\n pages 2071‚Äì2084. PMLR, 2021. \n S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn.\n Robonet: Large-scale multi-robot learning. In Conference on Robot Learning, 2019.\n \n \n 13 \n \n \n "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n M.Dehghani,J.Djolonga,B.Mustafa,P.Padlewski,J.Heek,J.Gilmer,A.Steiner,M.Caron,R.Geirhos,\n I.Alabdulmohsin,R.Jenatton,L.Beyer,M.Tschannen,A.Arnab,X.Wang,C.Riquelme,M.Minderer,\n J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver,\n F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink,\n A.Kolesnikov,F.Pavetiƒá,D.Tran,T.Kipf,M.Luƒçiƒá,X.Zhai,D.Keysers,J.Harmsen,and N.Houlsby.\n Scaling vision transformers to 22 billion parameters, 2023. \n D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\n T. Yu, et al. Palm-e: An embodied multimodal language model. ar Xiv preprint ar Xiv:2303.03378,\n \n 2023. \n M. Du, S. Nair, D. Sadigh, and C. Finn. Behavior retrieval: Few-shot imitation learning by querying\n unlabeled datasets. ar Xiv preprint ar Xiv:2304.08742, 2023 a. \n \n Y.Du,K.Konyushkova,M.Denil,A.Raju,J.Landon,F.Hill,N.de Freitas,and S.Cabi. Vision-language\n models as success detectors. ar Xiv preprint ar Xiv:2303.07280, 2023 b. \n \n C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International\n Conference on Robotics and Automation (ICRA), pages 2786‚Äì2793. IEEE, 2017.\n C.Finn,T.Yu,T.Zhang,P.Abbeel,and S.Levine. One-shotvisualimitationlearningviameta-learning.\n In Conference on robot learning, pages 357‚Äì368. PMLR, 2017. \n \n R. A. Fisher. Design of experiments. British Medical Journal, 1(3923):554, 1936.\n \n S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Clip on wheels: Zero-shot object\n navigation as object localization and exploration. ar Xiv preprint ar Xiv:2203.10421, 2022.\n Z.Gan,L.Li,C.Li,L.Wang,Z.Liu,J.Gao,etal. Vision-languagepre-training: Basics,recentadvances,\n and future trends. Foundations and Trends¬Æ in Computer Graphics and Vision, 14(3‚Äì4):163‚Äì352,\n 2022. \n \n G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Open-vocabulary image segmentation. ar Xiv preprint\n ar Xiv:2112.12143, 2021. \n \n K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,\n M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma,\n M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty,\n A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang,\n Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam,\n R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari,\n K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Z. Zhao,\n Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu,\n C.V.Jawahar,H.Joo,K.Kitani,H.Li,R.Newcombe,A.Oliva,H.S.Park,J.M.Rehg,Y.Sato,J.Shi,\n M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik. Ego 4 d: Around the world in 3,000\n hours of egocentric video, 2022. \n \n X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language\n knowledge distillation. ar Xiv preprint ar Xiv:2104.13921, 2021. \n \n N. Hansen, R. Jangir, Y. Sun, G. Aleny√†, P. Abbeel, A. A. Efros, L. Pinto, and X. Wang. Self-supervised\n policy adaptation during deployment. ar Xiv preprint ar Xiv:2007.04309, 2020.\n Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. Language models are\n general-purpose interfaces. ar Xiv preprint ar Xiv:2206.06336, 2022. \n \n \n 14 \n \n \n "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep reinforcement\n learning via transfer-learning from text. ar Xiv preprint ar Xiv:2005.09382, 2020.\n S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu,\n et al. Language is not all you need: Aligning perception with language models. ar Xiv preprint\n ar Xiv:2302.14045, 2023. \n \n W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\n actionableknowledgeforembodiedagents. In International Conferenceon Machine Learning,pages\n 9118‚Äì9147. PMLR, 2022. \n \n S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation\n learning. In Conference on robot learning, pages 783‚Äì795. PMLR, 2018. \n \n E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-\n shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages\n 991‚Äì1002. PMLR, 2021. \n \n Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,and L.Fan.\n Vima: General robot manipulation with multimodal prompts. ar Xiv preprint ar Xiv:2210.03094,\n 2022. \n L. P. Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915‚Äì916, 2020.\n \n S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang. Language-driven\n representation learning for robotics. ar Xiv preprint ar Xiv:2302.12766, 2023.\n \n A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,\n W.-Y. Lo, et al. Segment anything. ar Xiv preprint ar Xiv:2304.02643, 2023.\n \n I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep\n reinforcement learning from pixels. ar Xiv preprint ar Xiv:2004.13649, 2020.\n M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with\n augmented data. Advances in neural information processing systems, 33:19884‚Äì19895, 2020 a.\n \n M.Laskin,A.Srinivas,and P.Abbeel.Curl: Contrastiveunsupervisedrepresentationsforreinforcement\n learning. In International Conference on Machine Learning, pages 5639‚Äì5650. PMLR, 2020 b.\n \n S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,and D.Quillen. Learninghand-eyecoordinationforrobotic\n grasping with deep learning and large-scale data collection. The International journal of robotics\n research, 37(4-5):421‚Äì436, 2018. \n \n A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil,\n I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models.\n ar Xiv preprint ar Xiv:2206.14858, 2022. \n \n J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen\n image encoders and large language models. ar Xiv preprint ar Xiv:2301.12597, 2023.\n L.H.Li,M.Yatskar,D.Yin,C.-J.Hsieh,and K.-W.Chang. Visualbert: Asimpleandperformantbaseline\n for vision and language. ar Xiv preprint ar Xiv:1908.03557, 2019. \n \n H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained vision-\n language models. ar Xiv preprint ar Xiv:2210.13431, 2022. \n \n \n 15 \n \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations\n for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.\n C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. ar Xiv\n preprint ar Xiv:2005.07648, 2020. \n \n C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,and P.Florence.Interactive\n language: Talking to robots in real time. ar Xiv preprint ar Xiv:2210.06407, 2022.\n \n Y.J.Ma,S.Sodhani,D.Jayaraman,O.Bastani,V.Kumar,and A.Zhang. Vip: Towardsuniversalvisual\n reward and representation via value-implicit pre-training. ar Xiv preprint ar Xiv:2210.00030, 2022.\n Y. J. Ma, W. Liang, V. Som, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. Liv: Language-image\n representations and rewards for robotic control. ar Xiv preprint ar Xiv:2306.00958, 2023.\n \n J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0:\n Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. ar Xiv\n preprint ar Xiv:1703.09312, 2017. \n \n A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,J.Malik,\n et al. Where are we in the search for an artificial visual cortex for embodied intelligence? ar Xiv\n preprint ar Xiv:2303.18240, 2023 a. \n \n A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,J.Malik,\n et al. Where are we in the search for an artificial visual cortex for embodied intelligence? ar Xiv\n preprint ar Xiv:2303.18240, 2023 b. \n O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation\n learning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205‚Äì11212, 2022.\n \n M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran,\n A. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection with vision\n transformers. ar Xiv preprint ar Xiv:2205.06230, 2022. \n \n Y.Mu,Q.Zhang,M.Hu,W.Wang,M.Ding,J.Jin,B.Wang,J.Dai,Y.Qiao,and P.Luo. Embodiedgpt:\n Vision-language pre-training via embodied chain of thought. ar Xiv preprint ar Xiv:2305.15021,\n 2023. \n \n S.Nair,E.Mitchell,K.Chen,S.Savarese,C.Finn,etal. Learninglanguage-conditionedrobotbehavior\n fromofflinedataandcrowd-sourcedannotation. In Conferenceon Robot Learning,pages 1303‚Äì1315.\n PMLR, 2022 a. \n S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R 3 m: A universal visual representation for\n robot manipulation. ar Xiv preprint ar Xiv:2203.12601, 2022 b. \n \n Open AI. Gpt-4 technical report, 2023. \n \n J.Pari,N.M.Shafiullah,S.P.Arunachalam,and L.Pinto. Thesurprisingeffectivenessofrepresentation\n learning for visual imitation. ar Xiv preprint ar Xiv:2112.01511, 2021. \n \n L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50 k tries and 700 robot\n hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406‚Äì3413.\n IEEE, 2016. \n S.Polu,J.M.Han,K.Zheng,M.Baksys,I.Babuschkin,and I.Sutskever. Formalmathematicsstatement\n curriculum learning. ar Xiv preprint ar Xiv:2202.01344, 2022. \n \n \n 16 \n \n \n "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-fit: State-covering self-supervised\n reinforcement learning. ar Xiv preprint ar Xiv:1903.03698, 2019. \n A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\n J. Clark, et al. Learning transferable visual models from natural language supervision. In Interna-\n tional Conference on Machine Learning, pages 8748‚Äì8763. PMLR, 2021. \n \n S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,Y.Sulsky,\n J. Kay, J. T. Springenberg, et al. A generalist agent. ar Xiv preprint ar Xiv:2205.06175, 2022.\n \n M.Ryoo,A.Piergiovanni,A.Arnab,M.Dehghani,and A.Angelova. Tokenlearner: Adaptivespace-time\n tokenizationforvideos. Advancesin Neural Information Processing Systems,34:12786‚Äì12797,2021.\n \n D.Shah,B.Osi≈Ñski,b.ichter,and S.Levine. Lm-nav: Roboticnavigationwithlargepre-trainedmodels\n of language, vision, and action. In K. Liu, D. Kulic, and J. Ichnowski, editors, Proceedings of The 6 th\n Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 492‚Äì\n 504.PMLR,14‚Äì18 Dec 2023. URLhttps://proceedings.mlr.press/v 205/shah 23 b.html.\n \n R. Shah and V. Kumar. Rrl: Resnet as representation for reinforcement learning. ar Xiv preprint\n ar Xiv:2107.03380, 2021. \n M.Shridhar,L.Manuelli,and D.Fox. Cliport: Whatandwherepathwaysforroboticmanipulation. In\n Proceedings of the 5 th Conference on Robot Learning (Co RL), 2021. \n \n M.Shridhar,L.Manuelli,and D.Fox. Cliport: Whatandwherepathwaysforroboticmanipulation. In\n Conference on Robot Learning, pages 894‚Äì906. PMLR, 2022 a. \n \n M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic manipula-\n tion. ar Xiv preprint ar Xiv:2209.05451, 2022 b. \n \n I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\n Progprompt: Generating situated robot task plans using large language models. In ICRA, 2023.\n M. H. Smith and L. S. Coles. Design of a low cost, general purpose robot. In IJCAI, pages 324‚Äì336,\n 1973. \n \n A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia,\n C. Finn, et al. Open-world object manipulation using pre-trained vision-language models. ar Xiv\n preprint ar Xiv:2303.00905, 2023. \n \n T. Sumers, K. Marino, A. Ahuja, R. Fergus, and I. Dasgupta. Distilling internet-scale vision-language\n models into embodied agents. ar Xiv preprint ar Xiv:2301.12507, 2023. \n \n Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri,\n T. Schuster, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler. Ul 2: Unifying language learning\n paradigms, 2023. \n \n S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and model\n abilities. Microsoft Auton. Syst. Robot. Res, 2:20, 2023. \n J.Wang,Z.Yang,X.Hu,L.Li,K.Lin,Z.Gan,Z.Liu,C.Liu,and L.Wang.Git: Agenerativeimage-to-text\n transformer for vision and language. ar Xiv preprint ar Xiv:2205.14100, 2022.\n \n J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\n elicits reasoning in large language models. ar Xiv preprint ar Xiv:2201.11903, 2022.\n \n \n 17 \n \n \n "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n J. Wei, L. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le.\n Symbol tuning improves in-context learning in language models, 2023. \n J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser.\n Tidybot: Personalizedrobotassistancewithlargelanguagemodels.ar Xivpreprintar Xiv:2305.05658,\n 2023. \n \n T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman, S. Levine, and J. Tompson.\n Robotic skill acquisition via instruction augmentation with vision-language models. ar Xiv preprint\n ar Xiv:2211.11736, 2022 a. \n \n T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control. ar Xiv\n preprint ar Xiv:2203.06173, 2022 b. \n \n S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. In\n Conference on Robot Learning, pages 1992‚Äì2005. PMLR, 2021. \n \n K.-T.Yu,M.Bauza,N.Fazeli,and A.Rodriguez. Morethanamillionwaystobepushed.ahigh-fidelity\n experimental dataset of planar pushing. In 2016 IEEE/RSJ international conference on intelligent\n robots and systems (IROS), pages 30‚Äì37. IEEE, 2016. \n T.Yu,C.Finn,A.Xie,S.Dasari,T.Zhang,P.Abbeel,and S.Levine. One-shotimitationfromobserving\n humans via domain-adaptive meta-learning. ar Xiv preprint ar Xiv:1802.01557, 2018.\n \n X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings of the\n IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104‚Äì12113, 2022.\n \n X.Zhang,Y.Ding,S.Amiri,H.Yang,A.Kaminski,C.Esselink,and S.Zhang. Groundingclassicaltask\n planners via vision-language models. ar Xiv preprint ar Xiv:2304.08587, 2023.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 18 \n \n \n "
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n A. Contributions \n ‚Ä¢ Training and Evaluations (designing and executing procedures for training models, evalu-\n \n ating models in simulation and the real world, running ablations for algorithm design\n choices): Yevgen Chebotar, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey,\n Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han,\n Alexander Herzog, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee, Yao Lu, Henryk Michalewski,\n Igor Mordatch, Karl Pertsch, Michael Ryoo, Anikait Singh, Quan Vuong, Ayzaan Wahid, Paul\n Wohlhart, Fei Xia, Ted Xiao, and Tianhe Yu. \n ‚Ä¢ Network Architecture (designing and implementing model network modules, working on\n tokenization of actions, enabling inference of the model networks during experiments):\n Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Danny Driess, Pete Florence, Keerthana\n Gopalakrishnan, Kehang Han, Karol Hausman, Brian Ichter, Alex Irpan, Isabel Leal, Lisa Lee,\n Henryk Michalewski, Igor Mordatch, Kanishka Rao, Michael Ryoo, Anikait Singh, Quan Vuong,\n Ayzaan Wahid, Jialin Wu, Fei Xia, Ted Xiao, and Tianhe Yu. \n ‚Ä¢ Data Collection (collecting data on real robots, running real robot evaluations, executing\n operations required for running real robots): Noah Brown, Justice Carbajal, Tianli Ding,\n Krista Reymann, Grecia Salazar, Pierre Sermanet, Jaspiar Singh, Huong Tran, Stefan Welker,\n and Sichun Xu. \n ‚Ä¢ Leadership (leading the project efforts, managing the project staff, advising on project\n directions): Yevgen Chebotar, Chelsea Finn, Karol Hausman, Brian Ichter, Sergey Levine, Yao\n Lu, Igor Mordatch, Kanishka Rao, Pannag Sanketi, Radu Soricut, Vincent Vanhoucke, and\n Tianhe Yu. \n ‚Ä¢ Paper (working on the paper manuscript, designing paper visualizations and figures):\n Yevgen Chebotar, Danny Driess, Chelsea Finn, Pete Florence, Karol Hausman, Brian Ichter, Lisa\n Lee,Sergey Levine,Igor Mordatch,Karl Pertsch,Quan Vuong,Fei Xia,Ted Xiao,and Tianhe Yu.\n \n ‚Ä¢ Infrastructure (working on infrastructure and code base backbone needed for training\n models, running experiments, storing and accessing data): Anthony Brohan,Yevgen Chebo-\n tar,Danny Driess,Kehang Han,Jasmine Hsu,Brian Ichter,Alex Irpan,Nikhil Joshi,Ryan Julian,\n Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Yao Lu, Igor\n Mordatch, Quan Vuong, Ayzaan Wahid, Fei Xia, Ted Xiao, Peng Xu, and Tianhe Yu.\n \n \n B. Datasets \n The vision-language datasets are based on the dataset mixtures from Chen et al. (2023 b) and Driess\n et al. (2023). The bulk of this data consists of the Web LI dataset, which is around 10 B image-text\n \n pairs across 109 languages, filtered to the top 10% scoring cross-modal similarity examples to give\n 1 B training examples. Many other captioning and vision question answering datasets are included\n as well, and more info on the dataset mixtures can be found in Chen et al. (2023 b) for RT-2-Pa LI-X,\n and Driessetal.(2023)for RT-2-Pa LM-E.Whenco-fine-tuning RT-2-Pa LI-X,wedonotusethe Episodic\n Web LI dataset described by Chen et al. (2023 a). \n The robotics dataset is based on the dataset from Brohan et al. (2022). This consists of demon-\n stration episodes collected with a mobile manipulation robot. Each demonstration is annotated with\n anaturallanguageinstructionfromoneofsevenskills: \"Pick Object\",\"Move Object Near Object\",\n \"Place Object Upright\",\"Knock Object Over\",\"Open Drawer\",\"Close Drawer\",\"Place Objectinto\n Receptacle\", and \"Pick Object from Receptacle and place on the counter\". Further details can\n be found in Brohan et al. (2022). \n \n RT-2-Pa LI-X weights the robotics dataset such that it makes up about 50% of the training mixture\n \n \n 19 \n \n \n "
  },
  {
    "page_num": 20,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n for co-fine-tuning. RT-2-Pa LM-E weights the robotics dataset to be about 66% of the training mixture.\n Fortheresultson Language-Tablein Table 1,ourmodelistrainedonthe Language-Tabledatasets\n from Lynch et al. (2022). Our model is co-fine-tuned on several prediction tasks: (1) predict the\n action, given two consecutive image frames and a text instruction; (2) predict the instruction, given\n image frames; (3) predict the robot arm position, given image frames; (4) predict the number of\n timesteps between given image frames; and (5)predict whether the task wassuccessful, given image\n frames and the instruction. \n \n \n C. Baselines \n \n We compare our method to multiple state-of-the-art baselines that challenge different aspects of our\n method. All of the baselines use the exact same robotic data. \n \n \n ‚Ä¢ RT-1: Robotics Transformer 1 Brohan et al. (2022) is a transformer-based model that achieved\n state-of-the-art performance on a similar suite of tasks when it was published. The model does\n not use VLM-based pre-training so it provides an important data point demonstrating whether\n VLM-based pre-training matters. \n ‚Ä¢ VC-1: VC-1 Majumdar et al. (2023 a) is a visual foundation model that uses pre-trained visual\n representations specifically designed for robotics tasks. We use pre-trained representations\n from the VC-1 Vi T-L model. Since VC-1 does not include language conditioning, we add this by\n separatelyembeddingthelanguagecommandvia Universal Sentence Encoder Ceretal.(2018)\n to enable comparison to our method. In particular, we concatenate the resulting language\n embedding tokens to the image tokens produced by VC-1, and pass the concatenated token\n sequences through token learner Ryoo et al. (2021). The token sequences produced by token\n learner are then consumed by an RT-1 decoder-only transformer model to predict robot action\n tokens. We train the VC-1 baseline end-to-end and unfreeze the VC-1 weights during training,\n since this led to far better results than using frozen VC-1 weights. \n ‚Ä¢ R 3 M: R 3 M Nair et al. (2022 b) is a similar method to VC-1 in that R 3 M uses pre-trained\n visual-language representations to improve policy training. In this case the authors use Ego 4 D\n dataset Graumanetal.(2022)ofhumanactivitiestolearntherepresentationthatisusedbythe\n policy. Both VC-1 and R 3 M test different state-of-the-art representation learning methods as an\n alternative to using a VLM. To obtain a language-conditioned policy from the R 3 M pretrained\n representation, we follow the same procedure as described above for VC-1, except we use the\n R 3 M Res Net 50 model to obtain the image tokens, and unfreeze it during training.\n ‚Ä¢ MOO: MOO Stone et al. (2023) is an object-centric approach, where a VLM is first used to\n \n specifytheobjectofinterestinaformofasingle,coloredpixelintheoriginalimage. Thispixel-\n modified image is then trained with an end-to-end policy to accomplish a set of manipulation\n tasks. This baseline corresponds to a situation where a VLM is used as a separate module that\n enhances perception but its representations are not used for policy learning.\n \n \n D. VLMs for RT-2 \n The Pa LI-X model architecture consists of a Vi T-22 B Dehghani et al. (2023) to process images, which\n canacceptsequencesofùëõimages,leadingtoùëõ√óùëòtokensperimage,whereùëòisthenumberofpatches\n perimage. Theimagetokenspassingoveraprojectionlayeristhenconsumedbyanencoder-decoder\n \n backbone of 32 B parameters and 50 layers, similar to UL 2 Tay et al. (2023), which jointly processes\n text and images as embeddings to generate output tokens in an auto-regressive manner. The text\n \n \n 20 \n \n \n "
  },
  {
    "page_num": 21,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n inputusuallyconsistsofthetypeoftaskandanyadditionalcontext(e.g.,\"Generatecaptionin ‚ü®lang‚ü©\"\n for captioning tasks or \"Answer in ‚ü®lang‚ü©: question\" for VQA tasks). \n The Pa LI-3 B model trained on Language-Table (Table 1) uses a smaller Vi T-G/14 (Zhai et al.,\n 2022) (2 B parameters) to process images, and UL 2-3 B (Tay et al., 2023) for the encoder-decoder\n network. \n \n The Pa LM-E model is based on a decoder-only LLM that projects robot data such as images and\n text into the language token space and outputs text such as high-level plans. In the case of the\n used Pa LM-E-12 B, the visual model used to project images to the language embedding space is\n a Vi T-4 B Chen et al. (2023 b). The concatenation of continuous variables to textual input allows\n Pa LM-E to be fully multimodal, accepting a wide variety of inputs such as multiple sensor modalities,\n object-centric representations, scene representations and object entity referrals.\n \n \n E. Training Details \n \n We perform co-fine-tuning on pre-trained models from the Pa LI-X (Chen et al., 2023 a) 5 B & 55 B\n model, Pa LI (Chen et al., 2023 b) 3 B model and the Pa LM-E (Driess et al., 2023) 12 B model. For\n RT-2-Pa LI-X-55 B, we use learning rate 1 e-3 and batch size 2048 and co-fine-tune the model for\n 80 K gradient steps whereas for RT-2-Pa LI-X-5 B, we use the same learning rate and batch size and\n co-fine-tune the model for 270 K gradient steps. For RT-2-Pa LM-E-12 B, we use learning rate 4 e-4 and\n batch size 512 to co-fine-tune the model for 1 M gradient steps. Both models are trained with the\n next token prediction objective, which corresponds to the behavior cloning loss in robot learning. For\n RT-2-Pa LI-3 B model used for Language-Table results in Table 1, we use learning rate 1 e-3 and batch\n size 128 to co-fine-tune the model for 300 K gradient steps. \n \n \n F. Evaluation Details \n \n F.1. Evaluation Scenarios \n \n Forstudyingtheemergentcapabilitiesof RT-2 inaquantitativemanner, westudyvariouschallenging\n semanticevaluationscenariosthataimtomeasurecapabilitiessuchasreasoning,symbolunderstand-\n ing, and human recognition. A visual overview of a subset of these scenes is provided in Figure 8,\n and the full list of instructions used for quantiative evalution is shown in Table 3.\n \n \n F.2. Evaluation Instructions \n Table 2 listsnaturallanguageinstructionsusedinmodelevaluationsforunseenobjects,backgrounds,\n and environments. Each instruction was run between 1-5 times, depending on the number of total\n instructions in that evaluation set. Table 3 lists natural language instructions used to evaluate\n quantitative emergent evals. Each instruction was run 5 times. \n \n \n \n \n \n \n \n \n \n \n \n 21 \n \n \n "
  },
  {
    "page_num": 22,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n (a) Reasoning \n \n \n \n \n \n ‚Äúmove coke can to \n ‚Äúmove apple to cup with ‚Äúmove banna near the ‚Äúd√©placer les frites verts ‚Äúpick a healthy drink‚Äù Taylor Swift‚Äù\n same color‚Äù sum of two plus one‚Äù dans la tasse rouge‚Äù \n \n \n \n \n ‚Äúmove coke can to \n person with glasses‚Äù\n ‚Äúmove coke can ‚Äúput coke can close ‚Äúmove banana to ‚Äúmove apple to tree‚Äù\n near Y‚Äù to dog‚Äù android‚Äù (c) Human \n (b) Symbol Understanding Recognition \n Figure 8 | Anoverviewofsomeoftheevaluationscenariosusedtostudytheemergentcapabilitiesof RT-2.\n They focus on three broad categories, which are (a) reasoning, (b) symbol understanding, and (c) human\n recognition. Thevisualizedinstructionsareasubsetofthefullinstructions,whicharelistedin Appendix F.2.\n \n Task Group Tasks \n Symbol Understand- movecokecannear X,movecokecannear 3,movecokecannear Y\n ing: Symbol 1 \n Symbol Understand- moveappletotree,moveappletoduck,moveappletoapple,moveapple\n ing: Symbol 2 tomatchingcard \n Symbol Understand- putcokecanclosetodog,pushcokecanontopofheart,placecokecan\n ing: Symbol 3 abovestar \n \n Reasoning: Math move banana to 2, move banna near the sum of two plus one, move\n banana near the answer of three times two, move banana near the\n smallestnumber \n Reasoning: Logos movecuptogoogle,movecuptoandroid,movecuptoyoutube,move\n cuptoasearchengine,movecuptoaphone \n Reasoning: Nutrition getmeahealthysnack,pickahealthydrink,pickupasweetdrink,move\n thehealthysnacktothehealthydrink,pickupasaltysnack \n Reasoning: Colorand move apple to cup with same color, move apple to cup with different\n Multilingual color,movegreenchipstomatchingcolorcup,moveappletovasoverde,\n Bewegen Sieden Apfelindierote Tasse,movegreenchipstovasorojo,\n muevelamanzanaalvasoverde,d√©placerlesfritesvertsdanslatasse\n rouge \n Person Recognition: movecokecantotaylorswift,movecokecantotomcruise,movecoke\n Celebrities cantosnoopdog \n \n Person Recognition: movecokecantopersonwithglasses,movecokecantothemanwith\n Celeb A whitehair,movecokecantothebrunettelady \n \n Table 3 | Naturallanguageinstructionsusedforquantitativeemergentevalutions.\n \n \n \n \n 22 \n \n \n "
  },
  {
    "page_num": 23,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n G. Example Failure Cases \n In Fig.9 weprovideexamplesofanotabletypeoffailurecaseinthe Language Tablesetting,withthe\n \n RT-2 model not generalizing to unseen object dynamics. In these cases, although the model is able\n to correctly attend to the language instruction and move to the first correct object, it is not able to\n control the challenging dynamics of these objects, which are significantly different than the small set\n ofblockobjectsthathavebeenseeninthisenvironment Lynchetal.(2022). Thenpensimplyrollsoff\n the table (Fig. 9, left), while the banana‚Äôs center-of-mass is far from where the robot makes contact\n (Fig. 9, right). We note that pushing dynamics are notoriously difficult to predict and control Yu et al.\n (2016). We hypothesize that greater generalization in robot-environment interaction dynamics may\n be possible by further scaling the datasets across diverse environments and objects ‚Äì for example, in\n this case, datasets that include similar types of more diverse pushing dynamics Dasari et al. (2019).\n In addition, despite RT-2‚Äôs promising performance on real world manipulation tasks in qualitative\n and quantitative emergent evaluations, we still find numerous notable failure cases. For example,\n \n with the current training dataset composition and training method, RT-2 seemed to perform poorly\n at: \n \n ‚Ä¢ Grasping objects by specific parts, such as the handle \n ‚Ä¢ Novel motions beyond what was seen in the robot data, such as wiping with a towel or tool use\n ‚Ä¢ Dexterous or precise motions, such as folding a towel \n ‚Ä¢ Extended reasoning requiring multiple layers of indirection \n \n \n Push the red marker to the video game controller Push the banana to the apple\n \n \n \n \n \n Figure 9 | Qualitativeexamplefailurecasesinthereal-worldfailingtogeneralizetounseenobjectdynamics.\n \n \n H. Quantitative Experimental Results \n \n \n H.1. Overall Performance, for Section 4.1 \n Table 4 lists our quantitative overall evaluation results. We find that RT-2 performs as well or better\n than baselines on seen tasks and significantly outperforms baselines on generalization to unseen\n objects, backgrounds, and environments. \n \n Model Seen Tasks Unseen Objects Unseen Backgrounds Unseen Environments Unseen Average\n Easy Hard Easy Hard Easy Hard \n \n R 3 M(Nairetal.,2022 b) 45 32 14 13 9 0 2 12 \n VC-1(Majumdaretal.,2023 a) 63 34 10 13 3 0 0 10 \n RT-1(Brohanetal.,2022) 92 31 43 71 9 26 14 32 \n MOO(Stoneetal.,2023) 75 58 48 38 41 19 3 35 \n RT-2-Pa LI-X-55 B(ours) 91 70 62 96 48 63 35 62 \n RT-2-Pa LM-E-12 B 1(ours) 93 84 76 75 71 36 33 62 \n Table 4 | Overallperformanceoftwoinstantiationsof RT-2 andbaselinesacrossseentrainingtasksaswellas\n unseenevaluationsmeasuringgeneralizationtonovelobjects,novelbackgrounds,andnovelenvironments.\n \n 23 \n \n \n "
  },
  {
    "page_num": 24,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n H.2. Emergent Evaluation, for Section 4.2 \n Table 5 lists all of our quantitative emergent evaluation results. We find that RT-2 performs 2 x\n to 3 x better than RT-1 on these new instructions, without any additional robotic demonstrations.\n This showcases how our method allows us to leverage capabilities from pretraining on web-scale\n vision-language datasets. \n \n \n Model Symbol Understanding Reasoning Person Recognition Average\n Symbol 1 Symbol 2 Symbol 3 Average Math Logos Nutrition Color/Multilingual Average Celebrities Celeb A Average\n VC-1(Majumdaretal.,2023 a) 7 25 0 11 0 8 20 13 10 20 7 13 11 \n RT-1(Brohanetal.,2022) 27 20 0 16 5 0 32 28 16 20 20 20 17 \n RT-2-Pa LI-X-55 B(ours) 93 60 93 82 25 52 48 58 46 53 53 53 60 \n RT-2-Pa LM-E-12 B(ours) 67 20 20 36 35 56 44 35 43 33 53 43 40 \n Table 5 | Performanceof RT-2 andbaselinesonquantitativeemergentevaluations. \n \n H.3. Size and Training Ablations, for Section 4.3 \n Table 6 detailsquantitativeresultsforablationsacrossmodelsizeandtrainingapproach. Acrosseach,\n we see that model size plays an important role in performance and that co-fine-tuning outperforms\n fine-tuning, which outperforms training from scratch. \n \n Model Size Training Unseen Objects Unseen Backgrounds Unseen Environments Average\n \n Easy Hard Easy Hard Easy Hard \n RT-2-Pa LI-X 5 B fromscratch 0 10 46 0 0 0 9 \n RT-2-Pa LI-X 5 B fine-tuning 24 38 79 50 36 23 42 \n RT-2-Pa LI-X 5 B co-fine-tuning 60 38 67 29 44 24 44 \n RT-2-Pa LI-X 55 B fine-tuning 60 62 75 38 57 19 52 \n RT-2-Pa LI-X 55 B co-fine-tuning 70 62 96 48 63 35 63 \n Table 6 | Ablationsof RT-2 showcasingtheimpactofparametercountandtrainingstrategyongeneralization.\n \n \n I. Additional Chain-Of-Thought Reasoning Results \n \n Wepresentadditionalexamplesofchain-of-thoughtreasoningrolloutsaccomplishedwith RT-2-Pa LM-\n E, as described in Sec. 4.4, in Figure 10. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 1 Theoriginalpre-trainingdatamixtureusedin Pa LM-E-12 B(asdescribedin Driessetal.(2023))includesrobotimages\n forhigh-level VQAplanningtasksthatcanbesimilartoimagesencounteredingeneralizationscenarios.However,noneof\n thosetrainingexamplesincludelow-levelactionsthatareevaluatedinthisexperiment.\n \n 24 \n \n \n "
  },
  {
    "page_num": 25,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n User \n Pick up the \n object that is \n different from all \n other objects \n Plan: Pick rxbar \n chocolate \n User \n Move the green \n objects together \n Plan: Move green \n can near green \n rice chip bag \n User \n Move the blue \n objects together \n Plan: Move blue \n chip bag near \n pepsi \n User \n I am sleepy, bring \n me a drink to \n help. \n Plan: Pick redbull \n can \n Figure 10 | Additionalexamplesof RT-2 withchain-of-thoughtreasoning \n \n \n \n \n \n \n \n \n \n \n \n 25 \n \n \n "
  },
  {
    "page_num": 26,
    "text": " \n \n \n \n RT-2:Vision-Language-Action Models Transfer Web Knowledgeto Robotic Control\n \n \n Task Group Tasks \n Unseen Objects pickbanana,movebanananearcokecan,moveorangecannearbanana,\n (Easy) pickoreo,moveoreonearapple,moveredbullcannearoreo,pickpear,\n pickcoconutwater,movepearnearcoconutwater,movepepsicannear\n pear \n Unseen Objects pickcoldbrewcan,picklargeorangeplate,pickchewtoy,picklargeten-\n (Hard) nisball,pickbirdornament,pickfishtoy,pickgingerlemonkombucha,\n pick egg separator, pick wrist watch, pick green sprite can, pick blue\n microfibercloth,pickyellowpear,pickpretzelchipbag,pickdisinfectant\n wipes,pickpineapplehintwater,pickgreencup,pickpicklesnack,pick\n small blue plate, pick small orange rolling pin, pick octopus toy, pick\n catniptoy \n Unseen Back- pickgreenjalapenochipbag,pickorangecan,pickpepsican,pick 7 up\n grounds(Easy) can, pick apple, pick blue chip bag, pick orange, pick 7 up can, move\n orangenearsink,pickcokecan,picksponge,pickrxbarblueberry\n Unseen Back- pick wrist watch, pick egg separator, pick green sprite can, pick blue\n grounds(Hard) microfibercloth,pickyellowpear,pickpretzelchipbag,pickdisinfectant\n wipes,pickpineapplehintwater,pickgreencup,pickpicklesnack,pick\n small blue plate, pick small orange rolling pin, pick octopus toy, pick\n catniptoy,pickswedishfishbag,picklargegreenrollingpin,pickblack\n sunglasses \n Unseen Environ- pickcokecan,pickapple,pickrxbarblueberry,moveapplenearcokecan,\n ments(Easy) moverxbarblueberrynearapple,movecokecannearrxbarblueberry,\n pickblueplasticbottle,picksponge,pickbluechipbag,movesponge\n near blue plastic bottle, move blue chip bag near sponge, move blue\n plasticbottlenearbluechipbag,movecokecannearwhitemug,move\n spongenearwhitemug,movecokecannearyellowbowl,movesponge \n nearyellowbowl,movecokecanneargreencloth,movespongenear \n greencloth,movecokecannearplate,movespongenearplate,move\n coke can near spoon, move sponge near spoon, move coke can near\n orangecup,movespongenearorangecup,pickwhitemug,pickyellow\n bowl,pickgreencloth,movewhitemugnearsponge,moveyellowbowl\n nearsponge,movegreenclothnearsponge,pickplate,pickspoon,pick\n orange cup, move plate near sponge, move spoon near sponge, move\n orangecupnearsponge,putcokecanintosink,dropcokecanintosink,\n push coke can into sink, put sponge into sink, drop sponge into sink,\n pushspongeintosink,putgreenclothintosink,dropgreenclothinto\n sink,pushgreenclothintosink \n Unseen Environ- pickcokecan,pickapple,pickrxbarblueberry,moveapplenearcokecan,\n ments(Hard) moverxbarblueberrynearapple,movecokecannearrxbarblueberry,\n movecokecannearstapler,moveapplenearstapler,movecokecannear\n keyboard, move apple near keyboard, move coke can near tissue box,\n moveappleneartissuebox,movecokecannearpapers,moveapplenear\n papers,movecokecannearmouse,moveapplenearmouse,movecoke \n can near book, move apple near book, pick marker, pick stapler, pick\n mouse,movemarkernearapple,movestaplernearapple,movemouse\n nearapple,pushcokecantotheleft,pushcokecantotheright,push\n spongetotheleft,pushspongetotheright,pushtissueboxtotheleft,\n pushtissueboxtotheright,pointatcokecan,pointatsponge,pointat\n tissuebox \n Table 2 | Natural language instructions used for evaluations testing controlled distribution shifts along the\n dimension of novel objects, novel environments, and novel backgrounds. For each category, we introduce\n evaluationsettingswithsmallerdistributionshiftsaswellaslargerdistributionshifts. Avisualizationofthese\n scenariosifshownin Figure 3. \n 26 \n \n "
  }
]