[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n RL-GSBridge: 3 D Gaussian Splatting Based \n Real 2 Sim 2 Real Method for Robotic Manipulation Learning \n \n \n Yuxuan Wu∗, Lei Pan∗, Wenhua Wu, Guangming Wang, Yanzi Miao, Fan Xu# and Hesheng Wang#\n \n \n Abstract—Sim-to-Real refers to the process of transferring 1. Real 2 Sim by \n Soft Mesh Binding GS \n policieslearnedinsimulationtotherealworld,whichiscrucial \n for achieving practical robotics applications. However, recent \n Sim 2 real methods either rely on a large amount of augmented \n data or large learning models, which is inefficient for specific \n tasks. In recent years, with the emergence of radiance field \n reconstruction methods, especially 3 D Gaussian splatting, it \n hasbecomepossibletoconstructrealisticreal-worldscenes.To Dynamics-based GS Editing Grasp Pick&place\n this end, we propose RL-GSBridge, a novel real-to-sim-to-real \n framework which incorporates 3 D Gaussian Splatting into the \n conventional RL simulation pipeline, enabling zero-shot sim- \n to-real transfer for vision-based deep reinforcement learning. Render img RL-GS\n We introduce a mesh-based 3 D GS method with soft binding \n Bridge \n constraints, enhancing the rendering quality of mesh models. \n Thenutilizinga GSeditingapproachtosynchronizetherender- 2. Learn Policy at 3. Zero-shot Real-world\n ing with the physics simulator, RL-GSBridge could reflect the Simulator with Physic Robot Manipulation on\n visual interactions of the physical robot accurately. Through a Dynamics-based GS renderer Various Tasks\n series of sim-to-real experiments, including grasping and pick- \n Fig. 1. Pipeline of RL-GSBridge. (1) Real 2 Sim Environment Transfer.\n and-place tasks, we demonstrate that RL-GSBridge maintains \n Real-world scenarios is reconstructed through a novel soft mesh binding\n asatisfactorysuccessrateinreal-worldtaskcompletionduring \n GS model. (2) Learn Policy at Simulator with GS Render. With physical\n sim-to-realtransfer.Furthermore,aseriesofrenderingmetrics \n dynamics-based GS editing, RL policies learn through realistic rendered\n andvisualizationresultsindicatethatourproposedmesh-based images in simulation. (3) Zero-shot Real-world Robot Manipulation. We\n 3 DGSreducesartifactsinunstructuredobjects,demonstrating directlyapplythepolicytoreal-worldtaskswithoutfine-tuning.\n more realistic rendering performance. \n I. INTRODUCTION \n reality, or to train a highly generalized large model to learn\n Learningroboticactionpoliciesinsimulationandtransfer- \n knowledgefordifferenttasks.Thissignificantlyincreasesthe\n ring them to real-world represents an ideal robotic learning \n difficulty in training stage. \n strategythatbalancesboththecostandsafetyofthelearning \n process.However,asignificantbottleneckisthereliabilityof To avoid additional training burden and achieve ideal\n sim-to-realtransfer,whichimpactsthepotentialoftheentire Sim 2 Real performance on specific tasks, a novel framework\n framework towards substantial challenges. is needed. Recently, advances in radiance field-based recon-\n With the continuous development of the simulation-to- struction methods [5], [6], [7], [8] provide new directions\n reality(Sim 2 Real)field,extensiveworkisadvancingprogress for Sim 2 Real training. Based on a simple idea—using radi-\n from multiple perspectives [1], [2], [3], [4]. However, most ance field reconstruction to create a visually realistic robot\n Sim 2 Real methods attempt to expand the distribution of trainingenvironment—canweachievesatisfactory Sim 2 Real\n training data to cover various situations that may arise in performance? For this purpose, we design a Real 2 Sim 2 Real\n visualreinforcementlearningframework,RL-GSBridge,that\n Thisworkwassupportedinpartbythe Shenzhen Scienceand Technology bridges the real-to-sim gap by 3 D Gaussian splatting, as\n Program under Grant KJZD 20230923114812027. (Corresponding Author: \n shown in Fig. 1. Utilizing 3 D Gaussian splatting and editing\n Fan Xuand Hesheng Wang) \n *Equalcontributions techniques, RL-GSBridge provide a ‘virtual-reality’ simula-\n #Co-correspondingauthors tion platform for policy learning. \n Y. Wu, W. Wu, F. Xu, and H. Wang are with the Shenzhen Research \n Instituteof Shanghai Jiao Tong University,Shenzhen 518000,China. Forvision-basedrobottasks,itiscrucialtoavoidillusions\n Y. Wu, F. Xu, and H. Wang are also with the Department of Automa- caused by inconsistencies between visual perception and\n tion, Shanghai Jiao Tong University, Shanghai 200240, China. (e-mail: \n contact geometry. Thus, it is required to ensure an accurate\n furrygreen@sjtu.edu.cn;xufanlyra@sjtu.edu.cn;wanghesheng@sjtu.edu.cn) \n W. Wu is also with Mo E Key Lab of Artificial Intelligence, AI Institute, geometric representation of the model while achieving more\n Shanghai Jiao Tong University,Shanghai 200240,China. realisticrenderingresults.Ga Me S[9]hasdrawnourattention\n L. Pan and Y. Miao are with the School of Information and Control \n asitisamesh-based Gaussiansplatting(GS)method,which\n Engineering,China Universityof Miningand Technology,Xuzhou 221100, \n China. ensuresthattheoptimizationof GSunitsisperformedwithin\n G.Wangiswiththe Departmentof Engineering,Universityof Cambridge, the geometric mesh model. However, it enforces Gaussians\n Cambridge CB 21 PZ,U.K.(e-mail:gw 462@cam.ac.uk) \n to be aligned with the mesh grid planes, which could be\n Code is available at https://github.com/IRMV-Manipulation-Group/RL- \n GSBridge considered as ‘hard mesh binding’, thereby limiting the\n 5202 \n be F \n 22 \n ]OR.sc[ \n 2 v 19202.9042:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n flexibility of GS units. To address this, we propose a soft learning [20]. Meta-learning aims to teach robots the ability\n mesh binding method for Gaussian Splatting, which could to learn new tasks, whereas distillation learning trains a\n further enhance rendering quality while preserving editing student network using knowledge from expert networks.\n capabilities for both objects and the background. Ourapproachalignswiththefirstcategoryofgap-bridging\n Physics-based dynamics simulation is also a crucial and methods, but with a unique twist. We use soft mesh binding\n challenging aspect of sim 2 real. To tackle this, an off-the- GS to create realistic simulation environments for robot\n shelf physics simulator is used to provide dynamic changing training.Typically,achievingsuchfidelityrequiresexpensive\n information. The GS editing process simultaneously updates 3 D scanning equipment or CAD expertise. In contrast, GS\n the scene, ensuring that the rendering results align with modelsvisuallyrealisticsimulationenvironmentsusingonly\n physical interaction processes. multi-view images captured with consumer-grade devices.\n Based on the designed simulation platform, we train \n B. Radiance Field in Robotics \n roboticmanipulationpoliciesbydeepreinforcementlearning \n methods. The model is trained on grasping and pick-and- Neural Radiance Fields (Ne RF) [5] is an implicit repre-\n place tasks across scenarios that include diverse textures, sentation technique for novel view synthesis. It optimizes\n geometric shapes, and patterned desktop backgrounds. the parameters through multi-view images, and allows for\n Under the RL-GSBridge framework, the policy shows a the synthesis of any target views using volumetric ren-\n minor variation in success rates during Sim 2 Real transfer. dering. Ne RF’s representation has been applied to various\n This means that the policy maintains effective performance tasks, including Simultaneous Localization and Mapping\n on real-world tasks, reflecting a strong ability to generalize (SLAM) [7], [21], [22], scene reconstruction [23], scene\n from simulation to real-world environments. segmentation [24], navigation [25], and manipulation [26].\n To summarize our contributions: Ne RF-RL [27] treats the novel view synthesis task of\n Ne RF as a proxy task, where the learned encoding network\n • A Novel Sim 2 Real RL Framework: Leveraging the \n is directly used as feature input for reinforcement learning.\n high-fidelity rendering of 3 D GS and the convenience \n Y. Li et al. [28] uses Ne RF as a perceptual decoder for\n of modeling scenes with only consumer-grade cameras. \n the hidden states of a world model, training an additional\n • A Soft Mesh Binding GS Modeling Method: Propos- \n dynamicsestimationnetworktopredictfuturestatechanges.\n ing a soft mesh binding strategy to replace the hard \n Ne RF 2 Real [29] learns real-world contexts by converting\n mesh binding baseline, enhancing the flexibility and \n background meshes into a simulator, and trains robots to\n render quality. \n perform visual navigation, obstacle avoidance, and ball-\n • Physical Dynamics-Based GSEditing:Integratingdy- \n handling tasks. However, the training and rendering speed\n namicssignalsfromthesimulatortoedit 3 DGSmodels, \n of Vanilla Ne RF has consistently been a bottleneck limiting\n reflecting realistic physical robotic interactions. \n its further deployment in practical applications.\n • Validation on Real Physical Robots: Testing the RL- \n 3 D GS [30] is an explicit radiance field method that\n GSBridge framework on physical robots through grasp- \n directly updates the attributes of each 3 D Gaussian com-\n ing and pick-and-place tasks in real-world scenarios \n ponent to optimize scene representation. 3 D GS employs\n with complex textures and geometries. \n a splatting technique. for rendering, and achieve extremely\n II. RELATEDWORK fast training efficiency through CUDA parallel technology.\n Moreover, compared to the implicit representation of Ne RF,\n A. Sim 2 Real Transfer in RL \n the explicit representation facilitates tracking dynamic scene\n RL constructs an interactive learning model in which an modeling and editing scene content.\n agent learns to maximize rewards through trial and error. Many robotics works also incorporate 3 D Gaussian tech-\n By incorporating deep learning, deep reinforcement learn- niques for perception and motion learning [31]. Mani Gaus-\n ing(DRL) enhances the framework’s ability to tackle more sian [32] uses 3 D GS as visual and dynamic scene repre-\n complextasks[10].DRLhasshownimpressiveperformance sentation for policy learning. Quach et al. [33] combine GS\n across various domains, including games [11], finance [12], with Liquid networks for real-world drone flight navigation\n autonomous driving [13], and robotics [14]. However, most tasks, training in simulation and then deploying the policy\n applicationsarerestrictedtovirtualenvironmentsduetoreal- in the real world. \n world constraints related to safety, efficiency, and cost. Incontrastto Ne RF 2 Real[29],whichdirectlytexturesthe\n To improve the feasibility of deploying models in the real foreground objects in simulator, we model each foreground\n world, many researchers strive to bridge the gap between object with editable GS parameters. Also unlike the method\n simulation and reality[15]. These methods include domain designed by Quach et al. [33], which trains drone naviga-\n randomization [16] and domain adaptation [17], [18]. Do- tion policies for target-oriented tasks, our approach involves\n mainrandomizationinvolvesvaryingtask-relatedparameters robotic arm manipulation tasks with complex interactions.\n in the simulation to cover a broad range of real-world \n III. METHODS \n conditions, while domain adaptation focuses on extracting \n a unified feature space from both sources. Higher-level RL-GSBridgeaimstoharnessthepotentialofhigh-fidelity\n learning methods include metalearning [19] and distillation 3 DGSmodelsin Sim 2 Realforrobotactiontrainingtasks.In"
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n this paper, we focus on manipulation tasks for robotic arms. Smoother \n Asshownin Fig.1,theoverallframeworkisdividedintotwo GS representation \n parts:Real 2 Simand Sim 2 Real.In Real 2 Simstage,wecollect \n real-world image data {I }, where I represent the image \n k k \n sequences of the k-th object or background in the scenarios. \n We will model both the geometry and appearance of the \n MVS \n scenetobuildasimulationenvironmentforthemanipulation \n task. In Sim 2 Real stage, we use visual perception and deep \n reinforcementlearningtotrainapolicynetwork,anddirectly 2 2 \n transfer the policy to the real world. \n 3 \n A. Real 2 Sim: Building simulator with soft mesh binding GS 1 1 \n \n Mesh model Soft binding constraint\n To obtain a realistic simulation environment, we use \n consumer-grade cameras to capture 2 D image data I of Fig. 2. Mesh-based GS Reconstruction with Soft Binding Constraints:\n k \n Releasing the hard constraints of Ga Me S [9] in the normal direction for\n desktop-level operating platforms. Our goal is to model a \n smootherandmoreflexibleobjectsurfaces. \n geometricallyaccurateandtexture-realisticsimulationmodel \n fortrainingoperationaltasks.Morespecifically,weusemesh \n models {M k } to represent the accurate geometric informa- To address this, we propose a soft mesh binding method,\n tion, and Gaussian sets G k ={gi k } for high-quality texture. whichbuildsupon Ga Me S[9],butrelaxestheenforcedhard\n Below, we sequentially describe the steps to construct the mesh binding to a soft binding constraint. We introduce a\n Simulator with GS renderer. component along the normal direction into the vector of\n 1) Real-world Data Preparing: We use a monocular positions, specifically: \n cameraormobilephonetocapturea 1-2 minutevideoofthe \n µi(cid:0) αi,αi,αi,αi(cid:1) =αiv +αiv +αiv +αiv , (2)\n targetobjectontheexperimentalplatform.Weselectapprox- 1 2 3 n 1 1 2 2 3 3 n n\n imately 200 keyframes from the video and use COLMAP here, αi is a learnable weight parameter and v is the\n n n \n [34] to obtain the camera’s internal and external parameters normal vector. αi is constrained within the range of [−1,1],\n n \n foreachframe.Imagesegmentationalgorithmisalsoneeded \n ensuring the association between each mesh model element\n for extracting the target object, and we use an off-the-shelf \n and Gaussian pairs. As shown in Fig. 2, our method allows\n segmenter,SAM-track[35],forefficientobjectsegmentation. \n the Gaussian units within the mesh to float within a certain\n To complete the GS model reconstruction for the simula- range along the normal vector. This flexibility in Gaus-\n tor,itisalsonecessarytopre-modelageometricallyaccurate sian unit optimization could bring a smoother distribution\n mesh model as a prior model for GS. Here we consider a of Gaussian units on the object’s surface. Ultimately, the\n classic and stable open-source package open MVS to obtain algorithm not only ensures that the Gaussian model can\n the corresponding mesh model. still represent the accurate geometric structure according\n 2) Real 2 Simmodelingbysoftmeshbinding GS: Withthe to the mesh models, but also offers some tolerance and\n object mesh, we can define Gaussian units within triangular refinement space. Besides, the binds between the mesh grid\n faces as in Ga Me S [9], a method that binds Gaussians and Gaussians could even provide the possibility to handle\n onto the surface of meshes, and optimizes their properties non-rigid objects, as demonstrated in section IV-B.4.\n through multi-view consistency. Vanilla GS [30] optimizes 3) Physic Dynamics-Based GS Editing: After obtaining\n the attribute parameters of Gaussian units located at each the visual GS models {G } and geometry models {M }\n k k \n point cloud position, where θi = (µi,ri,si,σi,ci) denotes for real-world objects, we combine the dynamic simulation\n the position, rotation, scale, opacity, and color of the i- results from the simulator with real-time Gaussian model\n th Gaussian unit gi, respectively. To achieve controllable updates and render views to ensure that the visual represen-\n geometric editing effects, Ga Me S [9] constrains Gaussian tationfollowstheentirephysicalchangeprocess.Wefirstuse\n units within the triangular mesh of the object surface, using RANSAC plane regression and manual alignment methods\n the mean vector as the convex combination of the mesh to align GS models with mesh models in the simulator, and\n vertices to establish the positional relationship between the set the initial position of the GS model.\n Gaussian unit and the three vertices of the triangular mesh: With the aligned initial model, we read the real-time pose\n changes of each object in the operational scene from the\n µi(cid:0) α 1 i,α 2 i,α 3 i(cid:1) =α 1 iv 1 +α 2 iv 2 +α 3 iv 3 , (1) simulator. For the k-th object with its GS model {G k }, We\n acquire the rotation quaternion q and the homogeneous\n here, v ,v ,v represents the triangular mesh vertex posi- k \n 1 2 3 transformation matric T in the world coordinate system.\n tions, αi,αi,αi are learnable positional weight parameters k \n 1 2 3 Given Gaussian parameters θi=(µi,ri,si,σi,ci), The edit-\n forg.However,thisapproachofenforcedmesh-GSbinding k k k k k k \n i ing process of each Gaussian gi in the model set G could\n would diminish the flexibility of 3 D Gaussians, limiting the k k \n be formulated as: \n optimization of Gaussian units when the mesh model is not \n accurate, introducing certain undesirable defects. µi =T µi, ri =q ×ri. (3) \n k k k k k k "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n Physical Simulator \n \n \n Simulated \n Eye-in-hand Image \n ... \n Physic Dynamics-Based \n GS Editing \n render 1 , \n 2 \n , \n \n rendered image , \n \n Viewpoint \n Conv \n M M \n \n L Base L Q \n P Con trol le r P \n Actor Critic \n \n Fig.3. Policytrainingpipelinein RL-GSBridge.Intheupperhalfofthefigure,physicdynamics-based GSeditingreceivesthetransformationsignals\n of objects and synchronizes the states of GS models. In the lower half of the figure, an actor-critic RL network receives first-person perspective images\n renderedby GSmodelsasinput,tolearnavision-basedmanipulationpolicy. \n TABLEI \n here, (s ,a ,r ,s ) is a tuple belong to B, the y is\n t, t, t+1 t+1 t \n COMPARISONOFSUCCESSRATESBETWEENRL-GSBRIDGEAND \n calculated by target Q networks Q : \n RL-SIMINGRASPINGEXPERIMENTS,ALLCONDUCTINGUNDERFOAM φ,j \n PAD(FP)BACKGROUND.BOLDINDICATESBETTERRESULTS.VALUES y t =r t +γ(min Q φ,j (s t+1 ,a t+1 )−αlogπ θ (a t+1 |s t+1 )). (5)\n j=1,2 \n INPARENTHESESREPRESENTRELATIVECHANGEINSUCCESSRATE \n DURINGSIM 2 REALTRANSFER.(↓XX%)INDICATESADECREASE, In continuous action space, policy π θ outputs actions of\n WHILE(↑XX%)INDICATESANINCREASE. Gaussian distribution. The loss for actor is defined as:\n 1 \n Loss = ∑ \n Object Small cube Bear \n actor |B| (st,at,rt+1,st+1)∈B \n (6) \n Test scene Sim Real Sim Real (αlogπ θ (a (cid:101)t |s t )−min Q φ,j (s t ,a (cid:101)t )),\n j=1,2 \n RL-sim 96.88 12.50 (↓87%) 93.75 25.00 (↓73%) where a is sampled using the reparameterization trick, i.e.,\n (cid:101)t \n RL-GSBridge 96.88 96.88 87.50 100.00 (↑14%) a = f (ε;s), ε is Gaussian random noise. In practical\n t θ t t \n implementation, we set: f =tanh(µ (s)+σ (s)⊙ε).\n θ θ t θ t t \n Since vanilla SAC struggles to converge rapidly under\n sparse reward, to accelerate the learning process through\n As for local non-rigid transformations on the mesh grid, the \n automated guidance, we propose SACw B by referring to\n Gaussians could be updated according to Equation 2. After \n DDPGw B [38], and introduce a lightly designed baseline\n applying transformations to all object models, we concat the \n controllertoguidethepolicy.Thisapproachavoidscomplex\n Gaussian sets to acquire the Gaussian model of the whole \n reward designs while eliminating ineffective action spaces.\n scene G . Then we use rasterization to render G , \n scene scene In SACw B, the agent executes actions from the baseline\n obtaining the synchronized edited rendering view. \n controller with probability λ and selects the best option\n betweenthebaselinecontroller’sandtheactor’soutputswith\n probability 1−λ, the objective is typically defined as:\n B. Sim 2 Real: Train in simulation with physic dynamics- \n y =r +γmax((min Q (s ,a )−αlogπ (a |s )), \n based GS renderer and zero-shot transfer to reality t t φ,j t+1 t+1 θ t+1 t+1\n j=1,2 \n (min Q (s ,µ (s ))−αlogπ (µ (s )|s ))).\n We use Pybullet [36] as the simulation training platform, j=1,2 φ,j t+1 b t+1 θ b t+1 t+1\n (7) \n and employ SAC (Soft Actor-Critic) [37] algorithm for \n For the supervision of the actor network, we introduce an\n policy learning, due to its mature development in RL and \n additionalbehaviorcloneloss L in Equation 6 tosupervise\n widespread application in robotics. The SAC is an offline bc \n the mean values of the action distribution through base\n policy algorithm belonging to maximum entropy RL, which \n controller actions, with the probability λ:\n consistsoftwocriticnetworks,Q (s,a)and Q (s,a),and \n φ,1 φ,2 \n one actor network, π θ (s). For the sampled set B from the L bc =∥µ θ (s)−µ b (s)∥2, (8)\n replay buffer, the update loss of critic is defined as: \n and with the probability 1−λ: \n (cid:13) (cid:13)2 \n Loss critic = |B 1 | ∑ (st,at,rt+1,st+1)∈B (y t −Q φ,j (s t, a t ))2, (4) L bc = (cid:13) (cid:13) (cid:13) µ θ (s)−µ|argmax( j m =1 in ,2 Q φ,j (s,µ θ (s))) (cid:13) (cid:13) (cid:13) , (9)"
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n TABLEII \n SIM 2 REALRESULTSFORGRASPINGTASKINVARIOUSMANIPULATIONSCENARIOS.CONTENTSINPARENTHESESAFTERTHEOBJECTNAMES\n REPRESENTDIFFERENTBACKGROUNDS(BG),WHEREFPDENOTESFOAMPAD,ANDTCDENOTESTABLECLOTH.\n \n Object(Bg) Cake(FP) Banana(FP) Small cube(TC) Cake(TC) Banana(TC) Bear(TC) \n Testscene Sim Real Sim Real Sim Real Sim Real Sim Real Sim Real \n \n Successrate(%) 100.00 100.00 100.00 93.75(↓6%) 96.88 87.50(↓10%) 100.00 93.75(↓6%) 100.00 96.88(↓3%) 87.50 75.00(↓14%)\n \n \n Simulation Real World (Ours) Real World (RL-sim) \n Camera \n View \n \n Robot Arm \n Behaviour \n \n \n Fig.4. Comparisonofsim-to-realbehaviorconsistencybetween RL-GSBridgeand RL-sim.\n \n TABLEIII \n The Intel Real Sense D 435 i camera fixed on the robot arm\n THESIM 2 REALRESULTFORTHEPICKANDPLACETASK. \n captures the RGB images from the first-person perspective.\n 2) Tasks: As shown in Fig. 1, we design two types of\n Object Cake&Plate \n tasks from the robot’s first-person perspective: grasping and\n Testscene Sim Real pick-and-place operations. \n Successrate(%) 68.75 71.87(↑4%) For grasping, the robot grasps a target object and lifts it\n up.Theinitialpositionsoftheobjectsarerandomizedwithin\n a 30 × 30 cm section. We use Small cube, Cake, Banana,\n and Bear as objects. As for the operation platform, we use\n here, λ is the decay factor, which gradually approaches 0 as \n a foam pad with and without a tablecloth as two different\n training progresses. \n backgrounds. Success is considered when the object is lifted\n The whole training pipeline in the simulator with physic \n 10 cm above the table. \n dynamics-based GS renderer is shown in Fig. 3. The RL \n Forpick-and-placetasks,theroboticarmpickupthecake\n algorithm provides executable actions to the physics sim- \n model and place it on a plate. The position of the cake is\n ulator, which will return state information for actor-critic \n set similarly as described in the grasping task. The plate\n learning. The GS renderer simultaneously renders the realis- \n is placed in a fixed position on the foam pad. Success is\n tic images by physic dynamics-based GS rendering, serving \n considered when the cake is placed on the plate.\n as observation input to the actor network. We rely solely on \n 3) Evaluation Setup: For each task, we conduct both the\n thefirst-personperspectiveimageandproprioceptivestateof \n simulation and the real world experiments. During testing,\n the robotic arm, considering several advantages as described \n we divide the 30 × 30 cm section into four quadrants.\n in [39]. To enhance sim 2 real robustness, we add random \n For each task, we test the policy across a fixed number of\n noise to the rendered images of the GS model and randomly \n positions in each quadrant to calculate the success rate.\n alter certain image attribute parameters during training for \n 4) Baseline: To demonstrate that our method effectively\n environment challenges. \n reduces Sim 2 Real gap, we conduct a baseline experiment\n After policy training in the simulator, we directly deploy \n called RL-sim: using the same learning method but training\n the actor network onto the real robot arm, with the real- \n directly on images rendered from mesh models shaded in\n world eye-in-hand camera observations of the environment \n the Py Bullet simulator. We select two representative and\n serving as visual input. The trained policy subsequently \n easily shaded objects: the Small cube and Bear. Grasping\n outputsthepositionoftheend-effectorandthegripperstates \n experiments for RL-sim are conducted on a foam pad.\n as executable actions. \n B. Experiment Results \n IV. EXPERIMENTS 1) Grasping: In Table I,wecomparethegraspingperfor-\n mance of RL strategies trained with the baseline (RL-sim)\n A. Experiment Setup \n and RL-GSBridge in both simulation and real environments.\n 1) Robot Platform: We use a KUKA iiwa robot arm For both the Small cube with simple geometry and textures\n paired with a Robotiq 2 F-140 gripper as the platform. and the Bear with complex geometry and textures, RL-sim"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n TABLEIV \n OURSOFTMESHBINDINGMETHODCOMPAREDWITHGAMES[9]ONMULTIPLEFOREGROUNDOBJECTSANDBACKGROUNDRENDERING\n METRICS.SSIMISSCALESTRUCTURALSIMILARITYINDEX.PSNRISPEAKSIGNAL-TO-NOISERATIO.LPIPSISLEARNEDPERCEPTUALIMAGE\n PATCHSIMILARITY.BGREFERSTOTHEBACKGROUND,WHILECONTENTSINPARENTHESESEXPLAINTHEDETAILEDDIFFERENCES.\n \n Banana Bear Cake Small cube Bg (Foam Pad) Bg (Tablecloth) Bg (Plate)\n Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S Ours Ga Me S\n SSIM↑ 0.989 0.894 0.964 0.956 0.975 0.964 0.989 0.989 0.944 0.939 0.781 0.776 0.776 0.756\n PSNR↑ 35.46 26.53 29.82 28.18 33.38 30.73 37.18 36.64 28.40 27.32 22.88 21.86 21.86 20.80\n LPIPS↓ 0.026 0.049 0.034 0.040 0.066 0.079 0.012 0.012 0.144 0.149 0.248 0.308 0.308 0.311\n \n Banana Cake Bg (tablecloth) Bg (plate) Before Editing After Editing \n \n GT \n \n Ga Me S \n Fig. 6. The editing capability on non-rigid objects of our soft binding\n constraint GSmodelingmethod. \n Ours \n Fig.5. Oursoftbindingconstraintreconstructionmethodcomparedwith simulator and in real scenarios. With the same environment,\n Ga Me S[9]ontwoforegroundobjectsandtwobackgrounds. RL-GSBridgeexhibitsbehaviorhighlyconsistentwithsimu-\n lationtestsduringmanipulation,whereas RL-simwithoutthe\n GS model shows significant differences. Notably, blue lights\n shows a significant success rate drop when transferring to \n in the camera view of Fig. 4 is caused by the gripper indi-\n real world (an average decrease of 80%) due to visual dis- \n cator light. The image augmentation during policy training\n crepancies. In contrast, RL-GSBridge demonstrates a minor \n would mitigate this hue effect, ensuring the consistency of\n variation in success rates, maintaining high performance as \n policy behavior. Meanwhile, RL-GSBridge sitll ensure the\n in simulation. Notably, in the Bear grasping scenario, the \n consistency of texture details between simulated and real-\n success rate in the real world has increased by 14.28%. This \n world images. \n maybeattributedtothesuboptimalsimulationof Bear’ssoft \n 4) GS Rendering results of different Mesh Binding ap-\n material and non-structured shape in the simulation. \n proach: In Table IV,wecomparetheperformanceof Ga Me S\n Table II shows that RL-GSBridge experiences an average \n [9] and our soft mesh binding GS model under various\n drop of 6.6% of the success rate in sim-to-real transfer \n scenarios and achieve the SOTA performance. Furthermore,\n across various complex test scenarios, including diverse \n Fig. 5 shows that our method obtains fewer artifacts and\n objects and desktop backgrounds. This demonstrates that \n more detailed texture rendering. As a supplement, in Fig.\n the integration of the physic dynamics-based GS rendering \n 6, our soft binding constraint GS modeling method achieves\n effectively bridges the perception gap between simulation \n consistent results in editing a non-rigid toy bear. Unfortu-\n and real environments, maintaining stable strategy transfer \n nately, due to the limitations in simulating soft objects in\n across a range of scenarios. Additionally, we observe a \n Py Bullet,wedonotdemonstratecomprehensiveexperiments\n noticeable decrease in success rates for the Bear in the \n on deformable objects manipulation. However, this can be\n Tablecloth background scenario, both in simulation and real \n explored as a future direction. \n environments.Theprimaryreasonforthisdropisthechoice \n of a brown tablecloth, which has similar texture features to \n V. CONCLUSIONS \n the brown toy bear, causing difficulty in visual perception. \n 2) Pick-and-Place: As shown in Table III, we test RL- We propose RL-GSBridge, a real-to-sim-to-real frame-\n GSBridge’s Sim 2 Real performance in pick-and-place task work for robotic reinforcement learning. As an attempt\n where a cake is placed onto a plate. The results indicate a to apply the recently successful radiance field reconstruc-\n 4.54% increase of success rate in real environments, mainly tion methods to construct a realistic robotic simulator, RL-\n due to the differences in physical contacts between simula- GSBridge has shown promising sim-to-real success rates\n tion and reality. In simulation, even minor excess contacts in desktop-level tasks. This motivates us to explore future\n duringtheplacementprocessareconsideredastaskfailures. directions, such as investigating the simulation of realistic\n In contrast, some contacts in real environments that do not lighting [40], and integrating RL-GSBridge with advanced\n affectthetaskcanbetolerated,leadingtobetterperformance large-scalepolicymodels[41]andperceptionlearningmeth-\n since the task is ultimately completed successfully. ods [42], [43], [44].We hope RL-GSBridge will encourage\n 3) Comparison of Sim&Real Behavior Consistency: In moreattemptstoapplyradiancefieldreconstructionmethods\n Fig. 4, we compare the behavior of the robotic arm in the in robotics. "
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n REFERENCES IEEE/CVFConferenceon Computer Visionand Pattern Recognition,\n 2024,pp.21167–21177. \n [1] W. Zhao, J. P. Queralta, L. Qingqing, and T. Westerlund, “Towards [22] S.Zhu,R.Qin,G.Wang,J.Liu,and H.Wang,“Semgauss-slam:Dense\n closingthesim-to-realgapincollaborativemulti-robotdeepreinforce- semantic gaussian splatting slam,” ar Xiv preprint ar Xiv:2403.07494,\n mentlearning,”in 20205 th Internationalconferenceonroboticsand 2024. \n automationengineering(ICRAE). IEEE,2020,pp.7–12. [23] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, “Monosdf:\n [2] F. Muratore, C. Eilers, M. Gienger, and J. Peters, “Bayesian Exploringmonoculargeometriccuesforneuralimplicitsurfacerecon-\n domain randomization for sim-to-real transfer,” ar Xiv preprint struction,”Neur IPS,pp.25018–25032,2022.\n ar Xiv:2003.02471,2020. [24] S.Zhi,T.Laidlow,S.Leutenegger,and A.J.Davison,“In-placescene\n [3] K. Arndt, M. Hazara, A. Ghadirzadeh, and V. Kyrki, “Meta rein- labelling and understanding with implicit scene representation,” in\n forcementlearningforsim-to-realdomainadaptation,”in 2020 IEEE ICCV,2021,pp.15838–15847.\n internationalconferenceonroboticsandautomation(ICRA). IEEE, [25] M.Adamkiewicz,T.Chen,A.Caccavale,R.Gardner,P.Culbertson,\n 2020,pp.2725–2731. J.Bohg,and M.Schwager,“Vision-onlyrobotnavigationinaneural\n [4] R. Traore´, H. Caselles-Dupre´, T. Lesort, T. Sun, N. D´ıaz-Rodr´ıguez, radianceworld,”RA-L,pp.4606–4613,2022.\n and D. Filliat, “Continual reinforcement learning deployed in real- [26] Q.Dai,Y.Zhu,Y.Geng,C.Ruan,J.Zhang,and H.Wang,“Graspnerf:\n life using policy distillation and sim 2 real transfer,” ar Xiv preprint multiview-based 6-dof grasp detection for transparent and specular\n ar Xiv:1906.04452,2019. objectsusinggeneralizablenerf,”in ICRA,2023,pp.1757–1763.\n [5] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor- [27] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint, “Rein-\n thi, and R. Ng, “Nerf: Representing scenes as neural radiance fields forcementlearningwithneuralradiancefields,”Neur IPS,2022.\n forviewsynthesis,”in ECCV,2020. [28] Y.Li,S.Li,V.Sitzmann,P.Agrawal,and A.Torralba,“3 dneuralscene\n [6] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, “Nerf++: representationsforvisuomotorcontrol,”in Co RL,2022,pp.112–123.\n Analyzing and improving neural radiance fields,” ar Xiv preprint [29] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori,\n ar Xiv:2010.07492,2020. T. Haarnoja, B. Moran, S. Bohez, F. Sadeghi, B. Vujatovic, et al.,\n [7] Z.Zhu,S.Peng,V.Larsson,W.Xu,H.Bao,Z.Cui,M.R.Oswald, “Nerf 2 real: Sim 2 real transfer of vision-guided bipedal motion skills\n and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding for usingneuralradiancefields,”in ICRA,2023,pp.9362–9369.\n slam,”in CVPR,2022,pp.12786–12796. [30] B.Kerbl,G.Kopanas,T.Leimkuehler,and G.Drettakis,“3 dgaussian\n [8] S.Zhu,G.Wang,H.Blum,J.Liu,L.Song,M.Pollefeys,and H.Wang, splattingforreal-timeradiancefieldrendering,”ACMTrans.Graph.,\n “Sni-slam:Semanticneuralimplicitslam,”CVPR,2024. vol.42,no.4,2023. \n [9] J.Waczyn´ska,P.Borycki,S.Tadeja,J.Tabor,and P.Spurek,“Games: [31] S. Zhu, G. Wang, D. Kong, and H. Wang, “3 d gaussian splatting in\n Mesh-based adapting and modification of gaussian splatting,” ar Xiv robotics:Asurvey,”ar Xivpreprintar Xiv:2410.12262,2024.\n preprintar Xiv:2402.01459,2024. [32] G.Lu,S.Zhang,Z.Wang,C.Liu,J.Lu,and Y.Tang,“Manigaussian:\n [10] V.Franc¸ois-Lavet,P.Henderson,R.Islam,M.G.Bellemare,J.Pineau, Dynamicgaussiansplattingformulti-taskroboticmanipulation,”ar Xiv\n etal.,“Anintroductiontodeepreinforcementlearning,”Foundations preprintar Xiv:2403.08321,2024.\n and Trends® in Machine Learning, vol. 11, no. 3-4, pp. 219–354, [33] A.Quach,M.Chahine,A.Amini,R.Hasani,and D.Rus,“Gaussian\n 2018. splattingtorealworldflightnavigationtransferwithliquidnetworks,”\n [11] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van ar Xivpreprintar Xiv:2406.15149,2024.\n Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, [34] J.L.Scho¨nbergerand J.-M.Frahm,“Structure-from-motionrevisited,”\n M. Lanctot, et al., “Mastering the game of go with deep neural in Conferenceon Computer Visionand Pattern Recognition(CVPR),\n networks and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016. \n 2016. [35] Y. Cheng, L. Li, Y. Xu, X. Li, Z. Yang, W. Wang, and Y. Yang,\n [12] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep direct “Segmentandtrackanything,”ar Xivpreprintar Xiv:2305.06558,2023.\n reinforcementlearningforfinancialsignalrepresentationandtrading,” [36] E. Coumans and Y. Bai, “Pybullet, a python module for physics\n IEEE transactions on neural networks and learning systems, vol. 28, simulationforgames,roboticsandmachinelearning,”2016.\n no.3,pp.653–664,2016. [37] T.Haarnoja,A.Zhou,P.Abbeel,and S.Levine,“Softactor-critic:Off-\n [13] X. Pan, Y. You, Z. Wang, and C. Lu, “Virtual to real reinforcement policymaximumentropydeepreinforcementlearningwithastochastic\n learning for autonomous driving,” ar Xiv preprint ar Xiv:1704.03952, actor,”in Internationalconferenceonmachinelearning. PMLR,2018,\n 2017. pp.1861–1870. \n [38] G.Wang,M.Xin,W.Wu,Z.Liu,and H.Wang,“Learningoflong-\n [14] L.Pinto,M.Andrychowicz,P.Welinder,W.Zaremba,and P.Abbeel, \n horizonsparse-rewardroboticmanipulatortaskswithbasecontrollers,”\n “Asymmetric actor critic for image-based robot learning,” ar Xiv \n IEEETransactionson Neural Networksand Learning Systems,vol.35,\n preprintar Xiv:1710.06542,2017. \n no.3,pp.4072–4081,2022. \n [15] Y.Liu,W.Chen,Y.Bai,J.Luo,X.Song,K.Jiang,Z.Li,G.Zhao, \n [39] K. Hsu, M. J. Kim, R. Rafailov, J. Wu, and C. Finn, “Vision-based\n J.Lin,G.Li,etal.,“Aligningcyberspacewithphysicalworld:Acom- \n manipulators need to also see from their hands,” in International\n prehensivesurveyonembodiedai,”ar Xivpreprintar Xiv:2407.06886, \n Conferenceon Learning Representations,2021.\n 2024. \n [40] J.Gao,C.Gu,Y.Lin,Z.Li,H.Zhu,X.Cao,L.Zhang,and Y.Yao,\n [16] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,and P.Abbeel, \n “Relightable 3 d gaussians: Realistic point cloud relighting with brdf\n “Domain randomization for transferring deep neural networks from \n decompositionandraytracing,”in European Conferenceon Computer\n simulation to the real world,” in 2017 IEEE/RSJ international \n Vision. Springer,2024,pp.73–89. \n conference on intelligent robots and systems (IROS). IEEE, 2017, \n [41] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King, “A survey\n pp.23–30. \n on vision-language-action models for embodied ai,” ar Xiv preprint\n [17] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr- \n ar Xiv:2405.14093,2024. \n ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al., “Using \n [42] R.Mendonca,S.Bahl,and D.Pathak,“Structuredworldmodelsfrom\n simulation and domain adaptation to improve efficiency of deep \n humanvideos,”ar Xivpreprintar Xiv:2308.10901,2023.\n roboticgrasping,”in 2018 IEEEinternationalconferenceonrobotics \n [43] X. Fang, D. Liu, P. Zhou, and G. Nan, “You can ground earlier\n andautomation(ICRA). IEEE,2018,pp.4243–4250. \n than see: An effective and efficient pipeline for temporal sentence\n [18] X. Fang, A. Easwaran, B. Genest, and P. N. Suganthan, “Your data \n groundingincompressedvideos,”in CVPR,2023.\n is not perfect: Towards cross-domain out-of-distribution detection in \n [44] X. Fang, Z. Xiong, W. Fang, X. Qu, C. Chen, J. Dong, K. Tang,\n class-imbalanceddata,”ESWA,2024. \n P.Zhou,Y.Cheng,and D.Liu,“Rethinkingweakly-supervisedvideo\n [19] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, \n temporalgroundingfromagameperspective,”in ECCV,2025.\n R.Munos,C.Blundell,D.Kumaran,and M.Botvinick,“Learningto \n reinforcementlearn,”ar Xivpreprintar Xiv:1611.05763,2016. \n [20] A.A.Rusu,S.G.Colmenarejo,C.Gulcehre,G.Desjardins,J.Kirk- \n patrick,R.Pascanu,V.Mnih,K.Kavukcuoglu,and R.Hadsell,“Policy \n distillation,”ar Xivpreprintar Xiv:1511.06295,2015. \n [21] S.Zhu,G.Wang,H.Blum,J.Liu,L.Song,M.Pollefeys,and H.Wang, \n “Sni-slam: Semantic neural implicit slam,” in Proceedings of the "
  }
]