[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n Reconciling Reality through Simulation: A \n \n Real-to-Sim-to-Real Approach for Robust \n \n Manipulation \n \n \n Marcel Torne 1 Anthony Simeonov 1,4 Zechu Li 1,3,4 April Chan 1,4 \n Tao Chen 1,4 Abhishek Gupta 2∗ Pulkit Agrawal 1,4∗ \n 1 Massachusets Institute of Technology 2 University of Washington 3 TU Darmstadt\n 4 Improbable AI Lab \n \n \n Abstract—Imitation learning methods need significant human data across a massive range of scenes since content creation\n supervision to learn policies robust to changes in object poses, can be challenging in simulation and data collection can be\n physical disturbances, and visual distractors. Reinforcement \n challenging for the real world, (2) a widely general, robust\n learning, on the other hand, can explore the environment \n policy may be overly conservative, lowering its performance\n autonomously to learn robust behaviors but may require im- \n practical amounts of unsafe real-world data collection. To learn on the specific target domains encountered on deployment.\n performant, robust policies without the burden of unsafe real- Alternatively, we suggest that to maximally benefit a specific\n worlddatacollectionorextensivehumansupervision,wepropose user, it is more critical that the robot achieves high success\n Rial To, a system for robustifying real-world imitation learning \n in their particular home environment, showing robustness\n policies via reinforcement learning in “digital twin” simulation \n to various local disturbances and distractors that might be\n environmentsconstructedontheflyfromsmallamountsofreal- \n world data. To enable this real-to-sim-to-real pipeline, Rial To encountered in this setting. With this in mind, our goal is to\n proposes an easy-to-use interface for quickly scanning and develop a robot learning technique that requires minimal hu-\n constructingdigitaltwinsofreal-worldenvironments.Wealsoin- man effort to synthesize visuomotor manipulation controllers\n troduceanovel“inversedistillation”procedureforbringingreal- \n that are extremely robust for task performance in deployment\n world demonstrations into simulated environments for efficient \n environments. The question becomes - how do we acquire\n fine-tuning, with minimal human intervention and engineering \n required.Weevaluate Rial Toacrossavarietyofroboticmanipu- these robust controllers without requiring prohibitive amounts\n lationproblemsintherealworld,suchasrobustlystackingdishes of effort for data collection or simulation engineering?\n on a rack, placing books on a shelf, and six other tasks. Rial To A potential technique for data-driven learning of robotic\n increases (over 67%) in policy robustness without requiring \n control policies is to adopt the paradigm of imitation learning\n extensive human data collection. Project website and code at \n (IL), learning from expert demonstration data [58, 56, 25].\n https://real-to-sim-to-real.github.io/Rial To/. \n However, controllers learned via imitation learning tend to\n I. INTRODUCTION exhibit limited robustness unless a large number of demon-\n strations are collected. Furthermore, imitation learning does\n Imagine a robot that can de-clutter kitchens by putting \n notlearntorecoverfrommistakesorout-of-distributiondistur-\n dishesonadishrack.Consideralltheenvironmentalvariations \n bancesunlesssuchbehaviorswereintentionallydemonstrated.\n that might be encountered: different configurations of plates \n This makes direct imitation learning algorithms unsuitable for\n or changes in rack positions, a plate unexpectedly slipping in \n widespread, robust deployment in real-world scenarios.\n the gripper during transit, and visual distractions, including \n The alternative paradigm of reinforcement learning (RL)\n clutter and lighting changes. For the robot to be effective, \n allows robots to train on self-collected data, reducing the\n it must robustly solve the task across the various scene and \n burden on humans for extensive data collection [31] and\n object perturbations, without being brittle to transient scene \n to discover robust recovery behaviors beyond a set of pre-\n disturbances.Ourdesiderataisaframeworkthatmakesiteasy \n collected demonstrations (e.g., re-grasping when an object is\n for humans to program the robot to achieve a task robustly \n dropped, re-aligning when an object moves in the gripper,\n underthesevariationsordisturbances.Tobeascalablechoice \n adjusting to external perturbations, etc. — see examples in\n for deployment, the framework should not make task-specific \n Fig. 1). However, directly performing RL in the real world is\n assumptions and must seamlessly apply to many tasks. \n prohibitively slow, often results in unsafe data collection, and\n To design these types of robust robot controllers, one could \n is challenging due to problems like resets and reward specifi-\n attempt to train policies across a massive range of scenes and \n cation[75].Therefore,currently,it’simpracticalinmanycases\n with highly variable objects [12, 21]. This is hard-pressed \n to employ RL for learning robust control policies directly in\n to provide a scalable solution to robotic learning for two \n therealworld.Simulation,ontheotherhand,offerstheability\n reasons - (1) it is challenging to actually collect or synthesize \n to collect significant amounts of data broadly, cheaply, safely,\n *Equaladvising and withprivileged information [12,34, 61,40, 1].However,\n 4202 \n vo N \n 42 \n ]OR.sc[ \n 3 v 94930.3042:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n 1 Real-to-sim transfer of the scene 2 Real-to-sim transfer of policies \n Point cloud-based \n Policy \n \n 3 D \n reconstruction \n through API \n Articulated USD \n Imitation \n (Universal Scene Descriptor) \n learning \n 3 Simulation fine-tuning \n RL fine-tuning with sparse Simulation rollout \n rewards and demos \n \n Privileged information \n demos in sim \n Sim-to-real transfer \n Robust to disturbances and distractors in the real world 4\n \n \n \n \n \n \n New distractor Robot base Perturb closing Policy succeeds \n objects moved toaster opening the toaster \n \n Fig. 1. Rial To system overview. 1) Transfer the real-world scene to the simulator through an easy-to-use API (see Section III-B). 2) Transfer a policy\n learnedfromreal-worlddemonstrationstocollectasetofdemonstrationswithprivilegedinformationinsimulation.Wenotethisstepisoptional,and Rial To\n is compatible with skipping this step and providing demonstrations in simulation (see Section IV-C 2) 3) Use the collected set of demonstrations to bias\n explorationinthe RLfine-tuningwithsparserewardsofastate-basedpolicy(see Section III-C)4)Performteacher-studentdistillationanddeploythepolicy\n intherealworldobtainingrobustbehaviors(see Section III-D). \n manually constructing geometrically, visually, and physically engineering, we leverage a set of real-world demonstrations\n realistic simulation environments for problems like robotic thatbootstrapefficientfine-tuningwithreinforcementlearning.\n manipulation in the home can be time and labor-intensive, These real-world demonstrations help narrow the sim-to-real\n making it an impractical alternative at scale. gap and increase the performance of our policies, as shown in\n To safely and efficiently learn robust manipulation behav- Section IV-B.However,transferringreal-worlddemonstrations\n iors, our key insight is to train RL controllers on quickly into simulation is non-trivial because we do not have access\n constructed simulation scenes. By leveraging a video from to the Lagrangian state of theenvironment (e.g. object poses).\n the target deployment domain, we can obtain scenes complete We therefore propose a new “inverse-distillation” technique\n with accurate geometry and articulation that reflect the ap- thatenablestransferringreal-worlddemosintothesimulation.\n pearanceandkinematicsoftherealworld.These“in-domain” After using RL in constructed simulation environments to\n simulation environments can serve as a sandbox to safely robustify the real-world imitation learning policies, the fine-\n and quickly learn robust policies across various disturbances tuned policies can be transferred back to the real world with\n and distractors, without requiring expensive exploration in significantlyimprovedsuccessratesandrobustnesstotest-time\n the real world. We show how imitation learning policies disturbances. \n trained with small numbers of real-world demonstrations can Overall, our pipeline simultaneously improves the effec-\n be robustified via large-scale RL fine-tuning in simulation tiveness of both reinforcement and imitation learning. RL in\n on these constructed simulation environments, using minimal simulationhelpsmakeimitationlearningpoliciesdeployment-\n amounts of human effort in terms of environment design ready without requiring prohibitive amounts of unsafe, inter-\n and reward engineering. To remove the burden of reward active data collection in the real world. At the same time,\n \n \n "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n bootstrapping from real-world demonstration data via inverse improve the performance of models originally trained with\n distillation makes the exploration problem tractable for RL imitation learning. RL has exploded in its capacity for fine-\n fine-tuning in simulation. This minimizes the amount of task- tuning LLMs [49] and image generation models [4], learning\n specific engineering required by algorithm designers such as rewards from human feedback [14]. In robotics, prior work\n designingthedenserewardsormanuallydesigningthescenes. has explored techniques such as offline RL [70, 47, 33],\n Concretely, we propose Rial To, a system for robustifying learning world models [42, 18], and online fine-tuning in the\n real-worldimitationlearningpolicieswithoutrequiringsignifi- real world [2, 3, 22, 70]. Expert demonstrations have also\n canthumaneffort,byconstructingrealisticsimulationanalogs been used to bootstrap exploration and policy learning with\n for real-world environments on the fly and using these for RL [28, 27, 54, 74]. We similarly combine imitation and RL\n robust policy learning. Our contributions include: to guide exploration in sparse reward settings. However, our\n • A simple policy learning pipeline that synthesizes con- pipeline showcases how demonstrations additionally benefit\n trollers to perform diverse manipulation tasks in the RL by biasing policies toward physically plausible solutions\n real world that (i) reduces human effort in constructing that compensate for imperfect physics simulation.\n environments and specifying rewards, (ii) produces ro- Sim-to-real policy transfer: RL in simulation has been\n bust policies that transfer to real-world, cluttered scenes, used to synthesize impressive control policies in a variety\n showing robustness to disturbances and distractors, (iii) of domains such as locomotion [40, 34, 32], dexterous in-\n requires minimal amounts of expensive and unsafe data handmanipulation[11,12,1,23],anddroneflight[61].Many\n collection in the real world. simulation-based RL methods leverage some form of domain\n • A novel algorithm for transferring demonstrations from randomization [65, 51], system identification [26, 63], or\n the real world to the reconstructed simulation to boot- improved simulator visuals [55, 24] to reduce the simulation-\n strap efficient reinforcement learning from the low-level to-reality(sim-to-real)domaingap.Priorworkhasalsoshown\n Lagrangian state for policy fine-tuning. We show that the benefit of “teacher-student” distillation [12, 32, 60, 9],\n this real-to-sim transfer of human demonstrations both whereinprivileged“teacher”policieslearnedquicklywith RL\n improves efficiency and biases policies toward realistic are distilled into “student” policies that operate on sensor\n behavior in simulation which effectively transfers back observations. To acquire transferable controllers, we similarly\n to the real world. leverage GPU-acceleratedsimulation,teacher-studenttraining,\n • An intuitive graphical interface for quickly scanning anddomainrandomizationacrossparallelenvironments.How-\n and constructing digital twins of real-world scenes with ever, we address the more challenging scenario of household\n articulation, separated objects, and accurate geometries. manipulation, which is characterized by richer visual scenes,\n • We present extensive experimental evaluation showing and minimize the necessary engineering effort by relying on\n that Rial To produces reactive policies that solve several sparse rewards. We also simplify sim-to-real by training on\n manipulation tasks in real-world scenes under physical digital twin assets and co-training with real data [67].\n disturbances and visual distractions. Across eight diverse Real-to-sim transfer of scenes: Designing realistic sim-\n tasks, our pipeline provides an improvement of 67% ulation environments has been studied from the perspective\n over baselines in average success rate across scenarios of synthesizing digital assets that reflect real objects. Prior\n withvaryingobjectposes,visualdistractors,andphysical work has used tools from 3 D reconstruction [29] and inverse\n perturbations. graphics [10] for creating digital twins, and such real-to-sim\n pipelines have been used for both rigid and deformable [62]\n II. RELATEDWORK \n objects. These approaches are all compatible with our system\n Learning Visuomotor Control from Demonstrations: and could be used to automate real-to-sim scene transfer and\n Behavior cloning (BC) of expert trajectories can effectively reduce human effort. Our work similarly leverages advance-\n acquirerobotcontrolpoliciesthatoperateintherealworld[19, ments in 3 D vision [64] for reconstructing object geometry,\n 13, 72, 6, 20, 39]. While several works have used BC but we also introduce an easy-to-use GUI for building a\n to learn performant policies from small to moderately-sized URDF/USDwithaccuratearticulations.Furthermore,our GUI\n datasets [13, 72, 39], performance tends to drop when the could be used to improve the aforementioned methods by\n policy must generalize to variations in scene layouts and making it easier to collect a large dataset of human-annotated\n appearance. Techniques for improving BC often require much articulated scenes. The accuracy of the simulator could be\n larger-scaledatacollection[6,57],raisingscalabilityconcerns. improved further combining our GUI with the latest system\n Other techniques support generalization with intermediate identification research [41, 35].\n representations [20] and leverage generative models to add Real-to-sim-to-real transfer: Prior work has used\n visual distractors [71, 38]. These can improve robustness to Ne RF [43] and other 3 D reconstruction techniques to create\n visual distractors but do not address physical or dynamic realistic scene representations for improving manipulation\n disturbances, as these require producing actions not present [73], navigation [16, 8] and locomotion [7]. These works,\n in the data. however, only use the visual component of the synthetic\n Fine-tuning Imitation with RL and Improving RL with scene and do not involve any physical interaction with a\n Demonstrations: Reinforcement learning has been used to reconstructed geometry. As a result, these systems cannot"
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n adjust to environmental changes beyond visual distractions. we have verified the approach with a variety of scanning apps\n For instance, different grasp poses may require different (e.g., Polycam [52] and ARCode [15]) and 3 D reconstruction\n placements, and a policy cannot discover these novel pipelines [64, 45], each of which convert a set of multi-view\n behaviors without physically interacting with the environment 2 Dimages(oravideo)intoatextured 3 Dmesh.Therawmesh\n during training. A limited number of works have learned denoted G, is typically exported as a single globally-unified\n policies that interact with the reconstructed environments, geometry,whichisunsuitablefordirectpolicylearning.Scene\n but they either simplify the reconstructed shapes [37] or are objects are not separated and the kinematics of objects with\n limited to simple grasp motions [68]. internal joints are not reflected. Physical parameters like mass\n and friction are also required and unspecified. We therefore\n III. RIALTO:AREAL-TO-SIM-TO-REALSYSTEMFOR \n further process the raw mesh G into a set of separate bod-\n ROBUSTROBOTICMANIPULATION \n ies/links {G }M with kinematic relations K and physical\n i i=1 \n A. System Overview \n parameters P. \n Our goal is to obtain a control policy that maps real-world While there are various automated techniques for automat-\n sensory observations to robot actions. We only assume access ically segmenting and adding articulations to meshes [29],\n to a small set of demonstrations (∼ 15) containing (obser- in this work, we take a simple human-centric approach. We\n vation, action) trajectories collected by an expert, although in offer a simple graphical interface for humans to quickly\n principle Rial Tocanalsobeusedtorobustifylarge,expressive separate meshes and add articulations (see Fig. 2). Our GUI\n pretrainedmodelsaswell.Ourapproachrobustifiesreal-world allows users to upload their own meshes and drag/drop,\n imitationlearningpoliciesusingsimulation-based RLtomake reposition, and reorient them in the global scene. Users can\n learned controllers robust to disturbances and distractors not then separate meshes and add joints between different mesh\n presentinthedemos.Theproposedpipeline,Rial To,achieves elements, allowing objects like drawers, fridges, and cabinets\n this with four main steps (Fig 1): to be scanned and processed. Importantly, our interface is\n 1) We construct geometrically, visually, and kinematically lightweight, intuitive, and requires minimal domain-specific\n accurate simulation environments from real-world image knowledge. We conducted a study (Section VI) evaluating\n capture. We leverage 3 D reconstruction tools and develop six non-expert users’ experiences with the GUI and found\n an easy-to-use graphical interface for adding articulations they could scan complex scenes and populate them with a\n and physical properties. couple of articulated objects in under 15 minutes of active\n 2) We obtain a set of successful trajectories containing priv- interaction time. Examples of real-world environments with\n ileged information (such as Lagrangian state, e.g. object their corresponding digital twins are shown in Fig 4 and\n and joint poses) in simulation. We propose an “inverse Appendix Fig. 16. \n distillation” algorithm to transfer a policy learned from The next question is —how do we infer the physics\n real-worlddemonstrationstocreateadatasetoftrajectories parameters that faithfully replicate the real world? While\n (i.e., demos) in the simulation environment. accuratelyidentifyingphysicalparametersispossible,thiscan\n 3) The synthesized simulation demos bootstrap efficient fine- bechallengingwithoutconsiderableinteraction[5,69].While\n tuning with RL in simulation using an easy-to-design adapting to dynamics variations is an important direction for\n sparse reward function and low-dimensional state space, future work, in this system we set a single default value for\n with added randomization to make the policy robust to massandfrictionuniformlyacrossobjectsandcompensatefor\n environmental variations. thesim-to-realgaptoactualreal-worldvaluesbyconstraining\n 4) The learned policy is transferred to reality by distilling a the learned policy to be close to a small number of real-world\n state-based simulation policy into a policy operating from demonstrations as discussed in Section III-C.\n raw sensor observations available in the real world [9, 12]. This procedure produces a scene S = {{G i }M i=1 ,K,P}\n During distillation, we also co-trained with the original represented in a USD/URDF file that references the separated\n real-world demonstrations to capitalize on the combined meshes and their respective geometric (G i }M i=1 ), kinematics\n benefits of simulation-based robustification and in-domain (K) and physical parameters (P). This environment can sub-\n real-world data. sequently be used for large-scale policy robustification in\n The following sections describe each component in detail, simulation. \n along with a full system overview in Fig 1. \n C. Robustifying Real-World Imitation Learning Policies in\n B. Real-to-Sim Transfer for Scalable Scene Generation Simulation \n The first step of Rial To is to construct geometrically, Given the simulation environment generated in Section\n visually,andkinematicallyrealisticsimulatedscenesforpolicy III-B, the next step in Rial To involves learning a robust\n training. This requires (i) generating accurate textured 3 D policy in simulation that can solve desired tasks from a\n geometry from real-world images and (ii) specifying articu- wide variety of configurations and environmental conditions.\n lations and physical parameters. For geometry reconstruction, While this can be done by training policies from scratch in\n we use existing off-the-shelf 3-D reconstruction techniques. simulation,thisisoftenaprohibitivelyslowprocess,requiring\n Our pipeline is agnostic to the particular method used, and considerable manual engineering. Instead, we will adopt a"
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n 3 D reconstruction \n (Ne RFStudio, ARCode, Scene reconstruction GUI Articulated USD \n Polycam) \n Upload/Scale/Move scene Upload more objects Cut mesh Add joint\n \n \n \n \n \n \n Fig. 2. Overview of the real-to-sim pipeline for transfering scenes to the simulator. The first stage consists of scanning the environment, using off-the-\n shelf tools such as Ne RFStudio, ARCode, or Polycam. Each has its strengths and weaknesses and should be used appropriately (see Appendix XII for\n recommendations). The second stage consists of uploading the reconstructed scene into Rial To’s GUI where the user can cut the mesh, specify joints, and\n organizethesceneasdesired.Oncecomplete,thescenecanbedownloadedasa USDasset,whichcanbedirectlyimportedintothesimulator.\n \n \n \n \n \n \n \n \n \n \n {a 1 , . e . 1 . , x 1 , e a i : : e a e c ti p o o n s e (e e pose)\n i a T , e T , x T } x i : object poses \n desab-noisi V \n ycilop \n {a, e, o } D t t t real \n o t e t ee pose e t \n 3 D \n Conv \n MLP \n Succeeded \n PPO with \n Failed BC loss \n a tʼ \n ∩ \n Learning from \n 1 2 Simulation transfer 3 RL fine-tuning \n the real world \n Real-world \n demos \n o \n t point Vision-based \n cloud \n Si R m o u l l l a o t u e t d e t ee policy Sim Sc u e la n t e ed\n point Scene pose object x ee poses t pose cloud Collect fully\n simulated trajectory \n State-based MLP policy\n Train with \n Privileged information \n Supervised \n Learning demos a \n tʼ \n Fig. 3. Inverse distillation & RL fine-tuning. We introduce a novel procedure for going from point cloud-based policies trained from real-world\n demonstrations D real to a robust privileged state-based policy in simulation. 1) Train a vision-based policy with supervised learning on D real 2) Rollout\n thevision-basedpolicyonthesimulationrenderedpointcloudsandcollectasetof 15 privilegeddemonstrationswithobjectposes,D sim 3)Trainarobust\n state-basedpolicywith RLandasparsereward,addinga BClossfitting D sim tobiasexplorationandsetaprioronreal-world-transferablepolicies.\n fine-tuning-based approach, using reinforcement learning in information demonstrations can then be used to instantiate an\n simulationtofine-tuneapolicyinitializedfromasmallnumber efficient RL-based fine-tuning procedure (Section III-C 2) in\n of expert demonstrations collected in the real world. Since simulation to massively improve policy robustness.\n training RL directly from visual observations is challenging, \n we would ideally like to finetune simulation policies that 1) Inverse-distillation from Real-to-Sim for Privileged Pol-\n are based on a privileged Lagrangian state. However, real- icy Transfer: We assume a human provides a small\n worlddemonstrationsdonothaveaccesstothelow-levelstate number of demonstrations in the real world D real =\n informationintheenvironment.Toenablethebootstrappingof {(oi,ai),...,(oi ,ai )}N ,wheretrajectoriescontainobser-\n 1 1 H H i=1 \n RLfinetuninginsimulationfromaprivilegedstateusingreal- vations o (3 D point clouds) and actions a (delta end-effector\n world demonstrations, we introduce a novel “inverse distilla- pose). Considering that simulation-based RL fine-tuning is far\n tion”(Section III-C 1)procedurethatisabletotakereal-world moreefficientandperformantwhenoperatingfromacompact\n demonstrations with only raw sensor observations and actions staterepresentation[32,11](see Section V-C)andwewishto\n and transfer them to simulation demonstrations, complete use real-world human demonstrations to avoid the difficulties\n with low-level privileged state information. These privileged with running RL from scratch (see Section V-B), we want to\n transfer our observation-action demonstrations from the real"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n world to simulation in a way that allows for subsequent RL In addition to mitigating issues associated with explo-\n fine-tuning in simulation from compact state-based represen- ration [46, 54], leveraging the additional imitation learning\n tations. This presents a challenge because we do not have term in the objective helps bias the policy toward physically\n an explicit state estimation system that provides a Lagrangian plausible, safe solutions that improve transfer of behaviors to\n state for the collected demonstrations in the real world. We reality.Duringthisprocess,wecantrainthepolicyforrobust-\n insteadintroduceaprocedure,called“inverse-distillation”,for ness by randomizing initial robot/object/goal poses. Appendix\n converting our real-world set of demonstrations into a set of VIII contains complete details of our training procedure. The\n trajectories in simulation that are paired with privileged low- result is a robust policy π∗ (a|s) operating from Lagrangian\n sim \n level state information. state that is successful from a wide variety of configurations\n Given the demonstrations D , we can naturally train a and environmental conditions.\n real \n policy π (a|o) on this dataset via imitation learning. “In- \n real \n D. Teacher-Student Distillation with Co-Training on Real-\n verse distillation” involves executing this perception-based \n World Data for Sim-to-Real Transfer \n learned policy π (a|o) in simulation, based on simu- \n real \n lated sensor observations o, to collect a dataset D sim = In previous sections, we described a method for efficiently\n {(oi,ai,si)...,(oi ,ai ,si )}M of successful trajectories learning a robust policy π∗ (a|s) in simulation using privi-\n 1 1 1 H H H i=1 sim \n which contain privileged state information si. The key insight leged state information. However, in the real world, this priv-\n t \n here is that while we do not have access to the Lagrangian ileged information is unavailable. Policy deployment requires\n state in the real-world demonstrations when a learned real- operating directly from sensory observations (such as point\n world imitation policy is executed from perceptual inputs in clouds) in the environment. To achieve this, we build on\n simulation, low-level privileged Lagrangian state information the framework of teacher-student distillation (with interactive\n cannaturallybecollectedfromthesimulationaswellsincethe DAgger labeling)[57, 12] where the privileged information\n pairing between perceptual observations and Lagrangian state policy π∗ (a|s) serves as a teacher and the perceptual policy\n sim \n is known apriori in simulation. Since the goal is to improve π∗ (a|o) is the student. Since there is inevitable domain shift\n real \n beyond the real-world imitation policy π real (a|o), we can then between simulation and real domains, this training procedure\n perform RL fine-tuning, incorporating the privileged demon- can be further augmented by co-training the distillation ob-\n stration dataset D sim into the training process, as discussed in jective with a mix of the original real-world demonstration\n the following subsection. data D and simulation data drawn from π∗ (a|s) (via the\n real sim \n 2) Reinforcement Learning Fine-tuning in Simulation: DAgger objective [12]). This results in the following co-\n Given the privileged information dataset D , and the con- training objective for teacher-student policy learning:\n sim \n structed simulation environment the goal is to learn a robust \n policy π∗ (a|s) using reinforcement learning. There are two maxα (cid:88) π θ (π teacher (s i )|o i )\n key chal s l i e m nges in doing so in a scalable way: (1) resolving θ (si,oi,ai)∼τπθ (cid:80) ac π θ (a c |o i )\n (2) \n exploration challenges with minimal reward engineering, and \n (2) ensuring the policy learns behaviors that will transfer +β (cid:88) (cid:80) π θ (a i |o i )\n π (a |o ) \n to the real world. ‘ We find that both challenges can be (oi,ai)∈Dreal ac θ c i\n addressedbyasimpledemonstrationaugmentedreinforcement \n Here the first term corresponds to DAgger training in\n learning procedure [60, 46, 54], using the Lagrangian state- \n simulation, while the second term co-trains on real-world\n based dataset D . To avoid reward engineering, we define a \n sim expert data. This allows the policy to take advantage of\n simple reward function that detects if the scene is in a desired \n small amounts of high-quality real-world data to bridge the\n goalstate(detailedsparserewardfunctionsusedineachtaskin \n perceptual gap between simulation and real-world scenes and\n Appendix VIII).Webuildontheproximalpolicyoptimization \n improve generalization compared to only using the data from\n [59] algorithm with the addition of an imitation learning loss \n simulation. We empirically demonstrate (Section III-D) that\n asfollows(where Aˆ istheestimatoroftheadvantagefunction \n t thissignificantlyincreasestheresultingsuccessrateinthereal\n at step t [59], and V is the learned value function): \n ϕ world.Onapracticalnote,wereferthereaderto Appendix IX\n for additional details on the student-teacher training scheme\n maxα (cid:88) min( π θ (a t |s t ) Aˆ , thatenablesittobesuccessfulintheproposedproblemsetting.\n θ,ϕ (st,at,rt)∈τπθold π θold(at|st) t IV. EXPERIMENTALEVALUATION \n clip( π θ (a t |s t ) ,1−ϵ,1+ϵ)Aˆ ) Our experiments are designed to answer the following\n π t questions about Rial To: (a) Does Rial To provide real-world\n +β \n θ \n (cid:88) \n old(at|st) \n (V (s )−Vtarg)2 (1) policiesrobusttovariationsinconfigurations,appearance,and\n ϕ t t \n disturbances? (b) Does co-training policies with real-world\n (st,V \n t \n targ)∈τπθold \n +γ (cid:88) π θ (a i |s i ) Forthesakeofthiswork,wewillassumethattheoptimalactionsforthe\n (cid:80) π (a |s ) studentandteachercoincide,andtherearenoinformationgatheringspecific\n (si,ai)∈Dsim ac θ c i challengesinducedbypartialobservability[60]"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n Open toaster Plate on rack Book on shelf Mug on shelf Open Drawer Open cabinet\n \n \n \n \n \n \n \n \n \n \n \n \n \n htiw \n derettul C \n \n \n \n \n \n ksa T \n lanigir O \n noitalumi S \n secnaburtsid \n noitazimodna R \n Fig.4. Wedepictthesixtasksusedtoevaluate Rial To.Fromtoptobottom,wefirstshowtheoriginalenvironmentwherewecollectthedemonstrations,\n secondthesimulatedenvironment,thirdtheenvironmentwherewedoourfinalevaluationcontainingclutteranddisturbances,andfourththetaskrandomization\n overvieweachshadedareacorrespondstoanapproximationofhowmuchrandomizationeachobject/robothave.\n \n databenefitreal-worldevaluationperformance?(c)Isthereal- robot base when possible.\n to-sim transfer of scenes and policies necessary for training We conduct our experiments on a Franka Panda arm with\n efficiency and the resulting performance? (d) Does Rial To the default parallel jaw gripper, using 6 Do F Cartesian end\n scale up to more in-the-wild scenes? effector position control. For perceptual inputs, we obtain\n To answer these questions, we evaluate Rial To in eight 3 D point cloud observations from a single calibrated depth\n differenttasks,shownin Figure 4 and 8.Theseinclude 6-Do F camera. More details on the hardware setup can be found in\n grasping and reorientation of free objects (book on a shelf, Appendix X. All of the results in the real world are evaluated\n plate on a rack, mug on a shelf) and 6-Do F grasping and using the best policy obtained for each method, we report\n interacting with articulated objects (drawer and cabinet) on a the average across at least 10 rollouts and the bootstrapped\n tabletop and opening a toaster, plate on a rack, putting a cup standard deviation. Videos of highlights and evaluation runs\n in the trash in more uncontrolled scenes. More details on the are available in the website.\n tasks such as their sparse reward functions and randomization Throughout the next sections, we will evaluate Rial To\n setups are presented in Appendix VIII. For each task, we against the following set of baselines and ablations: 1) Im-\n consider three different disturbance levels in increasing order itation learning from 15 and 50 demos (Section IV-A); 2) No\n of difficulty (see Appendix VIII for more details): co-training on real-world data (Section IV-B); 3) Co-training\n 1) Randomizing object poses: at the beginning of each on demonstrations in simulation (Section IV-B); 4) Rial To\n episode we randomize the object and/or robot poses. from simulation demos (Section IV-C 2); 5) Learning from an\n 2) Adding visual distractors: at the beginning of each untargeted set of simulated assets (Section IV-C 1); 6) Rial To\n episode we also add visual distractors in a cluttered way. without distractors (Section V-A); 7) Rial To without demos\n 3) Applying physical disturbances: we apply physical dis- (Section V-B) \n turbances throughout the episode rollout. We change \n A. Rial To Learns Robust Policies via Real-to-Sim-to-Real\n the pose of the object being manipulated or the target \n location where the object needs to be placed, close the In this section, we aim to understand whether Rial To\n drawer/toaster/cabinet being manipulated, and move the can solve complex tasks, showing robustness to variations in\n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 50 100 50 100 50 100 50 100 50 100 50 100 \n 90 90 90 100 90 85 \n 30 10 10 10 40 50 \n 90 70 60 70 90 80 \n 20 \n 0 0 0 50 0 \n 90 60 50 80 90 80 \n 20 10 \n 0 0 0 0 \n Success Rate \n eso P \n srotcartsi D \n secnabrutsi D \n noitazimodnar \n Kitchen toaster Book on shelf Plate on rack Mug on shelf Open drawer Open cabinet\n Rial To Imitation learning \n Fig.5. Comparisonof Rial Toagainstimitationlearningbothfrom 15 demonstrations.Rial Toprovidesrobustpoliciesacrosstasksandlevelsofdistractions\n whileimitationlearningseverelysufferswhenaddingdistractorsanddisturbances. \n configurations, disturbances, and distractors. We compare our Onlyrandomization Distractors Disturbances\n approachofreal-to-sim-to-real RLfine-tuningagainstapolicy \n BC(15 demos) 10±9% 0±0% 0±0% \n trainedonlyonreal-worlddemosviastandardimitationlearn- BC(50 demos) 40±15% 30±16% 20±13%\n ing (BC). We report the results of running Rial To’s pipeline Rial To(15 demos) 90±9% 70±14% 60±16%\n startingfrom 15 demoscollecteddirectlyinsimulationandco- TABLEI \n training with 15 real-world demos during the teacher-student RIALTOANDIMITATIONLEARNINGONPLACINGABOOKONTHESHELF.\n distillation.In Section IV-C 2 weshowacomparisonofrunning \n Rial To uniquely on real or sim demos. \n morethanthehumaneffortfor Rial Toforwhichwecollect 15\n The results in Figure 5 show Rial To maintains high perfor- demos,in 30 minutes,andbuildtheenvironmentin 15 minutes\n mance across configuration levels, achieving on average 91% of active time (see Section VI). Although more data improves\n success across tasks for randomizing object poses, 77% with theperformanceofdirectimitationlearningfrom 10%to 40%,\n distractors, and 75% with disturbances. On the other hand, 0% to 30%, and 0% to 20% for the three different levels of\n the presence of distractors and disturbances severely reduces robustness, the results in Table I show that Rial To achieves\n the performance of pure imitation learning. For instance, approximately 2.5 times higher success rate than pure BC,\n when only randomizing the object poses, the BC baseline despiteusinglessthanonethirdthenumberofdemonstrations\n achieves an average of 25% success rate across tasks. Under and taking less than half of the human supervision’s time.\n more challenging conditions, the BC baseline drops to 11% \n and 5% overall performance on average for distractors and B. Impact of Co-Training with Real-World Data\n disturbances, respectively. \n Next, we assess the benefits offered by co-training with\n Figure 1, 10 and the videos in the website qualitatively real-world demonstrations during teacher-student distillation,\n show how the resulting policies are robust to many kinds of rather than just purely training policies in simulation. We\n environment perturbations, including moving the robot, mov- consider the book on shelf, plate on rack, mug on shelf, and\n ing the manipulated object and target positions, and adding open drawer tasks (the two first being two of the harder\n visual distractors that cause occlusion and distribution shift. tasks with lower overall performance). The results in Fig-\n The policy rollouts also demonstrate error recovery capabil- ure 6 illustrate that co-training the policy with 15 real-world\n ities, correcting the robot’s behavior in closed loop when, demonstrations significantly increases real-world performance\n e.g., objects are misaligned or a grasp must be reattempted. on some tasks(3.5 x and 2 x success rate increase for book on\n This highlights that Rial To provides robustness that does not shelf and plate on rack with disturbances, when comparing\n emerge by purely learning from demonstrations. co-training on real-world demos against co-training with sim\n We also compare Rial To against behavior cloning with 50 demos) while keeping the same performance on tasks that\n demonstrations to show that the problem is not simply one alreadyhaveasmallsim-to-realgap.Qualitatively,weobserve\n of slightly more data. Collecting the 50 demonstrations takes theco-trainedpolicyismoreconservativeandsafertoexecute.\n a total time of 1 hour and 45 minutes, which is significantly For instance, the policy without co-training usually comes"
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n \n eso P \n \n \n srotcartsi D \n \n \n \n \n secnabrutsi D \n noitazimodnar \n \n 50 100 50 100 50 100 \n Plate on rack Book on shelf Open drawer \n 80 70 100 \n 90 90 90 \n 50 50 100 \n 50 60 90 \n 70 70 80 \n 60 70 90 \n 30 30 90 \n 20 50 90 \n 60 70 80 \n 50 60 90 \n 30 30 80 \n 20 40 90 \n real demos and sim demos and sim demos and \n only sim demos \n real co-training real co-training sim co-training \n Fig. 6. Comparison between running Rial To on sim vs real data. The \n performanceonthemethodsdoingco-trainingwithreal-worlddemosishigher \n thanusingonlysimulateddemosornoreal-worldco-training,ontheharder \n tasks(plateonrackandbookonshelf),andmatchestheperformanceinthe \n easiertasks(opendrawerandmugonshelf).Furthermore,startingfromreal- \n worldorsimulateddemosdoesequallywell. \n very close to the plate or the book, occasionally causing \n it to fall. The policy with co-training data, however, leaves \n more space between the hand and the book before grasping, \n which is closer to the demonstrated behavior. The observation \n that sim co-training performs significantly worse than real- \n world co-training, indicates that co-training with real-world \n demonstrations is helping in reducing the sim-to-real gap for \n both the visual distribution shift between simulated and real \n point clouds and the sim-to-real dynamics gap. \n C. Is Real-to-Sim Transfer Necessary? \n 100 \n 50 \n Simulated Real world \n Objaverse Target \n etar \n sseccus \n reward \n nep O \n 1) Real-to-Sim Transfer of Scenes: Instead of reconstruct-\n ing assets from the target environment, one could train a\n policy on a diverse set of synthetic assets and hope the model\n generalizes to the real-world target scene [12, 21, 66]. While\n thishasshownpromisingresultsforobject-levelmanipulation,\n such as in-hand reorientation [12], it is still an active area of\n work for scene-level manipulation and rearrangement [21].\n Moreover, such methods require significant effort in creating\n a dataset of scenes and objects that enables the learned\n policies to generalize. Acquiring a controller that can act in\n many scenes is also a more challenging learning problem,\n requiringlongerwallclocktime,morecompute,andadditional\n engineering effort to train a performant policy on a larger and\n more diverse training set. \n To probe the benefits of Rial To over such a sim-only train-\n ing pipeline, we compared the performance against a policy\n trained using only synthetic assets. Using an amount of time\n effort roughly comparable to what is required from a single\n user following our real-to-sim approach (see Section VI), we\n collected a set of 4 drawers from the Objaverse dataset (see\n Figure 7). Although this is small compared to the growing\n size of 3 D object datasets, we found it non-trivial to transfer\n articulated objects into simulation-ready USDs and we leave\n it as future work. Given these manually constructed diverse\n simulation scenes, we then trained a multi-task policy using\n Rial To from 20 demonstrations to open the 4 drawers. See\n Appendix IX-C for the minor modifications to incorporate\n multi-task policy learning to Rial To. \n As shown in Figure 7, when evaluating the real target\n drawer, the policy trained on multiple drawers only achieves\n a 10% success rate, much lower than the 90% obtained\n by the policy trained on the target drawer in simulation.\n This leads us to conclude that to train a generalist agent,\n considerably more data and effort are needed as compared\n to the relatively simple real-to-sim procedure we describe\n for test time specialization. Moreover, this suggests that for\n Objaverse Drawers Target performance on particular deployment environments, targeted\n generation of simulation environments via real-to-simulation\n pipelines may be more effective than indiscriminate, diverse\n procedural scene generation. \n 2) Real-to-simtransferofpolicies: Weadditionallywantto\n understand the impact of transferring policies from real-world\n Trained on target (Rial To) Trained on Objaverse demonstrations in comparison to running the pipeline starting\n withdemoscollecteddirectlyinsimulation.Thishelpsanalyze\n whether instead of collecting demos both in simulation and\n in the real world (for the co-training) we can simply collect\n demos in the real world and do all the training with those.\n Figure 6 shows the real-world performance of policies\n trained using Rial To when starting the RL fine-tuning step\n 17 81 90 10 \n usingreal-worlddemonstrationsasexplainedin III-C 1 against\n using demonstrations provided directly in simulation. We\n observe that the performance for both cases is very close.\n Fig.7. Comparisonbetweentrainingwith Rial Toonthereconstructionofthe These results show that Rial To successfully learns policies\n targetdraweragainsttrainingonasetoffourdrawersfrom Objaverse[17].We \n with demonstrations from either source of supervision as long\n observe, that Rial To on the real-to-sim asset does significantly better (90% \n vs 10%) when testing in the real world on the target drawer compared to aswekeepco-trainingthepolicieswithreal-worlddatainthe\n trainingonthesetofrandomizeddrawers. teacher-student distillation step. Firstly, this indicates that we"
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n Pose Randomization Distractors \n \n Rial Towithoutdistractortraining 60±15% 30±15% \n Rial Towithdistractortraining 100±0% 70±15% \n TABLEII \n REAL-WORLDPERFORMANCEOFPOLICIESTRAINEDWITHANDWITHOUT \n DISTRACTORSONTHETASKOFPLACINGAMUGONASHELF. \n do not need to collect both demos in sim and real, but we \n can run Rial To uniquely from the demos in the real world. \n Furthermore, this flexibility is a strength of our pipeline, as \n the ease of acquiring different sources of supervision may \n varyacrossdeploymentscenarios–i.e.,onecouldusepolicies \n pretrainedfromlarge-scalereal-worlddataorobtaindatafrom \n a simulation-based crowdsourcing platform. \n D. Scaling Rial To to In-the-Wild Environments \n \n Open toaster Cup in trash Dish in rack \n \n Cup in trash \n \n \n \n \n \n \n \n \n \n 100 \n \n \n 50 \n \n 90 30 90 30 50 0 \n etar \n sseccu S \n robust policy that succeeds even in visual clutter. We analyze\n how this affects the final robustness of the learned policy.\n For the sake of analysis, we consider the performance on the\n mug on the shelf task. The small size of the mug and its\n resemblance in shape and size to other daily objects make the\n visual component of this task particularly challenging when\n other objects are also present. Our findings in Table II show\n that adding distractors during training increases the success\n ratefrom 30%to 70%whentestingthepolicyinenvironments\n with distractors. We also observe a performance improvement\n insetupswithnodistractorssuggestingthatsuchtrainingalso\n supports better sim-to-real policy transfer.\n B. Comparison to RL from Scratch \n We hypothesize two key advantages of incorporating\n demonstrations in the finetuning process: (1) aiding explo-\n ration,and(2)biasingthepolicytowardbehaviorsthattransfer\n welltoreality.Resultsin Table IIIshowthattrainingfrom PPO\n from scratch fails (0% success) in three out of five tasks and\n much poorer performance in the other two tasks. On tasks\n with non-zero success, we observed that the policy exploits\n simulator inaccuracies and learns behaviors that are unlikely\n to transfer to reality. (see Appendix Fig. 15). For example,\n the PPO policy opens the toaster by pushing on the bottom of\n the toaster, leveraging the slight misplacement of the joint on\n the toaster. Such behaviors are unsafe and would not transfer\n to reality, underlining the importance of using demonstrations\n during policy robustification. \n C. RL from Vision \n Rial To’s“inversedistillation”proceduretoacompactstate-\n spaceaddssomemethodologicaloverheadtothesystemwhen\n compared to the possibility of doing RL fine-tuning directly\n on visual observations. However, as reported in Appendix\n Fig. 14, on the task of drawer opening, RL from compact\n states achieves a 96% success rate after 12 hours of wall-\n clock time, while RL from vision only achieves a 1% success\n Rial To Imitation learning \n rate after 35 hours. Hence, inverse distilling to state space is\n necessarybecausetraining RLfromvisionwithsparserewards\n Fig.8. Wetest Rial Toonuncontrolledandin-the-wildscenes,andweseewe is prohibitively slow, motivating the methodology outlined in\n cancontinuetosolveavarietyoftasksmorerobustlythanimitationlearning \n Section III-C 1. \n techniques. \n Inthissection,wescaleup Rial Totomoreuncontrolledand \n VI. USERSTUDY \n in-the-wild environments. We test Rial To on three different We analyzed the usability of Rial To’s pipeline for bringing\n tasks:openthemicrowaveinakitchen(alsoshownin Section real-world scenes to simulation. We ran a user study over\n IV-A), put a cup in the trash, and bring the plate from 6 people, User 6 being an expert who used the GUI before\n the sink to the dishrack. We observe that Rial To scales and Users 1-5 never did any work on simulators before. Each\n up to these more diverse scenes and continues to perform participantwastaskedwithcreatinganarticulatedsceneusing\n significantlybetterthanstandardimitationlearningtechniques. the provided GUI. More precisely, their task was to: 1) scan\n In particular, Rial To brings on average a 57% improvement a big scene, 2) cut one object, 3) scan and upload a smaller\n upon standard imitation learning, see Fig 8. object, and 4) add a joint to the scene. From Figure 9, we\n found that the average total time to create a scene was 25\n V. FURTHERANALYSISANDABLATIONS \n minutes and 12 seconds of which only 14 minutes and 40\n A. Training with Distractors \n seconds were active work. We also observed that the expert\n When performing teacher-student distillation we performed user accomplished the task faster than the rest, and twice as\n randomizationwithadditionalvisualdistractorstotrainamore fast as the slowest user. This indicates that with practice, our"
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n Open Bookon Plateon Mugon Open \n toaster shelf rack shelf drawer \n RLfromscratchwith 0 demos 62±2% 0±0% 2±0% 0±0% 0±0% \n RLfine-tuningfrom 15 realdemos 91±1% 90±1% 81±2% 81±2% 96±1% \n RLfine-tuningfrom 15 simdemos 96±1% 89±1% 82±2% 82±2% 95±1% \n TABLEIII \n COMPARISONOFTRAININGRLFROMSCRATCHAGAINSTRLFROMREALANDSIMDEMOS.RLFROMSIMANDREALDEMOSSEEMTOBEEQUIVALENT\n INMOSTCASES,BUTRLFROMSCRATCHBARELYSOLVESTHETASK. \n \n GUI allows users to become faster at generating scenes. We days of wall-clock time end-to-end to train a policy for each\n concludethatdoingthereal-to-simtransferofthescenesusing task, this time bottleneck makes continual learning infeasible\n the proposed GUI seems to be an intuitive process that is and understanding how to obtain policies faster with minimal\n neither time nor labor-intensive when compared to collecting human supervision would be valuable. We expect with more\n many demonstrations in the real world. We provide more efficient techniques for learning with point clouds and better\n details about the study in Appendix XIII. parallelization, this procedure can be sped up significantly.\n Conclusion: This work presents Rial To, a system for\n acquiring policies that are robust to environmental varia-\n User 1 19:46 \n tions and disturbances on real-world deployment. Our system\n User 2 16:30 achieves robustness through the complementary strengths of\n real-world imitation learning and large-scale RL on digital\n User 3 16:07 \n twin simulations constructed on the fly. Our results show that\n User 4 12:27 byimporting 3-Dreconstructionsofrealscenesintosimulation\n and collecting a small amount of demonstration data, non-\n User 5 12:17 \n expertuserscanprogrammanipulationcontrollersthatsucceed\n User 6 10:54 under challenging conditions with minimal human effort,\n showing enhanced levels of robustness and generalization.\n 0 5 10 15 20 25 30 35 40 \n Minutes \n Scan Processing \n Scan Time Joint Time & Uploading Time ACKNOWLEDGMENTS \n 2 nd Scan Time Cut Time 10:54 Total Active Time \n The authors would like to thank the Improbable AI Lab\n Fig.9. 3 Dreconstruction GUI’suserstudybreakdowntimes.Onaverageit andthe WEIRDLabmembersfortheirvaluablefeedbackand\n takes 14 minutesand 40 secondsofactivetimeor 25 minutesand 12 seconds supportindevelopingthisproject.Inparticular,wewouldlike\n oftotaltimetocreateascenethroughourproposedpipeline. \n to acknowledge Antonia Bronars and Jacob Berg for helpful\n suggestions on improving the clarity of the manuscript, and\n VII. LIMITATIONSANDCONCLUSION Marius Memmel for providing valuable insights on learning\n frompointcloudsintheearlystagesoftheproject.Thiswork\n Limitations: Whileouruseof 3 Dpointcloudsinsteadof \n was partly supported by the Sony Research Award, the US\n RGB enables easier sim-to-real transfer, we require accurate \n Government, and Hyundai Motor Company. \n depth sensors that can struggle to detect thin, transparent, \n and reflective objects. Future work may investigate applying \n Rial Tototrainpoliciesthatoperateon RGBimagesor RGBD, Author Contributions \n as our framework makes no fundamental assumptions that Marcel Torne conceived the overall project goals, investi-\n prevent using different sensor modalities. We are also limited gatedhowtoobtainreal-to-simtransferofscenesandpolicies,\n to training policies for tasks that can be easily simulated and and robustly do sim-to-real transfer of policies, wrote all the\n for real-world objects that can be turned into digital assets. code for the policy learning pipeline and Rial To’s GUI for\n Currently, this is primarily limited to articulated rigid bodies, real-to-sim transfer of scenes, ran simulation and real-world\n but advancements in simulating and representing deformables experiments, wrote the paper, and was the primary author of\n should allow our approach to be applied to more challeng- the paper. \n ing objects. Even though we show Rial To works on fast Anthony Simeonov helped with setting up the robot hard-\n controllers, these are still relatively slow to minimize the ware, made technical suggestions on learning policies from\n sim-to-real gap in dynamics, thereafter there is potential to point clouds, helped with the task of placing the plate on the\n investigate tasks for which faster controllers are needed. In rack, and actively helped with writing the paper.\n this work, we consider relatively quasistatic problems, where Zechu Li assisted in the early stage of conceiving the\n exact identification of physics parameters is not necessary project and helped develop Rial To’s GUI for the real-to-sim\n for the constructed simulation. This will become important transfer of the scenes.\n as more complex environments are encountered. Finally, as April Chan led the user study experiments to analyze\n we explain in Section XIV, Rial To currently takes around 2 Rial To’s GUI. "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n Tao Chenprovidedvaluableinsightsandrecommendations via simulation and generative modeling. In Towards\n on sim-to-real transfer. Generalist Robots: Learning Paradigms for Scalable\n Abhishek Gupta was involved in conceiving the goals Skill Acquisition@ Co RL 2023, 2023.\n of the project, assisted with finding the scope of the paper, [11] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for\n suggested baselines and ablations, played an active role in general in-hand object re-orientation. In Conference on\n writing the paper and co-advised the project. Robot Learning, pages 297–307. PMLR, 2022.\n Pulkit Agrawal suggested the idea of doing real-to-sim [12] Tao Chen,Megha Tippur,Siyang Wu,Vikash Kumar,Ed-\n transfer of scenes, was involved in conceiving the goals of ward Adelson, and Pulkit Agrawal. Visual dexterity: In-\n the project, suggested baselines and ablations, helped edit the hand reorientation of novel and complex object shapes.\n paper, and co-advised the project. Science Robotics, 8(84):eadc 9244, 2023.\n [13] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric\n REFERENCES \n Cousineau, Benjamin Burchfiel, and Shuran Song. Dif-\n [1] Open AI: Marcin Andrychowicz, Bowen Baker, Maciek fusion policy: Visuomotor policy learning via action\n Chociej, Rafal Jozefowicz, Bob Mc Grew, Jakub Pa- diffusion. ar Xiv preprint ar Xiv:2303.04137, 2023.\n chocki, Arthur Petron, Matthias Plappert, Glenn Powell, [14] Paul FChristiano,Jan Leike,Tom Brown,Miljan Martic,\n Alex Ray, et al. Learning dexterous in-hand manipula- Shane Legg, and Dario Amodei. Deep reinforcement\n tion. The International Journalof Robotics Research,39 learning from human preferences. Advances in neural\n (1):3–20, 2020. information processing systems, 30, 2017.\n [2] Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey [15] AR Code. Ar code. https://ar-code.com/, 2022.\n Levine. Efficient online reinforcement learning with [16] Matt Deitke, Rose Hendrix, Ali Farhadi, Kiana Ehsani,\n offline data. ar Xiv preprint ar Xiv:2302.02948, 2023. and Aniruddha Kembhavi. Phone 2 proc: Bringing robust\n [3] Max Balsells,Marcel Torne,Zihan Wang,Samedh Desai, robots into our chaotic world. In Proceedings of the\n Pulkit Agrawal, and Abhishek Gupta. Autonomous IEEE/CVF Conference on Computer Vision and Pattern\n roboticreinforcementlearningwithasynchronoushuman Recognition, pages 9665–9675, 2023.\n feedback. ar Xiv preprint ar Xiv:2310.20608, 2023. [17] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca\n [4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Weihs, Oscar Michel, Eli Vander Bilt, Ludwig Schmidt,\n and Sergey Levine. Training diffusion models with re- Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.\n inforcement learning. ar Xiv preprint ar Xiv:2305.13301, Objaverse: A universe of annotated 3 d objects. In\n 2023. Proceedings of the IEEE/CVF Conference on Computer\n [5] Jeannette Bohg, Karol Hausman, Bharath Sankaran, Vision and Pattern Recognition, pages 13142–13153,\n Oliver Brock, Danica Kragic, Stefan Schaal, and Gau- 2023. \n rav S Sukhatme. Interactive perception: Leveraging [18] Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chan-\n action in perception and perception in action. IEEE dramouli Rajagopalan, and Xiaolong Wang. Finetuning\n Transactions on Robotics, 33(6):1273–1291, 2017. offline world models in the real world. ar Xiv preprint\n [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- ar Xiv:2310.16029, 2023.\n gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana [19] Pete Florence, Corey Lynch, Andy Zeng, Oscar A\n Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong,\n Hsu, et al. Rt-1: Robotics transformer for real-world Johnny Lee, Igor Mordatch, and Jonathan Tompson.\n control at scale. ar Xiv preprint ar Xiv:2212.06817, 2022. Implicit behavioral cloning. In Conference on Robot\n [7] Arunkumar Byravan,Jan Humplik,Leonard Hasenclever, Learning, pages 158–168. PMLR, 2022.\n Arthur Brussee, Francesco Nori, Tuomas Haarnoja, Ben [20] Peter Florence, Lucas Manuelli, and Russ Tedrake. Self-\n Moran, Steven Bohez, Fereshteh Sadeghi, Bojan Vuja- supervised correspondence in visuomotor policy learn-\n tovic,etal. Nerf 2 real:Sim 2 realtransferofvision-guided ing. IEEE Robotics and Automation Letters, 5(2):492–\n bipedal motion skills using neural radiance fields. In 499, 2019. \n 2023 IEEE International Conference on Robotics and [21] Ran Gong, Jiangyong Huang, Yizhou Zhao, Haoran\n Automation (ICRA), pages 9362–9369. IEEE, 2023. Geng, Xiaofeng Gao, Qingyang Wu, Wensi Ai, Zi-\n [8] Matthew Chang, Theophile Gervet, Mukul Khanna, Sri- heng Zhou, Demetri Terzopoulos, Song-Chun Zhu, et al.\n ram Yenamandra,Dhruv Shah,So Yeon Min,Kavit Shah, Arnold: A benchmark for language-grounded task learn-\n Chris Paxton, Saurabh Gupta, Dhruv Batra, et al. Goat: ing with continuous states in realistic 3 d scenes. ar Xiv\n Gotoanything. ar Xivpreprintar Xiv:2311.06430,2023. preprint ar Xiv:2304.04321, 2023.\n [9] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp [22] Abhishek Gupta,Justin Yu,Tony ZZhao,Vikash Kumar,\n Kra¨henbu¨hl. Learning by cheating. In Conference on Aaron Rovinsky,Kelvin Xu,Thomas Devlin,and Sergey\n Robot Learning, pages 66–75. PMLR, 2020. Levine. Reset-free reinforcement learning via multi-\n [10] Qiuyu Chen, Marius Memmel, Alex Fang, Aaron Wals- tasklearning:Learningdexterousmanipulationbehaviors\n man, Dieter Fox, and Abhishek Gupta. Urdformer: without human intervention. In 2021 IEEE International\n Constructinginteractiverealisticscenesfromrealimages Conference on Robotics and Automation (ICRA), pages"
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n 6664–6671. IEEE, 2021. Laskey, and Ken Goldberg. Planar robot casting with\n [23] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, real 2 sim 2 realself-supervisedlearning,2022. URLhttps:\n Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys //arxiv.org/abs/2111.04814.\n Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, [36] Yixin Lin, Austin S. Wang, Giovanni Sutanto, Ak-\n Balakumar Sundaralingam, et al. Dextreme: Transfer shara Rai, and Franziska Meier. Polymetis. https:\n of agile in-hand manipulation from simulation to reality. //facebookresearch.github.io/fairo/polymetis/, 2021.\n In 2023 IEEE International Conference on Robotics and [37] Naijun Liu, Yinghao Cai, Tao Lu, Rui Wang, and Shuo\n Automation (ICRA), pages 5977–5984. IEEE, 2023. Wang. Real–sim–real transfer for real-world robot con-\n [24] Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi trol policy learning with deep reinforcement learning.\n Khansari, and Yunfei Bai. Retinagan: An object-aware Applied Sciences, 10(5):1555, 2020.\n approach to sim-to-real transfer. In 2021 IEEE Interna- [38] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens,\n tional Conference on Robotics and Automation (ICRA), Shuran Song, Aravind Rajeswaran, and Vikash Ku-\n pages 10920–10926. IEEE, 2021. mar. Cacti: A framework for scalable multi-task\n [25] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, multi-scene visual imitation learning. ar Xiv preprint\n and Chrisina Jayne. Imitation learning: A survey of ar Xiv:2212.05711, 2022.\n learning methods. ACM Computing Surveys (CSUR), 50 [39] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\n (2):1–35, 2017. Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,\n [26] Jemin Hwangbo,Joonho Lee,Alexey Dosovitskiy,Dario Silvio Savarese, Yuke Zhu, and Roberto Mart´ın-Mart´ın.\n Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco What matters in learning from offline human demon-\n Hutter. Learning agile and dynamic motor skills for strations for robot manipulation. ar Xiv preprint\n legged robots. Science Robotics, 4(26):eaau 5872, 2019. ar Xiv:2108.03298, 2021.\n [27] Stephen James and Andrew J Davison. Q-attention: [40] Gabriel BMargolis,Ge Yang,Kartik Paigwar,Tao Chen,\n Enabling efficient learning for vision-based robotic ma- and Pulkit Agrawal. Rapidlocomotionviareinforcement\n nipulation. IEEE Robotics and Automation Letters, 7(2): learning. ar Xiv preprint ar Xiv:2205.02824, 2022.\n 1612–1619, 2022. [41] Marius Memmel, Andrew Wagenmaker, Chuning Zhu,\n [28] Stephen James, Kentaro Wada, Tristan Laidlow, and Patrick Yin, Dieter Fox, and Abhishek Gupta. Asid:\n Andrew J Davison. Coarse-to-fine q-attention: Efficient Active exploration for system identification in robotic\n learning for visual robotic manipulation via discretisa- manipulation. ar Xiv preprint ar Xiv:2404.12308, 2024.\n tion. In Proceedings of the IEEE/CVF Conference on [42] Russell Mendonca, Shikhar Bahl, and Deepak Pathak.\n Computer Visionand Pattern Recognition,pages 13739– Structured world models from human videos. ar Xiv\n 13748, 2022. preprint ar Xiv:2308.10901, 2023. \n [29] Zhenyu Jiang, Cheng-Chun Hsu, and Yuke Zhu. Ditto: [43] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\n Building digital twins of articulated objects from inter- Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\n action. In Proceedings of the IEEE/CVF Conference on Nerf: Representing scenes as neural radiance fields for\n Computer Vision and Pattern Recognition, pages 5616– view synthesis. Communications of the ACM, 65(1):99–\n 5626, 2022. 106, 2021. \n [30] Michael Kazhdan,Matthew Bolitho,and Hugues Hoppe. [44] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu,\n Poisson surface reconstruction. In Proceedings of the Nikita Rudin,David Hoeller,Jia Lin Yuan,Ritvik Singh,\n fourth Eurographicssymposiumon Geometryprocessing, Yunrong Guo, Hammad Mazhar, et al. Orbit: A unified\n volume 7, 2006. simulationframeworkforinteractiverobotlearningenvi-\n [31] Jens Kober,JAndrew Bagnell,and Jan Peters.Reinforce- ronments. IEEE Robotics and Automation Letters, 2023.\n ment learning in robotics: A survey. The International [45] Thomas Mu¨ller, Alex Evans, Christoph Schied, and\n Journal of Robotics Research, 32(11):1238–1274, 2013. Alexander Keller. Instant neural graphics primitives\n [32] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra with a multiresolution hash encoding. ACM Trans.\n Malik. Rma: Rapid motor adaptation for legged robots. Graph., 41(4):102:1–102:15, July 2022. doi: 10.\n ar Xiv preprint ar Xiv:2107.04034, 2021. 1145/3528223.3530127. URL https://doi.org/10.1145/\n [33] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey 3528223.3530127. \n Levine. Conservativeq-learningforofflinereinforcement [46] Ashvin Nair, Bob Mc Grew, Marcin Andrychowicz, Wo-\n learning. Advances in Neural Information Processing jciech Zaremba, and Pieter Abbeel. Overcoming ex-\n Systems, 33:1179–1191, 2020. ploration in reinforcement learning with demonstrations.\n [34] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, In 2018 IEEE international conference on robotics and\n Vladlen Koltun,and Marco Hutter.Learningquadrupedal automation (ICRA), pages 6292–6299. IEEE, 2018.\n locomotion over challenging terrain. Science robotics, 5 [47] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and\n (47):eabc 5986, 2020. Sergey Levine. Awac: Accelerating online reinforce-\n [35] Vincent Lim, Huang Huang, Lawrence Yunliang Chen, ment learning with offline datasets. ar Xiv preprint\n Jonathan Wang,Jeffrey Ichnowski,Daniel Seita,Michael ar Xiv:2006.09359, 2020. "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n [48] NVIDIA. Nvidia isaac-sim. Machine Learning, pages 31077–31093. PMLR, 2023.\n https://developer.nvidia.com/isaac-sim, May 2022. [61] Yunlong Song, Angel Romero, Matthias Mu¨ller, Vladlen\n [49] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Koltun, and Davide Scaramuzza. Reaching the limit\n Carroll Wainwright, Pamela Mishkin, Chong Zhang, in autonomous racing: Optimal control versus reinforce-\n Sandhini Agarwal, Katarina Slama, Alex Ray, et al. ment learning. Science Robotics, 8(82):eadg 1462, 2023.\n Training language models to follow instructions with [62] Priya Sundaresan, Rika Antonova, and Jeannette Bohgl.\n human feedback. Advances in Neural Information Pro- Diffcloud: Real-to-sim from point clouds with differen-\n cessing Systems, 35:27730–27744, 2022. tiablesimulationandrenderingofdeformableobjects. In\n [50] Songyou Peng, Michael Niemeyer, Lars Mescheder, 2022 IEEE/RSJ International Conference on Intelligent\n Marc Pollefeys, and Andreas Geiger. Convolutional Robots and Systems (IROS), pages 10828–10835. IEEE,\n occupancy networks. In Computer Vision–ECCV 2020: 2022. \n 16 th European Conference,Glasgow,UK,August 23–28, [63] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen,\n 2020,Proceedings,Part III 16,pages 523–540.Springer, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent\n 2020. Vanhoucke. Sim-to-real: Learning agile locomotion for\n [51] Xue Bin Peng, Marcin Andrychowicz, Wojciech quadruped robots. ar Xiv preprint ar Xiv:1804.10332,\n Zaremba, and Pieter Abbeel. Sim-to-real transfer of 2018. \n robotic control with dynamics randomization. In 2018 [64] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\n IEEE international conference on robotics and automa- Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake\n tion (ICRA), pages 3803–3810. IEEE, 2018. Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfs-\n [52] Polycam. Polycam. https://poly.cam, 2020. tudio: A modular framework for neural radiance field\n [53] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi development. In ACM SIGGRAPH 2023 Conference\n Kanervisto, Maximilian Ernestus, and Noah Dormann. Proceedings, pages 1–12, 2023.\n Stable-baselines 3: Reliable reinforcement learning im- [65] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider,\n plementations. Journal of Machine Learning Research, Wojciech Zaremba, and Pieter Abbeel. Domain ran-\n 22(268):1–8, 2021. URL http://jmlr.org/papers/v 22/ domization for transferring deep neural networks from\n 20-1364.html. simulation to the real world. In 2017 IEEE/RSJ in-\n [54] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, ternational conference on intelligent robots and systems\n Giulia Vezzani, John Schulman, Emanuel Todorov, and (IROS), pages 23–30. IEEE, 2017.\n Sergey Levine. Learning complex dexterous manipula- [66] Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shrid-\n tion with deep reinforcement learning and demonstra- har,Chen Bao,Yuzhe Qin,Bailin Wang,Huazhe Xu,and\n tions. ar Xiv preprint ar Xiv:1709.10087, 2017. Xiaolong Wang. Gensim: Generating robotic simulation\n [55] Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, tasks via large language models. In The Twelfth Interna-\n Julian Ibarz,and Mohi Khansari.Rl-cyclegan:Reinforce- tional Conference on Learning Representations, 2023.\n ment learning aware simulation-to-real. In Proceedings [67] Lirui Wang,Jialiang Zhao,Yilun Du,Edward HAdelson,\n of the IEEE/CVF Conference on Computer Vision and and Russ Tedrake. Poco: Policy composition from\n Pattern Recognition, pages 11157–11166, 2020. and for heterogeneous robot learning. ar Xiv preprint\n [56] Nathan Ratliff, J Andrew Bagnell, and Siddhartha S ar Xiv:2402.02511, 2024.\n Srinivasa. Imitation learning for locomotion and manip- [68] Luobin Wang,Runlin Guo,Quan Vuong,Yuzhe Qin,Hao\n ulation. In 20077 th IEEE-RASInternational Conference Su, and Henrik Christensen. A real 2 sim 2 real method for\n on Humanoid Robots, pages 392–397. IEEE, 2007. robustobjectgraspingwithneuralsurfacereconstruction.\n [57] Ste´phane Ross, Geoffrey Gordon, and Drew Bagnell. A In 2023 IEEE 19 th International Conferenceon Automa-\n reduction of imitation learning and structured prediction tion Science and Engineering (CASE), pages 1–8. IEEE,\n to no-regret online learning. In Proceedings of the 2023. \n fourteenth international conference on artificial intelli- [69] Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B Tenen-\n gence and statistics, pages 627–635. JMLR Workshop baum, and Shuran Song. Densephysnet: Learning dense\n and Conference Proceedings, 2011. physical object representations via multi-step dynamic\n [58] Stefan Schaal, Auke Ijspeert, and Aude Billard. Compu- interactions. ar Xiv preprint ar Xiv:1906.03853, 2019.\n tationalapproachestomotorlearningbyimitation.Philo- [70] Jingyun Yang, Max Sobol Mark, Brandon Vu, Archit\n sophical Transactions of the Royal Society of London. Sharma, Jeannette Bohg, and Chelsea Finn. Robot fine-\n Series B:Biological Sciences,358(1431):537–547,2003. tuning made easy: Pre-training rewards and policies for\n [59] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec autonomous real-world reinforcement learning. ar Xiv\n Radford,and Oleg Klimov. Proximalpolicyoptimization preprint ar Xiv:2310.15145, 2023.\n algorithms. ar Xiv preprint ar Xiv:1707.06347, 2017. [71] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson,\n [60] Idan Shenfeld,Zhang-Wei Hong,Aviv Tamar,and Pulkit Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan,\n Agrawal. Tgrl: An algorithm for teacher guided re- Jodilyn Peralta,Brian Ichter,etal. Scalingrobotlearning\n inforcement learning. In International Conference on with semantically imagined experience. ar Xiv preprint"
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n ar Xiv:2302.11550, 2023. \n [72] Tony ZZhao,Vikash Kumar,Sergey Levine,and Chelsea \n Finn. Learning fine-grained bimanual manipulation with \n low-cost hardware. ar Xiv preprint ar Xiv:2304.13705, \n 2023. \n [73] Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, \n and Chelsea Finn. Nerf in the palm of your hand: \n Correctiveaugmentationforroboticsvianovel-viewsyn- \n thesis. In Proceedings of the IEEE/CVF Conference on \n Computer Visionand Pattern Recognition,pages 17907– \n 17917, 2023. \n [74] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, \n Sergey Levine, and Vikash Kumar. Dexterous ma- \n nipulation with deep reinforcement learning: Efficient, \n general, and low-cost. In 2019 International Conference \n on Robotics and Automation (ICRA), pages 3651–3657. \n IEEE, 2019. \n [75] Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, \n Kristian Hartikainen, Avi Singh, Vikash Kumar, and \n Sergey Levine. The ingredients of real-world robotic re- \n inforcement learning. ar Xiv preprint ar Xiv:2004.12570, \n 2020. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n Next, we provide additional details of our work. More • Cupintrash:success=||cup site−trash site|| 2 <0.07\n concretely: && condition(gripper open) \n \n • Task Details VIII: provides more details on the tasks A. Simulation details\n used to evaluate Rial To and the baselines. \n For simulating each one of the tasks, we use the latest sim-\n • Implementation Details IX: provides more detailed in- \n ulator from NVIDIA, Isaac Sim [48]. Furthermore, to develop\n formation on the exact hyperparameters such as network \n our code we were inspired by the Orbit codebase [44], one of\n architectures, point cloud processing, and dataset sizes \n the first publicly available codebases that run Reinforcement\n using in Rial To. \n Learning and Robot Learning algorithms on Isaac Sim.\n • Further Analysis XI: we provide further details on \n Regarding the simulation parameters of the environments,\n Rial To, more concretely on running RL from vision, RL \n as mentioned in the text, we set default values in our GUI\n from scratch and on the sim-to-real gap. \n and these are the same that are used across the environments.\n • Hardware Setup X: Details on the robot hardware and \n In more detail, we use convex decomposition with 64 hull\n cameras used for the experiments. \n vertices and 32 convex hulls as the collision mesh for all\n • GUI for Real-to-Sim Transfer of Scenes XII: We \n objects. These values could vary in some environments, but\n provide further details on the GUI that we proposed \n wehavefoundtheyareingeneralagooddefaultvalue.There\n together with advice on which scanning methods to use \n is one exception, the dish on the rack task, where the rack\n for each scenario. \n needs to be simulated very precisely, in that case, we used\n • GUI User Study XIII: We explain how we ran the User \n SDF mesh decomposition with 256 resolution which returns\n Study together with visualizations of the scanned scenes. \n high-fidelity collision meshes. Note that all these options can\n • Compute Resources XIV:Wegivedetailsonthecompute \n be changed from our GUI. Regarding the physics parameters,\n used to run the experiments. \n we set the dynamic and static frictions of the objects to be\n VIII. TASKDETAILS 0.5, the joint frictions to be 0.1, and the mass of the objects\n to be 0.41 kg. Note thatin many ofthe tasks, wealso leverage\n In this section of the appendix, we describe additional \n setting fixed joints on the objects, to make sure these won’t\n details about each task. Across tasks, the state space consists \n move, for example, on the shelf or kitchen.\n of a concatenation of all of the poses of the objects present in \n the scenes together with the states of the joints and the state IX. IMPLEMENTATIONDETAILS\n of the robot. The action space consists of a discretized end- A. Network architectures\n effectordeltaposeofdimension 14.Moreconcretely,wehave \n 1) State-based policy: As described in Section III-C 2, we\n 6 actions for the delta position, which moves ±0.03 meters \n fine-tune a state-based policy with privileged information in\n in each axis, 6 more actions for rotating ±0.2 radians in each \n the simulator. This policy is a simple Multi-Layer Perceptron\n axis, and 2 final actions for opening and closing the gripper. \n (MLP)withtwolayersofsize 256 each.Thistakesasinputthe\n Asweexplainin Section III-B,wedefineasuccessfunction \n privileged state from the simulator and outputs a Categorical\n that will be used for selecting successful trajectories in the \n distribution of size 14 encoding the probabilities for sampling\n inverse distillation procedure and as a sparse reward in the \n each discrete end-effector action. For our PPO with BC loss\n RL fine-tuning phase. Next, we specify which are the success \n implementation, we build on top of the Stable Baselines 3\n functions for each of the tasks: \n repository [53]. The network for the value function shares the\n • Kitchen Toaster: success= first layer with the actor. See Table VI for more details.\n toaster joint>0.65 && condition(gripper open) 2) Point cloud policy: For both the inverse distillation pro-\n • Open Drawer: success= cedure(Section III-C 1)andthelastteacher-studentdistillation\n drawer joint>0.1 && condition(gripper open) steps (Section III-D) we train a policy that takes as input\n • Open Cabinet: success= the point cloud observation together with the state of the\n cabinet joint>0.1 && condition(gripper open) robot (end-effector pose and state) and outputs a Categorical\n • Plate on the rack: success= distribution of size 14 encoding the probabilities for each\n ||plate site − rack site|| < 0.2 && rack y axis · action. The network architecture consists of an encoder of the\n 2 \n plate z axis>0.9 && condition(gripper open) pointcloudsthatmapstoanembeddingofsize 128.Thenthis\n • Book on shelf: success= embeddingisconcatenatedtothestateoftherobot(size 9)and\n ||book site−shelf site|| <0.12 ispassedthroughan MLPofsize 256,256.Regardingthepoint\n 2 \n && condition(gripper open) cloud encoder, we use the same volumetric 3 D point cloud\n • Mug on shelf: success= encoderproposedin Convolutional Occupancy Networks[50],\n ||mug site − shelf site|| < 0.12 && mug z axis · consisting of a local point net followed by a 3 D U-Net which\n 2 \n shelf z axis>0.95 && condition(gripper open) outputsadensevoxelgridoffeatures.Thesefeaturesarethen\n • Plate on the rack in the kitchen: success= pooled with both a max pooling layer and an average pooling\n ||plate site − rack site|| < 0.2 && rack y axis · layer and the resulting two vectors are concatenated to obtain\n 2 \n plate z axis>0.9 && condition(gripper open) the final point cloud encoding of size 128."
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n \n \n \n \n \n move the mug \n move the robot around back down close back the cabinet \n perturbe the book \n pose after pick \n \n \n \n \n \n move the mug \n close back the drawer back down \n Fig.10. Overviewofthedisturbancesthat Rial Toisrobusttointhedifferenttasksthatweevaluatediton.\n \n Task USDName Episode Randomized Position Position Orientation Orientation\n length \n Parameters Object Ids Min(x,y,z) Max(x,y,z) Min(z-axis) Max(z-axis)\n Kitchentoaster kitchentoaster 3.usd 130 [267] [0.3,-0.2,- [0.7,0.1,0.2] [-0.1] [0.1]\n 0.2] \n Plateonrack dishinrackv 3.usd 150 [278, [-0.4,- [0,0.25,0] [-0.52,0] [0.52,0]\n [270,287]] 0.035,0] \n Mugonshelf mugandshelf 2.usd 150 [267,263] [[-0.3,0,0], [[0.25,0.3,0.07], [-0.52,-0.54] [0.52,0.54]\n [-0.1,0.25,0]] [0.4,0.4,0]] \n Bookonshelf booknshelve.usd 130 [277, [[-0.25,- [[0.15,0.28,0], [-0.52,0] [0.52,0]\n [268,272]] 0.12,0], [0.15,0.15,0]] \n [-0.15,- \n 0.05,0]] \n Opencabinet cabinet.usd 90 [268] [-0.5,- [0,0.3,-0.1] [-0.52] [0.52]\n 0.1,0.1] \n Opendrawer drawerbiggerhandle.usd 80 [268] [-0.26,-0.07,- [0.16,0.27,0] -0.5 0.5\n 0.05] \n Cupintrash cupntrash.usd 90 [263,266] [[[-0.2,-0.3, [[[0.2,0.1, [0,0] [0,0]\n -0.2],[-0.2,- 0.2], \n 0.12,0]]] [0.2,0.2,0]]] \n Plateonrackfromkitchen dishsinklab.usd 110 [[263,278, [[[-0.25,-0.1, [[[0.1,0.2, [0,-0.3,0] [0,0.3,0]\n 270]] -0.1], 0.1], \n [-0.1,0.05,0], [0.1,0.15,0], \n [-0.2,0,0]]] [0,0,0]]] \n TABLEIV \n SPECIFICPARAMETERSFOREACHONEOFTHETASKS. \n B. Teacher-student distillation mix 15000 trajectories rendering full point clouds (where all\n faces of the objects are visible, which is obtained through\n Given the state-based policy π (a|s) learned in the sim- \n sim directly sampling points from the mesh, as proposed in [12]),\n ulator, we wish to distill it into a policy π∗ (a|o) that takes \n sim 5000 trajectories rendered from a camera viewpoint that is\n the point cloud observation and outputs the action. We take \n approximately the same position as the camera in the real\n thestandardteacher-studentdistillationapproach[32,12].The \n world, a set of 2000 trajectories also generated from the same\n first step consists of doing imitation learning on a set of \n camera viewpoint in sim but adding distractor objects (see\n trajectories given by the expert policy π (a|s) rollout. This \n sim Figure 12), finally, we mix the 15 real-world trajectories. The\n set of trajectories needs to be carefully designed to build an \n four different splits in the dataset are sampled equally, with\n implicit curriculum so that we can learn the student policy \n 1/4 probability each. \n successfully. When designing this dataset of trajectories, we "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n Task Position(x,y,z) Rotation(quat) Crop Min Crop Max Size \n Parameters Camera Camera Camera Camera Image \n Kitchentoaster [0.0,-0.37,0.68] [0.82,0.34,-0.20,-0.41] [-0.8,-0.8,-0.8] [0.8,0.8,0.8] (640,480)\n Plateonrack [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480)\n Mugonshelf [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480)\n Bookonshelf [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480)\n Opencabinet [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480)\n Opendrawer [0.95,-0.4,0.68] [0.78,0.36,0.21,0.46] [-0.3,-0.6,0.02] [0.9,0.6,1] (640,480)\n Cupintrash [0.0,-0.37,0.68] [0.82,0.34,-0.20,-0.41] [-1,-1,-1] [1,1,1] (640,480)\n Plateonrackfromkitchen [0.0,-0.37,0.68] [0.82,0.34,-0.20,-0.41] [-0.8,-0.8,-0.8] [0.8,0.8,0.8] (640,480)\n TABLEV \n CAMERAPARAMETERSFOREACHTASK. \n MLPlayers PPOn steps PPObatchsize PPOBCbatchsize PPOBCweight Gradient Clipping\n \n 256,256 episodelength 31257 32 0.1 5 \n TABLEVI \n STATE-BASEDPOLICYTRAININGPARAMETERS.THERESTOFTHEPARAMETERSARETHEDEFAULTASDESCRIBEDINSTABLEBASELINES 3[53].\n \n \n Simulated Scenes \n \n \n \n \n Mug on shelf Kitchen - toaster Dish track \n \n Fig.12. Distractorobjectsusedtogetarobustpolicytovisualdistractors\n intheteacher-studentdistillationstep III-D.\n \n D. Imitation Learning Baseline \n Cabinet Book on shelf Drawer \n For the imitation learning baseline, we collect 15 (unless\n otherwise specified) real-world demonstrations using a key-\n Fig. 11. Overview of the scenes generated using our GUI and used for \n evaluating Rial To. board interface. We preprocess the point clouds in the same\n manner as for the teacher-student distillation training (see\n Section IX-B. We complete the point cloud sampling points\n Afterthisfirstdistillationstep,weperformastepof DAgger from the arm mesh leveraging the joints from the real robot.\n [57], where we roll out the policy π∗ (a|o) and relabel the We also add the same randomization: jitter, dropout, and\n sim \n actions with π (a|s). In this second and last step, we mix translation. \n sim \n the DAggerdatasetwiththetrajectorieswithdistractorsinsim 1) Imitation learning with new assets: We implemented an\n and the real-world trajectories and sample trajectories. Again additionalbaselinewhereweaddedpointcloudssampledfrom\n each dataset is sampled equally with 1/3 probability each. different object meshes (see Figure 12) into the real-world\n Finally,thedetailsforgeneratingandrandomizingthepoint point cloud to make the policy more robust to distractors.\n clouds are available in Table VII and were largely inspired However, no improvement in the robustness of this baseline\n by [12]. The parameters for training the point cloud-based was found as seen in Figure IX. We hypothesize that this is\n network are available in VIII. the case because the added meshes into the point cloud do\n not bring any occlusions which is one of the main challenges\n when adding distractors in point clouds. \n C. Simulated Assets Baseline Details \n To implement the baseline with multiple simulation assets \n X. HARDWARESETUP \n we had to incorporate two modifications for enabling the Our experiments are run on two different Panda Franka\n multi-task policy learning: 1) at each episode we select a arms. One is, the Panda Franka arm 2, which is mounted\n drawer randomly from the set of drawers 2) we expand the on a fixed table, we run the book on the shelf, mug on the\n observationspaceofthestate-basedpolicytoincludetheindex shelf, dish on the rack, open the cabinet, and open the drawer\n of the drawer selected to open. there. Then we also ran part of our experiments, on a Panda"
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n Totalpcd Sample Arm Dropout Jitter Jitter Sample Object Pcd Pcd Grid \n points Points(#) ratio ratio noise Meshes Points Normalization Scale Size \n 6000 3000 [0.1,0.3] 0.3 N(0,0.01) 1000 [0,0,0] 0.625(toaster) 32 x 32 x 32\n (toaster) 1(others) \n [0.35,0,0.4] \n (others) \n TABLEVII \n POINTCLOUDGENERATIONANDRANDOMIZATIONPARAMETERS. \n \n MLPlayers lr Optimizer Batch Size Nbfullpcdtraj Nbsimulatedpcd Nbsimulatedpcd Nbrealtraj\n traj traj(distractors) \n 256,256 0.0003 Adam W 32-64 15000 5000 1000 15 \n TABLEVIII \n POINTCLOUDTEACHER-STUDENTDISTILLATIONPARAMETERS. \n \n \n Pose Distractors Disturbances used for compact state policies. Rendering point clouds in\n randomization \n simulation is also approximately 10 x slower than running the\n IL 40±15% 50±17% 10±9% pipeline without any rendering. When adding these factors\n ILwithdistractors 50±17% 20±13% 10±9% up, RL from vision becomes much slower and practically\n TABLEIX infeasible given our setup with sparse rewards.\n COMPARISONOFTHEPLAINIMITATIONLEARNINGBASELINE(IL) \n AGAINSTADDINGNEWDISTRACTORS(ILWITHDISTRACTORS)ONTHE \n TASKOFOPENINGTHEDRAWER.NOIMPROVEMENTISOBSERVED. 1 \n R D e 4 a 3 l 5 sense Franka Research 3 R D e 4 a 5 l 5 sense 0.8 \n 0.6 \n Franka Emika Panda \n 0.4 \n 0.2 \n 0 \n Mobile table Fixed table 0 5 10 15 20 25 30 35 \n Wall Time (hours) \n Fig. 13. Overview of the hardware setup used for evaluating Rial To. left: \n usedforthekitchentoastertask,right:usedforthebookontheshelf,mug \n ontheshelf,dishontherack,opencabinet,andopendrawertasks. \n Frankaarm 3,mountedonamobiletable,moreconcretely,the \n open toaster in the kitchen was the task run on this arm. The \n communication between the higher and lower level controller \n of the arm is done through Polymetis [36]. \n We mount one calibrated camera per setup to extract the \n depth maps that will be passed to our vision policies. More \n concretely we use the Intel depth Realsense camera D 455 on \n the first setup and the Intel depth Realsense camera D 435 on \n the second setup. See Figure 13 for more details on the robot \n setup. \n XI. FURTHERANALYSIS \n A. RL from vision \n Part of the inefficiency of running RL from vision comes \n from the increased memory required to compute the policy \n loss for vision-based RL – on the same GPU, the batch size \n for vision-based policies is 100 x smaller than the batch size \n oita R \n sseccu S \n 96% \n RL from states \n RL from vision \n 1% \n 12 \n Fig. 14. Wall clock time comparison of running PPO from vision against\n fromcompactstates. \n B. RL from Scratch \n In Figure 15, we qualitatively observe the phenomena that\n we mention in III-C 2, where the policy trained from scratch,\n without demos, exploits the model’s inaccuracies. In this\n specific case, we observe that the policy leverages the slightly\n incorrectlyplacedjointtoopenthemicrowaveinanunnatural\n way that wouldn’t transfer to the real world.\n C. RL from different amounts of real-world data\n In this section, we analyze further how many real-world\n demonstrations are needed to successfully fine-tune policies\n with RL in simulation. We start with 0,5,10,15 real-world\n demonstrations and inverse-distill the policy by collecting 15\n simtrajectoriesfromthisreal-worldtrainedpolicy.Weobserve\n intable Xthatforthetaskofplacingabookontheshelf,there\n is a step function where the PPO has a 0% success rate until\n 15 demosareused.Thereasonisthatwithlessthan 15 demos"
  },
  {
    "page_num": 20,
    "text": " \n \n \n \n Open Mug \n drawer onshelf \n Imitationlearning 40±17% 10±9% \n Rial To 90±9% 100±0% \n Rial Tomultitask 90±9% 80±15% \n TABLEXII \n time COMPARISONOFTRAININGRIALTOONMULTIPLETASKSAGAINST\n SINGLE-TASKRIALTO.NOIMPROVEMENTISOBSERVED.\n Fig.15. Visualizationofarolloutofthefinalpolicylearnedwith RLwithout \n demosandachievinga 62%accuracyonopeningthetoasterinsimulation.We \n observe the resulting policy that learns without demos exploits the model’s 1) Train separate state-based single-task policies per task\n inaccuracies,thereafteritwillnottransfertotherealworld. \n 2) Collect trajectories from each one of the tasks with the\n state-based policies \n Bookon Open \n 3) Distill these trajectories into a single multi-task policy\n shelf drawer \n conditioned with the task-id \n RLfine-tuningfrom 0 realdemos 0±0% 0±0% \n 4) Run multiple iterations of DAgger on each task sequen-\n RLfine-tuningfrom 5 realdemos 0±0% 89±1% \n RLfine-tuningfrom 10 realdemos 0±0% 96±1% tially to obtain a final multi-task policy\n RLfine-tuningfrom 15 realdemos 90±2% 96±1% We evaluate this policy in the real world on two of the\n TABLEX tasksandobservein Table XIIthatinopeningthedrawer,the\n COMPARISONOFTRAININGRLFROMDIFFERENTAMOUNTSOF performance of multi-task Rial To matches single-task (90%\n REAL-WORLDDEMOS. \n success). However, the performance slightly decreases on the\n mug on the shelf task (from 100% on single-task to 80% on\n thereal-worldpolicydoesnottransfertothesimulationhence multi-task). Nevertheless, the performance is still above the\n no sim demos can be collected during the inverse distillation imitation learning baseline (40% for the drawer and 10% for\n procedure. Thereafter the RL fine-tuned policy starts from the mug on the shelf). We did not tune any hyperparameters,\n scratchwhenusing<15 real-worlddemos.Ontheotherside, andwekeptthesamenetworksizethatweusedforthe Rial To\n for the easier task of opening a drawer, we observe this step experiments. We should be able to bring the performance of\n functionearlier,whereat>5 demoswecando RLfine-tuning the mug on the shelf task to match the single-task policy with\n from demos and obtain successful policies. some hyperparameter tuning. \n We showed that Rial To can be easily adapted to train multi-\n D. Mixing Rial To with synthetic data \n task policies. We hypothesize that we need to train in more\n Werun Rial Tocombiningthedatafromthesyntheticassets environments to obtain multi-task generalization.\n experiment (see Figure 7) together with the simulated target \n F. Sim-to-real gap \n environment data and study whether we get any performance \n gain by combining these two sources of data on the task of We analyze and propose an explanation for the observed\n opening the drawer. We observe in Table XI that there is no sim-to-realgapin Table XIII,whereweshowtheperformance\n clear improvement when combining the simulated assets with of the final point cloud-based policy in both simulation and\n the target asset. One reason could be that more synthetic data therealworld.Weobservethatingeneral,thesim-to-realgap\n is needed to observe an increase in performance. The other doesnotseemtobepresent.Insomecasessuchasforthemug\n hypothesis is that learning only on the target environment onshelftask,weobservethattheperformanceinsimulationis\n (Rial To) is enough and the 10% left to reach 100% success worsethantheperformanceintherealworld.Themainreason\n rate in the real world comes from the sim-to-real gap. forthisdisparityisthatwewanttomakethesimulationharder\n than the real-world environment to make sure that we will be\n E. Rial To Multi-Task able to recover a good robust policy in the real world.\n We propose a multi-task version of Rial To. We train multi- \n XII. GUIFORREAL-TO-SIMTRANSFEROFSCENES \n task Rial Toonthetasksofopeningadrawer,puttingamugon \n the shelf, cup in the trash, and dish on the rack environments. In the main text and video, we provide an overview of\n Theproposedmulti-task Rial Toprocedureisthefollowing: the features and capabilities of our GUI. Additional valuable\n features include the ability to populate the scene with assets\n from object datasets such as Objaverse [17]. This allows for\n Pose Distractors randomizingsurroundingclutterandsupportingpolicytraining\n randomization that generalizes to distractor objects (see Section V-A).\n Rial To 90±9% 90±9% 1) 3 D reconstruction software used: We mainly used 3\n Rial To+syntheticassets 90±9% 80±13% different methods/apps for obtaining the 3 D meshes from\n videos: \n TABLEXI \n COMPARISONOFUSINGRIALTOWITHADDEDSYNTHETICASSETS 1) Polycam [52] is used to scan larger scenes, such as\n AGAINSTSTANDARDRIALTOONTHETASKOFOPENINGTHEDRAWERIN \n the kitchen. Polycam makes effective use of the built-in\n THEREALWORLD.NOIMPROVEMENTISOBSERVED. "
  },
  {
    "page_num": 21,
    "text": " \n \n \n \n Kitchen Bookon Plateon Mugon Open Open \n toaster shelf rack shelf drawer cabinet \n Performanceinsimulation 90±4% 84±5% 80±6% 72±6% 95±3% 92±4% \n Performanceintherealworld 90±9% 90±9% 90±9% 100±0% 90±9% 85±8% \n TABLEXIII \n COMPARISONOFPERFORMANCEINSIMULATION(TOP)ANDTHEREALWORLD(BOTTOM).\n \n \n i Phone depth sensor which helps extract realistic surface User Study Scanned Scenes\n geometry for large uniform flat surface (e.g., a kitchen \n counter). However, we find it struggles with fine-grained \n details. Polycam outputs a GLTF file, which we convert \n directly to a USD for loading into Isaac Sim using an \n online conversion tool. \n 2) AR Code [15] is used to extract high-quality meshes for \n single objects that can be viewed by images covering \n thefull 360 degreessurroundingtheobject(e.g.,cabinet, \n mug,microwave,drawer).While ARCodeleadstomore \n Phone cabin Printer room Table desk \n accurate geometry than Polycam for singulated objects, Bathroom 1 Kitchen Bathroom 2\n we still find it struggles on objects with very thin parts. \n AR Code directly outputs a USD file that can be loaded \n into Isaac Sim. \n 3) Ne RFStudio [64] is used to capture objects that re- \n quire significantly more detail to represent the geometry \n faithfully. For example, AR Code failed to capture the \n thin metal structures on the dish rack, whereas Ne RFs \n are capable of representing these challenging geometric \n parts. We use the default “nerfacto” model and training \n parameters. This method trains a relatively small model Added Joint Added Object Cut Object\n on a single desktop GPU in about 10 minutes. After \n training converges, we use the Ne RFStudio tools for Fig. 16. Overview of the scenes assembled by the Users during the user\n extractinga 3 Dpointcloudandobtainingatexturedmesh study,see Section VI. \n with Poisson Surface Reconstruction[30].Thisoutputsan \n OBJfile,whichweconvertintoa USDbyfirstconverting \n from OBJ to GLTF, and then converting from GLTF into \n the GLB file into the provided GUI, and the time required to\n USD(withbothfileconversionsperformedwithanonline \n complete these steps was recorded as “Scan Processing and\n conversion tool). \n Uploading Time.” Because the uploaded mesh was created\n using onescan, allobjects inthe sceneare connected, andthe\n XIII. GUIUSERSTUDY \n userisunabletomoveasingleitemwithoutshiftingtheentire\n To test the functionality and versatility of the real-to-sim background.Thus,inordertocreateamorerealisticscene,the\n generationpipeline,weranauserstudyoversixpeople,where participant was asked to use the GUI to cut an object out of\n each participant was tasked with creating an articulated scene the scene, allowing this item to be manipulated independently\n using the provided GUI. Every individual was given the same of the background. The time it took for the user to cut this\n set of instructions that would guide them through the process object from the original mesh was regarded as “Cut Time.”\n ofconstructingausableandaccuratescene.Atthestartofeach In an attempt to further the realistic nature of this scene, the\n trial,theparticipantwasinstructedtodownload Polycam[52], participantwastheninstructedtospecifyjointparametersand\n which uses a mobile device’s Li DAR to generate 3 D models. create a fixed joint that would allow an object in the scene\n The user then selected a location and captured their scene by to rotate about a specific point. For instance, a fixed joint at\n taking a sequence of images. The time required to complete a door would allow the door to rotate about its hinge and\n this step was recorded as “Scan Time.” Once the images were generate an accurate simulation of door movement. The time\n captured,Polycamneededtoprocessthepicturestotransform required to create a fixed joint in the scene was recorded as\n the scene into a three-dimensional mesh. Once the mesh had “Joint Time.” Lastly, to demonstrate the full capabilities of\n been generated, the participant was then instructed to upload the GUI, the participant was asked to add another object to\n thearticulated USDtoacomputerandconvertthisfileintothe their current scene. They were instructed to download another\n GLBformat(requiredbyour GUI).Finally,theuseruploaded 3 D scanning application, AR Code [15], which was used to"
  },
  {
    "page_num": 22,
    "text": " \n \n \n \n Scan Process+ Cut Joint 2 nd Scan Process+ Totaltime Totalactive \n Upload 1 st Scan Upload 2 nd time \n (idle) Scan(idle) \n User 1 2:25 5:41 4:15 4:56 8:10 10:45 36:12 19:46 \n User 2 6:30 12:57 3:32 3:51 2:37 4:19 33:46 16:30 \n User 3 3:52 5:52 4:35 4:14 3:26 4:15 26:14 16:07 \n User 4 2:34 2:06 2:48 1:41 5:14 4:33 19:06 12:27 \n User 5 1:32 2:33 4:43 1:28 4:34 3:50 18:40 12:17 \n User 6 2:30 3:52 2:08 1:17 4:59 2:26 17:12 10:54 \n TABLEXIV \n DETAILEDTIMESPENTBYEACHUSERINTHEUSERSTUDY,SEESECTIONVI. \n \n create the three-dimensional mesh of the additional object. processed, uploaded, and converted more quickly. However,\n The time required to generate this mesh was recorded as their speed did reduce the quality of their backgrounds, since\n “Scan Time (2).” Then the participant again converted their thedetailsinbothscansarenotaspreciseastheothers.Thus,\n mesh to GLB format and uploaded this file to the same GUI. it seems User 3 completed the tasks quickly with the most\n Once uploaded, the object was placed in a realistic position accurate scan. \n within the scene, and the time elapsed during this step was User 6 had previous experience with the real-to-sim\n addedtothe“Scan Processingand Uploading Time”category. pipeline, so they were able to use this expertise to quickly\n Through this user study, we found that it took an average complete the tasks. The only abnormality with User 6’s trial\n of 14.67 active minutes (excluding the “Scan Processing and was their longer Scan Time for object 2. They had trouble\n Uploading Time”category)tocreateascenethatincludedone with the “AR code” app during this trial, resulting in a longer\n cutobject,onefixedjoint,andoneadditionalobject.However, Scan Time (2). \n it is important to note that User 6 had previous experience \n A. Scaling laws of the Rial To GUI \n usingthis GUI,whileallotherusershadnoexperience.Thus, \n if we disregard theresults of User 6, we findthe average time \n to create a scene to be 15.42 active minutes, which is not \n total active time=t \n scanscene \n a significant difference. As a result, the real-to-sim transfer \n +t ·N \n using the provided GUI seems to be an intuitive process that scanobject objects (3)\n is neither time nor labor-intensive. +t cutobject ·N cutobjects \n User 1 took the longest time to complete this series of +t addjoint ·N joints\n tasks mostly due to their extensive upload period. Because \n We derive a relation to express the total active time needed\n User 1 scanned their environment for a lengthy period, their \n to create a scene with respect to the number of joints and\n articulated USDfilewaslargerthanallotherusers.Asaresult, \n objects there are in the scene. The total active time to create\n it took longer for them to upload their file to a computer and \n a scene increases linearly in complexity with the number of\n convertthisfileto GLBformat.Theabnormalsizeof User 1’s \n objects and joints present in the scene, as seen in Relation 3.\n file coupled with their difficulty operating the file conversion \n We define N as the number of scanned objects that we\n objects \n website led to a lengthy Scan Processing and Upload Time, \n want to add, N as the number of objects that we want\n cutobjects \n which led to the slowest overall performance. \n to extract from the scanned scene, N as the number of\n joints \n User 2 wastheonlyuserwhowassentinstructionsdigitally \n joints the scene has. Taking the average times from our user\n and completed the tasks remotely. An individual experienced study (see Table XIV) we find t =4:50, t =\n scanobject scanscene \n with the real-to-sim pipeline was present for all other trials. 3 : 14, t = 2 : 54, t = 3 : 40. Note that these\n addjoint cutobject \n Thus,thismayhavecontributedto User 2’slongercompletion \n valuesareontheconservativesidesinceonlyoneuserwasan\n time,astheirquestionshadtobeansweredremotely.However, \n expert,andwithincreasedexpertise,thesecoefficientsbecome\n User 2 did not have trouble with any particular section of the \n smaller. \n pipelinebutrathertookalongertimetocompleteeachsection. \n User 3’s experience with the real-to-sim pipeline went \n smoothly,astherewerenoobviousdifficultieswhilescanning, \n uploading, or using the GUI. They followed the instructions XIV. COMPUTERESOURCES\n quickly and precisely, resulting in a better completion time We run all of our experiments on an NVIDIA Ge Force\n than Users 1 and 2. RTX 2080 or an NVIDIA Ge Force RTX 3090. The first step\n Users 4 and 5 completed all tasks in the pipeline more of learning a vision policy from the real-world demos and\n quickly than User 3 because the background they chose was collecting a set of 15 demonstrations in simulation takes an\n smaller with fewer details. Thus, they were able to scan their average of 7 hours. The next step of RL fine-tuning from\n scenes faster, generating a smaller file that was able to be demonstrationstakesonaverage 20 hourstoconverge.Finally,"
  },
  {
    "page_num": 23,
    "text": " \n \n \n \n the teacher-student distillation step takes 24 hours between \n collecting the trajectories, distilling into the vision policy, and \n running the last step of DAgger. This adds up to a total of 2 \n daysand 3 hoursonaveragetotrainapolicyforagiventask. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n "
  }
]