[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n Open X-Embodiment: Robotic Learning Datasets and RT-X Models \n Open X-Embodiment Collaboration 0 \n robotics-transformer-x.github.io \n Abby O’Neill 34,Abdul Rehman 37,Abhinav Gupta 4,Abhiram Maddukuri 45,Abhishek Gupta 46,Abhishek Padalkar 10,Abraham Lee 34,\n Acorn Pooley 11,Agrim Gupta 28,Ajay Mandlekar 22,Ajinkya Jain 15,Albert Tung 28,Alex Bewley 11,Alex Herzog 11,Alex Irpan 11,\n Alexander Khazatsky 28,Anant Rai 23,Anchit Gupta 19,Andrew Wang 34,Andrey Kolobov 20,Anikait Singh 11,34,Animesh Garg 9,\n Aniruddha Kembhavi 1,Annie Xie 28,Anthony Brohan 11,Antonin Raffin 10,Archit Sharma 28,Arefeh Yavary 35,Arhan Jain 46,Ashwin Balakrishna 32,\n Ayzaan Wahid 11,Ben Burgess-Limerick 25,Beomjoon Kim 17,Bernhard Scho¨lkopf 18,Blake Wulfe 32,Brian Ichter 11,Cewu Lu 27,8,Charles Xu 34,\n Charlotte Le 34,Chelsea Finn 11,28,Chen Wang 28,Chenfeng Xu 34,Cheng Chi 5,28,Chenguang Huang 38,Christine Chan 11,\n Christopher Agia 28,Chuer Pan 28,Chuyuan Fu 11,Coline Devin 11,Danfei Xu 9,Daniel Morton 28,Danny Driess 11,Daphne Chen 46,Deepak Pathak 4,\n Dhruv Shah 34,Dieter Bu¨chler 18,Dinesh Jayaraman 42,Dmitry Kalashnikov 11,Dorsa Sadigh 11,Edward Johns 14,Ethan Foster 28,\n Fangchen Liu 34,Federico Ceola 16,Fei Xia 11,Feiyu Zhao 13,Felipe Vieira Frujeri 20,Freek Stulp 10,Gaoyue Zhou 23,Gaurav S.Sukhatme 43,\n Gautam Salhotra 43,15,Ge Yan 36,Gilbert Feng 34,Giulio Schiavi 7,Glen Berseth 41,21,Gregory Kahn 34,Guangwen Yang 33,\n Guanzhi Wang 3,22,Hao Su 36,Hao-Shu Fang 27,Haochen Shi 28,Henghui Bao 43,Heni Ben Amor 2,Henrik IChristensen 36,Hiroki Furuta 31,\n Homanga Bharadhwaj 4,19,Homer Walke 34,Hongjie Fang 27,Huy Ha 5,28,Igor Mordatch 11,Ilija Radosavovic 34,Isabel Leal 11,\n Jacky Liang 11,Jad Abou-Chakra 25,Jaehyung Kim 17,Jaimyn Drake 34,Jan Peters 29,Jan Schneider 18,Jasmine Hsu 11,Jay Vakil 19,\n Jeannette Bohg 28,Jeffrey Bingham 11,Jeffrey Wu 34,Jensen Gao 28,Jiaheng Hu 30,Jiajun Wu 28,Jialin Wu 12,Jiankai Sun 28,Jianlan Luo 34,\n Jiayuan Gu 36,Jie Tan 11,Jihoon Oh 31,Jimmy Wu 24,Jingpei Lu 36,Jingyun Yang 28,Jitendra Malik 34,Joa˜o Silve´rio 10,Joey Hejna 28,\n Jonathan Booher 28,Jonathan Tompson 11,Jonathan Yang 28,Jordi Salvador 1,Joseph J.Lim 17,Junhyek Han 17,Kaiyuan Wang 36,\n Kanishka Rao 11,Karl Pertsch 34,28,Karol Hausman 11,Keegan Go 15,Keerthana Gopalakrishnan 11,Ken Goldberg 34,Kendra Byrne 11,\n Kenneth Oslund 11,Kento Kawaharazuka 31,Kevin Black 34,Kevin Lin 28,Kevin Zhang 4,Kiana Ehsani 1,Kiran Lekkala 43,Kirsty Ellis 41,\n Krishan Rana 25,Krishnan Srinivasan 28,Kuan Fang 34,Kunal Pratap Singh 6,Kuo-Hao Zeng 1,Kyle Hatch 32,Kyle Hsu 28,Laurent Itti 43,\n Lawrence Yunliang Chen 34,Lerrel Pinto 23,Li Fei-Fei 28,Liam Tan 34,Linxi”Jim”Fan 22,Lionel Ott 7,Lisa Lee 11,Luca Weihs 1,\n Magnum Chen 13,Marion Lepert 28,Marius Memmel 46,Masayoshi Tomizuka 34,Masha Itkina 32,Mateo Guaman Castro 46,Max Spero 28,Maximilian Du 28,\n Michael Ahn 11,Michael C.Yip 36,Mingtong Zhang 39,Mingyu Ding 34,Minho Heo 17,Mohan Kumar Srirama 4,Mohit Sharma 4,\n Moo Jin Kim 28,Muhammad Zubair Irshad 32,Naoaki Kanazawa 31,Nicklas Hansen 36,Nicolas Heess 11,Nikhil JJoshi 11,Niko Suenderhauf 25,\n Ning Liu 13,Norman Di Palo 14,Nur Muhammad Mahi Shafiullah 23,Oier Mees 38,Oliver Kroemer 4,Osbert Bastani 42,Pannag RSanketi 11,\n Patrick”Tree”Miller 32,Patrick Yin 46,Paul Wohlhart 11,Peng Xu 11,Peter David Fagan 37,Peter Mitrano 40,Pierre Sermanet 11,Pieter Abbeel 34,\n Priya Sundaresan 28,Qiuyu Chen 46,Quan Vuong 11,Rafael Rafailov 11,28,Ran Tian 34,Ria Doshi 34,Roberto Mart’in-Mart’in 30,\n Rohan Baijal 46,Rosario Scalise 46,Rose Hendrix 1,Roy Lin 34,Runjia Qian 13,Ruohan Zhang 28,Russell Mendonca 4,Rutav Shah 30,\n Ryan Hoque 34,Ryan Julian 11,Samuel Bustamante 10,Sean Kirmani 11,Sergey Levine 11,34,Shan Lin 36,Sherry Moore 11,Shikhar Bahl 4,\n Shivin Dass 43,30,Shubham Sonawani 2,Shubham Tulsiani 4,Shuran Song 5,Sichun Xu 11,Siddhant Haldar 23,Siddharth Karamcheti 28,\n Simeon Adebola 34,Simon Guist 18,Soroush Nasiriany 30,Stefan Schaal 15,Stefan Welker 11,Stephen Tian 28,Subramanian Ramamoorthy 37,\n Sudeep Dasari 4,Suneel Belkhale 28,Sungjae Park 17,Suraj Nair 32,Suvir Mirchandani 28,Takayuki Osa 31,Tanmay Gupta 1,Tatsuya Harada 31,26,\n Tatsuya Matsushima 31,Ted Xiao 11,Thomas Kollar 32,Tianhe Yu 11,Tianli Ding 11,Todor Davchev 11,Tony Z.Zhao 28,\n Travis Armstrong 11,Trevor Darrell 34,Trinity Chung 34,Vidhi Jain 11,4,Vikash Kumar 4,Vincent Vanhoucke 11,Vitor Guizilini 32,Wei Zhan 34,\n Wenxuan Zhou 11,4,Wolfram Burgard 44,Xi Chen 11,Xiangyu Chen 13,Xiaolong Wang 36,Xinghao Zhu 34,Xinyang Geng 34,Xiyuan Liu 13,\n Xu Liangwei 13,Xuanlin Li 36,Yansong Pang 11,Yao Lu 11,Yecheng Jason Ma 42,Yejin Kim 1,Yevgen Chebotar 11,Yifan Zhou 2,\n Yifeng Zhu 30,Yilin Wu 4,Ying Xu 11,Yixuan Wang 39,Yonatan Bisk 4,Yongqiang Dou 33,Yoonyoung Cho 17,Youngwoon Lee 34,Yuchen Cui 28,\n Yue Cao 13,Yueh-Hua Wu 36,Yujin Tang 11,31,Yuke Zhu 30,Yunchu Zhang 46,Yunfan Jiang 28,Yunshuang Li 42,Yunzhu Li 39,\n Yusuke Iwasawa 31,Yutaka Matsuo 31,Zehan Ma 34,Zhuo Xu 11,Zichen Jeff Cui 23,Zichen Zhang 1,Zipeng Fu 28,Zipeng Lin 34\n \n \n \n Fig. 1: We propose an open, large-scale dataset for robot learning curated from 21 institutions across the globe. The dataset represents\n diverse behaviors, robot embodiments and environments, and enables learning generalized robotic policies.\n Abstract—Large, high-capacity models trained on diverse for many applications. Can such a consolidation happen in\n datasetshaveshownremarkablesuccessesonefficientlytackling robotics? Conventionally, robotic learning methods train a\n downstream applications. In domains from NLP to Computer separate model for every application, every robot, and even\n Vision, this has led to a consolidation of pretrained models, every environment. Can we instead train “generalist” X-robot\n with general pretrained backbones serving as a starting point policy that can be adapted efficiently to new robots, tasks,\n 5202 \n ya M \n 41 \n ]OR.sc[ \n 9 v 46880.0132:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n and environments? In this paper, we provide datasets in in environments and robots. Learning generalizable robot\n standardized data formats and models to make it possible to policies requires developing methods that can utilize X-\n explore this possibility in the context of robotic manipulation, \n embodiment data, tapping into datasets from many labs,\n alongside experimental results that provide an example of \n robots, and settings. Even if such datasets in their current\n effective X-robot policies. We assemble a dataset from 22 \n different robots collected through a collaboration between 21 size and coverage are insufficient to attain the impressive\n institutions, demonstrating 527 skills (160266 tasks). We show generalization results that have been demonstrated by large\n that a high-capacity model trained on this data, which we call language models, in the future, the union of such data can\n RT-X,exhibitspositivetransferandimprovesthecapabilitiesof \n potentiallyprovidethiskindofcoverage.Becauseofthis,we\n multiplerobotsbyleveragingexperiencefromotherplatforms. \n believethatenablingresearchinto X-embodimentrobotic\n The project website is robotics-transformer-x.github.io. \n learning is critical at the present juncture.\n I. INTRODUCTION \n Following this rationale, we have two goals: (1) Evaluate\n A central lesson from advances in machine learning and \n whether policies trained on data from many different robots\n artificial intelligence is that large-scale learning from di- \n and environments enjoy the benefits of positive transfer,\n verse datasets can enable capable AI systems by providing \n attaining better performance than policies trained only on\n for general-purpose pretrained models. In fact, large-scale \n data from each evaluation setup. (2) Organize large robotic\n general-purpose models typically trained on large and di- \n datasetstoenablefutureresearchon X-embodimentmodels.\n verse datasets can often outperform their narrowly targeted \n We focus our work on robotic manipulation. Addressing\n counterparts trained on smaller but more task-specific data. \n goal (1), our empirical contribution is to demonstrate that\n For instance, open-vocab classifiers (e.g., CLIP [1]) trained \n several recent robotic learning methods, with minimal mod-\n on large datasets scraped from the web tend to outperform \n ification, can utilize X-embodiment data and enable positive\n fixed-vocabulary models trained on more limited datasets, \n transfer. Specifically, we train the RT-1 [8] and RT-2 [9]\n and large language models [2, 3] trained on massive text \n models on 9 different robotic manipulators. We show that\n corpora tend to outperform systems that are only trained on \n the resulting models, which we call RT-X, can improve over\n narrowtask-specificdatasets.Increasingly,themosteffective \n policies trained only on data from the evaluation domain,\n way to tackle a given narrow task (e.g., in vision or NLP) \n exhibiting better generalization and new capabilities. Ad-\n is to adapt a general-purpose model. However, these lessons \n dressing (2), we provide the Open X-Embodiment (OXE)\n are difficult to apply in robotics: any single robotic domain \n Repository,whichincludesadatasetwith 22 differentrobotic\n mightbetoonarrow,andwhilecomputervisionand NLPcan \n embodiments from 21 different institutions that can enable\n leverage large datasets sourced from the web, comparably \n the robotics community to pursue further research on X-\n large and broad datasets for robotic interaction are hard to \n embodiment models, along with open-source tools to facili-\n come by. Even the largest data collection efforts still end \n tatesuchresearch.Ouraimisnottoinnovateintermsofthe\n up with datasets that are a fraction of the size and diversity \n particular architectures and algorithms, but rather to provide\n of benchmark datasets in vision (5-18 M) [4, 5] and NLP \n the model that we trained together with data and tools to\n (1.5 B-4.5 B)[6,7].Moreimportantly,suchdatasetsareoften \n energize research around X-embodiment robotic learning.\n still narrow along some axes of variation, either focusing on \n II. RELATEDWORK \n a single environment, a single set of objects, or a narrow \n Transfer across embodiments. A number of prior works\n range of tasks. How can we overcome these challenges in \n have studied methods for transfer across robot embodiments\n robotics and move the field of robotic learning toward large \n in simulation [10–22] and on real robots [23–29]. These\n data regime that has been so successful in other domains? \n methodsoftenintroducemechanismsspecificallydesignedto\n Inspired by the generalization made possible by pretrain- \n address the embodiment gap between different robots, such\n ing large vision or language models on diverse data, we \n as shared action representations [14, 30], incorporating rep-\n take the perspective that the goal of training generalizable \n resentation learning objectives [17, 26], adapting the learned\n robot policies requires X-embodiment training, i.e., with \n policy on embodiment information [11, 15, 18, 30, 31], and\n data from multiple robotic platforms. While each individual \n decouplingrobotandenvironmentrepresentations[24].Prior\n robotic learning dataset might be too narrow, the union of \n work has provided initial demonstrations of X-embodiment\n all such datasets provides a better coverage of variations \n training [27] and transfer [25, 29, 32] with transformer\n 01 Allen Institutefor AI;2 Arizona State University;3 California Instituteof Tech- models. We investigate complementary architectures and\n nology;4 Carnegie Mellon University;5 Columbia University;6 EPFL;7 ETHZu¨rich; \n providecomplementaryanalyses,and,inparticular,studythe\n 8 Flexiv Robotics; 9 Georgia Institute of Technology; 10 German Aerospace Center;\n 11 Google Deep Mind;12 Google Research;13 IO-AITECH;14 Imperial College Lon- interaction between X-embodiment transfer and web-scale\n don;15 Intrinsic LLC;16 Istituto Italianodi Tecnologia;17 Korea Advanced Institute \n pretraining. Similarly, methods for transfer across human\n of Science & Technology; 18 Max Planck Institute; 19 Meta AI; 20 Microsoft Re- \n search;21 Mila Quebec;22 NVIDIA;23 New York University;24 Princeton University; and robot embodiments also often employ techniques for\n 25 Queensland Universityof Technology;26 RIKEN;27 Shanghai Jiao Tong University; \n reducing the embodiment gap, i.e. by translating between\n 28 Stanford University;29 Technische Universita¨t Darmstadt;30 The Universityof Texas\n at Austin; 31 The University of Tokyo; 32 Toyota Research Institute; 33 Tsinghua domainsorlearningtransferablerepresentations[33–43].Al-\n University; 34 University of California, Berkeley; 35 University of California, Davis; ternatively, some works focus on sub-aspects of the problem\n 36 University of California, San Diego; 37 University of Edinburgh; 38 University of\n Freiburg; 39 University of Illinois Urbana-Champaign; 40 University of Michigan; such as learning transferable reward functions [17, 44–48],\n 41 University of Montreal; 42 University of Pennsylvania; 43 University of Southern goals [49, 50], dynamics models [51], or visual representa-\n California;44 Universityof Technology,Nuremberg;45 Universityof Texasat Austin; \n 46 Universityof Washington tions [52–59] from human video data. Unlike most of these"
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n Google \n Franka Robot \n Google \n x Arm Franka \n Robot \n Kuka iiwa \n x Arm \n Franka x Ar m S G a o w o y g e l r e Rob K o u t ka ii wa UR 5 Wi H do e w llo X Stretch P D R L 2 R SARA Jac U o n 2 x it A re r m e A B 1 i manua C l obo D t L ta R EDAN PA K M in Y o 2 va G F e a n n 3 uc Mate Jackal RC T C u a r r tle Bot 2 Baxter Widow X Jackal Hel K lo S in a S o w t v r y a e e t G c r h en 3 Widow X Sawyer\n (a) # Datasets per Robot Embodiment (b) # Scenes per Embodiment (c) # Trajectories per Embodiment\n Shapes \n Containers Food \n Furniture \n Appliances \n Utensils \n picking moving pushing placing sliding puttin n g avigat s in e g parating pointing opening nudging closin i g nsertin k g nockin d g ragging dropping wip a i s n s g e mbli t n u g rning on keeping hexag t o r n iangle heart cube tray bo wl pot box cu b p asket count d er ra wer tab c l a e binet door chair app o le rang b e an c a o n k a e c c a h n ip bag fauce f t ridge sink m st i o c v ro e wave oven for s k poon kni s f p e atula\n \n (d) Common Dataset Skills (e) Common Dataset Objects \n Fig.2:The Open X-Embodiment Dataset.(a):thedatasetconsistsof 60 individualdatasetsacross 22 embodiments.(b):the Frankarobot\n has the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): x Arm and Google Robot contribute\n the most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects.\n prior works, we directly train a policy on X-embodiment • Open X-Embodiment Dataset: robot learning dataset\n data,withoutanymechanismstoreducetheembodimentgap, with 1 M+ robot trajectories from 22 robot embodi-\n and observe positive transfer by leveraging that data. ments. \n Large-scale robot learning datasets. The robot learning • Pre-Trained Checkpoints: a selection of RT-X model\n community has created open-source robot learning datasets, checkpoints ready for inference and finetuning.\n spanninggrasping[60–71],pushinginteractions[23,72–74], We intend for these resources to form a foundation for X-\n setsofobjectsandmodels[75–85],andteleoperateddemon- embodiment research in robot learning, but they are just\n strations [8, 86–95]. With the exception of Robo Net [23], thestart.Open X-Embodimentisacommunity-driveneffort,\n these datasets contain data of robots of the same type, currently involving 21 institutions from around the world,\n whereas we focus on data spanning multiple embodiments. and we hope to further broaden participation and grow the\n The goal of our data repository is complementary to these initial Open X-Embodiment Dataset over time. In this sec-\n efforts: we process and aggregate a large number of prior tion, we summarize the dataset and X-embodiment learning\n datasets into a single, standardized repository, called Open framework, before discussing the specific models we use to\n X-Embodiment, which shows how robot learning datasets evaluate our dataset and our experimental results.\n can be shared in a meaningul and useful way. \n A. The Open X-Embodiment Dataset \n Language-conditioned robot learning. Prior work has \n The Open X-Embodiment Datasetcontains 1 M+realrobot\n aimed to endow robots and other agents with the ability to \n trajectories spanning 22 robot embodiments, from single\n understand and follow language instructions [96–101], often \n robot arms to bi-manual robots and quadrupeds. The dataset\n by learning language-conditioned policies [8, 40, 45, 102– \n was constructed by pooling 60 existing robot datasets from\n 106]. We train language-conditioned policies via imitation \n 34 robotic research labs around the world and converting\n learning like many of these prior works but do so using \n them into a consistent data format for easy download and\n large-scalemulti-embodimentdemonstrationdata.Following \n usage.Weusethe RLDSdataformat[119],whichsavesdata\n previous works that leverage pre-trained language embed- \n inserializedtfrecordfilesandaccommodatesthevarious\n dings [8, 40, 45, 103, 107–112] and pre-trained vision- \n action spaces and input modalities of different robot setups,\n language models [9, 113–115] in robotic imitation learning, \n such as differing numbers of RGB cameras, depth cameras\n we study both forms of pre-training in our experiments, \n and point clouds. It also supports efficient, parallelized data\n specifically following the recipes of RT-1 [8] and RT-2 [9]. \n loading in all major deep learning frameworks. For more\n III. THEOPENX-EMBODIMENTREPOSITORY details about the data storage format and a breakdown of all\n 60 datasets, see robotics-transformer-x.github.io.\n We introduce the Open X-Embodiment Repository \n (robotics-transformer-x.github.io) – an open-source reposi- B. Dataset Analysis\n torywhichincludeslarge-scaledataalongwithpre-trained Fig.2 analyzesthe Open X-Embodiment Dataset.Fig.2(a)\n model checkpointsfor X-embodiedrobotlearningresearch. shows the breakdown of datasets by robot embodiments,\n More specifically, we provide and maintain the following with the Franka robot being the most common. This is\n open-source resources to the broader community: reflected in the number of distinct scenes (based on dataset"
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n \n Pick apple from top drawer \n and place on counter Fi LM Efficient Net Transformer \n Discrete Action \n β \n )γ+1( \n 10 Hz \n Route cable DDiissccrreettee Closed Gripper\n Instruction AAccttiioonn Velocity \n · RT-1-X Z-Rot. Velocity Images + \n 3 Hz \n Gripper \n Position Delta \n Rotation Delta \n Discrete Instruction Action 5 Hz\n Pick up the orange fruit Image RT-2-X Gripper \n Vi T LLM De-Tokenizer Position Delta \n No Rotation \n Fig. 3: RT-1-X and RT-2-X both take images and a text instruction as input and output discretized end-effector actions. RT-1-X is an\n architecture designed for robotics, with a Fi LM [116] conditioned Efficient Net [117] and a Transformer [118]. RT-2-X builds on a VLM\n backbone by representing actions as another language, and training action text tokens together with vision-language data.\n metadata) per embodiment (Fig. 2(b)), where Franka dom- B. Policy architectures\n inates. Fig. 2(c) shows the breakdown of trajectories per We consider two model architectures in our experiments:\n embodiment. To further analyze the diversity, we use the (1) RT-1 [8], an efficient Transformer-based architecture\n language annotations present in our data. We use the Pa LM designedforroboticcontrol,and(2)RT-2[9]alargevision-\n language model [3] to extract objects and behaviors from language model co-fine-tuned to output robot actions as\n the instructions. Fig. 2(d,e) show the diversity of skills and natural language tokens. Both models take in a visual input\n objects. While most skills belong to the pick-place family, and natural language instruction describing the task, and\n the long tail of the dataset contains skills like “wiping” or output a tokenized action. For each model, the action is\n “assembling”.Additionally,thedatacoversarangeofhouse- tokenized into 256 bins uniformly distributed along each of\n hold objects, from appliances to food items and utensils. eightdimensions;onedimensionforterminatingtheepisode\n IV. RT-XDESIGN and seven dimensions for end-effector movement. Although\n both architectures are described in detail in their original\n To evaluate how much X-embodiment training can im- \n papers [8, 9], we provide a short summary of each below:\n prove the performance of learned policies on individual \n RT-1 [8] is a 35 M parameter network built on a Trans-\n robots, we require models that have sufficient capacity to \n former architecture [118] and designed for robotic control,\n productively make use of such large and heterogeneous \n as shown in Fig. 3. It takes in a history of 15 images\n datasets. To that end, our experiments will build on two \n along with the natural language. Each image is processed\n recently proposed Transformer-based robotic policies: RT- \n through an Image Net-pretrained Efficient Net [117] and the\n 1[8]and RT-2[9].Webrieflysummarizethedesignofthese \n naturallanguageinstructionistransformedintoa USE[120]\n models in this section, and discuss how we adapted them to \n embedding.Thevisualandlanguagerepresentationsarethen\n the X-embodiment setting in our experiments. \n interwoven via Fi LM [116] layers, producing 81 vision-\n A. Data format consolidation \n language tokens. These tokens are fed into a decoder-only\n One challenge of creating X-embodiment models is that \n Transformer, which outputs the tokenized actions.\n observation and action spaces vary significantly across \n RT-2 [9] is a family of large vision-language-action\n robots. We use a coarsely aligned action and observation \n models(VLAs)trainedon Internet-scalevisionandlanguage\n space across datasets. The model receives a history of \n dataalongwithroboticcontroldata.RT-2 caststhetokenized\n recent images and language instructions as observations and \n actions to text tokens, e.g., a possible action may be “1 128\n predicts a 7-dimensional action vector controlling the end- \n 91 241 5 101 127”. As such, any pretrained vision-language\n effector (x, y, z, roll, pitch, yaw, and gripper opening or the \n model(VLM[121–123])canbefinetunedforroboticcontrol,\n rates of these quantities). We select one canonical camera \n thusleveragingthebackboneof VLMsandtransferringsome\n viewfromeachdatasetastheinputimage,resizeittoacom- \n of their generalization properties. In this work, we focus on\n mon resolution and convert the original action set into a 7 \n the RT-2-Pa LI-Xvariant[121]builtonabackboneofavisual\n Do Fend-effectoraction.Wenormalizeeachdataset’sactions \n model, Vi T [124], and a language model, UL 2 [125], and\n prior to discretization. This way, an output of the model can \n pretrained primarily on the Web LI [121] dataset.\n be interpreted (de-normalized) differently depending on the \n embodimentused. Itshould benoted thatdespite this coarse C. Training and inference details\n alignment, the camera observations still vary substantially Both models use a standard categorical cross-entropy\n across datasets, e.g. due to differing camera poses relative objective over their output space (discrete buckets for RT-\n to the robot or differing camera properties, see Figure 3. 1 and all possible language tokens for RT-2).\n Similarly,fortheactionspace,wedonotalignthecoordinate We define the robotics data mixture used across all of\n framesacrossdatasetsinwhichtheend-effectoriscontrolled, the experiments as the data from 9 manipulators, and taken\n andallowactionvaluestorepresenteitherabsoluteorrelative from RT-1 [8], QT-Opt [66], Bridge [95], Task Agnostic\n positions or velocities, as per the original control scheme Robot Play[126,127],Jaco Play[128],Cable Routing[129],\n chosen for each robot. Thus, the same action vector may Robo Turk [86], NYU VINN [130], Austin VIOLA [131],\n induce very different motions for different robots. Berkeley Autolab UR 5 [132], TOTO [133] and Language"
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n \n \n \n \n \n Fig.4:RT-1-Xmeansuccessrateis 50%higherthanthatofeitherthe Original Methodor RT-1.RT-1 and RT-1-Xhavethesamenetwork\n architecture. Therefore the performance increase can be attributed to co-training on the robotics data mixture. The lab logos indicate the\n physical location of real robot evaluation, and the robot pictures indicate the embodiment used for the evaluation.\n tasks. We split our evaluation into two types: evaluation on\n Evaluation Setting Bridge Bridge RT-1 paper 6 skills \n domains that have small-scale datasets (Fig. 4), where we\n Evaluation Location IRIS(Stanford) RAILLab(UCB) Google Robotic Lab \n would expect transfer from larger datasets to significantly\n Robot Embodiment Widow X Widow X Google Robot \n Original Method LCBC[95] LCBC[95] - improve performance, and evaluation on domains that have\n Original Method 13% 13% - large-scale datasets (Table I), where we expect further im-\n RT-1 40% 30% 92% provement to be more challenging. Note that we use the\n RT-1-X 27% 27% 73% \n RT-2-X(55 B) 50% 30% 91% sameroboticsdatatrainingmixture(definedin Sec.IV-C)for\n all the evaluations presented in this section. For small-scale\n TABLEI:Parametercountscalingexperimenttoassesstheimpact \n dataset experiments, we use Kitchen Manipulation [128],\n of capacity on absorbing large-scale diverse embodiment data. For \n these large-scale datasets (Bridge and RT-1 paper data), RT-1-X Cable Routing [129], NYU Door Opening [130], AUTOLab\n underfits and performs worse than the Original Method and RT-1. UR 5 [132], and Robot Play [134]. We use the same evalua-\n RT-2-Xmodelwithsignificantlymanymoreparameterscanobtain tion and robot embodiment as in the respective publications.\n strong performance in these two evaluation scenarios. \n Forlarge-scaledatasetexperiments,weconsider Bridge[95]\n Table [91] datasets. RT-1-X is trained on only robotics \n and RT-1 [8] for in-distribution evaluation and use their\n mixture data defined above, whereas RT-2-X is trained via \n respective robots: Widow X and Google Robot.\n co-fine-tuning (similarly to the original RT-2 [9]), with an \n approximately one to one split of the original VLM data For each small dataset domain, we compare the perfor-\n and the robotics data mixture. Note that the robotics data mance of the RT-1-X model, and for each large dataset\n mixture used in our experiments includes 9 embodiments we consider both the RT-1-X and RT-2-X models. For\n which is fewer than the entire Open X-Embodiment dataset all experiments, the models are co-trained on the full X-\n (22)–thepracticalreasonforthisdifferenceisthatwehave embodimentdataset.Throughoutthisevaluationwecompare\n continued to extend the dataset over time, and at the time with two baseline models: (1) The model developed by\n of the experiments, the dataset above represented all of the the creators of the dataset trained only on that respective\n data. In the future, we plan to continue training policies on dataset. This constitutes a reasonable baseline insofar as\n the extended versions of the dataset as well as continue to it can be expected that the model has been optimized to\n growthedatasettogetherwiththerobotlearningcommunity. work well with the associated data; we refer to this baseline\n At inference time, each model is run at the rate required model as the Original Method model. (2) An RT-1 model\n for the robot (3-10 Hz), with RT-1 run locally and RT-2 trained on the dataset in isolation; this baseline allows us to\n hosted on a cloud service and queried over the network. assess whether the RT-X model architectures have enough\n capacity to represent policies for multiple different robot\n V. EXPERIMENTALRESULTS \n platforms simultaneously, and whether co-training on multi-\n Our experiments answer three questions about the effect \n embodiment data leads to higher performance.\n of X-embodiment training: (1) Can policies trained on our \n X-embodiment dataset effectively enable positive transfer, Small-scale dataset domains (Fig. 4). RT-1-X outper-\n such that co-training on data collected on multiple robots forms Original Method trained on each of the robot-specific\n improves performance on the training task? (2) Does co- datasets on 4 of the 5 datasets, with a large average im-\n training models on data from multiple platforms and tasks provement, demonstrating domains with limited data benefit\n improvegeneralizationtonew,unseentasks?(3)Whatisthe substantially from co-training on X-embodiment data.\n influenceofdifferentdesigndimensions,suchasmodelsize, \n model architecture or dataset composition, on performance Large-scale dataset domains (Table I). In the large-\n and generalization capabilities of the resulting policy? To dataset setting, the RT-1-X model does not outperform\n answerthesequestionsweconductthetotalnumberof 3600 the RT-1 baseline trained on only the embodiment-specific\n evaluation trials across 6 different robots. dataset, which indicates underfitting for that model class.\n However, the larger RT-2-X model outperforms both the\n A. In-distributionperformanceacrossdifferentembodiments \n Original Method and RT-1 suggesting that X-robot training\n To assess the ability of RT-X models to learn from X- can improve performance in the data-rich domains, but only\n embodimentdata,weevaluateperformanceonin-distribution when utilizing a sufficiently high-capacity architecture."
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n Row Model Size History Length Dataset Co-Trainedw/Web Initial Checkpoint Emergent Skills Evaluation RT-2 Generalization Evaluation\n (1) RT-2 55 B none Google Robotaction Yes Web-pretrained 27.3% 62% \n (2) RT-2-X 55 B none Roboticsdata Yes Web-pretrained 75.8% 61% \n (3) RT-2-X 55 B none Roboticsdataexcept Bridge Yes Web-pretrained 42.8% 54% \n (4) RT-2-X 5 B 2 Roboticsdata Yes Web-pretrained 44.4% 52% \n (5) RT-2-X 5 B none Roboticsdata Yes Web-pretrained 14.5% 30% \n (6) RT-2-X 5 B 2 Roboticsdata No Fromscratch 0% 1% \n (7) RT-2-X 5 B 2 Roboticsdata No Web-pretrained 48.7% 47% \n TABLE II: Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and\n emergent skills (skills from other datasets on the Google Robot), showing the importance of Web-pretraining, model size, and history.\n B. Improved generalization to out-of-distribution settings \n We now examine how X-embodiment training can enable \n better generalization to out-of-distribution settings and more \n complex and novel instructions. These experiments focus on \n the high-data domains, and use the RT-2-X model. \n Unseen objects, backgrounds and environments. We \n firstconductthesameevaluationofgeneralizationproperties \n as proposed in [9], testing for the ability to manipulate \n unseen objects in unseen environments and against unseen \n backgrounds.Wefindthat RT-2 and RT-2-Xperformroughly \n on par (Table II, rows (1) and (2), last column). This is not \n unexpected, since RT-2 already generalizes well (see [9]) \n along these dimensions due to its VLM backbone. \n Emergent skills evaluation. To investigate the transfer \n of knowledge across robots, we conduct experiments with \n the Google Robot, assessing the performance on tasks like \n the ones shown in Fig. 5. These tasks involve objects and Fig. 5: To assess transfer between embodiments, we evaluate the\n skillsthatarenotpresentinthe RT-2 datasetbutoccurinthe RT-2-X model on out-of-distribution skills. These skills are in\n Bridgedataset[95]foradifferentrobot(the Widow Xrobot). the Bridge dataset, but not in the Google Robot dataset (the\n embodiment they are evaluated on). \n Results are shown in Table II, Emergent Skills Evaluation \n across robotic datasets. Contrary to previous RT-2 findings,\n column. Comparing rows (1) and (2), we find that RT-2-X \n co-fine-tuning and fine-tuning have similar performance in\n outperforms RT-2 by ∼ 3×, suggesting that incorporating \n boththe Emergent Skillsand Generalization Evaluation(row\n data from other robots into the training improves the range \n (4)vsrow(7)),whichweattributetothefactthattherobotics\n of tasks that can be performed even by a robot that already \n datausedin RT-2-Xismuchmorediversethanthepreviously\n has large amounts of data available. Our results suggest that \n used robotics datasets. \n co-training with data from other platforms imbues the RT-2- \n X controller with additional skills for the platform that are \n VI. DISCUSSION,FUTUREWORK,ANDOPENPROBLEMS\n We presented a consolidated dataset that combines data\n not present in that platform’s original dataset. \n from 22 robotic embodiments collected through a collab-\n Our next ablation involves removing the Bridge dataset \n oration between 21 institutions, demonstrating 527 skills\n from RT-2-X training: Row (3) shows the results for RT-2- \n (160266 tasks). We also presented an experimental demon-\n X that includes all data used for RT-2-X except the Bridge \n stration that Transformer-based policies trained on this data\n dataset. This variation significantly reduces performance on \n canexhibitsignificantpositivetransferbetweenthedifferent\n thehold-outtasks,suggestingthattransferfromthe Widow X \n robots in the dataset. Our results showed that the RT-1-\n data may indeed be responsible for the additional skills that \n X policy has a 50% higher success rate than the original,\n can be performed by RT-2-X with the Google Robot. \n state-of-the-art methods contributed by different collabo-\n C. Design decisions rating institutions, while the bigger vision-language-model-\n Lastly, we perform ablations to measure the influence of based version (RT-2-X) demonstrated ∼ 3× generalization\n different design decisions on the generalization capabilities improvements over a model trained only on data from the\n of our most performant RT-2-X model, which are presented evaluation embodiment. In addition, we provided multiple\n in Table II. We note that including a short history of im- resources for the robotics community to explore the X-\n ages significantly improves generalization performance (row embodiment robot learning research, including: the unified\n (4) vs row (5)). Similarly to the conclusions in the RT-2 X-robot and X-institution dataset, sample code showing\n paper [9], Web-based pre-training of the model is critical how to use the data, and the RT-1-X model to serve as a\n to achieving a high performance for the large models (row foundation for future exploration.\n (4) vs row (6)). We also note that the 55 B model has While RT-X demonstrates a step towards a X-embodied\n significantlyhighersuccessrateinthe Emergent Skillscom- robot generalist, many more steps are needed to make this\n pared to the 5 B model (row (2) vs row (4)), demonstrating future a reality. Our experiments do not consider robots\n that higher model capacity enables higher degree of transfer with very different sensing and actuation modalities. They"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n do not study generalization to new robots, and provide a Advances in Neural Information Processing Systems,\n decision criterion for when positive transfer does or does 2018, pp. 9355–9366.\n not happen. Studying these questions is an important future [12] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg,\n workdirection.Thisworkservesnotonlyasanexamplethat J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia,\n X-robot learning is feasible and practical, but also provide “Graph networks as learnable physics engines for\n the tools to advance research in this direction in the future. inference and control,” in Proceedings of the 35 th\n International Conference on Machine Learning, ser.\n Proceedings of Machine Learning Research, J. Dy\n REFERENCES \n and A. Krause, Eds., vol. 80. PMLR, 10–15 Jul\n [1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, 2018, pp. 4470–4479. [Online]. Available: https://\n G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, proceedings.mlr.press/v 80/sanchez-gonzalez 18 a.html\n J. Clark et al., “Learning transferable visual models [13] D.Pathak,C.Lu,T.Darrell,P.Isola,and A.A.Efros,\n from natural language supervision,” in International “Learning to control self-assembling morphologies: a\n conference on machine learning. PMLR, 2021, pp. study of generalization via modularity,” Advances in\n 8748–8763. Neural Information Processing Systems,vol.32,2019.\n [2] Open AI, “GPT-4 technical report,” 2023. [14] R. Mart´ın-Mart´ın, M. Lee, R. Gardner, S. Savarese,\n [3] R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin, J. Bohg, and A. Garg, “Variable impedance control in\n A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen end-effector space. an action space for reinforcement\n et al., “Pa LM 2 technical report,” ar Xiv preprint learning in contact rich tasks,” in Proceedings of the\n ar Xiv:2305.10403, 2023. International Conference of Intelligent Robots and\n [4] T. Weyand, A. Araujo, B. Cao, and J. Sim, “Google Systems (IROS), 2019.\n landmarks dataset v 2 - a large-scale benchmark for [15] W.Huang,I.Mordatch,and D.Pathak,“Onepolicyto\n instance-level recognition and retrieval,” in Proceed- control them all: Shared modular policies for agent-\n ingsofthe IEEE/CVFConferenceon Computer Vision agnostic control,” in ICML, 2020.\n and Pattern Recognition (CVPR), June 2020. [16] V. Kurin, M. Igl, T. Rockta¨schel, W. Boehmer, and\n [5] B. Wu, W. Chen, Y. Fan, Y. Zhang, J. Hou, J. Liu, S. Whiteson, “My body is a cage: the role of mor-\n and T. Zhang, “Tencent ML-images: A large-scale phology in graph-based incompatible control,” ar Xiv\n multi-label image database for visual representation preprint ar Xiv:2010.01856, 2020.\n learning,” IEEE Access, vol. 7, 2019. [17] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg,\n [6] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, and D. Dwibedi, “XIRL: Cross-embodiment inverse\n D. Kontokostas, P. N. Mendes, S. Hellmann, reinforcement learning,” Conference on Robot Learn-\n M. Morsey, P. van Kleef, S. Auer, and C. Bizer, ing (Co RL), 2021. \n “DBpedia - a large-scale, multilingual knowledge [18] A. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn,\n base extracted from wikipedia.” Semantic Web, M.Bjo¨rkman,and D.Kragic,“Bayesianmeta-learning\n vol. 6, no. 2, pp. 167–195, 2015. [Online]. for few-shot policy adaptation across robotic plat-\n Available: http://dblp.uni-trier.de/db/journals/semweb/ forms,” in 2021 IEEE/RSJ International Conference\n semweb 6.html#Lehmann IJJKMHMK 15 on Intelligent Robots and Systems (IROS). IEEE,\n [7] H. Mu¨hleisen and C. Bizer, “Web data commons- 2021, pp. 1274–1280. \n extracting structured data from two large web cor- [19] A. Gupta, L. Fan, S. Ganguli, and L. Fei-Fei, “Meta-\n pora.” LDOW, vol. 937, pp. 133–145, 2012. morph:Learninguniversalcontrollerswithtransform-\n [8] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, ers,” in International Conference on Learning Repre-\n J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, sentations, 2021. \n A. Herzog, J. Hsu et al., “RT-1: Robotics transformer [20] I. Schubert, J. Zhang, J. Bruce, S. Bechtle,\n forreal-worldcontrolatscale,”Robotics:Scienceand E. Parisotto, M. Riedmiller, J. T. Springenberg,\n Systems (RSS), 2023. A. Byravan, L. Hasenclever, and N. Heess, “A gen-\n [9] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, eralist dynamics model for control,” 2023.\n X. Chen, K. Choromanski, T. Ding, D. Driess, [21] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and\n A. Dubey, C. Finn et al., “RT-2: Vision-language- S.Levine,“GNM:Ageneralnavigationmodeltodrive\n actionmodelstransferwebknowledgetoroboticcon- anyrobot,”in 2023 IEEEInternational Conferenceon\n trol,” ar Xiv preprint ar Xiv:2307.15818, 2023. Robotics and Automation (ICRA). IEEE, 2023, pp.\n [10] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and 7226–7233. \n S. Levine, “Learning modular neural network policies [22] Y. Zhou, S. Sonawani, M. Phielipp, S. Stepputtis, and\n formulti-taskandmulti-robottransfer,”in 2017 IEEE H. Amor, “Modularity through attention: Efficient\n international conference on robotics and automation training and transfer of language-conditioned policies\n (ICRA). IEEE, 2017, pp. 2169–2176. for robot manipulation,” in Proceedings of The 6 th\n [11] T. Chen, A. Murali, and A. Gupta, “Hardware con- Conference on Robot Learning, ser. Proceedings\n ditioned policies for multi-robot transfer learning,” in of Machine Learning Research, K. Liu, D. Kulic,"
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n and J. Ichnowski, Eds., vol. 205. PMLR, 14– controller,” Advances in Neural Information Process-\n 18 Dec 2023, pp. 1684–1695. [Online]. Available: ing Systems, vol. 32, 2019.\n https://proceedings.mlr.press/v 205/zhou 23 b.html [36] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and\n [23] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, S.Levine,“Avid:Learningmulti-stagetasksviapixel-\n K. Schmeckpeper, S. Singh, S. Levine, and C. Finn, level translation of human videos,” ar Xiv preprint\n “Robo Net: Large-scale multi-robot learning,” in Con- ar Xiv:1912.04443, 2019.\n ferenceon Robot Learning(Co RL),vol.100. PMLR, [37] A. Bonardi, S. James, and A. J. Davison, “Learning\n 2019, pp. 885–897. one-shot imitation from humans without humans,”\n [24] E. S. Hu, K. Huang, O. Rybkin, and D. Jayaraman, IEEE Robotics and Automation Letters, vol. 5, no. 2,\n “Know thyself: Transferable visual control policies pp. 3533–3539, 2020. \n throughrobot-awareness,”in International Conference [38] K.Schmeckpeper,O.Rybkin,K.Daniilidis,S.Levine,\n on Learning Representations, 2022. and C. Finn, “Reinforcement learning with videos:\n [25] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Combining offline observations with interaction,” in\n Lee, M. Bauza, T. Davchev, Y. Zhou, A. Gupta, Conference on Robot Learning. PMLR, 2021, pp.\n A. Raju et al., “Robo Cat: A self-improving founda- 339–354. \n tion agent for robotic manipulation,” ar Xiv preprint [39] H.Xiong,Q.Li,Y.-C.Chen,H.Bharadhwaj,S.Sinha,\n ar Xiv:2306.11706, 2023. and A. Garg, “Learning by watching: Physical imita-\n [26] J. Yang, D. Sadigh, and C. Finn, “Polybot: Training tion of manipulation skills from human videos,” in\n one policy across robots while embracing variability,” 2021 IEEE/RSJ International Conference on Intelli-\n ar Xiv preprint ar Xiv:2307.03719, 2023. gent Robots and Systems (IROS). IEEE, 2021, pp.\n [27] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, 7827–7834. \n A. Novikov, G. Barth-maron, M. Gime´nez, Y. Sul- [40] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert,\n sky, J. Kay, J. T. Springenberg, T. Eccles, J. Bruce, C. Lynch, S. Levine, and C. Finn, “BC-Z: Zero-shot\n A. Razavi, A. Edwards, N. Heess, Y. Chen, R. Had- task generalization with robotic imitation learning,”\n sell, O. Vinyals, M. Bordbar, and N. de Freitas, “A in Conference on Robot Learning (Co RL), 2021, pp.\n generalist agent,” Transactions on Machine Learning 991–1002. \n Research, 2022. [41] S. Bahl, A. Gupta, and D. Pathak, “Human-to-robot\n [28] G.Salhotra,I.-C.A.Liu,and G.Sukhatme,“Bridging imitation in the wild,” Robotics: Science and Systems\n action space mismatch in learning from demonstra- (RSS), 2022. \n tions,” ar Xiv preprint ar Xiv:2304.03833, 2023. [42] M. Ding, Y. Xu, Z. Chen, D. D. Cox, P. Luo,\n [29] I.Radosavovic,B.Shi,L.Fu,K.Goldberg,T.Darrell, J. B. Tenenbaum, and C. Gan, “Embodied concept\n and J. Malik, “Robot learning with sensorimotor pre- learner:Self-supervisedlearningofconceptsandmap-\n training,” in Conference on Robot Learning, 2023. ping through instruction following,” in Conference on\n [30] L. Shao, F. Ferreira, M. Jorda, V. Nambiar, J. Luo, Robot Learning. PMLR, 2023, pp. 1743–1754.\n E. Solowjow, J. A. Ojea, O. Khatib, and J. Bohg, [43] S. Bahl, R. Mendonca, L. Chen, U. Jain, and\n “Uni Grasp: Learning a unified model to grasp with D. Pathak, “Affordances from human videos as a\n multifingered robotic hands,” IEEE Robotics and Au- versatile representation for robotics,” in Proceedings\n tomation Letters, vol. 5, no. 2, pp. 2286–2293, 2020. ofthe IEEE/CVFConferenceon Computer Visionand\n [31] Z. Xu, B. Qi, S. Agrawal, and S. Song, “Adagrasp: Pattern Recognition (CVPR), June 2023, pp. 13778–\n Learning an adaptive gripper-aware grasping policy,” 13790. \n in 2021 IEEE International Conference on Robotics [44] P.Sermanet,K.Xu,and S.Levine,“Unsupervisedper-\n and Automation(ICRA). IEEE,2021,pp.4620–4626. ceptualrewardsforimitationlearning,”ar Xivpreprint\n [32] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, ar Xiv:1612.06699, 2016.\n K. Black, N. Hirose, and S. Levine, “Vi NT: A Foun- [45] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and\n dation Model for Visual Navigation,” in 7 th Annual J.Bohg,“Concept 2 Robot:Learningmanipulationcon-\n Conference on Robot Learning (Co RL), 2023. ceptsfrominstructionsandhumandemonstrations,”in\n [33] Y.Liu,A.Gupta,P.Abbeel,and S.Levine,“Imitation Proceedings of Robotics: Science and Systems (RSS),\n from observation: Learning to imitate behaviors from 2020. \n raw video via context translation,” in 2018 IEEE [46] A.S.Chen,S.Nair,and C.Finn,“Learninggeneraliz-\n International Conferenceon Roboticsand Automation able robotic reward functions from “in-the-wild” hu-\n (ICRA). IEEE, 2018, pp. 1118–1125. man videos,” ar Xiv preprint ar Xiv:2103.16817, 2021.\n [34] T.Yu,C.Finn,S.Dasari,A.Xie,T.Zhang,P.Abbeel, [47] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and\n and S.Levine,“One-shotimitationfromobservinghu- X.Wang,“Graphinversereinforcementlearningfrom\n mans via domain-adaptive meta-learning,” Robotics: diverse videos,” in Conference on Robot Learning.\n Science and Systems XIV, 2018. PMLR, 2023, pp. 55–66. \n [35] P. Sharma, D. Pathak, and A. Gupta, “Third-person [48] M. Alakuijala, G. Dulac-Arnold, J. Mairal, J. Ponce,\n visual imitation learning via decoupled hierarchical and C.Schmid,“Learningrewardfunctionsforrobotic"
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n manipulation by observing humans,” in 2023 IEEE 2015. \n International Conferenceon Roboticsand Automation [62] D. Kappler, J. Bohg, and S. Schaal, “Leveraging big\n (ICRA). IEEE, 2023, pp. 5006–5012. data for grasp planning,” in ICRA, 2015, pp. 4304–\n [49] Y. Zhou, Y. Aytar, and K. Bousmalis, “Manipulator- 4311. \n independent representations for visual imitation,” [63] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan,\n 2021. X. Liu, J. A. Ojea, and K. Goldberg, “Dex-Net 2.0:\n [50] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu, Deep learning to plan robust grasps with synthetic\n Y. Zhu, and A. Anandkumar, “Mimicplay: Long- point clouds and analytic grasp metrics,” in Robotics:\n horizon imitation learning by watching human play,” Science and Systems (RSS), 2017.\n in Conference on Robot Learning, 2023. [64] A. Depierre, E. Dellandre´a, and L. Chen, “Jacquard:\n [51] K. Schmeckpeper, A. Xie, O. Rybkin, S. Tian, A large scale dataset for robotic grasp detection,” in\n K. Daniilidis, S. Levine, and C. Finn, “Learning pre- 2018 IEEE/RSJ International Conference on Intelli-\n dictive models from observation and interaction,” in gent Robots and Systems (IROS). IEEE, 2018, pp.\n European Conference on Computer Vision. Springer, 3511–3516. \n 2020, pp. 708–725. [65] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and\n [52] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and D. Quillen, “Learning hand-eye coordination for\n A.Gupta,“R 3 m:Auniversalvisualrepresentationfor robotic grasping with deep learning and large-scale\n robot manipulation,” in Co RL, 2022. data collection,” The International journal of robotics\n [53] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik, research, vol. 37, no. 4-5, pp. 421–436, 2018.\n “Masked visual pre-training for motor control,” ar Xiv [66] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Her-\n preprint ar Xiv:2203.06173, 2022. zog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan,\n [54] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Ma- V. Vanhoucke et al., “QT-Opt: Scalable deep rein-\n lik, and T. Darrell, “Real-world robot learning with forcement learning for vision-based robotic manipu-\n masked visual pre-training,” in Conference on Robot lation,” ar Xiv preprint ar Xiv:1806.10293, 2018.\n Learning, 2022. [67] S. Brahmbhatt, C. Ham, C. Kemp, and J. Hays,\n [55] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, “Contactdb: Analyzing and predicting grasp contact\n V. Kumar, and A. Zhang, “Vip: Towards universal via thermal imaging,” 04 2019.\n visual reward and representation via value-implicit [68] H.-S. Fang, C. Wang, M. Gou, and C. Lu, “Graspnet-\n pre-training,” ar Xiv preprint ar Xiv:2210.00030, 2022. 1 billion: a large-scale benchmark for general object\n [56] A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen, grasping,”in Proceedingsofthe IEEE/CVFconference\n S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik oncomputervisionandpatternrecognition,2020,pp.\n etal.,“Whereareweinthesearchforanartificialvi- 11444–11453. \n sualcortexforembodiedintelligence?”ar Xivpreprint [69] C. Eppner, A. Mousavian, and D. Fox, “ACRONYM:\n ar Xiv:2303.18240, 2023. A large-scale grasp dataset based on simulation,” in\n [57] S.Karamcheti,S.Nair,A.S.Chen,T.Kollar,C.Finn, 2021 IEEE Int. Conf. on Robotics and Automation,\n D. Sadigh, and P. Liang, “Language-driven represen- ICRA, 2020. \n tation learning for robotics,” Robotics: Science and [70] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kel-\n Systems (RSS), 2023. cey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor,\n [58] Y. Mu, S. Yao, M. Ding, P. Luo, and C. Gan, “EC 2: K. Konolige, S. Levine, and V. Vanhoucke, “Using\n Emergent communication for embodied control,” in simulation and domain adaptation to improve effi-\n Proceedings of the IEEE/CVF Conference on Com- ciency of deep robotic grasping,” in ICRA, 2018, pp.\n puter Visionand Pattern Recognition,2023,pp.6704– 4243–4250. \n 6714. [71] X. Zhu, R. Tian, C. Xu, M. Huo, W. Zhan,\n [59] S. Bahl, R. Mendonca, L. Chen, U. Jain, and M. Tomizuka, and M. Ding, “Fanuc manipulation:\n D. Pathak, “Affordances from human videos as a A dataset for learning-based manipulation with fanuc\n versatile representation for robotics,” in Proceedings mate 200 i D robot,” https://sites.google.com/berkeley.\n ofthe IEEE/CVFConferenceon Computer Visionand edu/fanuc-manipulation, 2023. \n Pattern Recognition, 2023, pp. 13778–13790. [72] K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez,\n [60] Y. Jiang, S. Moseson, and A. Saxena, “Efficient “More than a million ways to be pushed. a high-\n grasping from RGBD images: Learning using a new fidelity experimental dataset of planar pushing,” in\n rectangle representation,” in 2011 IEEE International 2016 IEEE/RSJinternationalconferenceonintelligent\n conference on robotics and automation. IEEE, 2011, robots and systems (IROS). IEEE, 2016, pp. 30–37.\n pp. 3304–3311. [73] C.Finnand S.Levine,“Deepvisualforesightforplan-\n [61] L. Pinto and A. K. Gupta, “Supersizing self- ning robot motion,” in 2017 IEEE International Con-\n supervision: Learning to grasp from 50 k tries and ference on Robotics and Automation (ICRA). IEEE,\n 700 robothours,”2016 IEEEInternational Conference 2017, pp. 2786–2793. \n on Robotics and Automation (ICRA), pp. 3406–3413, [74] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and"
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n S. Levine, “Visual foresight: Model-based deep rein- E. Orbay, S. Savarese, and L. Fei-Fei, “Robo Turk:\n forcement learning for vision-based robotic control,” A crowdsourcing platform for robotic skill learning\n ar Xiv preprint ar Xiv:1812.00568, 2018. through imitation,” Co RR, vol. abs/1811.02790, 2018.\n [75] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser, [Online]. Available: http://arxiv.org/abs/1811.02790\n “Theprincetonshapebenchmark,”in Shape Modeling [87] P. Sharma, L. Mohan, L. Pinto, and A. Gupta, “Mul-\n Applications, 2004, pp. 167–388. tiple interactions made easy (MIME): Large scale\n [76] W. Wohlkinger, A. Aldoma Buchaca, R. Rusu, and demonstrations data for imitation,” in Conference on\n M. Vincze, “3 DNet: Large-Scale Object Class Recog- robot learning. PMLR, 2018, pp. 906–915.\n nitionfrom CADModels,”in IEEEInternational Con- [88] A. Mandlekar, J. Booher, M. Spero, A. Tung,\n ference on Robotics and Automation (ICRA), 2012. A. Gupta, Y. Zhu, A. Garg, S. Savarese, and L. Fei-\n [77] A. Kasper, Z. Xue, and R. Dillmann, “The kit object Fei, “Scaling robot supervision to hundreds of hours\n modelsdatabase:Anobjectmodeldatabaseforobject with Robo Turk:Roboticmanipulationdatasetthrough\n recognition, localization and manipulation in service human reasoning and dexterity,” in 2019 IEEE/RSJ\n robotics,” The International Journal of Robotics Re- International Conference on Intelligent Robots and\n search, vol. 31, no. 8, pp. 927–934, 2012. Systems (IROS). IEEE, 2019, pp. 1048–1055.\n [78] A. Singh, J. Sha, K. S. Narayan, T. Achim, and [89] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher,\n P. Abbeel, “Big BIRD: A large-scale 3 D database of G. Georgakis, K. Daniilidis, C. Finn, and S. Levine,\n object instances,” in IEEE International Conference “Bridgedata:Boostinggeneralizationofroboticskills\n on Robotics and Automation (ICRA), 2014, pp. 509– withcross-domaindatasets,”in Robotics:Scienceand\n 516. Systems (RSS) XVIII, 2022. \n [79] B. Calli, A. Walsman, A. Singh, S. Srinivasa, [90] A.Mandlekar,D.Xu,J.Wong,S.Nasiriany,C.Wang,\n P. Abbeel, and A. M. Dollar, “Benchmarking in ma- R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and\n nipulation research: Using the Yale-CMU-Berkeley R. Mart´ın-Mart´ın, “What matters in learning from\n object and model set,” IEEE Robotics & Automation offlinehumandemonstrationsforrobotmanipulation,”\n Magazine, vol. 22, no. 3, pp. 36–52, 2015. in ar Xiv preprint ar Xiv:2108.03298, 2021.\n [80] Zhirong Wu,S.Song,A.Khosla,Fisher Yu,Linguang [91] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker,\n Zhang, Xiaoou Tang, and J. Xiao, “3 D Shape Nets: A R. Baruch, T. Armstrong, and P. Florence, “Interac-\n deep representation for volumetric shapes,” in IEEE tive language: Talking to robots in real time,” IEEE\n Conference on Computer Vision and Pattern Recogni- Robotics and Automation Letters, 2023.\n tion (CVPR), 2015, pp. 1912–1920. [92] H.-S.Fang,H.Fang,Z.Tang,J.Liu,J.Wang,H.Zhu,\n [81] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, and C. Lu, “RH 20 T: A robotic dataset for learning\n R. Mottaghi, L. Guibas, and S. Savarese, “Object- diverse skills in one-shot,” in RSS 2023 Workshop on\n Net 3 D: A large scale database for 3 d object recog- Learning for Task and Motion Planning, 2023.\n nition,” in European Conference on Computer Vision [93] H.Bharadhwaj,J.Vakil,M.Sharma,A.Gupta,S.Tul-\n (ECCV). Springer, 2016, pp. 160–176. siani, and V. Kumar, “Robo Agent: Towards sample\n [82] D. Morrison, P. Corke, and J. Leitner, “Egad! an efficient robot manipulation with semantic augmenta-\n evolved grasping analysis dataset for diversity and re- tions and action chunking,” arxiv, 2023.\n producibility in robotic manipulation,” IEEE Robotics [94] M. Heo, Y. Lee, D. Lee, and J. J. Lim, “Furni-\n and Automation Letters, vol. 5, no. 3, pp. 4368–4375, turebench: Reproducible real-world benchmark for\n 2020. long-horizoncomplexmanipulation,”in Robotics:Sci-\n [83] R. Gao, Y.-Y. Chang, S. Mall, L. Fei-Fei, and J. Wu, ence and Systems, 2023.\n “Object Folder: A dataset of objects with implicit vi- [95] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du,\n sual, auditory, and tactile representations,” in Confer- C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong,\n ence on Robot Learning, 2021, pp. 466–476. A. He, V. Myers, K. Fang, C. Finn, and S. Levine,\n [84] L.Downs,A.Francis,N.Koenig,B.Kinman,R.Hick- “Bridgedatav 2:Adatasetforrobotlearningatscale,”\n man, K. Reymann, T.B. Mc Hugh, and V. Vanhoucke, 2023. \n “Googlescannedobjects:Ahigh-qualitydatasetof 3 D [96] T. Winograd, “Understanding natural language,”\n scannedhouseholditems,”in 2022 International Con- Cognitive Psychology, vol. 3, no. 1, pp. 1–191,\n ference on Robotics and Automation (ICRA). IEEE, 1972. [Online]. Available: https://www.sciencedirect.\n 2022, pp. 2553–2560. com/science/article/pii/0010028572900023\n [85] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swan- [97] M. Mac Mahon, B. Stankiewicz, and B. Kuipers,\n son, R. Jonschkowski, C. Finn, S. Levine, and “Walk the talk: Connecting language, knowledge, and\n K.Hausman,“MT-Opt:Continuousmulti-taskrobotic action in route instructions,” in Proceedings of the\n reinforcement learning at scale,” ar Xiv preprint Twenty-First AAAI Conference on Artificial Intelli-\n ar Xiv:2104.08212, 2021. gence, 2006. \n [86] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, [98] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward\n M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, understanding natural language directions,” in 2010"
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n 5 th ACM/IEEE International Conference on Human- [112] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and\n Robot Interaction (HRI), 2010, pp. 259–266. L. Fei-Fei, “Vox Poser: Composable 3 d value maps\n [99] D. L. Chen and R. J. Mooney, “Learning to interpret forroboticmanipulationwithlanguagemodels,”ar Xiv\n naturallanguagenavigationinstructionsfromobserva- preprint ar Xiv:2307.05973, 2023.\n tions,” in Proceedings of the Twenty-Fifth AAAI Con- [113] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What\n ference on Artificial Intelligence, 2011, p. 859–865. and where pathways for robotic manipulation,” in\n [100] F. Duvallet, J. Oh, A. Stentz, M. Walter, T. Howard, Conference on Robot Learning. PMLR, 2022, pp.\n S. Hemachandra, S. Teller, and N. Roy, “Inferring 894–906. \n maps and behaviors from natural language instruc- [114] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H.\n tions,” in International Symposium on Experimental Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia,\n Robotics (ISER), 2014. C.Finnetal.,“Open-worldobjectmanipulationusing\n [101] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foer- pre-trained vision-language models,” ar Xiv preprint\n ster, J. Andreas, E. Grefenstette, S. Whiteson, and ar Xiv:2303.00905, 2023.\n T. Rockta¨schel, “A survey of reinforcement learning [115] Y. Mu, Q. Zhang, M. Hu, W. Wang, M. Ding, J. Jin,\n informed by natural language,” in IJCAI, 2019. B.Wang,J.Dai,Y.Qiao,and P.Luo,“Embodied GPT:\n [102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, Vision-language pre-training via embodied chain of\n C. Baral, and H. Ben Amor, “Language-conditioned thought,” ar Xiv preprint ar Xiv:2305.15021, 2023.\n imitation learning for robot manipulation tasks,” Ad- [116] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and\n vances in Neural Information Processing Systems, A. Courville, “Film: Visual reasoning with a general\n vol. 33, pp. 13139–13150, 2020. conditioning layer,” 2017. \n [103] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn [117] M. Tan and Q. Le, “Efficient Net: Rethinking model\n et al., “Learning language-conditioned robot behavior scaling for convolutional neural networks,” in Inter-\n from offline data and crowd-sourced annotation,” in national conference on machine learning. PMLR,\n Conference on Robot Learning. PMLR, 2022, pp. 2019, pp. 6105–6114. \n 1303–1315. [118] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\n [104] O. Mees, L. Hermann, E. Rosete-Beas, and L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n W. Burgard, “CALVIN: A benchmark for language- “Attention is all you need,” Advances in neural infor-\n conditioned policy learning for long-horizon robot mation processing systems, vol. 30, 2017.\n manipulation tasks,” IEEE Robotics and Automation [119] S. Ramos, S. Girgin, L. Hussenot, D. Vincent,\n Letters, 2022. H. Yakubovich, D. Toyama, A. Gergely, P. Stanczyk,\n [105] O.Mees,L.Hermann,and W.Burgard,“Whatmatters R. Marinier, J. Harmsen, O. Pietquin, and N. Mom-\n in language conditioned robotic imitation learning chev,“RLDS:anecosystemtogenerate,shareanduse\n over unstructured data,” IEEE Robotics and Automa- datasets in reinforcement learning,” 2021.\n tion Letters, vol. 7, no. 4, pp. 11205–11212, 2022. [120] D. Cer, Y. Yang, S. yi Kong, N. Hua, N. Limti-\n [106] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver- aco, R. S. John, N. Constant, M. Guajardo-Cespedes,\n actor: A multi-task transformer for robotic manipu- S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and\n lation,” Conference on Robot Learning (Co RL), 2022. R. Kurzweil, “Universal sentence encoder,” 2018.\n [107] F. Hill, S. Mokra, N. Wong, and T. Harley, “Human [121] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa,\n instruction-following with deep reinforcement learn- S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman,\n ing via transfer-learning from text,” ar Xiv preprint X. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz,\n ar Xiv:2005.09382, 2020. M.Lucic,M.Tschannen,A.Nagrani,H.Hu,M.Joshi,\n [108] C. Lynch and P. Sermanet, “Grounding language in B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter,\n play,” Robotics: Science and Systems (RSS), 2021. A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters,\n [109] M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee,\n B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu,\n A.Herzogetal.,“Doas Ican,notas Isay:Grounding K. Rong, A. Kolesnikov, M. Seyedhosseini, A. An-\n languageinroboticaffordances,”Conferenceon Robot gelova, X. Zhai, N. Houlsby, and R. Soricut, “Pali-\n Learning (Co RL), 2022. x: On scaling up a multilingual vision and language\n [110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, model,” 2023. \n Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and [122] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,\n L. Fan, “VIMA: General robot manipulation with Y. Hasson, K. Lenc, A. Mensch, K. Millican,\n multimodal prompts,” International Conference on M.Reynolds,R.Ring,E.Rutherford,S.Cabi,T.Han,\n Machine Learning (ICML), 2023. Z. Gong, S. Samangooei, M. Monteiro, J. Menick,\n [111] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, S. Borgeaud, A. Brock, A. Nematzadeh, S. Shar-\n “Chat GPT for robotics: Design principles and model ifzadeh, M. Binkowski, R. Barreira, O. Vinyals,\n abilities,” Microsoft Auton. Syst. Robot. Res, vol. 2, A. Zisserman, and K. Simonyan, “Flamingo: a visual\n p. 20, 2023. language model for few-shot learning,” 2022."
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n [123] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, \n A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, \n Q.Vuong,T.Yu,W.Huang,Y.Chebotar,P.Sermanet, \n D.Duckworth,S.Levine,V.Vanhoucke,K.Hausman, \n M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and \n P. Florence, “Pa LM-E: An embodied multimodal lan- \n guage model,” 2023. \n [124] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis- \n senborn, X. Zhai, T. Unterthiner, M. Dehghani, \n M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, \n and N. Houlsby, “An image is worth 16 x 16 words: \n Transformers for image recognition at scale,” 2021. \n [125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, \n X.Wang,H.W.Chung,S.Shakeri,D.Bahri,T.Schus- \n ter,H.S.Zheng,D.Zhou,N.Houlsby,and D.Metzler, \n “UL 2: Unifying language learning paradigms,” 2023. \n [126] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, \n and W.Burgard,“Latentplansfortaskagnosticoffline \n reinforcement learning,” in Proceedings of the 6 th \n Conference on Robot Learning (Co RL), 2022. \n [127] O. Mees, J. Borja-Diaz, and W. Burgard, “Grounding \n language with visual affordances over unstructured \n data,” in Proceedings of the IEEE International Con- \n ference on Robotics and Automation (ICRA), London, \n UK, 2023. \n [128] S. Dass, J. Yapeter, J. Zhang, J. Zhang, K. Pertsch, \n S. Nikolaidis, and J. J. Lim, “CLVR jaco play \n dataset,” 2023. [Online]. Available: https://github. \n com/clvrai/clvr jaco play dataset \n [129] J. Luo, C. Xu, X. Geng, G. Feng, K. Fang, L. Tan, \n S. Schaal, and S. Levine, “Multi-stage cable rout- \n ing through hierarchical imitation learning,” ar Xiv \n preprint ar Xiv:2307.08927, 2023. \n [130] J. Pari, N. M. Shafiullah, S. P. Arunachalam, and \n L. Pinto, “The surprising effectiveness of representa- \n tion learning for visual imitation,” 2021. \n [131] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: \n Imitation learning for vision-based manipulation with \n object proposal priors,” 2023. \n [132] L. Y. Chen, S. Adebola, and K. Goldberg, “Berkeley \n UR 5 demonstration dataset,” https://sites.google.com/ \n view/berkeley-ur 5/home. \n [133] G. Zhou, V. Dean, M. K. Srirama, A. Rajeswaran, \n J. Pari, K. Hatch, A. Jain, T. Yu, P. Abbeel, L. Pinto, \n C. Finn, and A. Gupta, “Train offline, test online: A \n real robot learning benchmark,” 2023. \n [134] “Task-agnostic real world robot play,” https://www. \n kaggle.com/datasets/oiermees/taco-robot. \n \n \n \n \n \n \n \n "
  }
]