[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n GS-LTS: 3 D Gaussian Splatting-Based Adaptive Modeling \n for Long-Term Service Robots \n \n \n Bin Fu 1 and Jialin Li 1 and Bin Zhang 1 and Ruiping Wang 1,(cid:12) and Xilin Chen 1\n \n \n Abstract‚Äî3 D Gaussian Splatting (3 DGS) has garnered sig- \n nificant attention in robotics for its explicit, high fidelity \n dense scene representation, demonstrating strong potential for \n roboticapplications.However,3 DGS-basedmethodsinrobotics \n primarily focus on static scenes, with limited attention to the \n dynamic scene changes essential for long-term service robots. \n These robots demand sustained task execution and efficient \n scene updates‚Äîchallenges current approaches fail to meet. \n To address these limitations, we propose GS-LTS (Gaussian \n Splatting for Long-Term Service), a 3 DGS-based system en- \n abling indoor robots to manage diverse tasks in dynamic \n environments over time. GS-LTS detects scene changes (e.g., \n object addition or removal) via single-image change detection, \n employs a rule-based policy to autonomously collect multi- \n Addition Removal Relocation \n view observations, and efficiently updates the scene represen- \n tation through Gaussian editing. Additionally, we propose a Fig.1. Threecommontypesofscenechangesinindoorscenes.\n simulation-basedbenchmarkthatautomaticallygeneratesscene \n objects may be added, removed, or relocated over time. In\n changedataascompactconfigurationscripts,providingastan- \n dardized, user-friendly evaluation benchmark. Experimental such environments, the robot must continuously observe the\n results demonstrate GS-LTS‚Äôs advantages in reconstruction, scene, autonomously detect changes, and update its scene\n navigation, and superior scene updates‚Äîfaster and higher representation to maintain accuracy.\n quality than the image training baseline‚Äîadvancing 3 DGS \n A straightforward approach to handling scene changes\n for long-term robotic operations. Code and benchmark are \n available at: https://vipl-vsu.github.io/3 DGS-LTS would be to periodically recollect images and retrain or\n fine-tunethe 3 DGSrepresentationwhenevertheenvironment\n I. INTRODUCTION is modified. However, this method is computationally ex-\n 3 D Gaussian Splatting (3 DGS) [1] is an explicit radiance pensive, requiring frequent reprocessing of large-scale data,\n field representation based on 3 D Gaussians. It has been and lacks efficiency for real-time or long-term deployment.\n widely applied in fields such as dense visual SLAM [2] and To address this task, we propose GS-LTS, a 3 DGS-based\n 3 D reconstruction [3], benefiting from its explicit geometric system designed for Long-Term Service robots in indoor\n structure and real-time high-quality rendering. By further environments. The GS-LTS framework integrates four key\n embedding low-dimensional vision-semantic features into modules: (1) Gaussian Mapping Engine, which constructs\n each 3 DGaussian[4],acomprehensivescenerepresentation asemantic-aware 3 DGSrepresentation,integratinggeometry,\n integrating geometry, vision, andsemantics can be achieved, visualappearance,andsemantics;(2)Multi-Task Executor,\n which shows great potential in robotics applications, such which helps robots perform downstream tasks like object\n as navigation and instruction following. However, current navigation using the informative 3 DGS representation; (3)\n 3 DGSattemptsinthesefieldsprimarilyfocusonstaticscenes Change Detection Unit, a long-running module that detects\n [5], which fail to align with the dynamic nature of real- scene changes at a specified frequency by comparing the\n world environments involving object changes, as illustrated robot‚Äôs current RGB observations with historical 3 DGS-\n in Fig. 1, making these approaches inadequate for long- rendered images, locating altered regions and analyzing\n term service robots working in dynamic settings. A more change types and positions; and (4) Active Scene Updater,\n realistic scenario involves a robot utilizing a prebuilt 3 DGS which is guided by a rule-based policy, directs the robot to\n representation to perform tasks in an environment where collectmulti-viewimagesarounddetectedareas,andapplies\n pre-editing and fine-tuning to dynamically update the 3 DGS\n *This work is partially supported by National Key R&D Program of representation based on the detect change type and new\n China No.2021 ZD 0111901,and Natural Science Foundationof Chinaunder \n observations. Together, these components enable the robot\n contracts Nos.62495082,U 21 B 2025. \n 1 The authors are with the Key Laboratory of AI Safety of CAS, to adapt to evolving surroundings while maintaining robust\n Instituteof Computing Technology,Chinese Academyof Sciences(CAS), performance over extended periods.\n Beijing,100190,China,andalsowiththe Universityof Chinese Academy \n Evaluating this system places significant demands on\n of Sciences, Beijing, 100049, China. {bin.fu, jialin.li, \n bin.zhang}@vipl.ict.ac.cn, {wangruiping, both data availability and environmental support. Real-\n xlchen}@ict.ac.cn world settings struggle to support both robotic task execu-\n (cid:12)Correspondingauthor. tion and extensive variations, hindering large-scale dataset\n 5202 \n ra M \n 22 \n ]OR.sc[ \n 1 v 33771.3052:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n creation and standardized evaluation. To address this, we Gaussians [11]. This adaptability makes 3 DGS suited for\n propose a simulation-based benchmark that supports task modeling dynamic environments. Leveraging these proper-\n execution and policy learning via 3 DGS representations ties, this work explores the integration of 3 DGS with long-\n while enabling systematic generation of large-scale scene term service robot systems operating in dynamic settings.\n changedatathroughobjectinteractions.Thisbenchmarknot \n only facilitates large-scale evaluation but also serves as a B. Long-Term Robot Autonomy and Change Detection\n bridge for sim-to-real transfer, allowing models trained in \n Long-Term Autonomy (LTA) is a critical research area\n simulation to achieve enhanced performance in real-world \n in robotics, aimed at enabling robots to operate reliably\n environments. Our approach features two innovations: (1) \n in complex environments over extended periods [12]. This\n automated generation of customizable scene change data, \n capability is essential across various domains, including\n combining objects (e.g., cups), containers (e.g., tables), and \n underwater exploration [13], and service robotics [14]. A\n positions to produce diverse scene change tasks; and (2) \n major challenge in LTA is adapting to scene changes.\n storing scene change setups and environment metadata in \n Our work focuses on medium-term changes [12] in indoor\n configuration scripts, which ensures efficient storage, easy \n service environments, where robots must effectively model\n configuration,andaccuratereproductionofscenes.Thisscal- \n and update representations of daily object variations. While\n able, reproducible benchmark reduces data acquisition costs \n many LTA robotic systems have been deployed in service\n and provides standardized evaluation, advancing research on \n scenarios [14], [15], our work introduces a novel approach\n 3 DGS adaptability in dynamic environments. \n leveraging 3 DGS for scene representation to enable efficient\n We conduct extensive validation of the GS-LTS system \n adaptation to dynamic environments. \n through a series of experiments. First, we evaluate scene \n Scenechangedetectionisakeyresearchareaincomputer\n representation quality via image rendering for visual fidelity \n vision, aiming to identify scene changes such as object\n and 3 D localization for semantic accuracy. Additionally, ob- \n appearance, disappearance, or modifications. It is broadly\n jectnavigationresultsonanexistingbenchmark[6]highlight \n classified into 2 D and 3 D approaches based on data type.\n the potential of 3 DGS for embodied tasks. Finally, on our \n 2 D change detection employs pairs of before-and-after RGB\n custom Scene Change Adaptation Benchmark, we compare \n images [16], leveraging models from CNNs to foundation\n our Gaussian editing-based method with the baseline of \n models for feature extraction and change identification.\n direct image fine-tuning. Our approach significantly reduces \n Conversely, 3 D change detection incorporates spatial infor-\n scene update time while enhancing update quality. These \n mation, relying on multi-view RGB images [17] or point\n comprehensive experiments fully demonstrate the efficiency \n clouds [18]. Recent advances in 3 DGS-based novel-view\n and robustness of the GS-LTS system in scene reconstruc- \n synthesis [19] have demonstrated strong potential, whereas\n tion, embodied applications, and scene adaptability. \n our GS-LTS system adopts a distinct approach, leveraging a\n Insummary,thisworkintroduces GS-LTS,deliveringthree \n single egocentric RGB image for change detection to reduce\n key contributions: \n data and computational demands. \n ‚Ä¢ A 3 DGS-basedsystemenablingindoorrobotstohandle \n diverse tasks in dynamic environments over time. III. SYSTEMOVERVIEW \n ‚Ä¢ An automatic framework for object-level change detec- \n In this section, we first introduce the task formulation\n tion and adaptive scene update via Gaussian editing. \n for long-term service robot system working in dynamic\n ‚Ä¢ A scalable method for constructing a simulation bench- \n environmentsbasedon 3 DGaussian Splatting(3 DGS)scene\n mark for object-level scene change detection. \n representation. Subsequently, we present the overall frame-\n Together, these advancements enhance 3 DGS applications \n work of our proposed system designed to address this task.\n for long-term robotic operations in dynamic environments. \n II. RELATEDWORK A. Task Formulation \n A. 3 D Scene Representation The core objectives of the task are twofold: (1) in a\n Building accurate scene representations is crucial for dynamic environment, construct a 3 DGS representation of\n robotics, with various methods, such as semantic maps [7], the scene and utilize this representation to control the robot\n SLAM [8], and Ne RF [9] being widely used. Compared inaccomplishingdownstreamembodiedtasks,suchasobject\n to these representations, 3 D Gaussian Splatting (3 DGS) navigation; (2) enable the robot to detect object changes in\n provides an explicit, high-fidelity, and real-time renderable thesceneandautonomouslycollectdatatoupdatethe 3 DGS\n dense representation. Its ability to simultaneously encode representation. \n geometric, visual, and semantic information has driven its We focus on dynamic indoor settings where primary\n adoptionintaskssuchas 3 Dreconstruction[3],3 DGS-based structures (e.g., room layouts, large furniture like cabinets\n SLAM [2], and navigation [10]. However, most existing andrefrigerators)staystatic,whilecertainobjects(e.g.,cups,\n applications are restricted to static environments [5], where laptops)exhibitperiodicchanges.Weconsiderthreetypesof\n maps quickly become outdated in the face of scene changes. scene changes: relocation, addition, or removal of objects,\n Akeyadvantageof 3 DGSisitsinherentlyeditablenature, whichencompassthepredominantformsofobjectdynamics\n enabling dynamic updates through direct modifications of in real world environments."
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n Gaussian Mapping Engine Multi-Task Executor \n Semantic Map ((ùê∂+2)√óùëÄ√óùëÄ) \n SAM CLIP obstacles \n Segmentation category-wise \n binary grids \n Object Nav. \n Autoencoder \n Encoder Decoder \n RGB-D Position Feature \n 3 D Gaussians ùë† ùúô!\"#,ùúô$! \n Query: \n Pose GS-LTS Coffee Machine 3 D Localization \n Change Detection Unit Long-Term Active Scene Updater \n Adaptive Modeling \n Pixel Diff. Original \n Egocentric \n Gaussians Removal \n RGB obs. Scene Change \n Mask \n ? Addition \n 3 DGS Change Type Relocation \n rendering Change Position \n Feature Diff. Active Data Collecting \n Fig. 2. System Overview. GS-LTS is a modular system designed for long-term service robots, which can adapt to object changes in the dynamic\n environmentsandupdatethe 3 DGSrepresentationthroughperiodic,automatedoperationofthe Change Detection Unitandthe Active Scene Updater.\n During task execution, the robot can only access current methods such as object navigation.\n RGB-Ddataandposesfromtheenvironment.Consequently, Change Detection Unit. Since the robot must au-\n the robot must rely on single egocentric observations to tonomously detect scene changes relying solely on single\n actively perform change detection and determine whether egocentric observations, we develop the Change Detection\n objects in the scene have changed. Upon detecting scene Unit, a lightweight module designed for long-term standby\n alterations, the robot is further required to autonomously running in parallel with other downstream embodied tasks.\n collect data to update the 3 DGS scene representation. This module compares the robot‚Äôs current RGB observation\n with a rendered image generated from the 3 DGS at the\n B. GS-LTS System corresponding pose. By employing a dual-branch strategy\n To address the aforementioned challenges, we propose that analyzes both pixel-level differences and feature-level\n GS-LTS, a 3 DGS-based system tailored for Long-Term differences, the module effectively identifies the type and\n Servicerobotsoperatinginindoorenvironments.Thesystem location of changes between the two compared frames.\n integrates four key modules as shown in Fig. 2. In the fol- Active Scene Updater. Upon detecting scene changes\n lowing, we provide an overview of the system‚Äôs operational and their locations, the Active Scene Updater module au-\n workflow,withdetailsofeachmoduleelaboratedin Sec.IV. tonomously collects data and updates the long-term scene\n Gaussian Mapping Engine. This module is tasked with representation.Initially,therobotfollowsarule-basedheuris-\n generating the 3 DGS scene representation for the robot tic policy to navigate around the scene change region,\n duringthesystem‚Äôsinitializationphase.Givenasetofmulti- capturing multi-view images. Next, it applies 3 DGS editing\n view RGB images, depth maps, and their corresponding strategies to edit the target region. Finally, the 3 DGS repre-\n camera poses, the module trains a 3 DGS model to effec- sentationisrefinedbyfine-tuningwithcollectedimages.This\n tively capture the scene‚Äôs geometric and visual character- moduleenables GS-LTStoperformsceneupdatesefficiently\n istics. In addition, to incorporate semantic information, we with minimal computational overhead.\n leverage the Segment Anything Model (SAM) [20] and \n IV. METHODOLOGY \n open-vocabulary vision-language models (e.g., CLIP [21]) \n to embed semantic features into the 3 DGS representation. In this section, we introduce the implementation and\n Multi-Task Executor. This module serves as an interface technical details of the GS-LTS system.\n between the dense 3 DGS representation and downstream \n A. 3 DGS Mapping Engine \n tasks, enabling the robot to leverage the high-fidelity scene \n information encoded in the 3 DGS for task planning and In this module, we employ semantic-aware 3 D Gaussian\n execution. For instance, by matching textual features with Splatting (3 DGS) for scene reconstruction. 3 DGS provides\n 3 D Gaussian semantic features, this module facilitates 3 D an explicit scene representation through anisotropic Gaus-\n localization for arbitrary text queries. Additionally, it adapts sians characterized by center position ¬µ ‚àà R 3, covariance\n the 3 DGSintoa 2 Dsemanticmapformattosupportexisting matrix Œ£ ‚àà R 3√ó3, opacity o ‚àà R, and color c ‚àà SH"
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n represented by spherical harmonics. Through differentiable semantic point cloud is voxelized and flattened along the Z-\n rendering,3 DGSsynthesizespixelcolorsviaalphablending: axistoforma 2 Dsemanticmap,enablingpathplanningand\n navigation to target categories. \n N i‚àí1 \n (cid:88) (cid:89) \n C = c Œ± (1‚àíŒ± ), (1) \n i i j \n C. Change Detection Unit \n i=1 j=1 \n where i ‚â• 2 and Œ± depends on o and the projected 2 D Asmentionedin Sec.III-A,thismoduleisdesignedto:(1)\n i i \n Gaussian‚Äôs contribution to the current pixel. detect and classify three types of scene changes; (2) localize\n We refer [4] to extend 3 DGS for semantic fields. First, the target position p world in world coordinates.\n construct the vanilla 3 DGS scene representation. Second, 1) 3 DGS-based Change Detection: The most intuitive\n embedsemanticfeaturesbyleveraging SAM[20]togenerate method for detecting scene change is to analyze the discrep-\n multi-level masks {Ml}3 and extract high-dimensional anciesbetweenthereal-worldimageandthe 3 DGSrendered\n l=1 \n CLIP [21] features Fl = CLIP(I ‚äô Ml) ‚àà RD. An image,although 3 DGSenablesphoto-realisticimagerender-\n autoencoder is used to compresses these features into low- ing, there often exist pixel-wise errors with the real-world\n dimensional semantic features Sl = Encoder(Fl) ‚àà Rd observation,makingthedirectcomputationofabsolutepixel\n while preserving semantics through reconstruction regular- differencesbetweenthetwoimagesyieldsub-optimalresults.\n ization FÀÜl = Decoder(Sl). The low-dimensional semantic Therefore, following the practice of [19], we employ a\n features are used to supervise the Gaussians‚Äô semantic at- dual-branch strategy for scene change detection. As shown\n tribute sl ‚ààRd using view-consistent alpha blending: in Fig.2,giventhethereal-worldcameracapturedimage I real\n and 3 DGS rendered image I from the current viewpoint,\n N i‚àí1 GS \n SÀÜl = (cid:88) slŒ± (cid:89) (1‚àíŒ± ). (2) wefirstcalculatethesumofabsolutepixeldifferencesacross\n i i j all 3 channels between the two images, which is truncated\n i=1 j=1 \n via threshold œÑ to obtain the pixel-level binary mask:\n GS \n Thus, we obtain the semantic-aware 3 DGS representation \n (cid:18) (cid:19) \n G = {g }N which enables explicit 3 D semantic repre- (cid:88)3 \n i i=1 M = |I ‚àíI |>œÑ . (4) \n sentation while maintaining real-time rendering capabilities pixel chn=1 real,chn GS,chn GS\n through the Gaussian representation. \n Next, we compute normalized Efficient SAM [22] feature\n B. GS Multi-Task Executor maps I and I thatrobustlyrepresentsignificant\n real,SAM GS,SAM \n regions, then calculate their cosine similarity truncated by\n Below, we illustrate how the 3 DGS representation can be \n œÑ to obtain the feature-level binary mask:\n applied to robotic tasks, including 3 D localization and ob- feat \n ject navigation, which are critical capabilities for numerous \n M =(‚ü®I ,I ‚ü©>œÑ ). (5) \n applications and serve to assess the geometric and semantic feat GS,SAM real,SAM feat\n accuracy of 3 DGS scene representation. \n Finally, the combined binary mask can be obtained using\n 1) Semantic Querying and Localization: For an arbitrary \n pixel-by-pixelmultiplicationofthedual-branchbinarymasks\n textquery,therelevancescorer(œï ,œï )betweenthe CLIP \n embeddingœï andthesemanticf q e r a y tur g e i œï =Decoder(sl) M comb =M pixel ‚äôM feat . We hypothesize that when the total\n qry gi i area of M \n comb \n exceeds the threshold œÑ \n change \n , a scene change\n of each 3 D Gaussian is defined as: \n occurs, thereby triggering scene change prediction.\n min exp(œï gi ¬∑œï qry ) , (3) 2) Scene Change Prediction: We posit that all potential\n (cid:16) (cid:17) \n j exp(œï ¬∑œï )+exp œï ¬∑œïj change regions reside within M comp , where noise areas con-\n gi qry gi canon \n stitute a small portion. Therefore, we first extract connected\n where œïj canon is the CLIP embedding of a predefined set components {R i }N i=1 from M comp , sorted in descending\n of canonical phrases, including ‚Äúobject‚Äù, ‚Äúthings‚Äù, ‚Äústuff‚Äù, order by their area. Based on the distinct change types, we\n and ‚Äútexture‚Äù. The localization of the query is achieved by hypothesize that: (i) relocation operations are geometrically\n calculating the bounding box of matched Gaussians {g | constrained to the first two largest connected components\n i \n r(œï qry ,œï gi ) > œÑ sim }, where œÑ sim is a predefined threshold. (R 1 and R 2 ), while (ii) addition/removal operations mani-\n Duetothelargenumberof 3 DGaussiansinthescene,sparse fest exclusively within the dominant connected components\n samplingisappliedinpracticetoperformsemanticquerying. (R 1 ). We first formulate the following dual-region matching\n 2) 2 D Semantic Mapping and Navigation: The 3 DGS criterion to identify relocation operation:\n representationcanbeseamlesslyconvertedintoa 2 Dseman- \n min(A(R ),A(R )) \n tic map, ensuring compatibility with existing navigation and Œì match = max(A(R 1 ),A(R 2 )) >œÑ a (6)\n pathplanningmethods.The 2 Dsemanticmapisrepresented 1 2 \n (cid:124) (cid:123)(cid:122) (cid:125)\n as a (L+2)√óM√óM matrix, where M√óM represents the Areasimilarity \n map size, L is the number of semantic categories, and the ‚àß ‚à•Œ∑(R )‚àíŒ∑(R )‚à• <œÑ ,\n 1 2 2 d \n additionallayersrepresentobstaclesandexploredarea.For L (cid:124) (cid:123)(cid:122) (cid:125)\n Spatialdistance \n navigation-relevant categories, we assign each 3 D Gaussian \n the category with the highest relevance score. The resulting where A(¬∑)denotesregionarea,Œ∑(¬∑)forcentroidcoordinates."
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n Then, for the largest connected components R , we com- where œÄ(¬∑) is the projection function, the target Gaussians\n 1 \n pute its centroid p =(u ,v ), and sample depth from real- are selected based on the proportion œÅ of its presence within\n c c c \n world depth map D and 3 DGS rendered depth map D : the mask region: G ={g |V(g )>œÅ¬∑K}.\n cam GS target i i \n (cid:40) The workflow of our pre-editing method is as follows:\n d =D (p ) \n real real c (7) For addition: \n d =D (p ) \n GS GS c i. Generate object points P = {p } via depth map\n add j \n The change type is determined through depth difference: {D real,k ‚äôM comb,k }K k=1 .\n ii. Pass new semantic feature s . \n Ô£± manual \n Ô£¥Ô£≤ Addition, ‚àÜd<‚àíœµ iii. Find nearest neighbors and inherit attributes:\n ‚àÜd=d real ‚àíd GS ‚áíType= Removal, ‚àÜd>œµ (8) a) For each p j ‚ààP add : g n j n =argmin‚à•¬µ m ‚àíp j ‚à• 2 ,\n Ô£¥Ô£≥Unchanged, \n |‚àÜd|‚â§œµ gm‚ààG \n b) Extract covariance matrix Œ£j , opacity oj and color\n nn nn \n To obtain p world , here we construct a pseudo depth map cj nn from g n j n .\n D (u,v) = min(D (u,v),D (u,v)), then generate \n pseudo real GS iv. Create new Gaussians G and insert to G:\n camera-coordinate key point p ‚ààR 3 based on event type: add \n Ô£± cam Addition/ G add = (cid:8) g i (cid:12) (cid:12)(p j ,Œ£j nn ,oj nn ,cj nn ,s manual ) (cid:9) . (12)\n Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤ [u \n c \n ,v \n c \n ,D \n pseudo \n (u \n c \n ,v \n c \n )]‚ä§, \n Removal For removal: \n p cam = Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥ 1 2 \n + \n (cid:0) \n [ \n [ \n c \n c \n ( \n ( \n R \n R 1 \n ) \n ) \n ; \n ; \n D \n D pseudo \n ( \n ( \n c \n c \n ( \n ( \n R \n R 1 \n ) \n ) \n ) \n ) \n ] \n ] \n (cid:1)‚ä§ \n , Relocation (9) i i i . . D \n m \n G e e \n a \n l n \n p \n e e t \n { \n r e a \n D \n t G e ta h rg o et le \n ‚äô \n fr i o n \n M \n m pa G in . tin \n } \n g \n K \n poi \n . \n nts P \n fill \n = {p \n j \n } via depth \n 2 pseudo 2 real,k comb,k k=1 \n iii. Find nearest neighbors and inherit attributes:\n Then, p can be calculated using camera-to-world \n transformat w io or n ld matrix Tworld: a) For each p j ‚ààP fill : g n j n =argmin‚à•¬µ m ‚àíp j ‚à• 2 ,\n cam \n (cid:20) p (cid:21) b) Extract covariance matrix Œ£ g j m , ‚àà o G pacity oj , color cj\n p =Tworld¬∑ cam . (10) nn nn nn \n world cam 1 and semantic sj from gj . \n nn nn \n D. Active Scene Updater iv. Create new Gaussians G add and insert into G:\n 1) Active Data Collection: Wedesignarule-basedheuris- G \n add \n = (cid:8) g \n i \n (cid:12) (cid:12)(p \n j \n ,Œ£j \n nn \n ,oj \n nn \n ,cj \n nn \n ,sj \n nn \n ) (cid:9) . (13)\n ticpolicytoenabletherobottoautonomouslycaptureimages \n For relocation: \n of scene change regions. The core of this policy involves \n i. Obtain G and extract colors and semantics C,S =\n positioningtherobottofacethetargetregion,withtheregion target \n {c ,s |g ‚ààG }. \n as the circle‚Äôs center, and moving left or right along the j j j target \n ii. Delete G and generate target points P via depth\n tangential direction. The robot first adjusts its distance from target dest \n map {D ‚äôM }K at destination. \n thetargetregiontoensureasuccessfultangentialmovement. real,k comb,k k=1 \n After each movement, the robot reorients its viewpoint iii. For each p j ‚ààP dest , g n j n =argmin‚à•¬µ m ‚àíp j ‚à• 2 , create\n towardthetargetandreadjustsitsdistancetocaptureimages. new Gaussians G and inse g rt m i ‚àà n G to G:\n add \n Following this policy, the robot first moves K/2 steps to the \n left, then K steps to the right, collecting images {I real,i }K i=1 , G add = (cid:8) g i (cid:12) (cid:12)(p i ,Œ£j nn ,oj nn ,c nn ‚àºC,s nn ‚àºS) (cid:9) . (14)\n depth maps {D }K and camera poses {T }K from K \n real,i i=1 i i=1 iv. Hole inpainting to source region as aforementioned.\n viewpoints of the scene change region during the process. \n Finally, we perform post-training to further fine-tune the\n Next,therobotobtainscombinedmasks{M }K using \n comb,k k=1 Gaussians and refine the reconstruct quality.\n the method in Sec. IV-C.2. \n 2) Gaussian Editing based Scene Update: We adopt a V. EXPERIMENTS \n pre-editing and fine-tuning strategy to achieve scene update. \n Thissectiondetailsacomprehensiveevaluationof GS-LTS\n Distinct pre-editing protocols are implemented for different \n across simulation and real-world settings.\n scene change types: (i) For Addition, we directly instanti- \n ate new Gaussians in target region; (ii) For Removal, we A. Scene Change Adaptation Benchmark\n first localize target objects and then prune the associated 1) Settings: To assess the robot‚Äôs ability to adapt to\n Gaussians followed by scene inpainting to fix the hole; scene changes in dynamic environments, we present a novel\n (iii) For Relocation, we execute a delete-then-add strategy Scene Change Adaptation Benchmark constructed on the\n that removes the associated Gaussians and then instantiate AI 2-THOR simulation platform [23]. AI 2-THOR offers a\n new Gaussians in target region. To identify the associated comprehensive suite of APIs such as Initial Random Spawn,\n Gaussians, we refer to [11] for defining a voting function V Disable Object, and Place Object At Point that facilitate direct\n that localizes target Gaussians within {M comb,k }K k=1 , manipulation of scene objects, which we exploit to design\n three distinct types of scene update tasks: relocation, ad-\n K \n V(g )= (cid:88) I(cid:2) œÄ(T ¬µ )‚àà{M }K (cid:3) , (11) dition, and removal of objects. The benchmark is gener-\n i k i comb,k k=1 \n ated by automatically traversing combinations of editable\n k=1 "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n TABLEI \n SCENEUPDATEQUALITYEVALUATEDBYIMAGERENDERINGMETRICS(250 FINE-TUNINGITERATIONS).\n addition relocation removal overall \n Method View \n SSIM‚Üë PSNR‚Üë LPIPS‚Üì SSIM‚Üë PSNR‚Üë LPIPS‚Üì SSIM‚Üë PSNR‚Üë LPIPS‚Üì SSIM‚Üë PSNR‚Üë LPIPS‚Üì\n near 0.87 24.74 0.24 0.89 25.94 0.22 0.91 29.79 0.20 0.89 27.20 0.22 \n Baseline \n far 0.88 25.52 0.20 0.89 26.23 0.19 0.91 28.64 0.17 0.89 27.04 0.19 \n near 0.88 26.60 0.22 0.91 28.58 0.19 0.93 32.08 0.17 0.91 29.40 0.19 \n GS-LTS \n far 0.89 26.93 0.18 0.90 28.04 0.17 0.92 30.55 0.15 0.90 28.74 0.16 \n \u0000\u0016\u0000\u0013 \n \u0000\u0015\u0000\u001c \n \u0000\u0015\u0000\u001b \n \u0000\u0015\u0000\u001a \n \u0000\u0015\u0000\u0019 \n \u0000\u0015\u0000\u0018 \n \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \n \u0000,\u0000W\u0000H\u0000U\u0000D\u0000W\u0000L\u0000R\u0000Q \n \u00005\u00001\u00006\u00003 \n \u00001\u0000H\u0000D\u0000U \u0000)\u0000D\u0000U \n \u0000\u0016\u0000\u0013 \n \u0000\u0015\u0000\u001c \n \u0000\u0015\u0000\u001b \n \u0000\u0015\u0000\u001a \n \u0000\u0015\u0000\u0019 \n \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H \u0000%\u0000D\u0000V\u0000H\u0000O\u0000L\u0000Q\u0000H \n \u0000*\u00006\u0000\u0010\u0000/\u00007\u00006 \u0000\u0015\u0000\u0018 \u0000*\u00006\u0000\u0010\u0000/\u00007\u00006 \n \u0000\u0015\u0000\u0013\u0000\u0013 \u0000\u0017\u0000\u0013\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0013 \u0000\u001b\u0000\u0013\u0000\u0013 \u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013 \n \u0000,\u0000W\u0000H\u0000U\u0000D\u0000W\u0000L\u0000R\u0000Q \n Fig.3. Impactoffine-tuningiterationsonsceneupdatequality. \n objects, containers, and placement positions, which enables \n GT Baseline 100 Iter GS-LTS 100 Iter Baseline 250 Iter GS-LTS 250 Iter\n the sampling of an extensive range of scene changes. Each \n scene change task is recorded via a configuration script \n containing environment metadata (e.g., initial viewpoint) \n and a sequential action list specifying the operations to \n transformobjectsfromtheirdefaultstatetotheupdatedstate. \n Additionally,eachtaskinvolves 20 testviewpointscapturing \n the scene change region (10 near-range and 10 far-range). \n As changed objects typically occupy a small fraction of the \n field of view, we generate test images by expanding ground- \n truth change bounding boxes by 50 pixels in all directions. \n Scene update quality is then assessed using PSNR, SSIM, \n and LPIPS metrics. \n For evaluation, the robot is initialized at the starting pose \n of each scene change task. The Change Detection Unit is \n first executed to generate predictions, after which we assess \n whetherthepredictedscenechangetypematchestheground- \n truth type and whether the prediction error of the scene \n change region is within 1 meter. For tasks with accurate \n predictions,activedatacollectionand Gaussianediting-based \n scene update are performed. During scene representation \n updates, GS-LTS first employs pre-editing method, followed \n by fine-tuning of 3 DGS to refine object geometry and visual \n details. In contrast, the baseline method directly fine-tunes \n 3 DGS using multi-view data collected by GS-LTS. \n In this experiment, we sample 459 scene change tasks, \n achieving a 74.5% accuracy in predicting change type and \n target region with GS-LTS. Scene updates are tested on 342 \n tasks,withresultsafter 250 fine-tuningiterationsreportedin \n Table I. Experimental results demonstrate that our method \n achieves superior performance for both type of viewpoints, \n outperforming the baseline across all metrics. \n Additionally,asshownin Fig.3,weevaluatevariousfine- \n tuning iterations and statistically analyze the overall PSNR \n metrics for both type of viewpoints. The results demon- \n stratethatourapproachconsistentlyoutperformsthebaseline \n across all settings. Notably, GS-LTS achieves superior scene \n update quality with fewer fine-tuning iterations, highlighting \n noitidd A \n noitacole R \n lavome R \n Fig.4. Renderingresultsafterdifferentfine-tuningiterations.\n TABLEII \n 3 DLOCALIZATIONRESULTS(BOTTOMPARTFORABLATIONSTUDY).\n Feature Source m Io U Acc(Io U>0.5) Acc(Io U>0.3)\n CLIP 40.6 42.9 52.1 \n GT 60.9 73.6 81.8 \n CLIP(300√ó300 res) 24.6 29.0 30.7 \n CLIP(8 dim) 32.2 35.7 43.3 \n CLIP(60%data) 36.6 41.1 46.1 \n its ability to deliver efficient, low-cost scene updates. Fig. 4\n presents the quantitative results of GS-LTS, showcasing one\n representative case from each of three scene changes. The\n rendering results more intuitively demonstrate that GS-LTS\n achieves superior and faster scene update capabilities, while\n the baseline method requires significantly more iterations to\n obtain comparable outcomes. \n B. Multi-Task Experiment \n To assess the geometry and semantic fidelity of GS-LTS,\n we conduct experiments on 3 D localization and object nav-\n igation. We evaluate two 3 DGS representations embedding\n ground-truth semantics and CLIP semantics, denoted as GS-\n LTS (GT) and GS-LTS (CLIP), respectively.\n 1) 3 D Localization: For the 3 D localization task, se-\n mantic quality is quantitatively assessed by calculating the\n Intersection over Union (Io U) of the 3 D bounding boxes.\n A 3 D bounding box is deemed accurately localized if its\n Io U with the ground-truth bounding box exceeds a pre-\n defined threshold. Based on this criterion, we compute\n the Acc (Io U>threshold) metric to evaluate localization\n accuracy. We evaluate localization performance across 12\n differentobjectcategories,including Alarm Clock,Arm Chair,\n Bed, Bread, Chair, Coffee Machine, Desk Lamp, Dining Table,\n Dresser, Dumbbell, Remote Control and Sofa.\n Asshowninthetoppartof Table II,GS-LTS(GT)signif-\n icantly outperforms GS-LTS (CLIP) in terms of both m Io U"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n Bread Laptop Alarm Clock \n \n Robot \n View \n Fig. 5. 3 D Localization Examples. Red bounding boxes indicate the \n resultsfrom GS-LTS(GT),whilegreenonesfrom GS-LTS(CLIP). \n TABLEIII \n OBJECTNAVIGATIONRESULTSACROSSTHREEROOMTYPES. \n kitchen livingroom bedroom overall \n Method \n SPL SR SPL SR SPL SR SPL SR \n SAVN[6] 17.8 43.6 7.7 21.6 8.7 29.2 11.4 31.5 \n GVE[24] 17.9 45.6 9.4 25.2 8.1 27.6 11.8 32.8 \n HZG[25] 48.7 74.6 41.5 60.7 32.2 59.1 40.8 64.8 \n GS-LTS(CLIP) 45.7 59.0 40.0 52.2 41.5 54.6 42.2 55.3 \n GS-LTS(GT) 64.0 86.4 68.4 90.3 44.8 56.4 59.1 77.7 \n Fig. 6. Object Navigation Examples. The mid row displays semantic\n and accuracy metrics, highlighting the critical importance mapsandnavigationtrajectoriesgeneratedby GS-LTS(CLIP),thebottom\n rowillustratesthecorrespondingoutputsof GS-LTS(GT).\n of precise semantic cues. Fig. 5 presents qualitative results \n for the 3 D localization task. The 3 D bounding boxes gener- Step 1: Change Detection Step 3: 3 DGS Update\n 3 DGS Rendered Image Current Observation Old GS\n ated by GS-LTS (GT) generally exhibit a closer alignment \n with the objects compared to those generated by GS-LTS Change \n Detection \n (CLIP). These precise bounding boxes further highlight the \n advantages and potential of employing 3 DGS as a scene \n representation. \n 2) Object Navigation: For the object navigation task, we Change Detection Unit\n update \n adopt the experimental protocol proposed by SAVN [6], \n with a modification to limit the evaluation to three scene \n types: kitchens, living rooms, and bedrooms. Bathrooms are \n excludedduetotheirconstrainedspatialscaleandsimplistic \n layouts.Performanceisassessedusingthe Successweighted \n by Path Length (SPL) and Success Rate (SR). Step 2: Active Data Collection \n We compare GS-LTS with classical methods. Notably, New GS \n these classical methods trained on the AI 2 THOR involve Fig.7. Realrobotperformingscenechangeadaptation.\n exploration and navigation within a single episode. In con- \n an average of 670 images to 60% of the data lowers m Io U\n trast, GS-LTS leverages prebuilt 3 DGS representations and \n by 4.0%. Notably, CLIP features computed from SAM-\n performs training-free navigation directly from a 2 D seman- \n segmented masks rely heavily on high-resolution images for\n tic map, employing a deterministic policy (Fast Marching \n small object recognition. While smaller data volume affect\n Method). This experimental setup is designed to validate the \n fine details, the impact is minimal for objects visible from\n feasibility of 3 DGS-based robotic navigation using existing \n multiple viewpoints, such that overall performance decline\n benchmark. As shown in Table III, GS-LTS (GT) outper- \n remains limited. \n forms other approaches across most metrics, while GS-LTS \n (CLIP) also demonstrates competitive performance, particu- \n D. Application in Real-world Robot System\n larly on the SPL metric. Semantic maps and trajectories for \n three example navigation tasks are illustrated in Fig. 6. Todemonstratethereal-worldapplicabilityofthe GS-LTS\n system, we conducted experiments with a real robot. We\n C. Ablation Study \n utilize a Microsoft Azure Kinect DK camera to scan a pre-\n To examine the effect of initial training data on 3 DGS arrangedroom,capturingdatatotraina 3 DGSrepresentation\n representations, we perform an ablation study on the 3 D of the scene. Unlike simulation environments, where precise\n localization task, with results reported in the bottom part robot poses can be obtained directly from an environment\n of Table II. We analyze how reduced image resolution, API, such information is unavailable in real-world settings.\n feature dimension and data volume affect GS-LTS (CLIP) To address this, we augment the GS-LTS system with a\n performance. Experiments reveal that lowering resolution relocalizationmoduletailoredforreal-worldoperation.Here,\n from 1,000√ó1,000 to 300√ó300 decreases m Io U by 16.0% we first obtain a coarse pose estimation through ORB visual\n andsignificantlyreducesaccuracy.Decreasingthefeaturedi- featurematching,thenemployi Com Ma[26]toperformpose\n mensionfrom 32 to 8 resultsinaperformancedropofm Io U refinement to obtain an optimized precise pose estimation.\n from 40.6%to 32.2%,indicatingthatlower-dimensionalrep- To assess the robot‚Äôs ability to adapt to scene changes,\n resentationsdegradethequalityofthelearnedlatentspace,as we reposition three stacked colored storage bins within the\n convergenceofautoencodersbecomesmorechallengingwith room. As the robot approaches the vicinity of the bins,\n 8-dimensional features. Reducing the training dataset from the Change Detection Unit identifies discrepancies in the"
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n current scene. It then actively collects multi-view images to [4] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, ‚ÄúLangsplat: 3 d\n update 3 DGS. Fig. 7 illustrates the changes in the 3 DGS languagegaussiansplatting,‚Äùin CVPR,2024,pp.20051‚Äì20060.\n [5] S. Zhu, G. Wang, D. Kong, and H. Wang, ‚Äú3 d gaussian splatting in\n representation and rendered images before and after the \n robotics:Asurvey,‚Äùar Xivpreprintar Xiv:2410.12262,2024.\n adaptation. These results validate that the GS-LTS system [6] M.Wortsman,K.Ehsani,M.Rastegari,A.Farhadi,and R.Mottaghi,\n caneffectivelyoperateinreal-worldenvironmentsandadapt ‚ÄúLearningtolearnhowtolearn:Self-adaptivevisualnavigationusing\n meta-learning,‚Äùin CVPR,2019,pp.6750‚Äì6759.\n todynamicscenechanges.Foradetailedexperimentalvideo, \n [7] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov,\n please refer to our website. ‚ÄúObjectgoalnavigationusinggoal-orientedsemanticexploration,‚Äùin\n Neur IPS,2020,pp.4247‚Äì4258. \n VI. DISCUSSION [8] X.Chen,A.Milioto,E.Palazzolo,P.Giguere,J.Behley,and C.Stach-\n niss, ‚ÄúSuma++: Efficient lidar-based semantic slam,‚Äù in IROS, 2019,\n A. Resource Overhead pp.4530‚Äì4537. \n [9] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor-\n Theentiresystemoperatesefficientlyonasingle NVIDIA \n thi, and R. Ng, ‚ÄúNerf: Representing scenes as neural radiance fields\n Ge Force RTX 4090 GPU. The GS-LTS system completes forviewsynthesis,‚ÄùCommunicationsofthe ACM,vol.65,no.1,pp.\n vanilla 3 DGS reconstruction in ‚àº15 minutes, with subse- 99‚Äì106,2021. \n [10] R.Jin,Y.Gao,Y.Wang,Y.Wu,H.Lu,C.Xu,and F.Gao,‚ÄúGs-planner:\n quent 32 dimensional Gaussian semantic learning requiring \n Agaussian-splatting-basedplanningframeworkforactivehigh-fidelity\n ‚àº1 hour. Our experiments show 250 training iterations reconstruction,‚Äùin IROS. IEEE,2024,pp.11202‚Äì11209.\n achieve superior scene updates (0.91 SSIM / 29.07 PSNR) [11] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai,\n L.Yang,H.Liu,and G.Lin,‚ÄúGaussianeditor:Swiftandcontrollable\n versus 1,000-iteration baselines, with ‚â§10 s training. \n 3 deditingwithgaussiansplatting,‚Äùin CVPR,2024,pp.21476‚Äì21485.\n [12] L. Kunze, N. Hawes, T. Duckett, M. Hanheide, and T. Krajn¬¥ƒ±k,\n B. Limitation and Future Work \n ‚ÄúArtificialintelligenceforlong-termrobotautonomy:Asurvey,‚ÄùRAL,\n vol.3,no.4,pp.4023‚Äì4030,2018. \n The GS-LTS system advances adaptive modeling for \n [13] C.P.Jones,‚ÄúSlocumgliderpersistentoceanography,‚Äùin AUV. IEEE,\n 3 DGS-based robotic systems in long-term dynamic envi- 2012,pp.1‚Äì6. \n ronments, yet several challenges remain before achieving [14] N. Hawes, C. Burbridge, F. Jovan, L. Kunze, B. Lacerda, L. Mu-\n drova, J. Young, J. Wyatt, D. Hebesberger, T. Kortner, et al., ‚ÄúThe\n widespread real-world deployment. Below, we discuss key \n strandsproject:Long-termautonomyineverydayenvironments,‚ÄùIEEE\n limitations and promising directions for improvement. Robotics&Automation Magazine,vol.24,no.3,pp.146‚Äì156,2017.\n First, efficient large-scale representation is a challenge for [15] M.Hanheide,D.Hebesberger,and T.Krajn¬¥ƒ±k,‚ÄúThewhen,where,and\n how: An adaptive robotic info-terminal for care home residents,‚Äù in\n vanilla 3 DGS, which struggles with expansive scenes like \n HRI,2017,pp.341‚Äì349. \n factories, requiring more storage-efficient solutions. [16] P.F.Alcantarilla,S.Stent,G.Ros,R.Arroyo,and R.Gherardi,‚ÄúStreet-\n Second, robot control could be improved with learning- view change detection with deconvolutional networks,‚Äù Autonomous\n Robots,vol.42,pp.1301‚Äì1322,2018. \n based policies to enhance adaptability in complex scenarios. \n [17] E. Palazzolo and C. Stachniss, ‚ÄúFast image-based geometric change\n Finally, highly dynamic environments present an addi- detectiongivena 3 dmodel,‚Äùin ICRA. IEEE,2018,pp.6308‚Äì6315.\n tional challenge. GS-LTS focuses on medium-term changes, [18] J.Wald,A.Avetisyan,N.Navab,F.Tombari,and M.Nie√üner,‚ÄúRio:\n 3 d object instance re-localization in changing indoor environments,‚Äù\n not real-time dynamics like moving objects or human in- \n in ICCV,2019,pp.7658‚Äì7667. \n teractions. Future 3 DGS-based dynamic reconstruction will [19] Z. Lu, J. Ye, and J. Leonard, ‚Äú3 dgs-cd: 3 d gaussian splatting-based\n enhance support for tasks like cooking or household assis- change detection for physical object rearrangement,‚Äù IEEE Robotics\n and Automation Letters,2025. \n tance, improving more realistic long-term autonomy. \n [20] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\n T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., ‚ÄúSegment\n VII. CONCLUSIONS \n anything,‚Äùin ICCV,2023,pp.4015‚Äì4026. \n In this work, we introduce GS-LTS, a 3 DGS-based sys- [21] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,\n G.Sastry,A.Askell,P.Mishkin,J.Clark,etal.,‚ÄúLearningtransferable\n tem designed for long-term service robots operating in \n visualmodelsfromnaturallanguagesupervision,‚Äùin ICML. Pm LR,\n dynamic environments. By integrating object-level change 2021,pp.8748‚Äì8763. \n detection, multi-view observation, and efficient Gaussian [22] Y.Xiong,B.Varadarajan,L.Wu,X.Xiang,F.Xiao,C.Zhu,X.Dai,\n D.Wang,F.Sun,F.Iandola,etal.,‚ÄúEfficientsam:Leveragedmasked\n editing-basedsceneupdates,GS-LTSenablesrobotstoadapt \n imagepretrainingforefficientsegmentanything,‚Äùin CVPR,2024,pp.\n to scene variations over time. Additionally, we propose a 16111‚Äì16121. \n scalable simulation benchmark for evaluating object-level [23] E. Kolve, R. Mottaghi, W. Han, E. Vander Bilt, L. Weihs, A. Her-\n rasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, ‚ÄúAI 2-THOR:\n scenechanges,facilitatingsystematicassessmentandsim-to- \n An Interactive 3 D Environment for Visual AI,‚Äù ar Xiv preprint\n real transfer. Experimental results demonstrate that GS-LTS ar Xiv:1712.05474,2017.\n achieves faster and higher-quality scene updates, advancing [24] M. K. Moghaddam, Q. Wu, E. Abbasnejad, and J. Shi, ‚ÄúOptimistic\n agent: Accurate graph-based value estimation for more successful\n the applicability of 3 DGS for long-term robotic operations. \n visualnavigation,‚Äùin WACV,2021,pp.3733‚Äì3742.\n [25] Y. He and K. Zhou, ‚ÄúRelation-wise transformer network and re-\n REFERENCES inforcement learning for visual navigation,‚Äù Neural Computing and\n Applications,vol.36,no.21,pp.13205‚Äì13221,2024.\n [1] B.Kerbl,G.Kopanas,T.Leimku¬®hler,and G.Drettakis,‚Äú3 dgaussian \n [26] Y.Sun,X.Wang,Y.Zhang,J.Zhang,C.Jiang,Y.Guo,and F.Wang,\n splattingforreal-timeradiancefieldrendering.‚ÄùACMTrans.Graph., \n ‚Äúicomma:Inverting 3 dgaussiansplattingforcameraposeestimation\n vol.42,no.4,pp.139‚Äì1,2023. viacomparingandmatching,‚Äùar Xivpreprintar Xiv:2312.09031,2023.\n [2] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer, \n D.Ramanan,and J.Luiten,‚ÄúSplatam:Splattrack&map 3 dgaussians \n fordensergb-dslam,‚Äùin CVPR,2024,pp.21357‚Äì21366. \n [3] K. Wu, K. Zhang, Z. Zhang, M. Tie, S. Yuan, J. Zhao, Z. Gan, and \n W.Ding,‚ÄúHgs-mapping:Onlinedensemappingusinghybridgaussian \n representationinurbanscenes,‚ÄùRAL,2024. "
  }
]