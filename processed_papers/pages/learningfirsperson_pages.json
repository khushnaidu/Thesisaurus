[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n Learning Robot Activities from First-Person Human Videos \n Using Convolutional Future Regression \n \n \n Jangwon Lee and Michael S. Ryoo \n \n \n Abstract‚ÄîWe design a new approach that allows robot a limiting aspect particularly when we want to teach a robot\n learning of new activities from unlabeled human example new (i.e., previously unseen) activities.\n videos.Givenvideosofhumansexecutingthesameactivityfrom \n In this paper, we present a new CNN-based approach\n ahuman‚Äôsviewpoint(i.e.,first-personvideos),ourobjectiveisto \n that enables robot learning of its activities from ‚Äòhuman‚Äô\n maketherobotlearnthetemporalstructureoftheactivityasits \n futureregressionnetwork,andlearntotransfersuchmodelfor example videos. Human activity videos can be attractive\n itsownmotorexecution.Wepresentanewdeeplearningmodel: training resources because it does not require any hardware\n We extend the state-of-the-art convolutional object detection or professional software for teaching robots, even though\n network for the representation/estimation of human hands in \n it might create other difficulties like transferring learned\n trainingvideos,andnewlyintroducetheconceptofusingafully \n human-based models to the actual robots. Given videos\n convolutionalnetworktoregress(i.e.,predict)theintermediate \n scene representation corresponding to the future frame (e.g., of humans executing the same activity from a human‚Äôs\n 1-2 seconds later). Combining these allows direct prediction of viewpoint (i.e., first-person videos), our objective is to make\n futurelocationsofhumanhandsandobjects,whichenablesthe the robot learn the temporal structure of the activity as its\n robot to infer the motor control plan using our manipulation \n future regression network, and learn to transfer such model\n network. We experimentally confirm that our approach makes \n foritsownmotorexecution.Theideaisthatahuman‚Äôsfirst-\n learning of robot activities from unlabeled human interaction \n videos possible, and demonstrate that our robot is able to person video and the video a humanoid robot is expected to\n executethelearnedcollaborativeactivitiesinreal-timedirectly obtain during its activity execution should be very similar.\n based on its camera input. Providing first-person human videos to the robot is as if we\n are providing the robot ‚Äòvisual memory‚Äô of itself performing\n I. INTRODUCTION \n the activities previously. This enables the robot to directly\n One of the important abilities of humans (and animals) is \n learnwhatvisualobservationitisexpectedtoseeduringthe\n that they are able to learn new activities and their motor \n correctexecutionoftheactivityandhowitwillchangefrom\n controls from others‚Äô behaviors. When a person watches \n its viewpoint. \n others performing an activity, he/she not only learns to \n Therehavebeenpreviousworksonrobotactivitylearning\n visually predict future consequences of the motion during \n from human videos [3], [4], extending the previous concept\n the activity but also learns how to execute the activity \n of‚Äòrobotlearningfromdemonstration‚Äô[5]whichwasmostly\n himself/herself. \n done with direct motor control data. However, these works\n Recently, approaches taking advantage of ‚Äúdeep learning‚Äù \n focused on learning grammar representations of human ac-\n for robot manipulation have been gaining an increasing \n tivities, modeling human activities as a sequence of atomic\n amount of attention, directly learning motor control policies \n actions (e.g., grasping). These approaches were limited in\n given visual inputs (i.e., images and videos) [1]. The use of \n the aspect that activities were always represented in terms\n convolutionalneuralnetworks(CNNs)havebeenparticularly \n of pre-defined set of atomic actions, and the users had to\n successful,sincetheyareabletojointlylearnimagefeatures \n teach the robot how to recognize those atomic actions from\n optimized for the task based on their training data. Because \n humanactivityvideosbyprovidinglabeledtrainingdata(i.e.,\n of such ability, new models incorporating convolutional and \n supervised learning). This prevented the robot learning of\n recurrent neural networks (i.e., CNNs and RNNs) is likely \n activities from scratch, and was also limited in that human\n to become a major trend in robotics, just like what already \n had to define new atomic actions when a new activity is\n happened in computer vision and machine learning. \n added. Furthermore, since it was not trainable in an end-to-\n However, although these deep learning oriented ap- \n end fashion, the robot has to somehow figure out how to\n proaches showed very promising results on learning video \n execute those atomic actions, which was usually done by\n prediction [2] and actual motor control policy [1], they \n hand-coding the motion. \n have been limited to relatively simple actions such as object \n We introduce a new robot activity learning model using\n grasping and pushing. This is because a large amount of \n a fully convolutional network for future representation re-\n ‚Äòrobot‚Äôdataisnecessaryforthedirecttrainingofthese CNNs \n gression. We extend the state-of-the-art convolutional object\n and RNNs with millions of parameters. A large number of \n detection network (SSD [6]) for the representation of hu-\n samplesofhumans(ortherobotitself)motorcontrollingthe \n man hand-object information in a video frame, and newly\n robotisnecessaryforgeneratingtrainingdata[1],andthisis \n introduce the concept of using a fully convolutional network\n to regress (i.e., predict) how such intermediate scene repre-\n Schoolof Informaticsand Computing,Indiana University,Bloomington, \n IN 47408,USA.{leejang,mryoo}@indiana.edu sentation will change in the future frame (e.g., 1-2 seconds\n 7102 \n lu J \n 42 \n ]OR.sc[ \n 2 v 04010.3071:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n ùëì \n ùëî ‚Ñé \n VGG-16 Future hand \n Through Pool 5 layer Autoencoder Extra Feature Layers Location prediction\n \n Convolution Deconvolution \n \n 500 Replaced with \n Image predicted future \n feature map \n 500 \n ‚Ä¶ \n 512 x 63 x 63 512 x 63 x 63 \n Extracted feature map 256 x 25 x 25 256 x 25 x 25 \n Current Regression Network s \n s \n Feature map ùëü o L \n )2 \n L \n Concatenate (K) \n ( \n n \n a \n e \n d \n 5 x 5 5 x 5 5 x 5 1 x 1 ilc \n u \n E \n Predicted \n (256 x K) x 25 x 25 256 x 25 x 25 256 x 25 x 25 ‚Ä¶ 1024 x 25 x 25256 x 25 x 25 future\n feature map \n Future \n Feature map \n Fig. 1. Overview of our perception component: Our perception component consists of two fully convolutional neural networks: The first network is\n anextendedversionofthestate-of-the-artconvolutionalobjectdetectionnetwork(SSD[6])fortherepresentationofhumanhandsandestimationofthe\n boundingboxes(top).Thesecondnetworkisafutureregressionnetworktoregress(i.e.,predict)theintermediatescenerepresentationcorrespondingto\n thefutureframe.Thisnetworkdoesnotrequireactivitylabelsorhand/objectlabelsinvideosforitstraining.\n later). Combining these allows direct and explicit prediction from demonstration (Lf D) [5], [8], [9]. Since it enables\n of future hand locations (Figure 1), which then allows the robots automatically learn a new task from demonstration\n robot to infer the motor control plan. That is, not only by non-robotics expert, Lf D is very important in robotics.\n feature-level prediction of future representations (similar to However,therearelimitationssincemostoftheseapproaches\n [7])butalsosemantic-levelpredictionofexplicitfuturehand focused on making robots learn motor control polices from\n locations of humans and robots during the learned activity human data, which usually was in the form of direct control\n is being jointly performed in our new network. Such future sequences obtained with actual robots or simulation soft-\n handpredictionresultsareusedbyourmanipulationnetwork wares [10]. Moreover, it often requires a knowledge about\n that learns mapping of the 2-D hand locations in the image all primitive actions for teaching high-level tasks [11].\n coordinate to the actual motor control. There also have been previous works on robot activity\n Our activity learning is an unsupervised approach in the learning from visual data [3], [4], [12], extending the previ-\n aspect that it does not require activity labels or hand/object ous concept of Lf D. These works focused on learning gram-\n labels in the activity videos. It does require hand-annotated mar representations of human activities from conventional\n training data for the learning of its hand representation third-person videos (i.e., videos usually taken with static\n network, but there already exists public datasets for this cameras watching the actors), modeling human activities\n purpose and it does not require any labels for its future as a sequence of atomic actions (e.g., grasping). Having a\n regressionnetwork.Thefutureregressionnetworkislearned grammar representation composed of atomic actions allows\n without supervision by capturing changes in our hand-based transfer of human activity structure to robots, and the robot\n representations in the training videos. In addition, impor- replication of human activities was possible usually with\n tantly,allournetworksweredesignedtofunctioninreal-time hand-coded motion transfer from human atomic actions\n for the actual robot operation, and we show such capability to robot atomic actions. However, activity learning was\n with our experiments in this paper. generally done in a fully supervised fashion with human\n annotations in these approaches, and they assumed very\n II. RELATEDWORK \n reliable estimation of semantic features from videos such\n a) Robot learning from humans: There have been a human hands and human body skeletons. [13] studied an\n considerable amount of previous efforts on robot learning approach to directly learn object manipulation trajectories"
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n from human videos, but it was limited to one-robot-one- a hand-based scene representation and estimate bounding\n object scenarios unlike our approach focusing on very boxes,and(2)afutureregressionnetworktomodelhowsuch\n general human-robot collaboration scenarios (e.g., human- intermediate scene representation (should) change in future\n object-robot interactions). frames.Thesecondcomponentisamanipulationcomponent\n b) Video prediction: Our approach in this paper is to that maps 2-D hand locations in the image coordinate to the\n generate proper robot behaviors (particularly for human- actual motor control using fully connected layers.\n robot collaboration) by predicting ‚Äòfuture‚Äô visual represen- The key idea of our approach is that the proposed per-\n tation. The idea is that such representation leads to the ception component allows prediction of future (1-2 seconds\n estimationoffuturepositionsofobjectsandhandsofhumans later)handlocationsgivencurrentvideoinputfromacamera.\n and robots. Visual prediction is one of the core components Suchfuturepredictioncanbelearnedbasedonhumans‚Äôfirst-\n of our perception system. person activity videos by using them as training data, with\n Therehavebeenpreviousworksonthepredictionoffuture theassumptionthattherobotcamerahasasimilarviewpoint\n frames from the computer vision community [7], [14], [15]. with the human first-person videos. This allows the robot\n However, there has been very limited attempt on applying to directly predict its ideal future hand locations during the\n such future predictions for robotics systems, since these ap- activity, inferring how the hand should move if the activity\n proaches in general requires more components for interpret- were to be executed successfully. Next, the manipulation\n ing predicted representation to generate robot actions. In the componentgeneratesactualrobotcontrolcommandstomove\n above works, no robot manipulation was actually attempted. the robot‚Äôs hands to the predicted future locations.\n There exists a recent robotics work that attempted applying \n B. Perception Component \n visual prediction for generating robot control actions [16]. \n This study shows the potential in applying visual predic- Given a video frame XÀÜ t at time t, the goal of our\n tion for a robotic manipulation task; it enables transferring perception component is to predict the future hand locations\n the visual perception to robot manipulation component for YÀÜ t+‚àÜ . \n generating motor control commands without any additional a) Hand Representation Network: We first construct\n componentstointerprettherecognitionresults.However,this a network for the hand-based representation of the image\n requiresahugeamountoftrainingdatausingactualphysical scenebyextendingthe SSDobjectdetectionframework.We\n robots to make the robot learn activities, and thus is limited extended it by inserting a fully convolutional auto-encoder\n when the robot needs to learn many new activities. having five convolutional layers followed by five deconvo-\n c) First-person videos: First-person videos, also called lutional layers for dimensionality reduction. This allows the\n egocentric videos, are the videos taken from the actor‚Äôs own approach to abstract an image (with hands and objects) into\n viewpoint. Recognition of human/robot activities from such a lower dimensional intermediate representation.\n first-person videos has been actively studied particularly in Allourconvolutional/deconvolutionallayersuse 5√ó5 ker-\n thepast 5 years,includingrecognitionofhumanactionsfrom nels and the number of filters for each convolutional layer\n wearable cameras [17]‚Äì[20] and human-robot interactions are:512,256,128,64,256.Thegreenconvolutionallayersin\n from robot cameras [21], [22]. However, these focused on Fig. 1 correspond to them. After such convolutional layers,\n building discriminative video classifiers, and the attempt to there are deconvolutional layers (yellow layers in Fig. 1),\n learn‚Äòexecutable‚Äôrepresentationsofhumanactivitiesortheir each having the symmetric number of filters: 256, 64, 128,\n transfer to robots have been very limited. 256, 512. We do not use any pooling layer, and instead use\n The main contribution of this paper is in enabling robot stride 2 forthelastconvolutionallayerforthedimensionality\n activity learning from human interaction videos using our reduction. We thus increase the number of filters for the last\n newly proposed convolutional future regression. We believe convolutional layer to compensate loss of information.\n this is the first work to present a deep learning-based Let f denote the hand representation network given an\n (i.e., entirely CNN-based) method for learning human-robot image at time t. Then, this network can be considered as a\n interactions fromhuman-human videos. We alsobelieve this combination of two sub functions, f =g‚ó¶h:\n is the first paper to take advantage of human ‚Äòfirst-person YÀÜ =f(XÀÜ )=h(FÀÜ )=h(g(XÀÜ )), (1)\n videos‚Äô for the robot activity learning. t t t t \n where a function g : XÀÜ ‚Üí FÀÜ denotes a feature extractor\n III. APPROACH \n (from an input video frame to encoder) to get compressed\n A. System Overview intermediate visual representation (i.e., feature map) FÀÜ, and\n Given a sequence of current frames, our goal is to (i) pre- h : FÀÜ ‚Üí YÀÜ indicates a box estimator which uses the\n dict future hand locations and all interactive objects in front compressed representation as an input for locating hand\n of the robot, then to (ii) generate robot control commands boxes at time t. With the above formulation, the network\n for moving robot‚Äôs hands to the predicted hand locations. can predict hand locations YÀÜ at time t after the training.\n t \n Weemploytwocomponentsforachievingthegoal.Thefirst b) Future Regression Network: Although the above\n component is a perception component that consists of two hand representation network allows obtaining hand boxes in\n fully convolutional neural networks: (1) an extended version the ‚Äòcurrent‚Äô frame, our objective is to get the ‚Äòfuture‚Äô hand\n of the Single Shot Multi Box Detector (SSD) [6] to create locations YÀÜ instead of theirs current locations YÀÜ .\n t+‚àÜ t "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n g Hand representa,on network: t Fig. 2 summarizes data flow of our perception component\n YÀÜ during testing phase. Given a video frame XÀÜ t at time t, (1)\n t we extract the intermediate scene representation FÀÜ using\n t \n XÀÜ FÀÜ the feature extractor (g), and then (2) feed it into the future\n t t \n ‚Ä¶ regression network (r) to get future scene representation\n r FÀÜ t+‚àÜ . Next, (3) we feed FÀÜ t+‚àÜ into the box estimator (h),\n FÀÜ t FÀÜ t+Œî and finally obtain future position of hands YÀÜ t+‚àÜ at time t.\n YÀÜ =h(FÀÜ )=h(r(FÀÜ ))=h(r(g(XÀÜ ))) (5)\n t+‚àÜ t+‚àÜ t t \n h \n YÀÜ Furthermore, instead of using just a single frame (i.e., the\n t+Œî \n current frame) for the future regression, we extend our\n FÀÜ networktotakeadvantageoftheprevious K framestoobtain\n t+Œî \n ‚Ä¶ FÀÜ as illustrated in Fig. 1: \n t+‚àÜ \n Hand representa,on network: t+Œî YÀÜ =h(r([g(XÀÜ ),...,g(XÀÜ )])). (6) \n t+‚àÜ t t‚àí(K‚àí1) \n Fig.2. Dataflowofourperceptioncomponentduringtestphase.Itenables \n The advantage of our formulation is that it allows us to\n predictinghandscorrespondingtothefutureframe.Onlythecoloredlayers \n areusedforthepredictioninthetestphase. predict future hand locations while considering the implicit\n activity and object context, even without explicit detection\n ofobjectsinthescene.Ourauto-encoder-basedintermediate\n We formulate this problem as a regression problem. The representation FÀÜi abstracts the scene configuration by inter-\n t \n main idea is that the intermediate representation of the hand nally representing what objects/hands are currently in the\n representationnetwork FÀÜ abstractsthehand-objectinforma- scene and where they are, and our fully convolutional future\n t \n tioninthescene,andthatweareabletotakeadvantageofit regressor takes advantage of it for the prediction.\n to infer the future (intermediate) representation FÀÜ . Once \n t+‚àÜ C. Manipulation Component \n suchregressionbecomespossible,wecansimplyplug-inthe \n predicted future representation FÀÜ to the remaining part Although our perception component is able to predict fu-\n t+‚àÜ \n of the hand network (i.e., h) to obtain the final future hand ture hand locations of humans in first-person human activity\n predictionresults.Therefore,wenewlydesignanetworkfor videos,itisinsufficientfortherobotmanipulation.Here,we\n predicting the intermediate scene representation correspond- construct another regression network (m) for mapping the\n ing to the future frame FÀÜ , as a fully convolutional future predicted 2-Dhumanhandlocationsintheimagecoordinate\n t+‚àÜ \n regression network: Fig. 2. totheactualmotorcontrolcommands.Themainassumption\n Given a current scene representation FÀÜ from the hand is that a video frame from a robot‚Äôs camera will have a\n t \n network,ourfutureregressionnetwork(r)predictsthefuture similar viewpoint to our training data (first-person human\n the intermediate scene representation FÀÜ : videos), allowing us to take advantage of the learned model\n t+‚àÜ \n for the robot future hand prediction by assuming:\n FÀÜ =r (FÀÜ ). (2) \n t+‚àÜ w t YÀÜ (cid:39)YÀÜ (7) \n Rt t \n Ithassevenconvolutionallayershaving 2565√ó5 kernels.In \n where, YÀÜ represents robot hand locations.\n addition,ithasalayerwith 102413√ó13 kernelsfollowedby Rt \n Our manipulation component (m) predicts future robot\n thelastlayerthathas 2561√ó1 kernel.Wetrainedtheweights \n jointstates(ZÀÜ )givencurrentrobotjointstates(ZÀÜ ),robot\n (w) of the regression network with unlabeled first-person t+‚àÜ t \n hand locations (YÀÜ ), and future hand locations (YÀÜ )\n human activity videos using the following loss function: Rt Rt+‚àÜ \n tellingwheretherobot‚Äôshandsshouldmoveto.Thisnetwork\n (cid:88) \n w‚àó =argmin (cid:107)r (FÀÜi)‚àíFÀÜi (cid:107)2 can be formulated with the below function:\n w t t+‚àÜ 2 \n w \n i,t ZÀÜ =m (ZÀÜ ,YÀÜ ,YÀÜ ). (8) \n (cid:88) t+‚àÜ Œ∏ t Rt Rt+‚àÜ \n =argmin (cid:107)r (g(XÀÜi))‚àíFÀÜi (cid:107)2 (3) \n w w t t+‚àÜ 2 Our manipulation component consists of seven fully con-\n i,t \n nected layers having the following number of hidden units\n where XÀÜi indicates a video frame at time t from video i, for each layer: 32, 32, 32, 16, 16, 16, 7. The weights (Œ∏) of\n t \n and FÀÜi represents a feature map at time t from video i. this network can be obtained by the same way that used for\n t \n Our future regression network can use any intermediate our perception networks:\n scenerepresentationfromanyintermediatelayersofthehand (cid:88) \n Œ∏‚àó =argmin (cid:107)m (ZÀÜj,YÀÜj ,YÀÜj )‚àíZÀÜj (cid:107)2 (9)\n network, but we use the one from auto-encoder due to its Œ∏ t Rt Rt+‚àÜ t+‚àÜ 2 \n Œ∏ \n j,t \n lowerdimensionality.Finally,thefuturescenerepresentation \n FÀÜ t+‚àÜ isfedintothehandnetworkforestimatinghandboxes where ZÀÜj t indicates robot joint states at time t from training\n correspondingtothefutureframetogetfuturehandlocations episode j, and YÀÜj represents robot hand locations at time\n Rt \n YÀÜ t+‚àÜ . t from training episode j. Fig. 3 shows our manipulation\n YÀÜ =h(FÀÜ ) (4) component for generating robot control commands.\n t+‚àÜ t+‚àÜ "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n Predicted Future \n Manipulation Network \n Current frame Hand locations (2) \n Perception Motor torques (7)\n Component \n L L L L L L L \n L L L L L L L \n U U U U U U U \n F F F F F F F \n Current \n Hand locations (2) \n Current status of a robot \n 32 32 32 16 16 16 7 \n Current \n Arm joint angles (7) \n Fig.3. Robotmanipulationcomponentofourapproach.Itgeneratesrobotcontrolcommandsgivencurrentrobotjointstate,currentrobothandlocations,\n andpredictedfuturerobothandlocations. \n The combination of our perception component and ma- (u,v) in an image plane and the robot‚Äôs corresponding joint\n nipulation component provides a real-time robotics system angles at time t. We recorded these log files by making\n that takes raw video frames as its input and generates a human operator move the robot arms (i.e., the human\n motor control commands for its activity execution. Our grabbed the robot arms and moved them). We obtained such\n manipulation component can be replaced with a standard robot joint configuration sequences while moving the robot\n Inverse Kinematics, but our neural network-based model to cover possible arm motion during general human-robot\n generates more natural arm movements by considering the interactiontasks.Here,weassumethattherobotissupposed\n desired location of the robot‚Äôs end-effectors as well as joint to operate in a similar environment during the test phase.\n configuration sequences (i.e., unlabeled robot logs described Notethatthiswasnotrecordedundertheinteractionscenario\n in the next section). (i.e., just the robot itself was moving), and no annotation\n regarding the activity or motion was provided. We used\n IV. EXPERIMENTS \n a Baxter research robot for recording these files and the\n A. Datasets Baxterhassevendegrees-of-freedomarm:thefilecontains 9\n Ourapproachconsistsofthreedifferenttypesofnetworks variables for each arm. In order to estimate the robot‚Äôs hand\n (withinthetwocomponents),andweusethreedifferenttypes positionintheimageplane,weprojectedthe 3-Dpositionsof\n of datasets for training each model. the Baxter‚Äôs grippers into the image plane (based on camera\n Ego Hands [23]: This is a public dataset containing 48 calibration) and recorded the projected (u,v) positions with\n first-person videos of people interacting in four types of 7 joint angles at 30 Hz.\n activities(playingcards,playingchess,solvingapuzzle,and \n B. Baselines \n playing Jenga).Ithas 4,800 frameswith 15,053 ground-truth \n hand labels. Here, we added 466 frames with 1,267 ground- Inordertoprovidequantitativecomparisons,wecompared\n truth annotations to the original dataset to cover more hand our perception component with four different baselines:\n postures.Weusethisdatasettolearnourhandrepresentation (i) Hand-crafted representation uses a hand-crafted state\n network, which is trained to locate hand boxes in a video representation based on explicit object and hand detection.\n frame. It encodes relative distances between all interactive objects\n Unlabeled Human-Human Interaction Videos: We col- in our two scenarios, and uses it to predict the future\n lected a total of 47 first-person videos of human-human hand location using neural network-based regression. More\n collaboration scenarios, with each video clip ranging from specifically, it detects objects using KAZE features [24]\n 4 to 10 seconds. This dataset is a main dataset for teaching and hands using CNN based hand detector in [23], then\n a new task to our robot. It contains two types of tasks: (1) computesrelativedistancesbetweenallobjectsandhandsfor\n a person wearing the camera cleaning up all objects on a building the state representation which is a 20 dimensional\n tableasapartner(i.e.,theothersubject)approachesthetable vector. Then, we built a new network which has five fully\n while holding a heavy box (to make a room for her/him to connected layers trained using the state representations on\n put the heavy box on the table), and (2) a person wearing the same interaction dataset we use. (ii) Hands only uses\n the camera pushing a trivet on a table toward to a partner hand locations for the future regression. It predicts future\n when he/she is approaching the table while holding a hot hand locations solely based on current hand locations with-\n cooking pan. These videos are unlabeled videos without any out considering any other visual representations. In order\n activity/hand annotation and we trained our convolutional to train this baseline model, we extracted hand locations\n regression network using this dataset. from all frames of the interaction videos using our hand\n Unlabeled Robot Activity Log Files: We prepared this representation network, then made log files to store detected\n dataset to train our robot manipulation network. It contains handlocationsineachframeandtheirframenumbers.After\n 50 robot log files. Each log has the robot‚Äôs hand positions this, we trained another neural network model for the future"
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n TABLEI TABLEII \n EVALUATIONOFFUTUREHANDPREDICTION MEANPIXELDISTANCEBETWEENGROUNDTRUTHANDPREDICTED\n POSITIONSOFALLHANDS \n Evaluation \n Method \n Precision Recall F-measure Method Mean Pixel Distance \n Hand-craftedrepresentation 0.30¬±0.37 0.15¬±0.19 0.20¬±0.25 Hand-craftedrepresentation 143.85¬±48.77\n Handsonly 4.78¬±3.70 5.06¬±4.06 4.87¬±3.81 Handsonly 247.88¬±121.94 \n SSDwithfutureannotations 1 27.53¬±23.36 9.09¬±8.96 13.23¬±12.62 SSDwithfutureannotations 1 58.58¬±36.76\n SSDwithfutureannotations 2 29.21¬±19.16 7.92¬±6.45 12.10¬±9.42 \n SSDwithfutureannotations 2 79.95¬±102.07\n Deep Regressor(ours):K=1 27.04¬±16.50 21.71¬±14.71 23.45¬±14.99 \n Deep Regressor(ours):K=5 29.97¬±15.37 23.89¬±16.45 25.40¬±15.51 Deep Regressor(ours):K=1 51.31¬±39.10\n Deep Regressor(ours):K=10 36.58¬±16.91 28.78¬±17.96 30.90¬±17.02 Deep Regressor(ours):K=5 51.41¬±38.46\n Deep Regressor(ours):K=10 46.66¬±36.92 \n TABLEIII \n hand location prediction using the log files, which has seven \n fullyconnectedlayerswiththesamenumberofhiddenunits \n MEANPIXELDISTANCEBETWEENGROUNDTRUTHANDPREDICTED\n as our robot manipulation network. (iii) SSD with future POSITIONOFRIGHTHAND \n annotations 1 is a baseline that uses the original SSD model \n Method Mean Pixel Distance \n [6] trained based on Ego Hands dataset. Instead of training \n Hand-craftedrepresentation 121.48¬±87.36\n the model to infer the current hand locations given the Handsonly 264.52¬±148.15\n input frame, we fine-tuned this model on Ego Hands dataset SSDwithfutureannotations 1 48.63¬±39.04\n SSDwithfutureannotations 2 71.36¬±104.18\n after changing annotations of the dataset to have ‚Äúfuture‚Äù \n Deep Regressor(ours):K=1 40.08¬±32.72 \n locations of hands instead of making it to use current hand Deep Regressor(ours):K=5 40.46¬±39.52\n locations.Wealsousedadditionally 466 framesforthisfine- Deep Regressor(ours):K=10 36.78¬±36.70\n tuning since the original Ego Hands dataset was insufficient \n (too many repetitive hand movements) for this training. (iv) \n SSD with future annotations 2 is a baseline also using the tions of hands. The size of the image plane was 1280*720.\n original SSDmodel,butwetrainedthismodelfromscratch. We measured this mean pixel distance only when both the\n This time we changed all annotations of the Ego Hands ground truths and the predictions are present in the same\n dataset, then trained the model. After that we fine-tuned the frame. Table II shows the mean pixel distance errors for\n model as the same way that used for the ‚ÄúSSD with future all four types of hands (my left, my right, your left, and\n annotations 1‚Äù baseline. your right). Once more, we can confirm that our approaches\n greatly outperform the performance of all the baselines. The\n C. Evaluation of our future hand prediction \n overall average distance was a bit high due to changes\n We first evaluated the perception component of our ap- in human hand shapes and their variations, but they were\n proach in terms of precision, recall, and F-measure, and sufficient in terms of generating robot motion.\n compared them against the above baselines. In the first Wealsocomparedaccuraciesofthesemethodswhileonly\n evaluation,wemadeourapproachtopredictboundingboxes considering my right hand predictions, since position of my\n of human hands in the future frame given the current image right hand is more important for a robot manipulation than\n frame. We measured the ‚Äúintersection over union‚Äù ratio locationsofothertypesofhands.Thisisbecause,inourtest\n betweenareasofeachpredictedboxandgroundtruth(future) scenarios, the robot‚Äôs activities are very focused on its right\n hand locations. Only when the ratio was greater than 0.5, hand motion. Table III shows mean pixel distance between\n the predicted box was accepted as a true positive. In this ground truth and predicted position of ‚Äòmy right hand‚Äô. We\n experiment,werandomlysplitthesetofour Human-Human can see that performances of our approaches are superior to\n Interaction Videos into the training and testing sets, so 32 all the baselines. Examples of our visual predictions results\n videos were used for training sets and remaining 15 videos are illustrated in Fig. 4.\n were used for testing sets in a total of 47 videos. \n D. Real-time robot experiments \n Table I shows quantitative results of our future hand \n prediction. Here, the plus-minus sign (¬±) indicates standard Finally, we conducted a user study to evaluate the success\n deviation and K represents number of frames we used as an level of robot activities performed based on our proposed\n inputforourregressionnetwork.Our‚àÜwas 30 frames(i.e., approach, with human subjects. A total of 12 participants\n 1 sec). We are able to clearly observe that our approach (5 undergraduate and 7 graduate students) were recruited\n significantly outperforms all the baselines, including the from the campus, and were asked to perform one of the\n state-of-the-art object detector SSD modified for the hand two activities (clearing the table for a partner and preparing\n prediction. Our proposed network with K = 10 yielded the a trivet for a cooking pan) together with our robot. After\n best performance in terms of all three metrics, at about 30.9 such interactions, the participants were asked to complete a\n score in F-measure. The best performance we can get with questionnaire about the robot behaviors for each task. The\n SSD was only 13.23. questionnaire had two statements (one statement for each\n In our second evaluation, we measured mean pixel dis- activity)withscalesfrom 1(totallydonotagree)to 5(totally\n tancebetweenground truthlocationsandthe predictedposi- agree) to express their impression on the robot behaviors: ‚ÄúI"
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n Time (t) \n se \n m \n a \n rf \n tu \n p \n n \n I \n sn \n o \n itc \n id \n e \n r P \n Predictions overlaid \n on future frames \n Time (t) \n se \n m \n a \n rf \n tu \n p \n n \n I \n sn \n o \n itc \n id \n e \n r P \n Predictions overlaid \n on future frames \n Fig.4. Twoexamplesofourvisualprediction.Thefirstexampleistheactivityofclearingthetable,andthesecondexampleistheactivityofpushing\n the trivet toward the person holding a cooking pan. The first row shows the input frames and the second row shows our future hand prediction results.\n Inthethirdrow,weoverlaidourpredictionson‚Äúfuture‚Äùframes.Redboxescorrespondtothepredicted‚Äòmylefthand‚Äôlocations,blueboxescorrespond\n to ‚Äòmy right hand‚Äô, green boxes correspond to the opponent‚Äôs left hand, and the cyan boxes correspond to the opponent‚Äôs right hand. The frames were\n capturedeveryonesecond. \n TABLEIV \n think the robot cleared the table to make a space for me.‚Äù \n THESUCCESSLEVELOFOURHUMAN-ROBOTCOLLABORATION\n for the task 1 and ‚ÄúI think the robot passed a trivet closer to \n me so that I can put the cooking pan on it.‚Äù for the task 2. \n Method Task 1 Task 2 Average \n Inadditiontoourapproach(i.e.,ourperceptioncomponent Base SSD+Basecontrol 1.25¬±0.43 2.21¬±1.41 1.72¬±0.92\n Base SSD+Ourcontrol 1.5¬±0.96 2.33¬±1.60 1.92¬±1.28\n + manipulation component), we designed and implemented \n Ourperception+Basecontrol 2.33¬±1.18 2.25¬±1.36 2.29¬±1.27\n thefollowingthreebaselinesandcomparedtheirquantitative Ours 3.17¬±1.40 3.42¬±1.61 3.29¬±1.50\n results: (i) Base SSD + Base control uses the baseline \n SSD with future annotations 1 as a perception component \n and the base manipulation network trained using the same our real-time robot experiments with human subjects are\n robot activity log files. This base control network direct illustrated in Fig. 5.\n maps current hand locations in the image plane to current Our method operates in slow real-time with our unopti-\n seven joint angles for each robot arm, without the ZÀÜ term mized C++ code. It takes ‚àº100 ms per frame using one\n t \n in Eq. 8. (ii) Base SSD + Our control uses SSD with Nvidia Pascal Titan X GPU, and we were able to conduct\n future annotations 1 as a perception component and our ma- real-time human-robot collaboration experiments using it.\n nipulationcomponent(from Section III-C)togeneratemotor \n commands. (iii) Our perception + Base control used our V. CONCLUSION \n perception component to predict future hand locations and In this paper, we proposed a new robot activity learning\n thebasecontrolnetworkformanipulation.Inallthesecases, model using a fully convolutional network for future repre-\n the final control of our robot arm is performed by taking sentation regression. The main idea was to make the robot\n advantage of the Baxter API by providing the estimated learn the temporal structure of a human activity as its future\n future joint angle configuration. regression network, and learn to transfer such model for its\n Asaresult,eachparticipantinteractedwiththerobottotal own motor execution using our manipulation network. We\n of 8 timesinarandomorder.Table IVshowstheresults.The show that our approach enables the robot to infer the motor\n resultsindicatethatourparticipantsevaluatedtherobotwith control commands based on the prediction of future human\n our approach performed better on both tasks. We received a handlocationsinreal-time.Theexperimentalresultsconfirm\n higher average score of 3.29 compared to all the baselines that our approach not only predicts the future locations of\n (1.72, 1.92, and 2.29) from the participants. Examples of human/robot hands more reliably, but also is able to make"
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n Time (t) \n w \n e \n iv \n s‚Äôto \n b \n o \n R \n w \n e \n iv \n n \n o \n sre \n p \n dr 3 \n Time (t) \n w \n e \n iv \n s‚Äôto \n b \n o \n R \n w \n e \n iv \n n \n o \n sre \n p \n dr 3 \n Fig.5. Qualitativeresultsofourreal-timerobotexperiments.Similarto Fig.4,therearetwoexamples:clearingthetable,andpushingthetrivettoward\n theperson.Ineachexample,thefirstrowshowstheexactframesusedasinputstoourrobot(takenfromarobotcamera),andthesecondrowshowsthe\n robotandthehumanfroma 3 rdpersonviewpoint.Theframeswerecapturedeveryonesecond. \n robots execute the activities based on predictions. The paper [11] K. Mu¬®lling, J. Kober, O. Kroemer, and J. Peters, ‚ÄúLearning to\n focusesonrobotlearningoflocation-basedhandmovements select and generalize striking movements in robot table tennis,‚Äù The\n International Journalof Robotics Research,2013.\n (i.e., translations and natural rotations), and handling more \n [12] T. Shu, M. S. Ryoo, and S.-C. Zhu, ‚ÄúLearning social affordance\n dynamic hand posture changes remains as one of our future for human-robot interaction,‚Äù in International Joint Conference on\n challenges. Artificial Intelligence(IJCAI),2016. \n [13] H. Koppula and A. Saxena, ‚ÄúPhysically-grounded spatio-temporal\n Acknowledgement: This work was supported by the Army object affordances,‚Äù in European Conference on Computer Vision\n (ECCV),2014. \n Research Laboratory under Cooperative Agreement Number \n [14] J.Walker,A.Gupta,and M.Hebert,‚ÄúPatchtothefuture:Unsupervised\n W 911 NF-10-2-0016. visual prediction,‚Äù in IEEE Conference on Computer Vision and\n Pattern Recognition(CVPR),2014. \n [15] W. Lotter, G. Kreiman, and D. Cox, ‚ÄúDeep predictive coding net-\n REFERENCES \n worksforvideopredictionandunsupervisedlearning,‚Äùar Xivpreprint\n ar Xiv:1605.08104,2016. \n [1] S. Levine, C. Finn, T. Darrell, and P. Abbeel, ‚ÄúEnd-to-end training \n [16] C.Finnand S.Levine,‚ÄúDeepvisualforesightforplanningrobotmo-\n ofdeepvisuomotorpolicies,‚ÄùJournalof Machine Learning Research, \n tion,‚Äùin IEEEInternational Conferenceon Roboticsand Automation\n 2016. \n (ICRA),2017. \n [2] C. Finn, I. Goodfellow, and S. Levine, ‚ÄúUnsupervised learning for \n [17] K.M.Kitani,T.Okabe,Y.Sato,and A.Sugimoto,‚ÄúFastunsupervised\n physicalinteractionthroughvideoprediction,‚Äùin Advances In Neural \n ego-actionlearningforfirst-personsportsvideos,‚Äùin IEEEConference\n Information Processing Systems(NIPS),2016. \n on Computer Visionand Pattern Recognition(CVPR),2011.\n [3] K. Lee, Y. Su, T.-K. Kim, and Y. Demiris, ‚ÄúA syntactic approach \n [18] A. Fathi, A. Farhadi, and J. M. Rehg, ‚ÄúUnderstanding egocentric\n to robot imitation learning using probabilistic activity grammars,‚Äù \n activities,‚Äù in International Conference on Computer Vision (ICCV),\n Roboticsand Autonomous Systems,2013. \n 2011. \n [4] Y. Yang, Y. Li, C. Fermu¬®ller, and Y. Aloimonos, ‚ÄúRobot learning \n [19] H.Pirsiavashand D.Ramanan,‚ÄúDetectingactivitiesofdailylivingin\n manipulation action plans by‚Äù watching‚Äù unconstrained videos from \n first-person camera views,‚Äù in IEEE Conference on Computer Vision\n theworldwideweb.‚Äùin AAAI,2015. \n and Pattern Recognition(CVPR),2012. \n [5] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, ‚ÄúA survey [20] M. S. Ryoo, B. Rothrock, and L. Matthies, ‚ÄúPooled motion features\n of robot learning from demonstration,‚Äù Robotics and Autonomous forfirst-personvideos,‚Äùin IEEEConferenceon Computer Visionand\n Systems,2009. Pattern Recognition(CVPR),2015. \n [6] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and [21] M. S. Ryoo, T. J. Fuchs, L. Xia, J. K. Aggarwal, and L. Matthies,\n A.Berg,‚ÄúSSD:Singleshotmultiboxdetector,‚Äùin European Confer- ‚ÄúRobot-centricactivitypredictionfromfirst-personvideos:Whatwill\n enceon Computer Vision(ECCV),2016. they do to me?‚Äù in ACM/IEEE International Conference on Human-\n [7] C.Vondrick,H.Pirsiavash,and A.Torralba,‚ÄúAnticipatingvisualrep- Robot Interaction(HRI),2015.\n resentationswithunlabeledvideo,‚Äùin IEEEConferenceon Computer [22] I. Gori, J. K. Aggarwal, L. Matthies, and M. S. Ryoo, ‚ÄúMulti-type\n Visionand Pattern Recognition(CVPR),2016. activity recognition in robot-centric scenarios,‚Äù IEEE Robotics and\n [8] A.Billard,S.Calinon,R.Dillmann,and S.Schaal,‚ÄúRobotprogram- Automation Letters(RA-L),2016.\n mingbydemonstration,‚Äùin Springerhandbookofrobotics,2008. [23] S. Bambach, S. Lee, D. J. Crandall, and C. Yu, ‚ÄúLending a hand:\n [9] A.Gupta,C.Eppner,S.Levine,and P.Abbeel,‚ÄúLearningdexterous Detecting hands and recognizing activities in complex egocentric\n manipulationforasoftrobotichandfromhumandemonstrations,‚Äùin interactions,‚Äù in IEEE International Conference on Computer Vision\n IEEE/RSJInternational Conferenceon Intelligent Robotsand Systems (ICCV),2015. \n (IROS),2016. [24] P. F. Alcantarilla, A. Bartoli, and A. J. Davison, ‚ÄúKaze features,‚Äù in\n [10] A.L.Thomazand M.Cakmak,‚ÄúLearningaboutobjectswithhuman European Conferenceon Computer Vision(ECCV),2012.\n teachers,‚Äù in ACM/IEEE International Conference on Human-Robot \n Interaction(HRI),2009. "
  }
]