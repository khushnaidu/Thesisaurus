[
  {
    "page_num": 1,
    "text": " \n \n \n \n \n \n Humanoid Policy ∼ Human Policy \n \n \n Ri-Zhao Qiu*,1 Shiqi Yang*,1 Xuxin Cheng*,1 Chaitanya Chawla*,2 Jialong Li 1\n Tairan He 2 Ge Yan 4 David Yoon 3 Ryan Hoque 3 Lars Paulsen 1 \n Ge Yang 5 Jian Zhang 3 Sha Yi 1 Guanya Shi 2 Xiaolong Wang 1 \n 1 UCSan Diego,2 CMU,3 Apple,4 Universityof Washington,5 MIT \n https://human-as-robot.github.io/ \n \n \n Egocentric Vision Unified State-Action Space Robot Policies \n \n Small-scale \n Humanoid Data \n 1.5 k demos \n \n Fingers / Wrist \n \n Large-scale \n Human Data \n 27 k demos \n \n Figure 1: This paper advocates high-quality human data as a data source for cross-embodiment\n learning-task-orientedegocentrichumandata. Wecollectalarge-scaledataset,Physical Human-\n Humanoid Data (PH 2 D), with hand-finger 3 D poses from consumer-grade VR devices on well-\n definedmanipulationtasksdirectlyalignedwithrobots. Withoutrelyingonmodularperception,we\n train a Human Action Transformer (HAT) manipulation policy by directly modeling humans as a\n differenthumanoidembodimentinanend-to-endmanner. \n Abstract: Trainingmanipulationpoliciesforhumanoidrobotswithdiversedata\n enhancestheirrobustnessandgeneralizationacrosstasksandplatforms.However,\n learningsolelyfromrobotdemonstrationsislabor-intensive,requiringexpensive\n tele-operated data collection, which is difficult to scale. This paper investigates\n amorescalabledatasource,egocentrichumandemonstrations,toserveascross-\n embodiment training data for robot learning. We mitigate the embodiment gap\n between humanoids and humans from both the data and modeling perspectives.\n We collect an egocentric task-oriented dataset (PH 2 D) that is directly aligned\n with humanoid manipulation demonstrations. We then train a human-humanoid\n behavior policy, which we term Human Action Transformer (HAT). The state-\n action space of HAT is unified for both humans and humanoid robots and can\n bedifferentiablyretargetedtorobotactions. Co-trainedwithsmaller-scalerobot\n data,HATdirectlymodelshumanoidrobotsandhumansasdifferentembodiments\n withoutadditionalsupervision. Weshowthathumandataimprovebothgeneral-\n izationandrobustnessof HATwithsignificantlybetterdatacollectionefficiency.\n Keywords: Robot Manipulation,Cross-Embodiment,Humanoid \n 1 Introduction \n \n Learning from real robot demonstrations has led to great progress in robotic manipulation re-\n cently [1, 2, 3, 4]. One key advancement to enable such progress was hardware / software co-\n designs to scale up data collection using teleoperation [5, 6, 7, 8, 9, 10] and directly controlling\n \n \n 9 th Conferenceon Robot Learning(Co RL 2025),Seoul,Korea. \n 5202 \n tc O \n 5 \n ]OR.sc[ \n 3 v 14431.3052:vi Xra "
  },
  {
    "page_num": 2,
    "text": " \n \n \n \n \n \n the robot end effector [11, 12, 5, 6, 13, 7]. Instead of gathering data on a single robot, collective\n effortshavebeenmadetomergediverserobotdataandtrainfoundationalpoliciesacrossembodi-\n ments[11,14,2,1,3,4],whichhaveshowntoimprovecross-embodimentandcross-taskgeneral-\n izability. \n However, collecting struc- \n tured real-robot data Human Robot \n Dataset \n is expensive and time- \n #Frames #Demos #Frames #Demos \n consuming. We are still \n Dex Cap[15] ∼378 k 787 NA NA \n far away from building a \n Ego Mimic[16] ∼432 k† 2,150 1.29 M† 1,000 \n robust and generalizable \n PH 2 D(Ours) ∼3.02 M 26,824 ∼668 k 1,552 \n model as what has been \n achieved in Computer Table 1: Comparisons of task-oriented egocentric human\n Vision [17] and NLP [18]. datasets. Besides having the most demonstrations, PH 2 D is col-\n If we examine humanoid lected on various manipulation tasks, diverse objects and scenes,\n robot teleoperation more withaccurate 3 Dhand-fingerposesandlanguageannotations. †: es-\n timatedbasedonreporteddatacollectiontimewith 30 Hz; whereas\n closely, it involves robots \n Dex Cap[15]and PH 2 Dreportprocessedframesfortraining. \n mimicking human actions \n using geometric transforms or retargeting to control robot joints and end-effectors. From this\n perspective,weproposetomodelrobotsinahuman-centricrepresentation,andtherobotaction\n isjustatransformationawayfromthehumanaction. Ifwecanaccuratelycapturetheend-effector\n and head poses of humans, egocentric human demonstrations will be a more scalable source of\n training data,aswecancollectthemefficiently,inanyplace,andwithoutarobot.\n Inthispaper,weperformcross-humanandhumanoidembodimenttrainingforroboticmanipulation.\n Our key insight is to model bimanual humanoid behaviors by directly imitating human behaviors\n without using learning surrogates such as affordances [19, 20]. To realize this, we first collect\n an egocentric task-oriented dataset of Physical Humanoid-Human Data, dubbed PH 2 D. We adapt\n consumer-grade VRdevicestocollectegocentricvideoswithautomaticbutaccuratehandposeand\n endeffector(i.e.,hand)annotations. Comparedtoexistinghumandailybehaviordatasets[21,22],\n PH 2 D is task-oriented so that it can be directly used for co-training. The same VR hardwares\n arethenusedtoperformteleoperationtocollectsmaller-scalehumanoiddataforbetteralignment.\n We then train a Human-humanoid Action Transformer (HAT), which predicts future hand-finger\n trajectories in a unified human-centric state-action representation space. To obtain robot actions,\n wesimplyapplyinversekinematicsandhandretargetingtodifferentiablyconverthumanactionsto\n robotactionsfordeployment. \n We conduct real-robot evaluations on different manipulation tasks with extensive ablation studies\n toinvestigatehowtobestalignhumanandhumanoiddemonstrations. Inparticular,wefoundthat\n co-training with diverse human data improves robustness against spatial variance and background\n perturbation,generalizinginsettingsunseeninrobotdatabutseeninhumandata. Webelievethat\n thesefindingshighlightthepotentialofusinghumandataforlarge-scalecross-embodimentlearning.\n Insummary,ourcontributionsare: \n • Adataset,PH 2 D,whichisalargeegocentric,task-orientedhuman-humanoiddatasetwith\n accuratehandandwristposesformodelinghumanbehavior(see Tab.1). \n • A cross human-humanoid manipulation policy, HAT, that introduces a unified state-\n actionspaceandotheralignmenttechniquesforhumanoidmanipulation. \n • Improvedpolicyrobustnessandgeneralizationvalidatedbyextensiveexperimentsand\n ablationstudiestoshowthebenefitsofco-trainingwithhumandata. \n 2 Related Work \n Imitation Learningfor Robot Manipulation. Recently, learningrobotpolicywithdatagathered\n directlyfromthemultipleandtargetrobotembodimenthasshownimpressiverobustnessanddex-\n 2 "
  },
  {
    "page_num": 3,
    "text": " \n \n \n \n \n \n terity[23,2,24,1,25,26,9,27,28].Thescaleofdataforimitationlearninghasgrownsubstantially\n withrecentadvancementsindatacollection[29,9,7,8],wherehumanoperatorscanefficientlycol-\n lectlargeamountsofhigh-quality,task-orienteddata.Despitetheseadvances,achievingopen-world\n generalizationstillremainsasignificantchallengeduetolackofinternet-scaletrainingdata.\n Learningfrom Human Videos. Learningpoliciesfromhumanvideosisalong-standingtopicin\n bothcomputervisionandroboticsduetothevastexistenceofhumandata. Existingworkscanbe\n approximatelydividedintotwocategories: aligningobservationsoractions.\n Learn from Human - Aligning Observations. While teleoperating the actual robot platform al-\n lows learning policy with great dexterity, there is still a long way to go to achieve higher levels\n of generalization across diverse tasks, environments, and platforms. Unlike fields such as com-\n puter vision [17] and natural language processing [18] benefiting from internet-scale data, robot\n datacollectionintherealworldisfarmoreconstrained. Variousapproacheshaveattemptedtouse\n internet-scale human videos to train robot policies [30, 31, 32, 33, 34, 35]. Due to various dis-\n crepancies (e.g., supervision and viewpoints) between egocentric robot views and internet videos,\n mostexistingwork[19,20]usemodularapproacheswithintermediaterepresentationsassurrogates\n for training. The most representative ones are affordances [19, 20] for object interaction, object\n keypointspredictions[36,37,38,39,40],orothertypesofobjectrepresentations[41,42,43].\n Learn from Human - Aligning Actions. Beyond observation alignment, transferring human\n demonstrationstoroboticplatformsintroducesadditionalchallengesduetodifferencesinembodi-\n ment,actuation,andcontroldynamics. Specificalignmentofhumanandrobotactionsisrequiredto\n overcomethesedisparities. Approacheshaveemployedmaskinginegocentricviews[16],aligning\n motiontrajectoriesorflow[44,45],object-centricactions[46,47],orhandtrackingwithspecialized\n hardware[15].Mostcloselyrelatedtoourwork,Human Plus[48]designsaremappingmethodfrom\n 3 Dhumanposeestimationtotele-operatehumanoidrobots. Comparedto Human Plus, theinsight\n ofourmethodistowaivetherequirementforrobothardwareincollectinghumandataandcollect\n diversehumandatadirectlyforco-training. Incontrastto Human Plus, weintentionallyavoidper-\n formingretargetingonhumandemonstrationsanddesignedthepolicytodirectlyusehumanhand\n poses as states/actions. On the other hand, the ‘human shadowing’ retargeting in Human Plus is a\n teleoperationmethodthatstillrequiresrobots,leadingtolowercollectionefficiencythanours.\n Cross-Embodiment. Cross-embodimentpre-traininghasbeenshowntoimproveadaptabilityand\n generalizationoverdifferentembodiments[49,50,51,52,53,54,55,56,57,58,59,60,61]. When\n utilizinghumanvideos,introducingintermediaterepresentationscanbepronetocompositeerrors.\n Recent works investigate end-to-end approaches [2, 24, 1, 3] using cross-embodied robot data to\n reducesuchcompoundingperceptiveerrors. Noticeably,theseworkshavefoundthatsuchend-to-\n end learning leads to desired behaviors such as retrying [3]. Some other work [62, 38] enforces\n viewpoint constraints between training human demonstrations and test-time robot deployment to\n allowlearningonhumandatabutittradesoffthescalabilityofthedatacollectionprocess.\n Concurrent Work. Some concurrent work [15, 16, 63] also attempts to use egocentric human\n demonstrationsforend-to-endcross-embodimentpolicylearning. Dex Cap[15]usesglovestotrack\n 3 Dhandposeswithachest-mounted RGBDcameratocaptureegocentrichumanvideos. However,\n Dex Cap relies on 3 D inputs, whereas some recent works [3, 1] have shown the scalability of 2 D\n visualinputs. Mostrelatedtoourwork,Ego Mimic[16]alsoproposestocollectdatausingwearable\n device [64] with 2 D visual inputs. However, Ego Mimic requires strict visual sensor alignments;\n whereasweshowthatscalingupdiverseobservationswithdifferentcamerasmakesthepolicymore\n robust. In addition, PH 2 D is also greater in dataset scale and object diversity. We also show our\n policy can be deployed on real robots without strict requirements of visual sensors and heuristics,\n whichpavesthewayforscalabledatacollection. \n \n \n \n \n 3 \n \n "
  },
  {
    "page_num": 4,
    "text": " \n \n \n \n \n \n 3 Method \n \n To collect more data to train generalizable robot policies, recent research has explored cross-\n embodimentlearning, enablingpoliciestogeneralizeacrossdiversephysicalforms[3,1,4,2,65,\n 14]. This paper proposes egocentric human manipulation demonstrations as a scalable source of\n cross-embodimenttrainingdata. Sec.3.1 describesourapproachtoadaptconsumer-grade VRde-\n vicestoscaleuphumandatacollectionconvenientlyforadatasetoftask-orientedegocentrichuman\n demonstrations. Sec. 3.2 describesvarious techniques to handle domain gaps to align human data\n androbotdataforlearninghumanoidmanipulationpolicy. \n 3.1 PH 2 D:Task-oriented Physical Humanoid-Human Data \n \n Though there has been existing work that collects egocentric human videos [16, 22, 21, 15], they\n either (1) provide demonstrations mostly for non-task-oriented skills (e.g., dancing) and do not\n provideworld-frame 3 Dheadandhandposesestimationsforimitationlearningsupervision[21,22]\n or(2)requirespecializedhardwareorrobotsetups[15,16]. \n To address these issues, we propose PH 2 D. \n PH 2 Daddressthesetwoissuesby(1)collecting \n task-orientedhumandemonstrationsthataredi- \n rectly related to robot execution, (2) adapting \n well-engineered SDKs of VR devices (illus- \n Camera / \n tratedin Fig.2)toprovidesupervision,and(3) Pose Tracking Camera \n diversifying tasks, camera sensors, and reduc- \n Figure 2: Consumer-grade Devicesfor Data Collec-\n ing whole-body movement to reduce domain \n tion.Toavoidrelyingonspecializedhardwarefordata\n gapsinbothvisionandbehaviors. collectiontomakeourmethodscalable,wedesignour\n data collection process using consumer-grade VR de-\n Adapting Low-cost Commerical Devices vices. \n With development in pose estimation [66] and \n systemengineering,modernmobiledevicesarecapableofprovidingaccurateon-deviceworldframe\n 3 Dheadposetrackingand 3 Dhandkeypointtracking[9],whichhasprovedtobestableenoughto\n teleoperaterobotinreal-time[9,13]. Wedesignsoftwareandhardwaretosupportconvenientdata\n collectionacrossdifferentdevices. Differentcamerasprovidebettervisualdiversity.\n • Apple Vision Pro+Built-in Camera.Wedevelopeda Vision OSAppthatusesthebuilt-in\n cameraforvisualobservationandusesthe Apple ARKitfor 3 Dheadandhandposes.\n • Meta Quest 3/Apple Vision Pro+ZEDCamera.Wedevelopedaweb-basedapplication\n based on Open Television [9] to gather 3 D head and hand poses. We also designed a 3 D-\n printedholdertomount ZEDMini Stereocamerasonthesedevices. Thisconfigurationis\n bothlow-cost(<700$)andintroducesmorediversitywithstereocameras. \n Data Collection Pipeline We collect task-oriented egocentric human demonstrations by asking\n human operators to perform tasks overlapping with robot execution (e.g., grasping and pouring)\n when wearing the VR devices. For every demonstration, we provide language instructions (e.g.,\n graspacanofcokezerowithrighthand),andsynchronizeproprioceptioninputsandvisualinputs\n byclosesttimestamps. \n Action Domain Gap. Humanactionsandtele-operatedrobotactionsexhibittwodistinctcharacter-\n istics:(1)humanmanipulationusuallyinvolvesinvoluntarywhole-bodymovement,and(2)humans\n aremoredexterousthanrobotsandhavesignificantlyfastertaskcompletiontimethanrobots. We\n mitigatethefirstgapbyrequestingthehumandatacollectorstositinanuprightposition. Forthe\n secondspeedgap,weinterpolatetranslationandrotationsofhumandataduringtraining(effectively\n ‘slowingdown’actions). Theslow-downfactorsα areobtainedbynormalizingtheaveragetask\n slow \n completiontimeofhumansandhumanoids,whichisempiricallydistributedaround 4. Forconsis-\n tency,weuseα =4 inalltasks. \n slow \n 4 "
  },
  {
    "page_num": 5,
    "text": " \n \n \n \n \n \n \n \n HAT \n \n \n \n … \n remrofsnar T \n … \n Robot Observation Robot Human Robot Data / Deployment \n Human \n Both \n Teleoperator Dino V 2❄ \n Humanoid \n 6 Do FWrist Pose 6 Do F Wrist Pose \n 3 D Hand 3 D Hand \n Keypoints Keypoints \n Dino V 2❄ \n Human Observation Action Prediction \n Unified \n Distribution \n Inverse \n Kinematics \n Head Pose \n Inverse \n Forward Kinematics \n Kinematics \n Retargeting \n Human Human Data \n Demonstration \n Figure 3: Overviewof HAT.Human Action Transformer(HAT)learnsarobotpolicybymodeling\n humans. Duringtraining,wesampleastate-actionpairfromeitherhumandataorrobotdata. The\n images are encoded by a frozen Dino V 2 encoder [67]. The HAT model makes predictions in a\n human-centric action space using wrist 6 Do F poses and finger tips, which is retargeted to robot\n posesduringreal-robotdeployment. \n 3.2 HAT:Human Action Transformer \n HAT learns cross-embodied robot policy by modeling humans. We demonstrate that treating bi-\n manualhumanoidrobotsandhumansasdifferentrobotembodimentsviaretargetingimprovesboth\n generalizabilityandrobustnessof HAT. \n More concretely, let D = {(S ,A )}N be the set of data collected from real bimanual\n robot i i i=1 \n humanoid robots using teleoperation [9], where S is the states including proprioceptive and vi-\n i \n sual observations of i-th demonstration and A be the actions. The collected PH 2 D dataset,\n i \n D = {(S˜ ,A˜ )}M is used to augment the training process. Note that it is reasonable to\n human i i i=1 \n assume M ≫N duetothesignificantlybetterhumandatacollectionefficiency. \n Thegoalistodesignapolicyπ : S → Athatpredictsfuturerobotactionsa givencurrentrobot\n t \n observations attimet, wherethefutureactionsa isusuallyachunkofactionsformulti-step\n t t+1 \n execution (with slight abuse of notation). We model π using HAT, which is a transformer-based\n architecture predicting action chunks [5]. The overview of the model is illustrated in Fig. 3. We\n discusskeydesignchoicesof HATwithexperimentalablations. \n Unified State-Action Space. Both bimanual robots and humans have two end effectors. In our\n case,ourrobotsarealsoequippedwithanactuated 2 Do Fneckthatcanrotate,whichresemblesthe\n autonomous head movement when humans perform manipulation. Therefore, we design a unified\n state-actionspace(i.e., (S,A) ≡ (S˜,A˜))forbothbimanualrobotsandhumans. Moreconcretely,\n theproprioceptiveobservationisa 54-dimensionalvector(6 Drotations[68]ofthehead,leftwrist,\n andrightwrist; x/y/zofleftandrightwristsand 10 fingertips). Inthiswork,sincewedeployour\n policyonrobotswith 5-fingereddexteroushands(shownin Fig.4),thereexistsabijectivemapping\n betweenthefingertipsofrobothandsandhumanhands.Notethatinjectivemappingisalsopossible\n (e.g.,mappingdistancebetweenthethumbfingerandotherfingerstoparallelgripperdistance).\n Visual Domain Gap. Two types of domain gaps exist for co-training on human/humanoid data:\n camera sensors and end effector appearance. Since our human data collection process includes\n cameras different from robot deployment, this leads to camera domain gaps such as tones. Also,\n the appearances of human and humanoid end effectors are different. However, with sufficiently\n largeanddiversedata, wefinditnotastrictnecessitytoapplyheuristicprocessingsuchasvisual\n artifacts[16]orgenerativemethods[69]totrainhuman-robotpolicies-basicimageaugmentations\n suchascolorjitteringand Gaussianblurringareeffectiveregularization. \n 5 "
  },
  {
    "page_num": 6,
    "text": " \n \n \n \n \n \n Passing Horizontal Grasp Vertical Grasp Pouring Ovr.Succ.\n Meth. H.Data D.Norm \n I.D. O.O.D. I.D. O.O.D. I.D. O.O.D. I.D. O.O.D. I.D. O.O.D.\n ACT ✗ NA 19/20 36/60 8/10 7/30 7/20 15/70 8/10 1/10 42/60 59/170\n HAT ✓ ✗ 17/20 51/60 9/10 11/30 14/20 30/70 5/10 5/10 45/60 97/170\n HAT ✓ ✓ 20/20 52/60 8/10 12/30 13/20 29/70 8/10 8/10 49/60 101/170\n Typeof Generalization Background Texture Obj.Placement \n Table 2: Success rate of autonomous skill execution. Co-training with human data (H. Data)\n significantlyimprovesthe Out-Of-Distribution(O.O.D.)performancewithnearly 100%relativeim-\n provementonalltaskson Humanoid A.Wealsoablatethedesignchoiceofusingdifferentnormal-\n izations(D.Norm)fordifferentembodiments. Wedesignateeachtasksettingtoinvestigateasingle\n typeofgeneralization. Detailedanalysisofeachtypeofgeneralizationispresentedin Sec.C.\n Training. Thefinalpolicyisdenotedasπ : f (·) → Aforbothhumanandrobotpolicy,wheref\n θ θ \n isatransformer-basedneuralnetworkparametrizedbyθ. Thefinallossisgivenby,\n L=ℓ (π(s ),a )+λ·ℓ (π(s ) ,a ), (1) \n 1 i i 1 i EEF i,EEF \n where EEF are the indices of the translation vectors of the left and right wrists, and λ = 2 is\n an (insensitive) hyperparameter used to balance loss to emphasize the importance of end effector\n positionsoverlearningunnecessarilyprecisefingertipkeypoints. \n 4 Experiments \n Hardware Platforms. We run our experi- \n ments on two humanoid robots (Humanoid A \n and Humanoid B shown in Fig. 4) equipped \n with 6-DOF Inspire dexterous hands. Hu- \n manoid Aisa Unitree H 1 robotand Humanoid \n B is a Unitree H 1 2 robot with different arm \n configurations. Similartohumans,bothrobots \n (1)areequippedwithactuatednecks[9]toget (a) Humanoid A (b) Humanoid B\n make use of egocentric views and (2) do not \n have wrist cameras. Unless otherwise noted, Figure 4: Hardware Illustration. Most robot data\n mosthumanoiddatacollectionisdonewith Hu- attributes to Humanoid A, a Unitree H 1 robot. Hu-\n manoid B,a Unitree H 1-2 robotwithdifferentarmmo-\n manoid A.Weuse Humanoid Bmainlyfortest- \n tor configurations, is used to evaluate few-shot cross-\n ingcross-humanoidgeneralization. \n humanoidtransfer.Detailedcomparisonsin Sec.D\n Implementation Details. Weimplementpolicyarchitecturebyadoptingantransformer-basedar-\n chitecture predicting future action chunks [5]. We use a frozen Dino V 2 Vi T-S [67] as the visual\n backbone. Weimplementtwovariants: (1)ACT:baselineimplementationusingthe Action Chunk\n Transformer[5], trainedusingonlyrobotdata. Robotstatesarerepresentedasjointpositions. (2)\n HAT: same architecture as ACT, but the state encoder operates in the unified state-action space.\n Unless otherwise stated, HAT is co-trained on robot and human data. A checkpoint is trained for\n eachtaskwithapproximately 250-400 robotdemonstrations. \n Experimental Protocol. We collect robot and human demonstrations in different object sets.\n Sincehumandemonstrationsareeasiertocollect, thesettingsinhumandemonstrationsaregener-\n allymorediverse,whichincludebackground,objecttypes,objectpositions,andtherelativeposition\n ofthehumantothetable. \n We experimented with four different dexterous manipulation tasks and investigated in-distribution\n and out-of-distribution setups. The in-distribution (I.D.) setting tests the learned skills with back-\n groundsandobjectarrangementsapproximatelysimilartothetrainingdemonstrationspresentedin\n thereal-robotdata. Inthe Out-Of-Distribution(O.O.D.)setting,wetestgeneralizabilityandrobust-\n ness by introducing novel setups that were presented in human data but not in robot data. Fig. 7\n visualizesdifferentmanipulationtasksandhowwedefineout-of-distributionsettingsforeachtask.\n 6 "
  },
  {
    "page_num": 7,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n (a)Performanceof Humanoid Bco-trainedwith PH 2 D (b) Co-training consistently outperforms isolated\n onhorizontalgrasping.o 1 isseenby Humanoid B.o 2 training as Humanoid B demonstrations increase,\n ando 3 seeninhumandata.o 4 isunseeninalldata. achievinggoodsuccessrateseveninlow-dataregimes.\n Figure 5: Few-Shot Adaptation. Co-training consistently outperforms isolated training as Hu-\n manoid Bdemonstrationsincrease,achievingrobustsuccessrateseveninlow-dataregimes.\n 4.1 Main Evaluation \n \n Human data has minor effects on I.D. testing. From Tab. 2, we can see that I.D. performance\n with or without co-training with human data gives similar results. In the I.D. setting, we closely\n match the scene setups as training demonstrations, including both background, object types, and\n objectplacements. Thus,policiestrainedwithonlyasmallamountof Humanoid Adataperformed\n wellinthissetting. Thisfindingisconsistentwithrecentwork[9,7]thatfrozenvisualfoundation\n models[17,67]improverobustnessagainstcertainexternalperturbationssuchaslighting.\n Humandataimprovesthe O.O.D.settingswithmanygeneralizations. Onecommonchallenge\n inimitationlearningisoverfittingtoonlyin-distributiontasksettings.Hence,itiscrucialforarobot\n policy to generalize beyond the scene setups seen in a limited set of single-embodiment data. To\n demonstrate how co-training with human data reduces such overfitting, we introduce O.O.D. task\n settings to evaluate such generalization. From Tab. 2, we can see that co-training drastically im-\n proves O.O.D.settings,achievingnearly 100%relativeimprovementinsettingsunseenbytherobot\n data. In particular, we find that human data improves three types of generalization: background,\n objectplacement, andappearance. Toisolatetheeffectofeachvariable, eachtaskfocusesona\n specifictypeofgeneralizationaslistedin Tab.2,within-depthanalysesin Sec.C.\n 4.2 Few-Shot Transferacross Heterogenous Embodiments \n \n Weconductedfew-shotgeneralizationexperimentsonadistincthumanoidplatform(Humanoid B),\n contrastingitwithourprimaryplatform,Humanoid A.Notably,Humanoid B’sdemonstrationdata\n werecollectedinanentirelyseparateenvironment,introducingbothembodimentandenvironmental\n shifts. We highlight two key advantages of our approach: (1) the ability to unify heterogeneous\n human-centricdatasources(humanoidsandhumans)intoageneralizablepolicyframework,and(2)\n thecapacitytorapidlyadapttonewembodimentswithdrasticallyreduceddatarequirements.\n Experiment 1: Cross-embodiment co-training efficacy Using only 20 demonstrations from Hu-\n manoid B, we trained 3 policies - respectively on data from (i) Humanoid B only, (ii) Humanoid\n B + Humanoid A (cross-embodiment), and (iii) Humanoid B + Humanoid A + Human (cross-\n embodimentandhumanpriors). Asshownin Fig.5 a,co-trainedpolicies(ii)and(iii)substantially\n outperformedthe Humanoid B-onlybaselinesonalltasksettings,underscoringthemethod’sability\n totransferlatenttaskstructureacrossembodiments. \n Experiment 2: Scaling Demonstrationsfor Few-Shot Adaptation Wefurtherquantifiedtherelation-\n shipbetweenrequiredforfew-shotgeneralization. Wehold Humanoid Aandhumandatasetsfixed\n \n 7 \n \n "
  },
  {
    "page_num": 8,
    "text": " \n \n \n \n \n \n Robot Only –28/90 Co-Trained –35/90 \n \n Task State Space Action Speed Success \n ✓ ✗ 1/10 \n Vertical Grasping ✗ ✓ 0/10 \n ✓ ✓ 4/10 \n Table 3: Importance of unifying policy inputs and out-\n Figure 6: Human data has better puts. Wereportthenumberofsuccessesofverticalgrasp-\n sampling efficiency. Per-grid vertical ing objects in the upper-left block as illustrated in Fig. 8.\n graspingsuccessesoutof 10 trialswith Baselinesusejointpositionsasstateinputordonotinter-\n modelstrainedwithrobot-onlydataand polatehumanmotions. \n mixed data. Red boxes indicate where \n trainingdataiscollected. \n for the horizontal grasping task and ablate number of demonstrations required for Humanoid B in\n Fig. 5. Co-training (Humanoid B + A + Human) consistently outperformed isolated training on\n Humanoid Bacrossallsettings,especiallyinthefew-dataregime. \n 4.3 Ablation Study \n \n Sampling Efficiencyof Humanand Humanoid Data. Conceptually,collectinghumandataisless\n expensive, not just because it can be done faster, but also because it can be done in in-the-wild\n scenes;reducessetupcostbeforeeverydatacollection;andavoidsthehardwarecosttoequipevery\n operatorwithrobots. \n Weperformadditionalexperimentstoshowthateveninthelabsetting,humandatacanhavebetter\n samplingefficiencyinunittime. Inparticular,weprovideasmall-scaleexperimentonthevertical\n grasping task. Allocating 20 minutes for two settings, we collected (1) 60 Humanoid A demon-\n strations,(2)30 Humanoid Ademonstrations,and 120 humandemonstrations. Toavoidconflating\n diversityanddatasize,theobjectplacementsinalldemonstrationsareevenlydistributedatthebot-\n tom 6 cells. The results are given in Fig. 6. The policy trained with mixed robot and human data\n performssignificantlybetter,whichvalidatesthesamplingefficiencyofhumandataoverrobotdata.\n Eachcellrepresentsa 10 cm×10 cmregionwheretherobotattemptstopickupabox. \n State-Action Design. In Tab.3,weablatethedesignchoicesoftheproprioceptionstatespaceand\n the speed of output actions. In particular, using the same set of robot and human data, we imple-\n ment two baselines: 1) a unified state-action space, but does not interpolate (i.e., slow down) the\n humanactions;and 2)abaselinethatinterpolateshumanactionsbutusesseparatestaterepresenta-\n tionforhumanoid(jointpositions)andhumans(EEFrepresentation). Thepoliciesexhibitdifferent\n failurepatternsduringtherolloutofthesetwobaselines. Withoutinterpolatinghumanactions,the\n speed of the predicted actions fluctuates between fast (resembling humans) and slow (resembling\n teleoperation),whichleadstoinstability. Withoutaunifiedstatespace,thepolicyisgivena‘short-\n cut’ to distinguish between embodiments, which leads to on-par in-distribution performance and\n significantlyworse OODperformance. \n More Ablation Study. Duetospacelimit,pleaserefertotheappendixandthesupplementaryfor\n morequalitativevisualizationandquantitativeablationstudies. \n 5 Conclusions \n \n Thispaperproposes PH 2 D,anefforttoconstructalarge-scalehumantask-orientedbehaviordataset,\n alongwiththetrainingpipeline HAT,whichleverages PH 2 Dandrobotdatatoshowhowhumanscan\n betreatedasadatasourceforcross-embodimentlearning.Weshowthatitispossibletodirectlytrain\n animitationlearningmodelwithmixedhuman-humanoiddatawithoutanytrainingsurrogateswhen\n thehumandataarealignedwiththerobotdata. Thelearnedpolicyshowsimprovedgeneralization\n androbustnesscomparedtothecounterparttrainedusingonlyreal-robotdata. \n \n 8 \n \n "
  },
  {
    "page_num": 9,
    "text": " \n \n \n \n \n \n 6 Limitations \n Althoughwealsocollectlanguageinstructionsin PH 2 D,duetoourfocusoninvestigatingtheem-\n bodiment gap between humans and humanoids, one limitation of the current version of the paper\n uses a relatively simple architecture for learning policy. In the near future, we plan to expand the\n policylearningprocesstotrainalargelanguage-conditionedcross-embodimentpolicytoinvestigate\n generalizationtonovellanguageusinghumandemonstrations. Thecollectionofhumandatarelies\n onoff-the-shelf VRhardwaresandtheirhandtracking SDKs.Sincethese SDKsweretrainedmostly\n for VR applications, hand keypoint tracking can fail for certain motions with heavy occlusion. In\n addition, though the proposed method conceptually extends to more robot morphologies, current\n evaluationsaredoneonrobotsequippedwithdexteroushands. \n \n 7 Acknowledgment \n \n Thisworkwassupported,inpart,by NSFCAREERAward IIS-2240014,NSFCCF-2112665(TI-\n LOS),andgiftsfrom Amazon,Metaand Apple. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 9 \n \n "
  },
  {
    "page_num": 10,
    "text": " \n \n \n \n \n \n References \n [1] S.Liu,L.Wu,B.Li,H.Tan,H.Chen,Z.Wang,K.Xu,H.Su,and J.Zhu. Rdt-1 b: adiffusion\n foundationmodelforbimanualmanipulation. ar Xivpreprintar Xiv:2410.07864,2024.\n \n [2] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna,\n C. Xu, J. Luo, T. Kreiman, Y. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh,\n C. Finn, and S. Levine. Octo: An open-source generalist robot policy. In Proceedings of\n Robotics: Scienceand Systems,2024. \n [3] K.Black,N.Brown,D.Driess,A.Esmail,M.Equi,C.Finn,N.Fusai,L.Groom,K.Hausman,\n B. Ichter, et al. π : A vision-language-action flow model for general robot control. ar Xiv\n 0 \n preprintar Xiv:2410.24164,2024. \n [4] S.Dasari,O.Mees,S.Zhao,M.K.Srirama,and S.Levine. Theingredientsforroboticdiffu-\n siontransformers. ar Xivpreprintar Xiv:2410.10088,2024. \n [5] T.Z.Zhao, V.Kumar, S.Levine, and C.Finn. Learningfine-grainedbimanualmanipulation\n withlow-costhardware. ar Xivpreprintar Xiv:2304.13705,2023. \n [6] Z.Fu, T.Z.Zhao, and C.Finn. Mobilealoha: Learningbimanualmobilemanipulationwith\n low-costwhole-bodyteleoperation. ar Xivpreprintar Xiv:2401.02117,2024.\n \n [7] C.Chi,Z.Xu,C.Pan,E.Cousineau,B.Burchfiel,S.Feng,R.Tedrake,and S.Song. Universal\n manipulationinterface: In-the-wildrobotteachingwithoutin-the-wildrobots. ar Xivpreprint\n ar Xiv:2402.10329,2024. \n [8] S. Yang, M. Liu, Y. Qin, R. Ding, J. Li, X. Cheng, R. Yang, S. Yi, and X. Wang. Ace: A\n cross-platformvisual-exoskeletonssystemforlow-costdexterousteleoperation.ar Xivpreprint\n ar Xiv:2408.11805,2024. \n [9] X.Cheng,J.Li,S.Yang,G.Yang,and X.Wang. Open-television: Teleoperationwithimmer-\n siveactivevisualfeedback. In Conferenceon Robot Learning(Co RL),2024. \n \n [10] T.He,Z.Luo,X.He,W.Xiao,C.Zhang,W.Zhang,K.Kitani,C.Liu,and G.Shi. Omnih 2 o:\n Universal and dexterous human-to-humanoid whole-body teleoperation and learning. ar Xiv\n preprintar Xiv:2406.08858,2024. \n [11] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and\n C.Finn. Robonet: Large-scalemulti-robotlearning. ar Xivpreprintar Xiv:1910.11215,2019.\n \n [12] H.Bharadhwaj,J.Vakil,M.Sharma,A.Gupta,S.Tulsiani,and V.Kumar. Roboagent: Gener-\n alizationandefficiencyinrobotmanipulationviasemanticaugmentationsandactionchunking.\n In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 4788–\n 4795.IEEE,2024. \n [13] H.Ha,Y.Gao,Z.Fu,J.Tan,and S.Song. Umionlegs: Makingmanipulationpoliciesmobile\n withmanipulation-centricwhole-bodycontrollers. ar Xivpreprintar Xiv:2407.10353,2024.\n [14] A.O’Neill,A.Rehman, A.Gupta, A.Maddukuri, A.Gupta, A.Padalkar, A.Lee, A.Pooley,\n A.Gupta,A.Mandlekar,etal.Openx-embodiment:Roboticlearningdatasetsandrt-xmodels.\n ar Xivpreprintar Xiv:2310.08864,2023. \n \n [15] C.Wang,H.Shi,W.Wang,R.Zhang,L.Fei-Fei,and C.K.Liu.Dexcap:Scalableandportable\n mocap data collection system for dexterous manipulation. ar Xiv preprint ar Xiv:2403.07788,\n 2024. \n [16] S. Kareer, D. Patel, R. Punamiya, P. Mathur, S. Cheng, C. Wang, J. Hoffman, and D. Xu.\n Egomimic: Scalingimitationlearningviaegocentricvideo. ar Xivpreprintar Xiv:2410.24221,\n 2024. URLhttps://arxiv.org/abs/2410.24221. \n \n 10 \n \n "
  },
  {
    "page_num": 11,
    "text": " \n \n \n \n \n \n [17] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n P.Mishkin,J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervi-\n sion. In ICML.PMLR,2021. \n [18] Open AI. Gpt-4 technicalreport. Technicalreport,Open AI,2023. \n \n [19] R.Mendonca,S.Bahl,and D.Pathak. Structuredworldmodelsfromhumanvideos. In RSS,\n 2023. \n [20] S.Bahl,R.Mendonca,L.Chen,U.Jain,and D.Pathak. Affordancesfromhumanvideosasa\n versatilerepresentationforrobotics. In CVPR,2023. \n [21] K.Grauman,A.Westbury,E.Byrne,Z.Chavis,A.Furnari,R.Girdhar,J.Hamburger,H.Jiang,\n M.Liu,X.Liu,etal. Ego 4 d: Aroundtheworldin 3,000 hoursofegocentricvideo. In CVPR,\n 2022. \n \n [22] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti,\n J. Munro, T. Perrett, W. Price, and M. Wray. Scaling egocentric vision: The epic-kitchens\n dataset. In ECCV,2018. \n [23] T.Z.Zhao,J.Tompson,D.Driess,P.Florence,K.Ghasemipour,C.Finn,and A.Wahid.Aloha\n unleashed: Asimplerecipeforrobotdexterity. ar Xivpreprintar Xiv:2410.13126,2024.\n \n [24] L.Wang,X.Chen,J.Zhao,and K.He. Scalingproprioceptive-visuallearningwithheteroge-\n neouspre-trainedtransformers. ar Xivpreprintar Xiv:2409.20537,2024. \n [25] C.Chi,Z.Xu,S.Feng,E.Cousineau,Y.Du,B.Burchfiel,R.Tedrake,and S.Song. Diffusion\n policy:Visuomotorpolicylearningviaactiondiffusion. The International Journalof Robotics\n Research,page 02783649241273668,2023. \n [26] R.-Z.Qiu,Y.Song,X.Peng,S.A.Suryadevara,G.Yang,M.Liu,M.Ji,C.Jia,R.Yang,X.Zou,\n etal.Wildlma:Longhorizonloco-manipulationinthewild.ar Xivpreprintar Xiv:2411.15131,\n 2024. \n \n [27] C. Lu, X. Cheng, J. Li, S. Yang, M. Ji, C. Yuan, G. Yang, S. Yi, and X. Wang. Mobile-\n television: Predictivemotionpriorsforhumanoidwhole-bodycontrol. In ICRA,2025.\n [28] Y. Ze, Z. Chen, W. Wang, T. Chen, X. He, Y. Yuan, X. B. Peng, and J. Wu. Generalizable\n humanoidmanipulationwithimproved 3 ddiffusionpolicies.ar Xivpreprintar Xiv:2410.10803,\n 2024. \n \n [29] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto. Dexterous imitation made easy: A\n learning-based framework for efficient dexterous manipulation. In 2023 ieee international\n conferenceonroboticsandautomation(icra),pages 5954–5961.IEEE,2023. \n [30] A.S. Chen, S. Nair, and C.Finn. Learninggeneralizable roboticreward functionsfrom” in-\n the-wild”humanvideos. ar Xivpreprintar Xiv:2103.16817,2021. \n [31] J. Lee and M. S. Ryoo. Learning robot activities from first-person human videos using con-\n volutionalfutureregression. In Proceedingsofthe IEEEConferenceon Computer Visionand\n Pattern Recognition Workshops,pages 1–2,2017. \n \n [32] K. Lee, Y. Su, T.-K. Kim, and Y. Demiris. A syntactic approach to robot imitation learning\n usingprobabilisticactivitygrammars. Roboticsand Autonomous Systems,61(12):1323–1334,\n 2013. \n [33] A. Nguyen, D. Kanoulas, L. Muratore, D. G. Caldwell, and N. G. Tsagarakis. Translating\n videos to commands for robotic manipulation with deep recurrent neural networks. In 2018\n IEEEInternational Conferenceon Roboticsand Automation(ICRA),pages 3782–3788.IEEE,\n 2018. \n \n 11 \n \n "
  },
  {
    "page_num": 12,
    "text": " \n \n \n \n \n \n [34] J.Rothfuss,F.Ferreira,E.E.Aksoy,Y.Zhou,and T.Asfour. Deepepisodicmemory: Encod-\n ing,recalling,andpredictingepisodicexperiencesforrobotactionexecution. IEEERobotics\n and Automation Letters,3(4):4007–4014,2018. \n [35] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot learning manipulation action plans\n by” watching” unconstrained videos from the world wide web. In Proceedings of the AAAI\n conferenceonartificialintelligence,volume 29,2015. \n [36] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani. Track 2 act: Predicting point tracks\n frominternetvideosenablesdiversezero-shotrobotmanipulation. In ECCV,2024.\n \n [37] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,and P.Abbeel. Any-pointtrajectorymodeling\n forpolicylearning. ar Xivpreprintar Xiv:2401.00025,2023. \n [38] J.Li,Y.Zhu,Y.Xie,Z.Jiang,M.Seo,G.Pavlakos,and Y.Zhu. Okami: Teachinghumanoid\n robots manipulation skills through single video imitation. ar Xiv preprint ar Xiv:2410.11792,\n 2024. \n \n [39] N. Das, S. Bechtle, T. Davchev, D. Jayaraman, A. Rai, and F. Meier. Model-based inverse\n reinforcementlearningfromvisualdemonstrations. In Conferenceon Robot Learning,pages\n 1930–1942.PMLR,2021. \n [40] H.Xiong, Q.Li, Y.-C.Chen, H.Bharadhwaj, S.Sinha, and A.Garg. Learningbywatching:\n Physicalimitationofmanipulationskillsfromhumanvideos. In 2021 IEEE/RSJInternational\n Conferenceon Intelligent Robotsand Systems(IROS),pages 7827–7834.IEEE,2021.\n [41] S. Pirk, M. Khansari, Y. Bai, C. Lynch, and P. Sermanet. Online object representations with\n contrastivelearning. ar Xivpreprintar Xiv:1906.04312,2019. \n \n [42] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,and A.Gupta. R 3 m: Auniversalvisualrepresen-\n tationforrobotmanipulation. ar Xivpreprintar Xiv:2203.12601,2022. \n [43] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards\n universal visual reward and representation via value-implicit pre-training. ar Xiv preprint\n ar Xiv:2210.00030,2022. \n \n [44] L.-H.Lin,Y.Cui,A.Xie,T.Hua,and D.Sadigh. Flowretrieval:Flow-guideddataretrievalfor\n few-shotimitationlearning. ar Xivpreprintar Xiv:2408.16944,2024. \n [45] J. Ren, P. Sundaresan, D. Sadigh, S. Choudhury, and J. Bohg. Motion tracks: A uni-\n fied representation for human-robot transfer in few-shot imitation learning. ar Xiv preprint\n ar Xiv:2501.06994,2025. \n [46] Y. Zhu, A. Lim, P. Stone, and Y. Zhu. Vision-based manipulation from single human video\n withopen-worldobjectgraphs. ar Xivpreprintar Xiv:2405.20321,2024. \n \n [47] C.-C.Hsu,B.Wen,J.Xu,Y.Narang,X.Wang,Y.Zhu,J.Biswas,and S.Birchfield. Spot: Se\n (3)posetrajectorydiffusionforobject-centricmanipulation.ar Xivpreprintar Xiv:2411.00965,\n 2024. \n [48] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn. Humanplus: Humanoid shadowing and\n imitationfromhumans. In Co RL,2024. \n \n [49] W. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular\n policiesforagent-agnosticcontrol. In International Conferenceon Machine Learning,pages\n 4455–4464.PMLR,2020. \n [50] L. Y. Chen, K. Hari, K. Dharmarajan, C. Xu, Q. Vuong, and K. Goldberg. Mirage: Cross-\n embodimentzero-shotpolicytransferwithcross-painting. ar Xivpreprintar Xiv:2402.19249,\n 2024. \n \n 12 \n \n "
  },
  {
    "page_num": 13,
    "text": " \n \n \n \n \n \n [51] J.Yang,C.Glossop,A.Bhorkar,D.Shah,Q.Vuong,C.Finn,D.Sadigh,and S.Levine. Push-\n ingthelimitsofcross-embodimentlearningformanipulationandnavigation. ar Xivpreprint\n ar Xiv:2402.19432,2024. \n [52] J.Yang,D.Sadigh,and C.Finn. Polybot: Trainingonepolicyacrossrobotswhileembracing\n variability. ar Xivpreprintar Xiv:2307.03719,2023. \n [53] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and\n S.Levine. Bridgedata: Boostinggeneralizationofroboticskillswithcross-domaindatasets.\n ar Xivpreprintar Xiv:2109.13396,2021. \n \n [54] T.Franzmeyer,P.Torr,and J.F.Henriques. Learnwhatmatters: cross-domainimitationlearn-\n ingwithtask-relevantembeddings. Advancesin Neural Information Processing Systems,35:\n 26283–26294,2022. \n [55] A.Ghadirzadeh,X.Chen,P.Poklukar,C.Finn,M.Bjo¨rkman,and D.Kragic. Bayesianmeta-\n learningforfew-shotpolicyadaptationacrossroboticplatforms. In 2021 IEEE/RSJInterna-\n tional Conferenceon Intelligent Robotsand Systems(IROS),pages 1274–1280.IEEE,2021.\n \n [56] T.Shankar,Y.Lin,A.Rajeswaran,V.Kumar,S.Anderson,and J.Oh. Translatingrobotskills:\n Learning unsupervised skill correspondences across robots. In International Conference on\n Machine Learning,pages 19626–19644.PMLR,2022. \n [57] M.Xu,Z.Xu,C.Chi,M.Veloso,and S.Song. Xskill: Crossembodimentskilldiscovery. In\n Conferenceon Robot Learning,pages 3536–3555.PMLR,2023. \n [58] Z.-H.Yin,L.Sun,H.Ma,M.Tomizuka,and W.-J.Li. Crossdomainrobotimitationwithin-\n variantrepresentation. In 2022 International Conferenceon Roboticsand Automation(ICRA),\n pages 455–461.IEEE,2022. \n \n [59] K.Zakka,A.Zeng,P.Florence,J.Tompson,J.Bohg,and D.Dwibedi.Xirl:Cross-embodiment\n inverse reinforcement learning. In Conference on Robot Learning, pages 537–546. PMLR,\n 2022. \n [60] G.Zhang,L.Zhong,Y.Lee,and J.J.Lim. Policytransferacrossvisualanddynamicsdomain\n gapsviaiterativegrounding. ar Xivpreprintar Xiv:2107.00339,2021. \n [61] Q.Zhang,T.Xiao,A.A.Efros,L.Pinto,and X.Wang.Learningcross-domaincorrespondence\n forcontrolwithdynamicscycle-consistency. ar Xivpreprintar Xiv:2012.09811,2020.\n \n [62] S.Bahl,A.Gupta,and D.Pathak. Human-to-robotimitationinthewild. In RSS,2022.\n [63] C.Wang,L.Fan,J.Sun,R.Zhang,L.Fei-Fei,D.Xu,Y.Zhu,and A.Anandkumar.Mimicplay:\n Long-horizonimitationlearningbywatchinghumanplay. ar Xivpreprintar Xiv:2302.12422,\n 2023. \n \n [64] J.Engel,K.Somasundaram,M.Goesele,A.Sun,A.Gamino,A.Turner,A.Talattof,A.Yuan,\n B.Souti,B.Meredith,etal. Projectaria: Anewtoolforegocentricmulti-modalairesearch.\n ar Xivpreprintar Xiv:2308.13561,2023. \n [65] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany,\n M.K.Srirama,L.Y.Chen,K.Ellis,etal. Droid: Alarge-scalein-the-wildrobotmanipulation\n dataset. ar Xivpreprintar Xiv:2403.12945,2024. \n \n [66] W. Zhu, X. Ma, Z. Liu, L. Liu, W. Wu, and Y. Wang. Motionbert: A unified perspective on\n learninghumanmotionrepresentations. In ICCV,2023. \n [67] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haz-\n iza,F.Massa,A.El-Nouby,etal.Dinov 2:Learningrobustvisualfeatureswithoutsupervision.\n ar Xivpreprintar Xiv:2304.07193,2023. \n \n 13 \n \n "
  },
  {
    "page_num": 14,
    "text": " \n \n \n \n \n \n [68] Y.Zhou,C.Barnes,J.Lu,J.Yang,and H.Li. Onthecontinuityofrotationrepresentationsin\n neuralnetworks. In CVPR,2019. \n [69] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta,\n B.Ichter,etal. Scalingrobotlearningwithsemanticallyimaginedexperience. ar Xivpreprint\n ar Xiv:2302.11550,2023. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 14 \n \n "
  },
  {
    "page_num": 15,
    "text": " \n \n \n \n \n \n Paper Wooden Red Green #1 #2 \n \n \n Diff. Background \n (a)Therobotperformsthecuppassingtaskacrossfourdifferentbackgrounds. Theleftsideshowsthefour\n backgroundvariations,whiletherightsideillustratesthetwopassingdirections: (#1-Righthandpassesthe\n cuptothelefthand,#2-Lefthandpassesthecuptotherighthand). \n Diff. Item #1 #2 #3 #4 #5 \n \n \n \n \n (b)Therobotperformsthehorizontalgraspingtaskwithfourdifferentitems: bottle,box 1,box 2,andcan,\n asshownontheleft. Therightsideillustratestheprocess: (#1-#3-Therobotgraspsthebottle,#4-#5-The\n robotplacesitintotheplasticbin). \n Diff. Position #1 #2 #3 #4 #5 \n \n \n \n (c)Therobotperformstheverticalgraspingtask. Asshownontheleft,the Dynamixelboxisplacedinnine\n differentpositionsforgrasping.Therightsideillustratestheprocess:(#1-#3-Therobotgraspsthebox,#4-#5\n -Therobotplacestheboxintotheplasticbin). \n \n Diff. Setting #1 #2 #3 #4 #5 \n \n \n \n (d)Therobotperformsthepouringtask.Theleftsideshowsdifferentsettingsachievedbyvaryingtherobot’s\n rotation and the table’s position. The right side illustrates the pouring process: (#1 - Right hand grasps the\n bottle,#2-Lefthandgraspsthecup,#3-Pouringthedrink,#4-Lefthandplacesthecupdown,#5-Right\n handplacesthebottledown). \n Figure 7: Illustrations of tasks used in quantitative evaluations. From top to bottom: cup passing,\n horizontalgrasping,verticalgrasping,andpouring. \n Bottle Box Box Can \n Method 1 2 Ovr. Succ. \n I.D. H.D. H.D. H.D. \n Withoutwhole-body 8/10 6/10 0/10 7/10 21/40 \n Withwhole-body 9/10 3/10 3/10 3/10 18/40 \n Table 4: Ablation of how human whole-body movement in training demonstrations affects\n policy rollout. We collect the same number of demonstrations on the same set of objects for the\n graspingtaskwithorwithoutwhole-bodymovement.Sincetherobotdoesnothaveanaturalwhole-\n bodymovementlikehumans,itnegativelyinfluencesthemanipulationsuccessrate.\n A More Ablation Study-Data Collection \n \n Autonomous Whole-body Movement. In Tab.4,wejustifythenecessitytominimizebodymove-\n mentinhumandatacollection.Humanstendtomovetheirupperbodyunconsciouslyduringmanip-\n ulation(includingshoulderandwaistmovement). However, existinghumanoidrobotshaveyetto\n reachsuchalevelofdexterity.Thus,havingthesedifficult-to-replicateactionsinthehumandemon-\n strations leads to degraded performance. We hypothesize that such a necessity would be greatly\n reduced with the development of both whole-body locomotion methods and mechanical designs,\n \n 15 \n \n "
  },
  {
    "page_num": 16,
    "text": " \n \n \n \n \n \n Method Grasping(secs) Pouring(secs) \n Human Demo 3.79±0.27 4.81±0.35 \n Human Demowith VR 4.09±0.30 4.90±0.26 \n Humanoid Demo(VRTeleop) 19.72±1.65 37.31±6.25 \n Table 5:Amortizedmeanandstandarddeviationofthetimerequiredtocollectasingledemon-\n stration, including scene resets. The first row shows the time for regular human to complete cor-\n responding tasks in real world. The second row represents our human data when wearing VR for\n datacollection, demonstratingthategocentrichumandemonstrationsprovideamorescalabledata\n sourcecomparedtorobotteleoperation. \n \n butforthecurrentlyavailableplatforms,weinstructoperatorstominimizebodymovementasmuch\n aspossibleinourdataset. \n Efficiencyof Data Collection. In Tab.5,wecomparetaskcompletiontimesacrossdifferentsetups,\n includingstandardhumanmanipulation,humandemonstrationsperformedwhilewearinga VRde-\n vice,androbotteleoperation. Thisanalysishighlightshowtask-orientedhumandemonstrationscan\n be a scalable data source for cross-embodiment learning. Notably, wearing a VR device does not\n significantlyimpacthumanmanipulationspeed,asthecompletiontimeremainsnearlythesameas\n instandardhumandemonstrations. \n Amongdifferentdatacollectionschemes, wefindthatmostoverheadarisesduringtheretargeting\n process from human actions to robot actions. This is primarily due to latency and the constrained\n workspaceof 7-Do Froboticarms,whichareinherentchallengesinexistingdatacollectionmethods\n suchas VRteleoperation[9],motiontracking[48,10],andpuppeting[8,5]. \n Beyonddatacollectionspeed,humandemonstrationsofferseveraladditionaladvantagesovertele-\n operation. They provide a safer alternative, reducing risks associated with real-robot execution.\n Theyarealsomorelabor-efficient,astheydonotrequireadditionalpersonnelforsupervision. Fur-\n thermore, human demonstrations allow for greater flexibility in settings, enabling a diverse range\n ofenvironmentswithoutrequiringrobot-specificadaptations. Additionally,humandemonstrations\n achieveahigherdemonstrationsuccessrate,andtherequiredhardware(suchasmotioncaptureor\n VR devices) is more accessible and cost-effective compared to full robotic setups. These factors\n collectivelymakehumandataamorescalablesolutionforlarge-scaledatacollection.\n B Normalizationofdifferentembodiments. \n \n Tab. 2 suggests minor differences between using different normalization coefficients for the states\n andactionsvectorsofhumansandhumanoids. Wetakeacloserlookin Fig.8,whereweinvestigate\n theimpactofdifferentnormalizationstrategiesintheverticalgrasping(picking)task. Noticeably,\n thesamenormalizationapproachachievedthehighestoverallsuccessrate,butthesuccessdistribu-\n tionisbiasedtowardstheupper-rightregionofthegrid. \n Wehypothesizethatthisisbecausehumanshavealargerworkspacethanhumanoidrobots. Thus,\n humandataencompasseshumanoidproprioceptionasasubset,whichresultsinarelativelysmaller\n distributionfortherobotstate-actionspace. \n \n C In-Depth Analysisof Different Typesof Generalization \n \n Humandataimprovesbackgroundgeneralization. Wechosetousethecuppassingtasktotest\n background generalization. We prepared four different tablecloths as backgrounds, as shown in\n Fig. 7 a. In terms of training data distribution, the teleoperation data for this task was collected\n exclusivelyonthepaperbackgroundshownin Fig.7 a,whereasthehumandataincludesmorethan\n five different backgrounds. This diverse human dataset significantly enhances the generalization\n ability of the co-trained HAT policy. As shown in Tab. 7. , HAT consistently outperforms across\n all four backgrounds, demonstrating robustness to background variations. In addition, the overall\n \n 16 \n \n "
  },
  {
    "page_num": 17,
    "text": " \n \n \n \n \n \n Bottle Box Box Can \n Method 1 2 Ovr. Succ. \n I.D. H.D. O.O.D. O.O.D. \n ACT 8/10 5/10 1/10 1/10 16/40 \n HAT 8/10 7/10 1/10 4/10 21/40 \n Table 6: Object Appearance Generalization: In the horizontal grasping task, we evaluated the\n graspingperformancebyattemptingtograspeachobject 10 timesandrecordedthesuccessrate.\n \n \n \n success rate increases by nearly 50% compared to training without human data, highlighting the\n advantageofutilizingdiversehumandemonstrations. \n Humandataimprovesappearancegeneralization. Totesthowco-trainingimprovesrobustness\n toperturbationsinobjecttextures, weevaluatethehorizontalgraspingpolicyonnovelobjects, as\n shown in Fig. 7 b. Specifically, we compare the policy’s performance on the bottle, box 1, box 2,\n andcan,asshownlefttorightinthefirstimagein Fig.7 b. Theseobjectsdiffersignificantlyinboth\n colorandshapefromthebottleusedintheteleoperationdatadistribution. \n Sincegraspingisarelativelysimpletask,ouradjustedpolicydemonstratesstronglearningcapabil-\n itiesevenwithonly 50 teleoperationdatasamples. Thepolicycansuccessfullygraspmostbottles\n despitethelimitedtrainingset.Tobetterhighlighttheimpactofhumandata,weselectedmorechal-\n lengingobjectsforevaluation. Asshownin Tab.6,humandatasignificantlyenhancesthepolicy’s\n abilitytograspthesemoredifficultobjects. \n Notably,box 1 appearsinthehumandata,whilebox 2 doesnot. Despitethis,weobservethatco-\n trainingwithhumandatastillimprovesoverallperformance,evenonbox 2,thoughitssuccessrate\n doesnotincrease.Thissuggeststhat,beyonddirectexperiencewithspecificobjects,thehumandata\n helpsthepolicylearnbroadervisualpriorsthatenablemoreproactiveandstablegraspingbehaviors.\n Forbox 2,whilethesuccessrateremainslow—partiallyduetoitslowheightandcolorsimilarity\n tothetable—theco-trained HATpolicydemonstratesfewerout-of-distribution(OOD)failuresand\n more actively searches for graspable regions. The failures on box 2 are primarily due to unstable\n graspingandthesmallboxslippingfromthehand,ratherthantheinabilitytoperceiveorlocatethe\n object. \n Furthermore, adding more human data not only improves performance on objects seen in human\n training demonstrations (e.g., box 1) but also enhances generalization to completely novel objects\n (e.g., box 2 and can). We hypothesize that, as the number of objects grows, HAT starts to learn\n inter-categoryvisualpriorsthatguideittograspobjectsmoreeffectively,evenwhentheywerenot\n explicitlypresentinthetrainingset. \n Humandataimprovesobjectplacementgeneralization.Finally,weintroducevariationsinobject\n placementsthatarenotpresentinthereal-robottrainingdemonstrationsandspecificallyinvestigate\n this in the vertical grasping (picking) task. In this task, we intentionally constrain the robot data\n collectiontoobjectplacementswithinasubsetofcells,whilehumanverticalgraspingdatacoversa\n muchmorediverserangeofsettings. \n Tosystematicallyanalyzetheimpactofhumandata,weevaluatemodelperformanceonastructured\n 3×3 grid,whereeachcellrepresentsa 10 cm×10 cmregionforgraspingattempts. Thenumbersin\n eachcellindicatethenumberofsuccessfulpicksoutof 10 trials.Real-robottrainingdataiscollected\n fromonlytwospecificcells,highlightedwithdashedlines. \n Akeydetailinourteleoperationdatadistributionisthat 50 pickingattemptsarecollectedfromthe\n right-handsidegridandonly 10 fromtheleft-handsidegrid. Thisimbalanceexplainswhypolicies\n trainedpurelyonteleoperationdatastruggletograspobjectsintheleft-sidegrid. Weobservethat\n models trained solely on robot data fail to generalize to unseen cells, whereas cross-embodiment\n learningwithhumandatasignificantlyimprovesgeneralization,doublingtheoverallsuccessrate.\n \n 17 \n \n "
  },
  {
    "page_num": 18,
    "text": " \n \n \n \n \n \n ACT HAT (diff. norm) HAT (same norm) \n \n \n \n \n \n \n \n \n \n Figure 8:Object Placement Generalization.Performancecomparisonsofmodelstrainedwithand\n without human data on vertical grasping (picking). Each cell in the 3×3 grid represents a 10 cm ×\n 10 cmregionwheretherobotattemptstopickupabox,withnumbersindicatingsuccessfulattempts\n outof 10. Thereal-robotdataiscollectedintwocellsinsidethedashedlines. Notably,ourteleop-\n erationdataisintentionallyimbalanced. \n \n Paper Wooden Red Green \n Method Ovr. Succ. \n I.D. H.D. O.O.D. O.O.D. \n ACT 19/20 14/20 12/20 10/20 55/80 \n HAT 20/20 16/20 18/20 18/20 72/80 \n Table 7:Background Generalization:Inthecuppassingtask,weevaluatethepassingperformance\n byrecordingthenumberoffailuresorretriesneededtocomplete 20 cup-passingtrials.\n \n D In-Depth Comparisonbetween Humanoid Aand Humanoid B \n configurations \n \n Thissectionpresentsadetailedcomparisonofthetwohumanoidplatforms,referredtoas Humanoid\n Aand Humanoid B,withafocusonjointstructureandimplicationsformanipulationcapabilities.\n We restrict our analysis to the arm configurations, as other parts of the body were not exclusively\n exploredinthiswork. \n While morphologically similar, these two humanoids have drastically different arm configura-\n tions that create hurdles in direct policy transfer. Besides differences in motor technical specs\n such as torque and types of encoder (Humanoid B has absolute motor position encoders), they\n also have different mechanical limits. The range of motion (ROM) for the first four proximal\n joints—shoulder pitch, shoulder roll, shoulder yaw, and elbow—differs across the two platforms.\n Humanoid Bexhibitsaconsistentlywider ROM,whichallowsawidersetofreachableconfigura-\n tionsandincreasesthemanipulabilityofthearminconstrainedenvironments. Table 8 summarizes\n the ROMvaluesforthesesharedjoints. \n Amoresignificantarchitecturaldivergenceisobservedatthewrist. Humanoid Aincludesasingle\n distal joint—wrist roll—providing limited wrist articulation. This restricts end-effector dexterity\n andconstrainsin-handmanipulationstrategiestoasinglerotationaldegreeoffreedom. Incontrast,\n Humanoid Bisequippedwithacompletewristmechanismcomposedofthreeindependentlyactu-\n atedjoints:wrist pitch,wrist roll,andwrist yaw.Theseadditionaldegreesoffreedomallowforfull\n orientation control of the end-effector, enabling tasks that require precise alignment, rotation, and\n fineadjustmentofobjectposes. \n \n \n \n \n \n 18 \n \n "
  },
  {
    "page_num": 19,
    "text": " \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Joint Humanoid A Humanoid B \n shoulder pitch −164◦to+164◦ −180◦to+90◦ \n shoulder roll −19◦to+178◦ −21◦to+194◦ \n shoulder yaw −74◦to+255◦ −152◦to+172◦ \n elbow −71◦to 150◦ −54◦to 182◦ \n wrist roll −175◦to 175◦ −172◦to 157◦ \n Table 8: Joint Rangeof Motion Comparisonbetween Humanoid Aand B(indegrees)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 19 \n \n "
  }
]