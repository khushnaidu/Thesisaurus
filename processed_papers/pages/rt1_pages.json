[
  {
    "page_num": 1,
    "text": " \n \n Preprint \n \n \n \n RT-1: ROBOTICS TRANSFORMER \n \n FOR REAL-WORLD CONTROL AT SCALE \n 1 Anthony Brohan‚àó,Noah Brown‚àó,Justice Carbajal‚àó,Yevgen Chebotar‚àó,Joseph Dabis‚àó,\n Chelsea Finn‚àó,Keerthana Gopalakrishnan‚àó,Karol Hausman‚àó,Alex Herzog‚Ä†,Jasmine Hsu‚àó,\n Julian Ibarz‚àó,Brian Ichter‚àó,Alex Irpan‚àó,Tomas Jackson‚àó,Sally Jesmonth‚àó,Nikhil JJoshi‚àó,\n Ryan Julian‚àó,Dmitry Kalashnikov‚àó,Yuheng Kuang‚àó,Isabel Leal‚àó,Kuang-Huei Lee‚Ä°,Sergey Levine‚àó,\n Yao Lu‚àó,Utsav Malla‚àó,Deeksha Manjunath‚àó,Igor Mordatch‚Ä°,Ofir Nachum‚Ä°,Carolina Parada‚àó,\n Jodilyn Peralta‚àó,Emily Perez‚àó,Karl Pertsch‚àó,Jornell Quiambao‚àó,Kanishka Rao‚àó,Michael Ryoo‚àó,\n Grecia Salazar‚àó,Pannag Sanketi‚àó,Kevin Sayed‚àó,Jaspiar Singh‚àó,Sumedh Sontakke‚Ä°,Austin Stone‚àó,\n Clayton Tan‚àó,Huong Tran‚àó,Vincent Vanhoucke‚àó,Steve Vega‚àó,Quan Vuong‚àó,Fei Xia‚àó,Ted Xiao‚àó,\n Peng Xu‚àó,Sichun Xu‚àó,Tianhe Yu‚àó,Brianna Zitkovich‚àó \n ‚àóRoboticsat Google,‚Ä†Everyday Robots,‚Ä°Google Research,Brain Team \n ABSTRACT \n Bytransferringknowledgefromlarge,diverse,task-agnosticdatasets,modernma-\n chinelearningmodelscansolvespecificdownstreamtaskseitherzero-shotorwith\n smalltask-specificdatasetstoahighlevelofperformance. Whilethiscapability\n has been demonstrated in other fields such as computer vision, natural language\n processing or speech recognition, it remains to be shown in robotics, where the\n generalization capabilities of the models are particularly critical due to the dif-\n ficulty of collecting real-world robotic data. We argue that one of the keys to\n the success of such general robotic models lies with open-ended task-agnostic\n training,combinedwithhigh-capacityarchitecturesthatcanabsorballofthedi-\n verse, robotic data. In this paper, we present a model class, dubbed Robotics\n Transformer, that exhibits promising scalable model properties. We verify our\n conclusionsinastudyofdifferentmodelclassesandtheirabilitytogeneralizeas\n a function of the data size, model size, and data diversity based on a large-scale\n datacollectiononrealrobotsperformingreal-worldtasks. Theproject‚Äôswebsite\n andvideoscanbefoundatrobotics-transformer 1.github.io \n 1 INTRODUCTION \n End-to-end robotic learning, with either imitation or reinforcement, typically involves collecting\n task-specific data in either single-task (Kalashnikov et al., 2018; Zhang et al., 2018) or multi-\n task (Kalashnikov et al., 2021 b; Jang et al., 2021) settings that are narrowly tailored to the tasks\n thattherobotshouldperform. Thisworkflowmirrorstheclassicapproachtosupervisedlearningin\n otherdomains,suchascomputervisionand NLP,wheretask-specificdatasetswouldbecollected,\n labeled, and deployed to solve individual tasks, with little interplay between the tasks themselves.\n Recentyearshaveseenatransformationinvision,NLP,andotherdomains,awayfromsiloed,small-\n scale datasets and models and towards large, general models pre-trained on broad, large datasets.\n Thekeystothesuccessofsuchmodelsliewithopen-endedtask-agnostictraining,combinedwith\n high-capacityarchitecturesthatcanabsorballoftheknowledgepresentinlarge-scaledatasets. Ifa\n model can ‚Äúsponge up‚Äù experience to learn general patterns in language or perception, then it can\n bring them to bear on individual tasks more efficiently. While removing the need for large task-\n specific datasets is appealing generally in supervised learning, it is even more critical in robotics,\n wheredatasetsmightrequireengineering-heavyautonomousoperationorexpensivehumandemon-\n strations. Wethereforeask: canwetrainasingle,capable,largemulti-taskbackbonemodelondata\n consistingofawidevarietyofrobotictasks? Anddoessuchamodelenjoythebenefitsobservedin\n otherdomains,exhibitingzero-shotgeneralizationtonewtasks,environments,andobjects?\n Buildingsuchmodelsinroboticsisnoteasy. Althoughrecentyearshaveseenseverallargemulti-\n taskrobotpoliciesproposedintheliterature(Reedetal.,2022;Jangetal.,2021),suchmodelsoften\n havelimitedbreadthofreal-worldtasks,aswith Gato(Reedetal.,2022),orfocusontrainingtasks\n ratherthangeneralizationtonewtasks,aswithrecentinstructionfollowingmethods(Shridharetal.,\n 2021;2022),orattaincomparativelylowerperformanceonnewtasks(Jangetal.,2021).\n 1 Authorslistedinalphabeticalorder.Contributionsin Appendix A. \n Correspondingemails:{keerthanapg,kanishkarao,karolhausman}@google.com.\n 1 \n 3202 \n gu A \n 11 \n ]OR.sc[ \n 2 v 71860.2122:vi Xra "
  },
  {
    "page_num": 2,
    "text": " Instruction \n Pick apple from top drawer and place on counter Mode Arm Base \n Images \n Fi LM \n Efficient Net Token Learner Transformer \n Preprint \n ‚Ä¶ \n RT-1 \n 3 Hz \n Œ≤ \n )Œ≥+1( \n Action \n ¬∑ \n + \n Instruction \n Pick apple from top drawer and place on counter Mode Arm Base \n Images \n Fi LM \n Efficient Net Token Learner Transformer \n ‚Ä¶ \n RT-1 \n 3 Hz \n Œ≤ \n )Œ≥+1( \n Action \n ¬∑ \n + \n (a)RT-1 takesimagesandnaturallanguageinstructionsandoutputsdiscretizedbaseandarmactions. Despite\n itssize(35 Mparameters),itdoesthisat 3 Hz,duetoitsefficientyethigh-capacityarchitecture:a Fi LM(Perez\n et al., 2018) conditioned Efficient Net (Tan & Le, 2019), a Token Learner (Ryoo et al., 2021), and a Trans-\n former(Vaswanietal.,2017). \n (b)RT-1‚Äôslarge-scale,real-worldtraining(130 kdemonstrations)andevaluation(3000 real-worldtrials)show\n impressivegeneralization,robustness,andabilitytolearnfromdiversedata.\n Figure 1: Ahigh-leveloverviewof RT-1‚Äôsarchitecture,dataset,andevaluation.\n Thetwomainchallengesliein assemblingtherightdatasetanddesigningtherightmodel. While\n data collection and curation is often the ‚Äúunsung hero‚Äù of many large-scale machine learning\n projects(Radfordetal.,2021;Rameshetal.,2021),thisisespeciallytrueinrobotics,wheredatasets\n areoftenrobot-specificandgatheredmanually(Dasarietal.,2019;Ebertetal.,2021). Aswewill\n showinourevaluations,goodgeneralizationrequiresdatasetsthatcombinebothscaleandbreadth,\n coveringavarietyoftasksandsettings. Atthesametime, thetasksinthedatasetshouldbesuffi-\n ciently well-connected to enable generalization, such that the model can discover the patterns be-\n tweenstructuralsimilartasksandperformnewtasksthatcombinethosepatternsinnovelways. We\n utilizeadatasetthatwegatheredoverthecourseof 17 monthswithafleetof 13 robots,containing\n ‚àº130 kepisodesandover 700 tasks,andweablatevariousaspectsofthisdatasetinourevaluation.\n The second challenge lies in the design of the model itself. Effective robotic multi-task learning\n requiresahighcapacitymodel,and Transformer(Vaswanietal.,2017)modelsexcelinthisregard,\n particularlywhenitisnecessarytolearnmanytasksconditioned,asinourcase,onlanguageinstruc-\n tions. However,roboticcontrollersmustalsobeefficientenoughtoruninrealtime,whichpresents\n amajorchallengefor Transformersinparticular. Weproposeanovelarchitecturethatwecall RT-1\n (Robotics Transformer 1),whichbyencodinghigh-dimensionalinputsandoutputs,includingcam-\n eraimages,instructionsandmotorcommandsintocompacttokenrepresentationstobeusedbythe\n Transformer,allowsforefficientinferenceatruntimetomakereal-timecontrolfeasible.\n Ourcontributionisthe RT-1 modelandexperimentswiththismodelonalargeandbroaddatasetof\n real-worldrobotictasks. Ourexperimentsnotonlydemonstratethat RT-1 canexhibitsignificantly\n improvedgeneralizationandrobustnesscomparedtopriortechniques,butalsoevaluateandablate\n manydesignchoicesinboththemodelandinthecompositionofthetrainingset. Ourresultsshow\n that RT-1 canperformover 700 traininginstructionsat 97%successrate,andcangeneralizetonew\n tasks, distractors, and backgrounds 25%, 36% and 18% better than the next best baseline, respec-\n tively. Thislevelofperformanceallowsustoexecuteverylong-horizontasksinthe Say Can(Ahn\n etal.,2022)framework,withasmanyas 50 stages. Wefurthershowthat RT-1 canincorporatedata\n fromsimulationorevenotherrobottypes,retainingperformanceontheoriginaltasksandimproving\n generalizationtonewscenarios. Ashortoverviewof RT-1 capabilitiesispresentedin Fig.1 b 2.\n 2 Helperrobotsshownin Fig.1-5 arefrom Everyday Robots \n 2 "
  },
  {
    "page_num": 3,
    "text": " \n \n Preprint \n \n \n \n 2 RELATED WORK \n \n \n A number of recent works have proposed Transformer-based policies for robotic control. As in\n RT-1, several works use language commands processed with Transformers as a robust framework\n for specifying and generalizing to new tasks (Zhang & Chai, 2021; Pashevich et al., 2021; Silva\n et al., 2021; Jang et al., 2021; Ahn et al., 2022; Nair et al., 2022). Our work takes the application\n of Transformersastepfurtherandtreatsthemappingoflanguageandvisionobservationstorobot\n actions as a sequence modelling problem, using a Transformer to learn this mapping. This idea\n is directly inspired by successes in game-playing (Chen et al., 2021; Lee et al., 2022 a) as well\n as simulated robot navigation (Fang et al., 2019), locomotion (Janner et al., 2021; Gupta et al.,\n 2022),andmanipulation(Jiangetal.,2022)environments. Wenotethatseveraloftheseworksgo\n beyond only text conditioning and use Transformers to also generalize across robot morphologies\n (e.g.,Guptaetal.(2022))andothermodalitiesfortaskspecifications(e.g.,Jangetal.(2021);Jiang\n etal.(2022)). Theseextensionsarepromisingfuturedirectionsfor RT-1. \n Beyond Transformer-basedpolicies,thefocusofourworkisongeneralizableandrobustreal-world\n roboticmanipulationatscale.Existingworksonreal-world Transformer-basedroboticmanipulation\n focus on efficiently learning tasks from a set of demonstrations per task (Shridhar et al., 2022).\n Behavior Transformer (Shafiullah et al., 2022) and Gato (Reed et al., 2022) advocate for training\n a single model on large-scale robotic and non-robotic datasets. However, these works are limited\n intheirreal-worldrobotictasks;e.g.,Gatolearnseffectivelyasingletask(coloredblockstacking)\n withoutevaluatinggeneralizationtonewtasksoravarietyofreal-worldsettings. Onthetechnical\n side,ourworkexamineshow Transformer-basedpoliciescanbebuiltsoastocombinehighcapacity\n andgeneralizationwiththecomputationalefficiencynecessaryforreal-timecontrol.\n Whiletheuseofhigh-capacity Transformermodelstolearnroboticcontrolpoliciesisafairlyrecent\n innovation, robotics has a long history of multi-task and language-conditioned learning, and RT-1\n buildsonthesefoundations. Asignificantbodyofworkdealswithlearningpoliciesandpredictive\n models for robotic grasping (Saxena et al., 2006; Lenz et al., 2015; Pinto & Gupta, 2016; Gupta\n et al., 2018; Viereck et al., 2017), with the aim of generalizing to new objects. Prior works have\n sought to address robotic language understanding through pipelined approaches that combine lan-\n guageparsing,vision,androboticcontrol(Mac Mahonetal.,2006;Kollaretal.,2010;Tellexetal.,\n 2011)andwithend-to-endapproaches(Meietal.,2016;Stepputtisetal.,2020;Lynch&Sermanet,\n 2020;Ahnetal.,2022). Multi-taskroboticlearninghasalsobeenapproachedfromtheperspective\n of learning to reach goals (Chung et al., 2015; Raffin et al., 2019; Jurgenson et al., 2020; Huang\n etal.,2020), aswellaslearningpoliciesthatcanperformtasksinadiscretesetorsomeotherpa-\n rameterizedform(Deisenrothetal.,2014;Devinetal.,2017;Foxetal.,2019;Kalashnikovetal.,\n 2021 a). A number of prior works in robotics have also focused on collecting datasets containing\n demonstrationsortrialsthatillustrateavarietyofdifferenttasks(Sharmaetal.,2018;Dasarietal.,\n 2019; Yu et al., 2020; Singh et al., 2020; James et al., 2020). Our work adds further evidence in\n supportofthepowerofmulti-task,language-conditionedroboticlearning,presentingexperimental\n results at a larger scale and with a greater variety of behaviors, objects, and scenes and proposing\n newarchitecturesanddesignchoicesthatenableroboticlearningatasignificantlylargerscale.\n 3 PRELIMINARIES \n Robotlearning. Weaimtolearnrobotpoliciestosolvelanguage-conditionedtasksfromvision.\n Formally,weconsiderasequentialdecision-makingenvironment. Attimestept = 0,thepolicyœÄ\n ispresentedwithalanguageinstructioniandaninitialimageobservationx . Thepolicyproduces\n 0 \n an action distribution œÄ(¬∑ | i,x ) from which an action a is sampled and applied to the robot.\n 0 0 \n Thisprocesscontinues,withthepolicyiterativelyproducingactionsa bysamplingfromalearned\n t \n distributionœÄ(¬∑|i,{x }t )andapplyingthoseactionstotherobot. Theinteractionendswhena\n j j=0 \n terminationconditionisachieved. Thefullinteractioni,{(x ,a )}T fromfromthestartingstep\n j j j=0 \n t = 0 to terminating step T is referred to as an episode. At the end of an episode, the agent will\n begivenabinaryrewardr ‚àà {0,1}indicatingwhethertherobotperformedtheinstructioni. The\n goalistolearnapolicyœÄ thatmaximizestheaveragereward,inexpectationoveradistributionof\n instructions,startingstatesx ,andtransitiondynamics. \n 0 \n 3 "
  },
  {
    "page_num": 4,
    "text": " \n \n Preprint \n \n \n \n Transformers. RT-1 usesa Transformer(Vaswanietal.,2017)toparameterizethepolicyœÄ. Gener-\n allyspeaking,a Transformerisasequencemodelmappinganinputsequence{Œæ }H toanoutput\n h h=0 \n sequence{y }K usingcombinationsofself-attentionlayersandfully-connectedneuralnetworks.\n k k=0 \n While Transformerswereoriginallydesignedfortextsequences,whereeachinputŒæ andoutputy\n j k \n represents a text token, they have been extended to images (Parmar et al., 2018) as well as other\n modalities (Lee et al., 2022 a; Reed et al., 2022). As detailed in the next section, we parameterize\n œÄ by first mapping inputs i,{x }t to a sequence {Œæ }H and action outputs a to a sequence\n j j=0 h h=0 t \n {y }K beforeusinga Transformertolearnthemapping{Œæ }H ‚Üí{y }K . \n k k=0 h h=0 k k=0 \n Imitation learning. Imitation learning methods train the policy œÄ on a dataset D of demonstra-\n tions (Pomerleau, 1988; Zhang et al., 2018; Jang et al., 2021). Specifically, we assume access to\n adataset D = {(i(n),{(x(n),a(n))}T(n))}N ofepisodes,allofwhicharesuccessful(i.e.,havea\n t t t=0 n=0 \n finalrewardof 1). WelearnœÄ usingbehavioralcloning(Pomerleau,1988), whichoptimizesœÄ by\n minimizingthenegativelog-likelihoodofactionsa giventheimagesandlanguageinstructions.\n t \n 4 SYSTEM OVERVIEW \n The goal of this work is to build and demonstrate a general robot learning system that can ab-\n sorblargeamountsofdataandgeneralizeeffectively. Weusemobilemanipulatorsfrom Everyday\n Robots 3, which have a 7 degree-of-freedom arm, a two-fingered gripper, and a mobile base (see\n Fig.2(d)). Tocollectdataandevaluateourmethod,weusethreekitchen-basedenvironments: two\n real office kitchens and a training environment modelled off these real kitchens. The training en-\n vironment, shown in Fig. 2 (a), consists of partial counters and is constructed for large scale data\n collection. Thetworealenvironments,shownin Fig.2(b,c),havesimilarcountertopstothetrain-\n ingenvironment,butvaryinlighting,background,andfullkitchengeometry(e.g.,theremaybea\n cabinetinsteadofadrawerorasinkmaybevisible). Weevaluatetheperformanceofourpolicies\n acrossthesedifferentenvironments,measuringthepolicy‚Äôsperformanceandabilitytogeneralize.\n Ourtrainingdataconsistsofhuman-provideddemonstrations,andweannotateeachepisodewitha\n textualdescriptionoftheinstructionthattherobotjustperformed. Theinstructionsusuallycontain\n averbandoneormorenounsdescribingthetargetobjects. Togrouptheseinstructionstogether,we\n splitthemintoanumberofskills(e.g.,verbssuchas‚Äúpick‚Äù,‚Äúopen‚Äùor‚Äúplaceupright‚Äù)andobjects\n (e.g., nounssuchas‚Äúcokecan‚Äù, ‚Äúapple‚Äù, or‚Äúdrawer‚Äù). Wedescribethedetailsofourdatacollec-\n tionstrategyatscalein Sec.5.2. Ourlargestdatasetcontainsover 130 kindividualdemonstrations\n constitutingover 700 distincttaskinstructionsusingalargevarietyofobjects(see Fig.2(f)). We\n describethedetailsofthedatacollectedin Sec.5.2. \n One of the main contributions of our system is the network architecture, Robotics Transformer 1\n (RT-1),anefficientmodelthatcanabsorblargeamountsofdata,effectivelygeneralize,andoutput\n actions at real-time rates for practical robotic control. RT-1 takes a short sequence of images and\n a natural language instruction as input and outputs an action for the robot at each time step. To\n this end, the architecture (shown in Figure 1 a) leverages several elements: first the images and\n textareprocessedviaan Image Netpretrainedconvolutionalnetwork(Tan&Le,2019)conditioned\n on a pretrained embedding of the instruction via Fi LM (Perez et al., 2018), followed by a Token\n Learner(Ryooetal.,2021)tocomputeacompactsetoftokens,andfinallya Transformer(Vaswani\n etal.,2017)toattendoverthesetokensandproducediscretizedactiontokens. Theactionsconsist\n ofsevendimensionsforthearmmovement(x, y, z, roll, pitch, yaw, openingofthegripper), three\n dimensionsforbasemovement(x,y,yaw)andadiscretedimensiontoswitchbetweenthreemodes:\n controlling the arm, the base, or terminating the episode. RT-1 performs closed-loop control and\n commandsactionsat 3 Hzuntiliteitheryieldsa‚Äúterminate‚Äùactionorhitsapre-settimesteplimit.\n 5 RT-1: ROBOTICS TRANSFORMER \n Inthissection,wedescribehowwetokenizetheimages,text,andactions,andthendiscussthe RT-1\n modelarchitecture.Wethendescribehowweattaintheruntimespeedrequiredforreal-timecontrol.\n Lastly,wedescribethedatacollectionprocedureandtheskillsandinstructionsinourdataset.\n 3 everydayrobots.com \n 4 "
  },
  {
    "page_num": 5,
    "text": " \n \n Preprint \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 2:(a)Robotclassroomwherewecollectdataatscale;(b)arealofficekitchen,oneofthetwo\n realisticenvironmentsusedforevaluation(named Kitchen 1 intherestofthepaper);(c)adifferent\n officekitchenusedforevaluation(named Kitchen 2 intherestofthepaper);(d)mobilemanipulator\n usedthroughoutthepaper; (e)asetofobjectsusedformostoftheskillstoexpandskilldiversity;\n (f)amorediversesetofobjectsusedmostlytoexpandobjectdiversityofthepickingskill.\n \n 5.1 MODEL \n \n Ourmodelisbuiltona Transformerarchitecture(Vaswanietal.,2017)andtakesahistoryofimages\n andtaskdescriptionasinputanddirectlyoutputstokenizedactions,asshownin Fig.1 aandindetail\n in Fig. 3. In the following we describe the components of the model, following the top-to-bottom\n orderin Fig.3. Moredetailonmodelselectionatscaleareprovidedin Appendix C.3.\n Instructionandimagetokenization. The RT-1 architecturereliesonadata-efficientandcompact\n tokenizationofimagesand languageinstruction. RT-1 tokenizesahistoryof 6 imagesbypassing\n images through an Image Net pretrained Efficient Net-B 3 (Tan & Le, 2019) model, which takes 6\n imagesofresolution 300√ó300 asinputandoutputsaspatialfeaturemapofshape 9√ó9√ó512 from\n the final convolutional layer. Unlike Reed et al. (2022), we do not patchify the images into visual\n tokenspriortofeedingthemtoour Transformerbackbone.Weinsteadflattentheoutputfeaturemap\n fromthe Efficient Netinto 81 visualtokenswhicharepassedontothelaterlayersofthenetwork.\n To include the language instruction, we condition the image tokenizer on the natural language in-\n structionintheformofapretrainedlanguageembedding,allowingextractionoftask-relevantimage\n featuresearlyonandimprovingperformanceof RT-1. Theinstructionisfirstembeddedvia Univer-\n sal Sentence Encoder(Ceretal.,2018). Thisembeddingisthenusedasinputtoidentity-initialized\n Fi LM layers (Perez et al., 2018) added to the pretrained Efficient Net to condition the image en-\n coder. Normally,insertinga Fi LMlayerintotheinteriorofapretrainednetworkwoulddisruptthe\n intermediate activations and negate the benefit of using pretrained weights. To overcome this, we\n initialize the weights of the dense layers (f and h ) which produce the Fi LM affine transforma-\n c C \n tiontozero,allowingthe Fi LMlayertoinitiallyactasanidentityandpreservethefunctionofthe\n pretrainedweights. Wefindthatidentity-initialized Fi LMalsoproducesbetterresultswhentraining\n withan Efficient Netinitializedfromscratch,without Image Netpretraining,butitdoesnotsurpass\n theinitializationdescribedabove. Thearchitectureoftheimagetokenizerispresentedin Fig.3.\n RT-1‚Äôsimageandinstructiontokenizationvia Fi LMEfficient Net-B 3 isatotalof 16 Mparameters,\n with 26 layersof MBConvblocksand Fi LMlayers,whichoutput 81 vision-languagetokens.\n Token Learner. Tofurthercompressthenumberoftokensthat RT-1 needstoattendoverandthus\n speed up inference, RT-1 uses Token Learner (Ryoo et al., 2021). Token Learner is an element-\n wise attention module that learns to map a large number of tokens into a much smaller number\n of tokens. This allows us to soft-select image tokens based on their information, passing only the\n importanttokencombinationstothesubsequent Transformerlayers. Theinclusionof Token Learner\n subsamplesthe 81 visualtokensthatcomeoutofthepre-trained Fi LM-Efficient Netlayerstojust 8\n finaltokensthatarethenpassedontoour Transformerlayers. \n 5 "
  },
  {
    "page_num": 6,
    "text": " \n \n Preprint \n \n \n \n \n \n \n \n \n 1 Œ≥) Œ≤ \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n ¬∑ + ‚Ä¶ \n 1 Œ≥) Œ≤ \n 1 Œ≥) Œ≤ \n 1 Œ≥) Œ≤ \n Figure 3: Thearchitecturediagramof RT-1. Theinstructionistransformedintoa USEembedding\n and used to condition a pre-trained Efficient Net via Fi LM layers. The resulting vision-language\n tokens are reduced by the Token Learner and fed into a decoder-only Transformer, which outputs\n tokenizedactions. \n Transformer. These 8 tokensper-imagearethenconcatenatedwiththeotherimagesinthehistory,\n forming 48 totaltokens(withaddedpositionencoding)tobefedintothe Transformerbackboneof\n RT-1. The Transformerisadecoder-onlysequencemodelwith 8 self-attentionlayersand 19 Mtotal\n parametersthatoutputsactiontokens. \n Action tokenization. To tokenize actions, each action dimension in RT-1 is discretized into ùú∏ùú∑\n 256 bins. As mentioned previously, the action dimensions we consider include seven variables\n for the arm movement (x, y, z, roll, pitch, yaw, opening of the gripper), three variables for base\n movement(x,y,yaw)andadiscretevariabletoswitchbetweenthreemodes: controllingarm,base\n orterminatingtheepisode. Foreachvariable,wemapthetargettooneofthe 256 bins,wherethe\n binsareuniformlydistributedwithintheboundsofeachvariable. \n Loss. We use a standard categorical cross-entropy entropy objective and causal masking that was\n utilizedinprior Transformer-basedcontrollers(Reedetal.,2022;Leeetal.,2022 a).\n Inference speed. In contrast to many applications of large models, such as natural language or\n imagegeneration,oneoftheuniquerequirementsforamodelthatneedstorunonrealrobotsinreal\n time is fast and consistent inference speed. Given the human speeds of executing the instructions\n 6 "
  },
  {
    "page_num": 7,
    "text": " \n \n Preprint \n \n \n \n consideredinthiswork(whichwemeasuredtobeinthe 2‚àí4 secsrange),wewantthemodeltobe\n notsignificantlyslowerthanthat. Basedonourexperimentsthisrequirementcorrespondstoatleast\n 3 Hzcontrolfrequencyandtheresultinginferencetimebudgetforthemodel,givenotherlatencies\n inthesystem,tobelessthan 100 ms. \n This requirement limits the size of the model that we can use. We further explore the impact of\n modelsizeoninferencespeedintheexperiments. Weemploytwotechniquestospeedupinference:\n (i) reduce the number of tokens generated by a pre-trained Efficient Net model by using Token-\n Learner(Ryooetal.,2021), (ii)computethesetokensonlyonceandreusethemforthefollowing\n windowsthatoverlapforthefutureinferences. Bothoftheseallowustospeedupthemodelinfer-\n enceby 2.4 and 1.7 times,respectively. Additionaldetailsonmodelinferencearein Appendix C.1.\n \n 5.2 DATA \n \n Skill Count Description Example Instruction \n Pick Object 130 Lifttheobjectoffthesurface pickicedteacan \n Move Object Near Object 337 Movethefirstobjectnearthesecond movepepsicannearrxbarblueberry\n Place Object Upright 8 Placeanelongatedobjectupright placewaterbottleupright\n Knock Object Over 8 Knockanelongatedobjectover knockredbullcanover \n Open Drawer 3 Openanyofthecabinetdrawers openthetopdrawer \n Close Drawer 3 Closeanyofthecabinetdrawers closethemiddledrawer \n Place Objectinto Receptacle 84 Placeanobjectintoareceptacle placebrownchipbagintowhitebowl\n Pick Objectfrom Receptacle 162 Pickanobjectupfromalocationandthen pickgreenjalapenochipbagfrompaper\n and Placeonthe Counter placeitonthecounter bowlandplaceoncounter \n Section 6.3 and 6.4 tasks 9 Skillstrainedforrealistic,longinstructions openthelargeglassjarofpistachios\n pullnapkinoutofdispenser \n grabscooper \n Total 744 \n Table 1: The list of skills collected for RT-1 together with their descriptions and example instruc-\n tions. \n Our goal is to build a system that exhibits high performance, generalization to new tasks, and ro-\n bustnesstodistractorsandbackgrounds. Wethereforeaimtocollectalarge,diversedatasetofrobot\n trajectoriesthatincludesmultipletasks,objectsandenvironments. Ourprimarydatasetconsistsof\n ‚àº130 krobotdemonstrations,collectedwithafleetof 13 robotsoverthecourseof 17 months. We\n conductedthislarge-scaledatacollectioninaseriesofofficekitchensegments,whichwerefertoas\n robotclassrooms,shownin Fig.2. Moredetailsondatacollectionarein Appendix C.2.\n Skills and instructions. While the definition of a task remains inconsistent in the literature, in\n this work we count the number of language instructions that the system can perform, where an\n instructioncorrespondstoaverbsurroundedbyoneormultiplenouns,suchas‚Äúplacewaterbottle\n upright‚Äù,‚Äúmovethecokecantothegreenchipbag‚Äùor‚Äúopenthedrawer‚Äù. RT-1 isabletoperform\n over 700 languageinstructionsinmultiplerealisticofficekitchenenvironmentsthatweevaluateand\n describeindetailintheexperiments. Inordertogrouptheevaluationsanddrawconclusionsonthe\n performanceofthesystem,wegrouptheinstructionsbytheverbsusedinthem,whichwereferto\n asskills. Amoredetailedlistofinstructionsisshownin Table 1,withexamplesandthenumberof\n instructionsperskill. \n Thecurrentsetofskillsincludespicking,placing,openingandclosingdrawers,gettingitemsinand\n outdrawers,placingelongateditemsup-right,knockingthemover,pullingnapkinsandopeningjars.\n Theskillswerechosentodemonstratemultiplebehaviorswithmanyobjects(seenin Fig.2(e))to\n testaspectsof RT-1 suchasgeneralizationtonewinstructionsandabilitytoperformmanytasks.We\n thengreatlyexpandedtheobjectdiversityforthe‚Äúpick‚Äùskilltomakesurethattheskillsgeneralize\n to varied objects (see the expanded set of objects in Fig. 2(f)). The skills were further expanded\n while we conducted the ablations to include instructions added in the last row of Table 1, which\n were used for the experiments described in Sec. 6.4 and 6.3. These additional skills focused on\n realistic,long-horizoninstructionsinanofficekitchen. Theentireprocessofaddingtasksanddata\n is described in the Appendix C.4. Since we do not make any assumptions about particular skills\n when adding new instructions, the system is easily extendable, and we can continuously provide\n morediversedatatoimproveitscapabilities. \n 7 "
  },
  {
    "page_num": 8,
    "text": " \n \n Preprint \n \n \n \n 6 EXPERIMENTS \n \n Ourexperimentsseektoanswerthefollowingquestions: \n \n 1. Can an RT-1 learn to perform a large number of instructions, as well as to generalize in\n zeroshottonewtasks,objectsandenvironments? (Section 6.2) \n 2. Canwepushtheresultingmodelevenfurtherbyincorporatingheterogeneousdatasources,\n suchassimulateddataordatafromdifferentrobots? (Section 6.3) \n 3. Howdovariousmethodsgeneralizetolong-horizonroboticscenarios? (Section 6.4)\n 4. How do generalization metrics change with varying amounts of data quantity and data\n diversity? (Section 6.5) \n 5. Whataretheimportantandpracticaldecisionsinthedesignofthemodelandhowdothey\n affectperformanceandgeneralization? (Appendix Section D.4) \n Throughoutthissectionwewillcomparetotwobaselinestateoftheartarchitectures, Gato(Reed\n etal.,2022)and BC-Z(Jangetal.,2021).Importantlybothofthesearetrainedonourdatadescribed\n in detail in Sec. 5.2 (which is an important part of our system) since the original models in these\n publicationswouldnotexhibitgeneralizationpropertiesrequiredforourevaluationtasks. Gatois,\n similarly to RT-1, based on a Transformer architecture, but varies from RT-1 in multiple aspects.\n First,itcomputesimagetokenswithoutthenotionoflanguageandeachimagetokenembeddingis\n computed separately for each image patch, as opposed to early language fusion and global image\n embedding in our model. Second, it does not use a pre-trained text embedding to encode the lan-\n guagestring. Italsodoesnotincludeinferencetimeconsiderationsthatarenecessaryforrealrobots\n asdiscussedin Sec.5.1 suchas Token Learnerandtheremovalofauto-regressiveactions.Inorderto\n run Gatoonrealrobotsatahighenoughfrequency,wealsolimitthesizeofthemodelcomparedto\n theoriginalpublication,whichwas 1.2 Bparameters(resultinginonrobotinferencetimeof 1.9 s),\n to be of similar size to RT-1 (37 M parameters for Gato vs. 35 M for RT-1). BC-Z is based on a\n Res Netarchitecture,andwasusedin Say Can(Ahnetal.,2022). BC-Zdiffersfrom RT-1 inthatitis\n afeedforwardmodelthatdoesnotuseprevioustimesteps,anditusescontinuousactionsratherthan\n discrete action tokens. In addition to the original BC-Z model size, we also compare our method\n toalargerversionof BC-Zthathasasimilarnumberofparametersto RT-1 andrefertoitas BC-Z\n XL. We study and analyze how each of these design decisions changes performance in Appendix\n Sections D.4 and D.5. \n Weevaluatethesuccessrateinexperimentstomeasureperformanceontraininginstructions, gen-\n eralization to unseen instructions, robustness to backgrounds and distractors, and performance in\n long-horizon scenarios, as detailed below. Throughout this section, we evaluate our approach and\n baselines with over 3000 real-world trials, making one of the largest scale evaluation of a robot\n learningsystemto-date. \n 6.1 EXPERIMENTALSETUP \n As mentioned in Section 4, we evaluate RT-1 with a set of mobile manipulators from Everyday\n Robotsinthreeenvironments:tworealofficekitchensandatrainingenvironmentmodelledoffthese\n realkitchens. Thetrainingenvironment, shownin Fig.2(a), consistsofpartialcounterswhilethe\n tworealenvironments,shownin Fig.2(b,c),havesimilarcountertopstothetrainingenvironment,\n butvaryinlighting,background,andfullkitchengeometry(e.g.,theremaybeacabinetinsteadof\n a drawer or a sink may be visible). The policies are evaluated for performance on training tasks\n as well as generalization to new tasks, robustness to unseen environments, and performance when\n chainedtogetherforlong-horizontasks,asdetailedbelow. \n Seentaskperformance.Toevaluateperformanceonseeninstructions,weevaluateperformanceon\n instructionssampledfromthetrainingset. Note,however,thatthisevaluationstillinvolvesvarying\n theplacementofobjectsandotherfactorsofthesetup(e.g.,timeofday,robotposition),requiring\n the skills to generalize to realistic variability in the environment. In all, we test over 200 tasks in\n thisevaluation: 36 forpickingobjects,35 forknockingobjects,35 forplacingthingsupright,48 for\n movingobjects,18 foropeningandclosingvariousdrawers,and 36 forpickingoutofandplacing\n objectsintodrawers. \n Unseentasksgeneralization. Toevaluategeneralizationtounseentasks,wetest 21 novel,unseen\n instructions. These instructions are distributed across skills and objects. This ensures that at least\n 8 "
  },
  {
    "page_num": 9,
    "text": " \n \n Preprint \n \n \n \n someinstancesofeachobjectandskillwerepresentinthetrainingsetbuttheywillbecombinedin\n novelways. Forexample,if‚Äúpickuptheapple‚Äùisheldout,thenthereareothertraininginstructions\n thatincludetheapple. Thelistofallunseeninstructionscanbefoundinthe Appendix D.1.\n Robustness. Toevaluaterobustness, weperform 30 real-worldtasksfordistractorrobustnessand\n 22 tasks for background robustness. The background robustness was tested by evaluating in new\n kitchens(whichhavedifferentlightingandbackgroundvisuals)andwithdifferentcountersurfaces\n (e.g., a patterned table cloth). Example configurations of the robustness evaluation scenarios are\n depictedin Fig.4. \n Long-horizonscenarios. Wealsoevaluategeneralizationtomorerealisticlong-horizonscenarios,\n whicheachrequireexecutingasequenceofskills.Thegoalofthisevaluationistocombinemultiple\n generalization axes such as new tasks, objects, environments and test the overall generalization\n capabilitiesinrealisticsettings.Theseevaluationsconsistof 15 long-horizoninstructionsintworeal\n kitchens, which require executing sequences of skills consisting of ‚àº 10 distinct steps, with each\n stepofroughlycomparablescopeasthetraininginstructions.Thesestepsareobtainedautomatically\n fromhigherlevelinstructions,suchas‚Äúhowwouldyouthrowawayalltheitemsonthetable?‚Äù by\n usingthe Say Cansystem(Ahnetal.,2022),asdescribedindetailin Section 6.4 and Appendix D.3.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 4: Evaluation scenarios for distractors (first row), from left to right: easy (0-5 distractors),\n medium (9 distractors), hard (9 distractors and occluded object); background (second row), from\n lefttoright: originalenvironment,patternedtablecloth,newkitchen;andrealisticscenariosinthe\n realkitchen(thirdrow),generalizationlevelsfromlefttoright: L 1,L 2 and L 3.\n 6.2 CANRT-1 LEARNTOPERFORMALARGENUMBEROFINSTRUCTIONS,ANDTO \n GENERALIZETONEWTASKS,OBJECTSANDENVIRONMENTS? \n \n To answer our first question, we analyze the overall performance, generalization, and robustness\n capabilities of RT-1 compared to previously proposed models. Specifically, we compare to the\n model architectures used by Gato (Reed et al., 2022) and BC-Z (Jang et al., 2021), as well as a\n largerversionof BC-Z,whichwerefertoas BC-ZXL.Note, however, thatallmodelsaretrained\n on the same data as RT-1, and the evaluation only compares the model architectures, not the task\n sets,datasets,oroverallroboticsystems. Thecapabilitiesof RT-1 aredeterminedtoalargeextent\n by the dataset and task set, which we believe improves significantly over prior works (e.g. BC-Z\n uses 100 tasksandtheoriginal Gatomodeltrainsastackingtaskwithvariousshapes),andthusthis\n comparison should be viewed as rather favorable to the prior models, which also benefit from the\n largeanddiversedatasetandtasksetthatwecollected. \n The results are shown in Table 2. Across each category, we find that RT-1 outperforms the prior\n models significantly. On seen tasks, RT-1 is able to perform 97% of the more than 200 instruc-\n 9 \n "
  },
  {
    "page_num": 10,
    "text": " \n \n Preprint \n \n \n \n \n \n Model Seen Tasks Unseen Tasks Distractors Backgrounds \n Gato(Reedetal.,2022) 65 52 43 35 \n BC-Z(Jangetal.,2021) 72 19 47 41 \n BC-ZXL 56 43 23 35 \n RT-1(ours) 97 76 83 59 \n \n \n Table 2: Overall performance of RT-1 and baselines across seen tasks, generalization to unseen\n tasks,androbustnesstodistractorsandbackgrounds. \n \n tionssuccessfully,whichis 25%morethan BC-Zand 32%morethan Gato. Onunseentasks,RT-1\n showsitiscapableofgeneralizingtonovelinstructions,performing 76%ofthenever-before-seen\n instructions,24%morethanthenextbestbaseline. Whilesuchgeneralizationtonovelinstructions\n ismadepossibleduetonaturallanguageconditioningofthepolicy, asthepolicyisabletounder-\n stand new combinations of previously seen concepts, all of the baselines are also conditioned on\n naturallanguageandinprincipleenjoythesamebenefits. Wefurtherablatedifferentcomponents\n of RT-1 inthenextsectiontobetterunderstandwhataspectsofourmethodcontributethemostto\n thisdifference. Ondistractorsandbackgrounds,wefindthat RT-1 isquiterobust,successfullyexe-\n cuting 83%ofthedistractorrobustnesstasksand 59%ofthebackgroundrobustnesstasks(36%and\n 18%higherthanthenextbestalternative,respectively). Overall,wefindthat RT-1 hashighgeneral\n performance,whileexhibitingimpressivedegreesofgeneralizationandrobustness. Weshowexam-\n pletrajectoriesofthe RT-1 agentincludinginstructionsthatcoverdifferentskills,environmentsand\n objectsin Fig.5. Wealsopresentadditionaltrajectoryexamplesfordifferentgeneralizationtestsin\n the Appendix,whichincludebackgrounds(Fig.10),anddistractors(Fig.12). \n Generalization to realistic instructions. Next, we test whether our method generalizes enough\n across all the different axes that we evaluated previously to be deployed in a real kitchen, which\n poses multiple distribution shifts all at once such as new tasks combinations, object distractors as\n wellasanovelenvironment. \n To evaluate our algorithm in realistic scenarios in a real kitchen, we construct task sequences to\n accomplish a number of realistic goals. The robot restocks several snacks in drawers, tidies up\n knocked over condiment bottles and closes drawers left open by humans, prepares a snack with\n an orange and a napkin and fetches lost sunglasses and an octopus toy from several places in the\n kitchen. The detailed instructions used in these scenarios are listed in the Appendix D.1. The\n officekitcheninvolvesadramaticshiftfromthetrainingenvironmentandwecategorizetasksacross\n thesescenarioswithvaryinglevelsofgeneralization: L 1 forgeneralizationtothenewcounter-top\n layout and lighting conditions, L 2 for additionally generalization to unseen distractor objects, L 3\n foradditionalgeneralizationtodrasticallynewtasksettings, newtaskobjectsorobjectsinunseen\n locations such as near a sink. The three levels that correspond to the three tasks of restocking,\n preparingasnackandfetchingalostobjectintherealkitchenaredepictedinthelastrowof Fig.4.\n Exampletrajectoriesfordifferentlevelsarepresentedinthe Appendixin Fig.11.\n Wereporttheper-tasksuccessrateintheserealisticscenariosalongwiththevaryinggeneralization\n levelsin Table 3 andfind RT-1 tobethemostrobustonalllevels. Gatogeneralizesfairlywellatthe\n first level but it performs significantly drops for the more difficult generalization scenarios. BC-Z\n andits XLequivalentperformfairlywellat L 2 levelandbetterthan Gatoat L 3 buttheyarestillnot\n atthegeneralizationlevelof RT-1. \n 6.3 CANWEPUSHTHERESULTINGMODELFURTHERBYINCORPORATINGHETEROGENEOUS \n DATASOURCESSUCHASSIMULATIONORDATAFROMDIFFERENTROBOTS? \n Next,weexplorethelimitsof RT-1 forutilizinghighlyheterogeneousdata.Wedemonstratehow RT-\n 1 canincorporateandlearnfromvastlydifferentdatasourcesandimprovefromsuchdatawithout\n sacrificingitsoriginal-tasksperformanceacrossthevariedtasksinherentinthisdata.Tothisend,we\n conducttwoexperiments: (1)RT-1 trainedandtestedonbothrealdataandsimulationdataand(2)\n 10 "
  },
  {
    "page_num": 11,
    "text": " \n \n Preprint \n \n \n \n \n ‚Äúpick water bottle \n from the bottom \n drawer and put it \n on the counter‚Äù \n \n ‚Äúmove sponge to \n green jalapeno \n chips‚Äù \n \n ‚Äúplace red bull \n can in middle \n drawer‚Äù \n \n ‚Äúpull napkin out \n of dispenser‚Äù \n \n \n ‚Äúplace coke can \n upright‚Äù \n \n \n ‚Äúopen top \n drawer‚Äù \n \n \n ‚Äúpick apple from \n bowl‚Äù \n \n Figure 5: Exampleevaluationtrajectoriesfor RT-1 acrossvariousinstructions.\n \n \n \n \n Generalization Scenario Levels \n Models All L 1 L 2 L 3 \n Gato Reedetal.(2022) 30 63 25 0 \n BC-ZJangetal.(2021) 45 38 50 50 \n BC-ZXL 55 63 75 38 \n RT-1(ours) 70 88 75 50 \n \n Table 3: Realistic generalization scenarios: we compare model success rate in a realistic Google kitchen\n scenariosacrossthreelevelsofgeneralization:L 1 forgeneralizationtothenewcounter-toplayoutandlighting\n conditions,L 2 foradditionallygeneralizationtounseendistractorobjects,L 3 foradditionallygeneralization\n todrasticallynewtasksettings,newtaskobjectsorinunseenlocationslikenearasink.\n \n \n RT-1 trainedacrosslargedatasetsofdifferenttasks, originallycollectedbydifferentrobots. More\n informationoneachisprovidedin Appendix D.2. \n Absorbingsimulationdata. Table 4 showstheabilityof RT-1, andbaselines,toabsorbbothreal\n andsimulationdata. Totestthis,wetakealloftherealdemonstrationdatabutwealsoprovidead-\n \n 11 \n "
  },
  {
    "page_num": 12,
    "text": " \n \n Preprint \n \n \n \n \n \n 60% \n 50% \n Real Objects Sim Objects(notseeninreal) \n 40% \n Seen Skill Seen Skill Unseen Skill \n Models Training Data w/Objects w/Objects w/Objects 30% \n RT-1 Real Only 92 23 7 20% \n RT-1 Real+Sim 90(-2) 87(+64) 33(+26) \n 10% \n 0% \n Sim-seen Objects Sim-seen Objects Real Tasks\n w/ Skills w/o Skills \n ot \n derapmo C \n eta R \n sseccu S \n ylno \n lae R \n Real +Sim Data \n +64% \n +26% \n -2% \n Table 4: Experimental results for incorporating simulation data in RT-1. Adding simulation data\n doesnotimpacttheperformanceonrealobjects,whilesignificantlyimprovingrealperformanceon\n objectsthatwereonlyintroducedinsimulation(+64%). Italsoimprovesreal-worldgeneralization\n onsimulatedobjectsusedwithskillsseenonlyintherealworld(+26%),e.g. ‚Äúmove Xto Y‚Äùwhere\n Xonlyappearedinsimulated‚Äúpick X‚Äùtask. \n ditionalsimulationdatathatincludesobjectsthattherobothasneverseenintherealworld. Specifi-\n cally,wespecifydifferentgeneralizationscenarios: forseenskillswithrealobjectsthetrainingdata\n hasrealdataofthatinstruction(i.e.,performanceonseentasks),forseenskillswithsimobjectsthe\n trainingdatahassimdataofthatinstruction(e.g. ‚Äúpickupasimobject‚Äù,whichwaspresentinsim),\n andforunseenskillswithsimobjectsthetrainingdatahassimdataofthatobjectbutthereareno\n examplesoftheinstructiondescribingtheskillwiththatobjecteitherinsimorinreal(e.g.,‚Äúmove\n asimobjecttoapple‚Äù,eventhoughtherobothasonlypracticedinpickingthatsimobjectandnot\n movingitnearotherobjects). Allevaluationsaredoneintherealworldbuttolimitthenumberof\n instructionsevaluated,wefocusonpickandmove-toskills. \n We find in Table 4 that for RT-1, we do not lose performance adding simulation data compared\n to the Real Only dataset. We do however, see a significant increase in performance (from 23% to\n 87%)onobjectsandtasksseenonlyinsimulation, toapproximatelytheperformanceofthethose\n in real, demonstrating an impressive degree of domain transfer. We also see a significant increase\n inperformanceonunseeninstructionsfrom 7%to 33%;impressivegiventheobjectinquestionhas\n never been seen in real and the instruction never seen at all. Overall, we find that RT-1 is able to\n efficientlyabsorbnewdata,evenfromaverydifferentdomain. \n Absorbing data from different robots. To push the data absorption limits of RT-1, we conduct\n an additional set of experiments where we combine two data sources that originate from different\n robots: Kuka IIWA as well as the Everyday Robots mobile manipulators used in the experiments\n sofar. The Kukadatacontainsallthesuccessfulexamplescollectedin QT-Opt(Kalashnikovetal.,\n 2018),whichcorrespondsto 209 kepisodes,wheretherobotwasindiscriminatelygraspingobjects\n inabin(seeanexampleofa Kukaepisodein Table.5). Totestwhether RT-1 caneffectivelyabsorb\n thesetwoverydifferentdatasets,whichwerefertoasthestandard‚ÄúClassroomeval‚Äù,aswellasthe\n performance on the newly constructed tasks that reflect the bin-picking setup present in the Kuka\n data,whichwerefertoasthe‚ÄúBin-pickingeval‚Äù(see Fig.6). \n Wewouldliketoemphasizethedifficultyofthissettingbynotingthemajordifferencesbetweenthe\n datasets. Notonlyaretherobotsthatcollectedthedatadifferentinappearanceandactionspace,but\n alsotheenvironmenttheyweredeployedinhasdifferentappearanceanddynamics. Inadditionthe\n QT-Optdatapresentsacompletelydifferentactiondistribution‚Äìitwascollectedbyan RLagentas\n opposedtohumandemonstrationspresentinourdataset. \n The results are presented in Table 5. We observe that the model that mixes the RT-1 data and the\n Kukadatahasonlyaminimaldecreaseintheoriginaltasks‚Äôperformance(i.e. Classroomeval),i.e.\n 2%. Even more importantly, in the Bin-picking eval, we observe that the model trained on multi-\n robotdataperformsat 39%comparedtothe 22%ofthemodelthatwastrainedonlyonthe RT-1 data.\n Thisisa 17%performancedifference(almost 2 x). Additionally,RT-1 trainedon Kukabin-picking\n data and evaluated on the bin-picking tasks with the Everyday Robots (EDR) robot achieves 0%\n performance, confirming that it is difficult to transfer a behavior from another robot morphology.\n However, mixing the data from both robots allows RT-1 to infer the correct actions of the EDR\n 12 "
  },
  {
    "page_num": 13,
    "text": " \n \n Preprint \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 6: In Table 5,RT-1 istrainedwithdatafromtworoboticsplatformsandlearnstogeneralize\n acrossthem. \n \n 17.5% \n 15.0% \n 12.5% Models Training Data Classroomeval Bin-pickingeval\n 10.0% \n RT-1 Kukabin-pickingdata+EDRdata 90(-2) 39(+17) \n 7.5% \n RT-1 EDRonlydata 92 22 \n 5.0% RT-1 Kukabin-pickingonlydata 0 0\n 2.5% \n 0.0% \n 2.5% \n Bin-picking Eval Classroom Eval\n yln O \n RDE \n ot \n derapmo C \n eta R \n sseccu S \n EDR +Kuka Data \n +17% \n -2% \n Table 5: Experimental results for mixing data from two different robots. Incorporating Kuka bin-\n pickingdatafrom QT-Opt(Kalashnikovetal.,2018)in RT-1 minimallyimpactsthestandardclass-\n roomevaluationperformanceandresultsinalmosta 2 ximprovementingeneralizationtothe Bin-\n pickingevaluation(thatissimilartothesetupinthe Kukadata)onthe Everyday Robotsmanipulator.\n Thisdemonstratesaneffectivetransferacrosstwodifferentrobotmorphologies.\n robot even when faced with the states observed by Kuka robots. This is achieved without explicit\n demonstrationsofbin-pickingon EDRrobotandbytakingadvantageofpastexperiencescollected\n by Kukarobots. Theseresultsindicatethat RT-1‚Äôsabsorptionpropertiesalsoincludetheabilityto\n acquire new skills through observing other robots‚Äô experiences and present an exciting avenue of\n futureworkwherewecombinemanymoremulti-robotdatasetstoenhancetherobotcapabilities.\n 6.4 HOWDOVARIOUSMETHODSGENERALIZELONG-HORIZONROBOTICSCENARIOS? \n In the next set of experiments we evaluate whether our method generalizes enough to be used in\n long-horizonrealistickitchensettings. Toanswerthisquestion,weexecute RT-1 andvariousbase-\n lineswithinthe Say Can(Ahnetal.,2022)frameworkintwodifferentrealkitchens. Since Say Can\n combines many low-level instructions to perform high-level instructions, the number of possible\n high-level instructions increases combinatorially with skills, so the skill-breadth of RT-1 can be\n fullyseen(formoredetailsonthe Say Canalgorithmpleasereferto Ahnetal.(2022)). Thesuccess\n rateoflong-horizontasksalsodecreasesexponentiallywiththelengthofthetask,sohighsuccess\n rates in manipulation skills are particularly important. Furthermore, as mobile manipulation tasks\n requirebothnavigationandmanipulation,thepoliciesabilitytoberobusttobasepositioniscrucial.\n Moredetailisprovidedin Appendix D.3. \n Table 6 shows our results (on instructions in Appendix Table 12). Except for original Say Can, all\n methodsget 87%asplanningsuccessrate,and RT-1 performsthebest,with 67%executionsuccess\n rate in Kitchen 1. Kitchen 2 constitutes a much more challenging generalization scene, since the\n Robot Classroom training scenes are modeled after Kitchen 1 (see the pictures of the kitchens in\n Fig.2). Duetothisgeneralizationdifficulty,Say Canwith Gatoisnotabletofinishanylonghorizon\n task, and Say Canwith BC-Zisabletoachieve asuccessrateof 13%. Theoriginal Say Can paper\n didnotevaluateperformanceinanewkitchen. Surprisingly,themanipulationperformancedoesnot\n 13 "
  },
  {
    "page_num": 14,
    "text": " \n \n Preprint \n \n \n \n seeavisibledropfrom Kitchen 1 to Kitchen 2 forourmethod. Inthesupplementaryvideo,weshow\n thatthisenablesustooperateunseendrawersin Kitchen 2,andthatwecanuse Say Can-RT 1 toplan\n andexecuteultra-longhorizontasks,withasmanyas 50 steps. \n Say Cantasksin Kitchen 1 Say Cantasksin Kitchen 2 \n Planning Execution Planning Execution \n Original Say Can(Ahnetal.,2022)‚àó 73 47 - - \n Say Canw/Gato(Reedetal.,2022) 87 33 87 0 \n Say Canw/BC-Z(Jangetal.,2021) 87 53 87 13 \n Say Canw/RT-1(ours) 87 67 87 67 \n Table 6: Say Canstylelonghorizontasksin Kitchen 1 and Kitchen 2. (*Original Say Canevalusesa\n slightlydifferentpromptsotheplanningsuccessrateislower.) \n \n \n 6.5 HOWDOGENERALIZATIONMETRICSCHANGEWITHVARYINGAMOUNTSOFDATA \n QUANTITYANDDATADIVERSITY? \n While previous works have shown the scaling abilities of Transformer-based models (Lee et al.,\n 2022 a;Reedetal.,2022;Jiangetal.,2022)withthenumberofmodelparameters,inmanyrobotics\n works the model size is often not the primary bottleneck, and the maximum size is limited by the\n latency requirement for running such models on real robots. Instead, in this study we focus on\n ablatingtheinfluenceofdatasetsizeanddiversity,astheyplayanimportantroleinthetraditionally\n data-limited robot learning field. Since data collection is particularly expensive for real robots, it\n is important to quantify what kind of data our models need to achieve a certain performance and\n generalization. Thus,ourlastquestionfocusesonthescalingpropertiesof RT-1 withdifferentdata\n properties. \n Generalization \n Models %Tasks %Data Seen Tasks All Unseen Tasks Distractors Backgrounds\n Smaller Data \n RT-1(ours) 100 100 97 73 76 83 59 \n RT-1 100 51 71 50 52 39 59 \n RT-1 100 37 55 46 57 35 47 \n RT-1 100 22 59 29 14 31 41 \n Narrower Data \n RT-1(ours) 100 100 97 73 76 83 59 \n RT-1 75 97 86 54 67 42 53 \n \n \n \n \n \n \n \n \n \n Table 7: Various data ablations of RT-1 across seen tasks, generalization to unseen tasks, and ro-\n bustnesstodistractorsandbackgrounds. Datadiversityhasahigherimpactontheperformanceand\n generalizationthandataquantity. \n \n In Table 7 we show the performance, generalization, and robustness of RT-1 as we decrease the\n dataset size (% data) and the dataset diversity (% tasks). To separate the axes of dataset size and\n diversity, we create smaller datasets with the same task diversity by removing data from the tasks\n withthelargestdata,cappingthenumberofexamplespertaskat 200(resultingin 51%ofthedata),\n \n 14 \n "
  },
  {
    "page_num": 15,
    "text": " \n \n Preprint \n \n \n \n 100(37%ofthedata),and 50(22.5%ofthedata). Tocreateanarrowdataset,weremovethetasks\n withtheleastdata,thuskeeping 97%oftheoveralldatabutonly 75%ofthetasks. Aswedecrease\n dataset size, we see a general trend of decreasing performance and a steeper trend of decreasing\n generalization. Aswemakethedatasetmorenarrow,weseemuchsteeperperformancereductions,\n particularlyintermsofgeneralization. Infact,removing 25%ofthetaskswhilekeeping 97%ofthe\n dataachievesanequivalentgeneralizationperformancetoreducingthedatasetsizebyasmuchas\n 49%. Ourkeytakeawayisthusthatdatadiversityismoreessentialthandataquantity.\n \n 7 CONCLUSIONS, LIMITATIONS AND FUTURE WORK \n \n We presented Robotics Transformer 1, RT-1, a robot learning method that can effectively absorb\n largeamountsofdataandscaleswithdataquantityanddiversity. Wetrained RT-1 onalargedataset\n of demonstrations containing over 130 k episodes collected over the course of 17 months with 13\n robots. Inourbroadsetofexperiments,wedemonstratedthatourmethodthatcanperformover 700\n instructionsat 97%successrateandeffectivelygeneralizetonewtasks, objectsandenvironments\n betterthanpreviouslypublishedbaselines. Wealsodemonstratedthat RT-1 cansuccessfullyabsorb\n heterogeneousdatafromsimulationandotherrobotmorphologieswithoutsacrificingoriginal-tasks\n performanceandwhileimprovinggeneralizationtonewscenarios.Lastly,weshowedhowthislevel\n ofperformanceandgeneralizationallowedustoexecuteverylong-horizontasksinthe Say Can(Ahn\n etal.,2022)framework,withasmanyas 50 steps. \n While RT-1 presents a promising step towards large-scale robot learning with an data-absorbent\n model, it comes with a number of limitations. First, it is an imitation learning method, which\n inheritsthechallengesofthatclassofapproachessuchasthefactthatitmaynotbeabletosurpass\n theperformanceofthedemonstrators. Second, thegeneralizationtonewinstructionsislimitedto\n thecombinationsofpreviouslyseenconceptsand RT-1 isnotyetabletogeneralizetoacompletely\n newmotionthathasnotbeenseenbefore. Lastly, ourmethodispresentedonalargebutnotvery\n dexteroussetofmanipulationtasks. Weplantocontinueextendingthesetofinstructionsthat RT-1\n enablesandgeneralizestotoaddressthischallenge. \n Asweexplorefuturedirectionsforthiswork,wehopetoscalethenumberofrobotskillsfasterby\n developingmethodsthatallownon-expertstotraintherobotviadirecteddatacollectionandmodel\n prompting. While the current version of RT-1 is fairly robust especially to distractor objects, its\n robustness to backgrounds and environments could be further improved by greatly increasing the\n environmentdiversity. Wealsohopetoimprovethereactionspeedsandcontextretentionof RT-1\n throughscalableattentionandmemory. \n Toallowtheresearchcommunitytobuildontopofthiswork,wehaveopen-sourcedthecodefor RT-\n 14,whichwehopewillprovideresearcherswithavaluableresourceforfutureresearchforscaling\n uprobotlearning. \n ACKNOWLEDGMENTS \n We would like to acknowledge Aleksandra Faust, Andy Christiansen, Chuyuan Fu, Daniel Kap-\n pler,David Rendleman,Eric Jang,Jessica Gomez,Jessica Lin,Jie Tan,Josh Weaver,Justin Boyd,\n Krzysztof Choromanski,Matthew Bennice,Mengyuan Yan,Mrinal Kalakrishnan,Nik Stewart,Paul\n Wohlhart, Peter Pastor, Pierre Sermanet, Wenlong Lu, Zhen Yu Song, Zhuo Xu, and the greater\n teamsat Roboticsat Googleand Everyday Robotsfortheirfeedbackandcontributions.\n REFERENCES \n Michael Ahn,Anthony Brohan,Noah Brown,Yevgen Chebotar,Omar Cortes,Byron David,Chelsea\n Finn,Keerthana Gopalakrishnan,Karol Hausman,Alex Herzog,etal. Doas Ican,notas Isay:\n Groundinglanguageinroboticaffordances. ar Xivpreprintar Xiv:2204.01691,2022.\n Daniel Cer,Yinfei Yang,Sheng-yi Kong,Nan Hua,Nicole Limtiaco,Rhomni St John,Noah Con-\n stant,Mario Guajardo-Cespedes,Steve Yuan,Chris Tar,etal. Universalsentenceencoder. ar Xiv\n preprintar Xiv:1803.11175,2018. \n 4 http://github.com/google-research/robotics_transformer \n \n 15 \n "
  },
  {
    "page_num": 16,
    "text": " \n \n Preprint \n \n \n \n Lili Chen,Kevin Lu,Aravind Rajeswaran,Kimin Lee,Aditya Grover,Misha Laskin,Pieter Abbeel,\n Aravind Srinivas,and Igor Mordatch.Decisiontransformer:Reinforcementlearningviasequence\n modeling. Advancesinneuralinformationprocessingsystems,34:15084‚Äì15097,2021.\n \n Michael Jae-Yoon Chung,Abram LFriesen,Dieter Fox,Andrew NMeltzoff,and Rajesh PNRao.\n Abayesiandevelopmentalapproachtoroboticgoal-basedimitationlearning. Plo Sone,10(11):\n e 0141965,2015. \n Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper,\n Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning.\n In Conferenceon Robot Learning,2019. \n Marc Peter Deisenroth, Peter Englert, Jan Peters, and Dieter Fox. Multi-task policy search for\n robotics. In 2014 IEEEinternationalconferenceonroboticsandautomation(ICRA),pp.3876‚Äì\n 3881.IEEE,2014. \n Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learningmod-\n ularneuralnetworkpoliciesformulti-taskandmulti-robottransfer. In 2017 IEEEinternational\n conferenceonroboticsandautomation(ICRA),pp.2169‚Äì2176.IEEE,2017. \n Miroslav Dud¬¥ƒ±k,John Langford,and Lihong Li.Doublyrobustpolicyevaluationandlearning.ar Xiv\n preprintar Xiv:1103.4601,2011. \n Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas\n Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic\n skillswithcross-domaindatasets. ar Xivpreprintar Xiv:2109.13396,2021. \n Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for\n embodiedagentsinlong-horizontasks.In Proceedingsofthe IEEE/CVFConferenceon Computer\n Visionand Pattern Recognition,pp.538‚Äì547,2019. \n \n Roy Fox,Ron Berenstein,Ion Stoica,and Ken Goldberg. Multi-taskhierarchicalimitationlearning\n forhomeautomation. In 2019 IEEE 15 th International Conferenceon Automation Scienceand\n Engineering(CASE),pp.1‚Äì8.IEEE,2019. \n Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashchand Gandhi, and Lerrel Pinto. Robot\n learninginhomes:Improvinggeneralizationandreducingdatasetbias. Advancesinneuralinfor-\n mationprocessingsystems,31,2018. \n Agrim Gupta,Linxi Fan,Surya Ganguli,and Li Fei-Fei.Metamorph:Learninguniversalcontrollers\n withtransformers. ar Xivpreprintar Xiv:2203.11931,2022. \n Josiah PHanna,Peter Stone,and Scott Niekum. Bootstrappingwithmodels: Confidenceintervals\n foroff-policyevaluation. In Thirty-First AAAIConferenceon Artificial Intelligence,2017.\n Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. Retina GAN:\n An object-aware approach to sim-to-real transfer, 2020. URL https://arxiv.org/abs/\n 2011.03148. \n De-An Huang, Yu-Wei Chao, Chris Paxton, Xinke Deng, Li Fei-Fei, Juan Carlos Niebles, Ani-\n mesh Garg, and Dieter Fox. Motionreasoningforgoal-basedimitationlearning. In 2020 IEEE\n International Conferenceon Roboticsand Automation(ICRA),pp.4878‚Äì4884.IEEE,2020.\n \n Alexander Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, and Sergey\n Levine. Off-policyevaluationviaoff-policyclassification. Advancesin Neural Information Pro-\n cessing Systems,32,2019. \n Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLBench: The robot\n learningbenchmark&learningenvironment. IEEERoboticsand Automation Letters,5(2):3019‚Äì\n 3026,2020. \n Eric Jang,Alex Irpan,Mohi Khansari,Daniel Kappler,Frederik Ebert,Corey Lynch,Sergey Levine,\n and Chelsea Finn. Bc-z: Zero-shottaskgeneralizationwithroboticimitationlearning. In Confer-\n enceon Robot Learning,pp.991‚Äì1002.PMLR,2021. \n \n 16 \n "
  },
  {
    "page_num": 17,
    "text": " \n \n Preprint \n \n \n \n Michael Janner,Qiyang Li,and Sergey Levine. Reinforcementlearningasonebigsequencemod-\n elingproblem. In ICML 2021 Workshopon Unsupervised Reinforcement Learning,2021.\n \n Yunfan Jiang,Agrim Gupta,Zichen Zhang,Guanzhi Wang,Yongqiang Dou,Yanjun Chen,Li Fei-\n Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with\n multimodalprompts. ar Xivpreprintar Xiv:2210.03094,2022. \n Tom Jurgenson,Or Avner,Edward Groshev,and Aviv Tamar. Sub-goaltreesaframeworkforgoal-\n basedreinforcementlearning.In International Conferenceon Machine Learning,pp.5020‚Äì5030.\n PMLR,2020. \n Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\n Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce-\n mentlearningforvision-basedroboticmanipulation. In Conferenceon Robot Learning,pp.651‚Äì\n 673.PMLR,2018. \n Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,\n Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic re-\n inforcementlearningatscale. ar Xivpreprintar Xiv:2104.08212,2021 a. \n Dmitry Kalashnikov, Jake Varley, Yevgen Chebotar, Ben Swanson, Rico Jonschkowski, Chelsea\n Finn,Sergey Levine,and Karol Hausman. MT-opt: Continuousmulti-taskroboticreinforcement\n learningatscale. ar Xiv,2021 b. \n Thomas Kollar,Stefanie Tellex,Deb Roy,and Nicholas Roy.Towardunderstandingnaturallanguage\n directions. In 20105 th ACM/IEEEInternational Conferenceon Human-Robot Interaction(HRI),\n pp.259‚Äì266.IEEE,2010. \n Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio\n Guadarrama,Ian Fischer,Eric Jang,Henryk Michalewski,etal. Multi-gamedecisiontransform-\n ers. ar Xivpreprintar Xiv:2205.15241,2022 a. \n Kuang-Huei Lee, Ted Xiao, Adrian Li, Paul Wohlhart, Ian Fischer, and Yao Lu. PI-QT-Opt: Pre-\n dictive information improves multi-task robotic reinforcement learning at scale. ar Xiv preprint\n ar Xiv:2210.08217,2022 b. \n Ian Lenz, Honglak Lee, and Ashutosh Saxena. Deep learning for detecting robotic grasps. The\n International Journalof Robotics Research,34(4-5):705‚Äì724,2015. \n Corey Lynchand Pierre Sermanet. Languageconditionedimitationlearningoverunstructureddata.\n ar Xivpreprintar Xiv:2005.07648,2020. \n \n Matt Mac Mahon,Brian Stankiewicz,and Benjamin Kuipers. Walkthetalk: Connectinglanguage,\n knowledge,andactioninrouteinstructions. Def,2(6):4,2006. \n Hongyuan Mei,Mohit Bansal,and Matthew RWalter. Listen,attend,andwalk: Neuralmappingof\n navigationalinstructionstoactionsequences. In Thirtieth AAAIConferenceon Artificial Intelli-\n gence,2016. \n Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-\n conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on\n Robot Learning,pp.1303‚Äì1315.PMLR,2022. \n Niki Parmar,Ashish Vaswani,Jakob Uszkoreit,Lukasz Kaiser,Noam Shazeer,Alexander Ku,and\n Dustin Tran. Image transformer. In International conference on machine learning, pp. 4055‚Äì\n 4064.PMLR,2018. \n Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-\n language navigation. In Proceedings of the IEEE/CVF International Conference on Computer\n Vision,pp.15942‚Äì15952,2021. \n Ethan Perez,Florian Strub,Harmde Vries,Vincent Dumoulin,and Aaron Courville. Film: Visual\n reasoning with a general conditioning layer. Proceedings of the AAAI Conference on Artificial\n Intelligence, 32(1), Apr. 2018. doi: 10.1609/aaai.v 32 i 1.11671. URL https://ojs.aaai.\n org/index.php/AAAI/article/view/11671. \n \n 17 \n "
  },
  {
    "page_num": 18,
    "text": " \n \n Preprint \n \n \n \n Lerrel Pintoand Abhinav Gupta. Supersizingself-supervision:Learningtograspfrom 50 ktriesand\n 700 robothours. In 2016 IEEEinternationalconferenceonroboticsandautomation(ICRA),pp.\n 3406‚Äì3413.IEEE,2016. \n \n Dean APomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. Advancesinneural\n informationprocessingsystems,1,1988. \n Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n models from natural language supervision. In International Conference on Machine Learning,\n pp.8748‚Äì8763.PMLR,2021. \n Antonin Raffin,Ashley Hill,Rene¬¥Traore¬¥,Timothe¬¥e Lesort,Natalia D¬¥ƒ±az-Rodr¬¥ƒ±guez,and David Fil-\n liat. Decouplingfeatureextractionfrompolicylearning:assessingbenefitsofstaterepresentation\n learningingoalbasedrobotics. ar Xivpreprintar Xiv:1901.08651,2019. \n Aditya Ramesh,Mikhail Pavlov,Gabriel Goh,Scott Gray,Chelsea Voss,Alec Radford,Mark Chen,\n and Ilya Sutskever. Zero-shottext-to-imagegeneration. In International Conferenceon Machine\n Learning,pp.8821‚Äì8831.PMLR,2021. \n \n Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\n Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.\n Ageneralistagent. ar Xivpreprintar Xiv:2205.06175,2022. \n Michael Ryoo, AJPiergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Token-\n learner:Adaptivespace-timetokenizationforvideos.Advancesin Neural Information Processing\n Systems,34:12786‚Äì12797,2021. \n Ashutosh Saxena, Justin Driemeyer, Justin Kearns, and Andrew Ng. Robotic grasping of novel\n objects. Advancesinneuralinformationprocessingsystems,19,2006. \n Nur Muhammad Mahi Shafiullah,Zichen Jeff Cui,Ariuntuya Altanzaya,and Lerrel Pinto.Behavior\n transformers: Cloningkmodeswithonestone. ar Xivpreprintar Xiv:2206.11251,2022.\n \n Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta. Multiple interactions made\n easy(mime): Largescaledemonstrationsdataforimitation. In Conferenceonrobotlearning,pp.\n 906‚Äì915.PMLR,2018. \n Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\n manipulation. In Proceedingsofthe 5 th Conferenceon Robot Learning(Co RL),2021.\n Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for\n roboticmanipulation. ar Xivpreprintar Xiv:2209.05451,2022. \n \n Andrew Silva, Nina Moorman, William Silva, Zulfiqar Zaidi, Nakul Gopalan, and Matthew Gom-\n bolay.Lancon-learn:Learningwithlanguagetoenablegeneralizationinmulti-taskmanipulation.\n IEEERoboticsand Automation Letters,7(2):1635‚Äì1642,2021. \n Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, Murtaza Dalal, Sergey Levinev, Mohi\n Khansari, and Chelsea Finn. Scalable multi-task imitation learning with autonomous improve-\n ment. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 2167‚Äì\n 2173.IEEE,2020. \n Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni\n Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. Advances\n in Neural Information Processing Systems,33:13139‚Äì13150,2020. \n Mingxing Tanand Quoc Le. Efficient Net: Rethinkingmodelscalingforconvolutionalneuralnet-\n works. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36 th In-\n ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning\n Research, pp. 6105‚Äì6114. PMLR, 09‚Äì15 Jun 2019. URL https://proceedings.mlr.\n press/v 97/tan 19 a.html. \n \n 18 \n "
  },
  {
    "page_num": 19,
    "text": " \n \n Preprint \n \n \n \n Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller,\n and Nicholas Roy. Understandingnaturallanguagecommandsforroboticnavigationandmobile\n manipulation. In Proceedingsofthe AAAIConferenceon Artificial Intelligence,volume 25,pp.\n 1507‚Äì1514,2011. \n Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\n tionprocessingsystems,30,2017. \n \n Ulrich Viereck,Andreas Pas,Kate Saenko,and Robert Platt. Learningavisuomotorcontrollerfor\n realworldroboticgraspingusingsimulateddepthimages. In Conferenceonrobotlearning,pp.\n 291‚Äì300.PMLR,2017. \n Ted Xiao,Eric Jang,Dmitry Kalashnikov,Sergey Levine,Julian Ibarz,Karol Hausman,and Alexan-\n der Herzog. Thinkingwhilemoving:Deepreinforcementlearningwithconcurrentcontrol. ar Xiv\n preprintar Xiv:2004.06089,2020. \n Tianhe Yu,Deirdre Quillen,Zhanpeng He,Ryan Julian,Karol Hausman,Chelsea Finn,and Sergey\n Levine.Meta-world:Abenchmarkandevaluationformulti-taskandmetareinforcementlearning.\n In Conferenceonrobotlearning,pp.1094‚Äì1100.PMLR,2020. \n Tianhao Zhang,Zoe Mc Carthy,Owen Jow,Dennis Lee,Xi Chen,Ken Goldberg,and Pieter Abbeel.\n Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In\n 2018 IEEEInternational Conferenceon Roboticsand Automation(ICRA),pp.5628‚Äì5635.IEEE,\n 2018. \n Yichi Zhang and Joyce Chai. Hierarchical task learning from language instructions with unified\n transformersandself-monitoring. ar Xivpreprintar Xiv:2106.03427,2021. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 19 \n "
  },
  {
    "page_num": 20,
    "text": " \n \n Preprint \n \n \n \n APPENDIX \n \n A AUTHOR CONTRIBUTIONS \n \n ‚Ä¢ Evaluations (ablations, designing procedures, implementations, and running abla-\n tions): Yevgen Chebotar,Keerthana Gopalakrishnan,Karol Hausman,Julian Ibarz,Brian\n Ichter, Alex Irpan, Isabel Leal, Kuang-Huei Lee, Yao Lu, Ofir Nachum, Kanishka Rao,\n Sumedh Sontakke,Austin Stone,Quan Vuong,Fei Xia,Ted Xiao,and Tianhe Yu.\n ‚Ä¢ Network Architecture (tokenizer, training, inference): Yevgen Chebotar, Keerthana\n Gopalakrishnan, Julian Ibarz, Alex Irpan, Kuang-Huei Lee, Yao Lu, Karl Pertsch, Kan-\n ishka Rao,Michael Ryoo,Sumedh Sontakke,Austin Stone,and Quan Vuong. \n ‚Ä¢ Developed Infrastructure(data,training,collect,simulation,evaluations,storage,and\n operations): Anthony Brohan,Keerthana Gopalakrishnan,Karol Hausman,Alex Herzog,\n Jasmine Hsu,Alex Irpan,Nikhil Joshi,Ryan Julian,Dmitry Kalashnikov,Yuheng Kuang,\n Isabel Leal,Yao Lu,Fei Xia,Ted Xiao,Peng Xu,Sichun Xu,and Tianhe Yu. \n ‚Ä¢ Leadership(managedoradvisedontheproject): Chelsea Finn,Karol Hausman,Julian\n Ibarz,Sally Jesmonth,Sergey Levine,Yao Lu,Igor Mordatch,Carolina Parada,Kanishka\n Rao,Pannag Sanketi,Vincent Vanhoucke. \n ‚Ä¢ Paper (figures, vizualizations, writing): Keerthana Gopalakrishnan, Karol Hausman,\n Brian Ichter,Sergey Levine,Ofir Nachum,Karl Pertsch,Kanishka Rao,Austin Stone,Fei\n Xia,and Ted Xiao. \n ‚Ä¢ Data collection and evaluations: Noah Brown, Justice Carbajal, Joseph Dabis, Tomas\n Jackson,Utsav Malla,Deeksha Manjunath,Jodily Peralta,Emily Perez,Jornell Quiambao,\n Grecia Salazar, Kevin Sayed, Jaspiar Singh, Clayton Tan, Huong Tran, Steve Vega, and\n Brianna Zitkovich. \n B MODEL CARD \n \n Wepresentthe Model Cardfor RT-1 in Fig.7. \n \n C MODEL AND DATA \n \n C.1 MODELINFERENCE \n \n In addition to the inference speed requirement, we need to ensure that our system outputs actions\n at a consistent frequency, avoiding jitter. To accomplish this, we introduce a fixed-time waiting\n mechanismthatwaitsacertainamountoftime(280 ms,themaxobservedlatencyofallcomponents)\n afterthestate,thatwasusedtocomputethenextaction,hasbeencaptured,butbeforeapplyingthe\n action,similarlytotheproceduredescribedby Xiaoetal.(2020). \n C.2 DATACOLLECTIONATSCALE. \n \n Each of the robots autonomously approaches its station at the beginning of the episode and com-\n municates to the operator the instruction that they should demonstrate to the robot. To ensure a\n balanced dataset as well as randomization of the scene, we created a software module responsible\n for sampling the instructions to be demonstrated as well as the randomization of the background\n configuration. Each of the robots tells the demonstrator how to randomize the scene and which\n instructiontodemonstrate. \n Demonstrations are collected with direct line-of-sight between operator and robot using 2 virtual\n reality remotes. We map remote controls onto our policy action space to preserve consistency of\n thetransition-dynamics. 3 Dpositionandrotationaldisplacementsoftheremotearemappedto 6 d\n displacementsoftherobottool. Thex,ypositionofthejoystickismappedtoaturningangleand\n drivingdistanceofthemobilebase. Wecomputeandtracktrajectoriestothetargetposesthatwe\n obtainfromthejoystickcommands. \n 20 \n "
  },
  {
    "page_num": 21,
    "text": " \n \n Preprint \n \n \n \n \n Model Cardfor RT-1(Robotics Transformer) \n Model Details \n ‚Ä¢ Developedbyresearchersat Roboticsat Googleand Everyday Robots,2022,v 1.\n ‚Ä¢ Transformer-basedmodel,builtupona Fi LM-conditioned Efficient Net(Tan&Le,\n 2019),a Token Learner(Ryooetal.,2021),anda Transformer(Vaswanietal.,2017).\n ‚Ä¢ Trainedwithimitationlearningwithinputsofnaturallanguagetasksandimagesand\n outputrobotactions. \n Intended Use \n ‚Ä¢ Intendedtobeusedforcontrollingan Everyday Robotformanipulationtasks.\n ‚Ä¢ Unclearsuitabilityasalearnedrepresentationfordifferentroboticembodiments,\n environments,orsignificantlyvarieddownstreamtasks. \n ‚Ä¢ Notsuitableforinteractionwithhumans. \n Factors \n ‚Ä¢ Factorsincludevaryingbackgrounds,lighting,scenes,baseposition,andnovel\n naturallanguagetasks. Hardwarefactorsincludecameraandrobotembodiment.\n Metrics \n ‚Ä¢ Evaluationmetricsincludeseentaskperformance,unseentaskperformance,\n robustnesstobackgroundsanddistractors,andperformanceinlong-horizon\n scenarios. Eachmeasuresthesuccessrateofthemodelperformingnaturallanguage\n specifiedtaskswithrandomizedobjectsandobjectlocationsandvaryingscenes.\n Training Data \n ‚Ä¢ Trainedon 130 ktele-operationdemonstrationsover 13 robotsand 744 tasks.\n Skill Count Description Example Instruction \n Pick Object 130 Lifttheobjectoffthesurface pickicedteacan \n Move Object Near Object 337 Movethefirstobjectnearthesecond movepepsicannearrxbarblueberry\n Place Object Upright 8 Placeanelongatedobjectupright placewaterbottleupright\n Knock Object Over 8 Knockanelongatedobjectover knockredbullcanover \n Open/Close Drawer 6 Openorcloseanyofthecabinetdrawers openthetopdrawer\n Place Objectinto Receptacle 84 Placeanobjectintoareceptacle placebrownchipbagintowhitebowl\n Pick Objectfrom Receptacle 162 Pickanobjectupfromalocationandthen pickgreenjalapenochipbagfrompaper\n and Placeonthe Counter placeitonthecounter bowlandplaceoncounter \n Additionaltasks 9 Skillstrainedforrealistic,longinstructions pullnapkinoutofdispenser\n Total 744 \n Evaluation Data \n ‚Ä¢ Evaluatedonreal-worldrandomizedscenesandover 3000 totalrolloutsinthe\n environmentitwastrainedonaswellastwonewofficekitchenenvironments.\n Quantitative Analyses \n ‚Ä¢ RT-1 showshigh-performanceandrobustnessandcanlearnfromheterogenousdata.\n \n \n \n \n Ethical Considerations \n ‚Ä¢ Earlyresearch,modelhasnotyetbeenevaluatedforsuitabilitytouseoutsideofits\n currentresearchsetting. \n Caveatsand Recommendations \n ‚Ä¢ Whilethecurrentmodelcoversonlyasmallportionofpossibleroboticmanipulation\n tasks,itpresentsarecipeforscalableroboticlearningandanarchitecturethatshows\n favorablegeneralizationanddataabsorptionproperties. \n Figure 7: Model Cardfor RT-1. \n \n 21 \n "
  },
  {
    "page_num": 22,
    "text": " \n \n Preprint \n \n \n \n C.3 MODELSELECTIONATSCALE \n \n \n Asrobotlearningsystemsbecomemorecapableandthenumberofinstructionstheycanhandlein-\n creases,evaluationofthesemodelsbecomesdifficult(Kalashnikovetal.,2021 a;Jangetal.,2021).\n Thisisanimportantconsiderationnotonlyforevaluatingdifferentmodelclassesanddatadistribu-\n tionsduringthedevelopmentprocess,butalsoforselectingthemostperformantmodelcheckpoints\n for a particular training run. While there have been a number of proposed solutions to this prob-\n lem (Dud¬¥ƒ±k et al., 2011; Irpan et al., 2019; Hanna et al., 2017), mostly known in the offline rein-\n forcementlearningliteratureas‚Äúoff-policyevaluation‚Äù,itstillremainsanopenresearchchallenge\n toevaluatemulti-taskrobotlearningsystemsatscale. \n Inthiswork,weproposeleveragingsimulationfor‚Äúrealtosim‚Äùtransferasascalabletoolthatpro-\n videsanapproximateestimateofmodelperformanceduringtrainingacrossmanyrealtasks.Werun\n policiestrainedfromrealdatainasimulatortotestthefullrolloutperformance. Notethatallofour\n trainingdatacomesfromtherealworld(excepttheexperimentin Section 6.3),andthesimulatoris\n usedonlyformodelselection. Toaccomplishthis,weexpandthesimulationenvironmentproposed\n by Leeetal.(2022 b)tosupport 551 ofthetasksdescribedin Section 5.2. Foreachofthesetasks,\n we define a set of scene setup randomizations, robot pose randomizations, and success detection\n criteria. Tobridgethevisualdistributionshiftbetweentherealworldandthesimulation, wetrain\n a Retina GAN (Ho et al., 2020) model that transforms simulated images into realistic looking im-\n ages. Then,wedeploypoliciestrainedonrealdatadirectlyintothesesimulationenvironmentsby\n applying Retina GANvisualtransformationsateachtimestepandmeasuringrolloutsimulatedtask\n successrates. \n Whilemodelstrainedonlyonrealworlddataperformbetterintherealworldthantheydoinsim-\n ulation,wefindthatthesimulationsuccessratesofhigh-performingrealworldpoliciesarehigher\n thanthesimulationsuccessratesoflow-performingrealworldpolicies.Inotherwords,theordering\n of simulation policy success rates are informative for predicting the ordering of real world policy\n successrates. Wenotethatinthisreal-to-simevaluationsetting, wehavealessstrictrequirement\n for simulation accuracy compared to sim-to-real settings; as long as simulation success rates are\n directionallycorrelatedwithrealsuccessrates,wecanacceptamoderateorevenhighgapbetween\n realandsimulationsuccessrates. \n Wepresentexamplecameraimagesfromsimulationaswellastheir Retina GAN-basedtransforma-\n tionsin Fig.8. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 8:Examplecameraimagesshowcasingrawsimulation,simulationwith Retina GANapplied,\n andtherealworld. \n \n \n \n 22 \n "
  },
  {
    "page_num": 23,
    "text": " \n \n Preprint \n \n \n \n C.4 DATACOLLECTIONPROCESS \n \n Figure 9 showsthegrowthofdata,numberoftasks,andthesuccessrateofthepolicyovertime.The\n numberoftasks/instructionsthatoursystemiscapableofgrowsovertimeasmoredataiscollected.\n Thesameistruewiththeperformanceofseentasks.Oneoftheimportantaspectsofthefuturework\n isdeveloptechniquesthatallowustogrowthedataaswellastherobotsperformanceandgeneral\n capabilitiesatafasterrate. \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 9: Thegrowthofdata,numberoftasks,andseeninstructionperformanceovertime.\n \n \n D EXPERIMENTS \n \n D.1 EVALUATIONDETAILS \n \n In Section 6.2, westudythezero-shotgeneralizationcapabilitiesof RT-1 todifficultscenariosnot\n present in the training dataset. To fairly evaluate different ablations of RT-1 as well as baseline\n policies,wedesignstandardizedevaluationproceduresthatcoverarangeofincrementaldifficulty\n levels. \n Seen tasks. We evaluate on 744 tasks present in the training dataset. The breakdown between 12\n skillsisshownin Table 1. Forall‚ÄúSeen‚Äùevaluations, weusethesameclassroomsettingusedfor\n datacollectionasdescribedin Section 5.2. Foreachpolicy,wereportasinglerepresentativemetric\n thattakesaskill-weightedaverageacrossindividualskillevaluations. \n Unseentasks. Weevaluatepolicyperformanceon 53 tasksthatareheldoutduringtraining. While\n the unseen instructions‚Äô specific combinations of skills and objects are not seen during training,\n othercombinationsofthesameskillsandobjectsarepresentinthetrainingset. Weevaluatethese\n unseentasksinthesameenvironmentandthesamerandomizationprocedureasthe Seentasks. A\n fulllistoftheseunseentasksisshownin Table 8. \n Distractorrobustness. Wetestthreetasks(‚Äúpickcokecan‚Äù,‚Äúplacecokecanupright‚Äù,‚Äúmovecoke\n canneargreenricechipbag‚Äù)withincrementallymoredistractorobjectsaddedtothescene. The\n easysettingincludes 0,2,or 5 distractorobjects. Themediumsettingincludes 9 distractorobjects,\n but the coke can is never obscured. The hard setting includes 9 distractor objects, but the scene\n is more crowded and the coke can is partially occluded. Both the medium are hard setting are\n more difficult than scenarios in the training dataset, which contained between 0 and 4 distractors.\n Examplesofthesedifficultysettingsandpolicyevaluationrolloutsareshownin Figure 12.\n Background robustness. We test six tasks (‚Äúpick coke can‚Äù, ‚Äúmove blue chip bag near or-\n ange‚Äù, ‚Äúknock redbull can over‚Äù, ‚Äúpick green jalapeno chip bag‚Äù, ‚Äúmove sponge near brown chip\n bag‚Äù,‚Äúplace redbull can upright‚Äù) with incrementally more challenging backgrounds and counter\n textures. Intheeasysetting,weutilizethesamebackgroundenvironmentsandcountertexturesas\n thetrainingdataset. Inthemediumsetting,weutilizethesamebackgroundenvironmentbutadda\n patternedtableclothtochangethecountertexture.Inthehardsetting,weutilizeabrandnewkitchen\n environmentwithanewcountertop;thischangesthecountertexture,drawermaterialandcolor,and\n 23 "
  },
  {
    "page_num": 24,
    "text": " \n \n Preprint \n \n \n \n backgroundvisuals. Examplesofthesedifficultysettingsandpolicyevaluationrolloutsareshown\n in Figure 10. \n \n Realistic instructions. To study how RT-1 performs in more realistic scenarios, we propose an\n evaluation setting in a real office kitchen that is a dramatic shift from the original training class-\n room environment. We propose a variety of skills that combine aspects of the previous zero-shot\n evaluations, including adding new distractors, including new backgrounds, and new combinations\n ofobjectswithskills. Werefertotheeasiestscenarioas L 1 generalization,whichintroducesanew\n countertopandlightingconditionbutkeepstheskillsandobjectsthesame. Next,L 2 generalization\n additionallyaddsnoveldistractorobjectssuchaskitchenjarcontainers. Finally,L 3 generalization\n addsnewobjectsornewlocationssuchasnearasink. Whilesomeofthesedistributionshiftsare\n tested in Section 6.2, these realistic instructions aim to test multiple dimensions simultaneously.\n Examplesoftheseinstructionsarepresentedin Fig.11. \n Easy \n same background, \n same texture \n \n Medium \n same background, \n new texture \n Hard \n new background, \n new texture \n \n Figure 10: ‚ÄúBackgrounds‚Äù evaluations focus on testing the performance of RT-1 on settings with\n differenttabletexturesanddifferentbackgrounds,suchasthosefoundinkitchensnevertrainedon.\n These visual differences are quite pronounced, which in the most challenging case entails a new\n kitchenwithdifferentcountertexture,differentlightingconditions,differentcountermaterial,anda\n differentbackground. \n \n \n \n \n \n \n \n \n \n \n \n \n \n Figure 11:‚ÄúRealisticinstructions‚Äùevaluationsproposerealisticscenariosmultipledistributionshifts\n thatincrementallyincreaseindifficulty. L 1 generalizationintroducesanewrealofficekitchenwith\n newlightingconditions. L 2 generalizationadditionallyaddsunseendistractorobjects. Finally,L 3\n generalizationincludesnewobjectsorobjectsinnewlocations,suchasnexttoasink.\n \n D.2 HETEROGENEOUSDATA \n \n Wealsoexplorethelimitsof RT-1 forutilizinghighlyheterogeneousdata.Wedemonstratehow RT-\n 1 canincorporateandlearnfromvastlydifferentdatasourcesandimprovefromsuchdatawithout\n \n 24 \n "
  },
  {
    "page_num": 25,
    "text": " \n \n Preprint \n \n \n \n \n \n \n Instruction \n pickcokecanfromtopdrawerandplaceoncounter \n pickgreencanfromtopdrawerandplaceoncounter \n pickgreenricechipbagfrommiddledrawerandplaceoncounter \n pickredbullcanfromtopdrawerandplaceoncounter \n place 7 upcanintobottomdrawer \n placebrownchipbagintotopdrawer \n placegreencanintomiddledrawer \n move 7 upcannearredbullcan \n moveappleneargreenricechipbag \n moveapplenearpaperbowl \n moveapplenearredbullcan \n movebluechipbagnearblueplasticbottle \n movebluechipbagnearpepsican \n movebluechipbagnearsponge \n movebrownchipbagnearapple \n movebrownchipbagneargreenricechipbag \n movebrownchipbagnearredbullcan \n movecokecanneargreenjalapenochipbag \n movecokecannearwaterbottle \n movegreencannear 7 upcan \n movegreencannearapple \n movegreencannearcokecan \n movegreenjalapenochipbagnearbluechipbag \n movegreenricechipbagnearorange \n movegreenricechipbagnearorangecan \n movegreenricechipbagnearpaperbowl \n moveorangecannearbrownchipbag \n movepepsicannearorangecan \n moveredbullcannearcokecan \n moverxbarblueberrynearblueplasticbottle \n moverxbarblueberrynearorangecan \n moverxbarchocolatenearpaperbowl \n moverxbarchocolatenearrxbarblueberry \n movespongenearapple \n movewaterbottlenear 7 upcan \n movewaterbottlenearsponge \n movewhitebowlnearorangecan \n pickblueplasticbottle \n pickgreenricechipbag \n pickorange \n pickrxbarchocolate \n picksponge \n placepepsicanupright \n knockorangecanover \n pickblueplasticbottlefrompaperbowlandplaceoncounter \n pickbrownchipbagfromwhitebowlandplaceoncounter \n pickgreencanfrompaperbowlandplaceoncounter \n pickgreenjalapenochipbagfromwhitebowlandplaceoncounter \n pickorangecanfromwhitebowlandplaceoncounter \n pickredbullcanfromwhitebowlandplaceoncounter \n placeblueplasticbottleintopaperbowl \n placecokecanintopaperbowl \n placeorangecanintopaperbowl \n Table 8: Listof Unseen Instructionsin Sec.6.2. Forthe‚ÄúUnseen Tasks‚Äùevaluation,weexcludea\n totalof 53 tasksduringtraining. Whiletheseexactinstructionswerenotpresentinthetrainingset,\n theobjectsandskillscontainedintheseinstructionswerestillpresentinthetrainingset.\n 25 "
  },
  {
    "page_num": 26,
    "text": " \n \n Preprint \n \n \n \n \n \n \n Easy \n 2 - 5 distractors, \n no occlusion \n \n \n Medium \n 9 distractors, \n no occlusion \n \n \n \n Hard \n 9 distractors, \n occlusion \n \n \n Figure 12: ‚ÄúDistractors‚Äùevaluationsfocusondiversifyinginitialsceneconfigurationswellbeyond\n thedistributionscontainedinthetrainingdataset,whichcontainbetween 2 and 4 distractorobjects.\n Inthemostchallengingscenarios, thesceneisextremelyclutteredandcontainsocclusionsforthe\n objectsofinterest. \n \n \n sacrificingitsoriginal-tasksperformanceacrossthevariedtasksinherentinthisdata. Tothisend,\n weconducttwoexperiments: (1)RT-1 trainedandtestedonbothrealdataandsimulationdataand\n (2)RT-1 trainedacrosslargedatasetsofdifferenttasks,originallycollectedbydifferentrobots.\n Absorbingsimulationdata. Table 9 showstheabilityof RT-1, andbaselines,toabsorbbothreal\n and simulation data. To test this, we take all of the real demonstration data but we also provide\n additionalsimulationdatathatincludesobjectsthattherobothasneverseenintherealworld. We\n addasetofsimobjectsandonlyshowthemonasubsetoftasks,specificallythepickingtasks,in\n simulation. To accomplish this, we run our real 2 sim method described in Sec. C.3 to bootstrap a\n simulation policy from the real world policy that is then trained with multi-task RL (Kalashnikov\n etal.,2021 a)withadditionalobjectsinsimulation. Fromthisprocess, weextract 518 ksuccessful\n trajectories of picking new objects and mix them with the real data that was used in the previous\n experiments. Thegoalofthisexperimentistodemonstratethatbyexpandingthedatasetofsimu-\n lationtrajectories,wecanbenefit RT-1‚Äôsgeneralizationcapabilitieswithoutsacrificingtheoriginal\n trainingperformance‚Äìadesiredpropertyofanabsorbentmodel. \n Toevaluatethepropertiesofthismodel,wespecifydifferentgeneralizationscenarios:forseenskills\n withrealobjectsthetrainingdatahasrealdataofthatinstruction(i.e.,performanceonseentasks),\n forseenskillswithsimobjectsthetrainingdatahassimdataofthatinstruction(e.g. ‚Äúpickupasim\n object‚Äù,whichwaspresentinsim),andforunseenskillswithsimobjectsthetrainingdatahassim\n dataofthatobjectbuttherearenoexamplesoftheinstructiondescribingtheskillwiththatobject\n eitherinsimorinreal(e.g.,‚Äúmoveasimobjecttoapple‚Äù,eventhoughtherobothasonlypracticed\n inpickingthatsimobjectandnotmovingitnearotherobjects). Allevaluationsaredoneinthereal\n worldbuttolimitthenumberofinstructionsevaluated,wefocusonpickandmove-toskills.\n We find in Table 9 that for RT-1, we do not lose performance adding simulation data compared\n to the Real Only dataset. We do however, see a significant increase in performance (from 23% to\n 87%)onobjectsandtasksseenonlyinsimulation, toapproximatelytheperformanceofthethose\n in real, demonstrating an impressive degree of domain transfer. We also see a significant increase\n inperformanceonunseeninstructionsfrom 7%to 33%;impressivegiventheobjectinquestionhas\n never been seen in real and the instruction never seen at all. Overall, we find that RT-1 is able to\n efficiently‚Äúspongeup‚Äùnewdata,evenfromaverydifferentdomain. \n 26 "
  },
  {
    "page_num": 27,
    "text": " \n \n Preprint \n \n \n \n \n \n 60% \n 50% \n Real Objects Sim Objects(notseeninreal) \n 40% \n Seen Skill Seen Skill Unseen Skill \n Models Training Data w/Objects w/Objects w/Objects 30% \n RT-1 Real Only 92 23 7 20% \n RT-1 Real+Sim 90 87 33 \n 10% \n 0% \n Sim-seen Objects Sim-seen Objects Real Tasks\n w/ Skills w/o Skills \n ot \n derapmo C \n eta R \n sseccu S \n ylno \n lae R \n Real +Sim Data \n +64% \n +26% \n -2% \n Table 9: Experimental results for incorporating simulation data in RT-1. Adding simulation data\n doesnotimpacttheperformanceonrealobjects,whilesignificantlyimprovingrealperformanceon\n objectsthatwereonlyintroducedinsimulation. \n Absorbing data from different robots. To push the data absorption limits of RT-1, we conduct\n an additional set of experiments where we combine two data sources that originate from different\n robots: Kuka IIWA as well as the Everyday Robots mobile manipulators used in the experiments\n sofar. The Kukadatacontainsallthesuccessfulexamplescollectedin QT-Opt(Kalashnikovetal.,\n 2018),whichcorrespondsto 209 kepisodes,wheretherobotwasindiscriminatelygraspingobjects\n inabin(seeanexampleofa Kukaepisodein Table.10). Ourgoalinthisexperimentistoanalyze\n whethertheperformanceonthe RT-1 tasksdropswhenaddingtheadditionaldataand,moreimpor-\n tantly,whetherwecanobserveanytransferfromdatacollectedbyadifferentrobotmorphology.\n Wewouldliketoemphasizethedifficultyofthissettingbynotingthemajordifferencesbetweenthe\n datasets. Notonlyaretherobotsthatcollectedthedatadifferentinappearanceandactionspace,but\n alsotheenvironmenttheyweredeployedinhasdifferentappearanceanddynamics. Inadditionthe\n QT-Optdatapresentsacompletelydifferentactiondistribution‚Äìitwascollectedbyan RLagentas\n opposedtohumandemonstrationspresentinourdataset. \n Tomixthe Kukadatatogetherwiththe RT-1 data,wefirsttransformtheoriginal Kuka 4-DOFaction\n spaceintothesameactionspaceas RT-1,namelywesettherollandpitchto 0,whilekeepingtheyaw\n valuesthatwerepresentintheoriginal Kukadata.Inaddition,wetransformthebinarygripper-close\n commandintoacontinuousgripper-closednesscommandthatispresentinthe RT-1 data. Wealso\n needtextinstructionscorrespondingtothetaskperformedandsincethe Kukadatadoesnotcontain\n thenameoftheobjectthatwasgrasped, werelabelallthedatatothe‚Äúpickanything‚Äùinstruction.\n Withthesemodifications,wemixbothdatasetswiththe 2:1(RT-1 data: Kukadata)ratioandtrain\n RT-1 toobtainthefinalmodel. \n Totestwhether RT-1 caneffectivelyabsorbthesetwoverydifferentdatasets, weevaluatetheper-\n formance on the original RT-1 tasks (in this case, we also focus on ‚Äúpick‚Äù and ‚Äúmove to‚Äù skills),\n whichwerefertoasthestandard‚ÄúClassroomeval‚Äù,aswellastheperformanceonthenewlycon-\n structed tasks that reflect the bin-picking setup present in the Kuka data, which we refer to as the\n ‚ÄúBin-pickingeval‚Äù. Forthe Bin-pickingevaltobeclosetotheoriginaldataset,weputinthesame\n looking bin for the objects as well as modify the robot to be similar to the Kuka manipulators by\n adding extra wires and coloring the gripper gray. For all of the evaluations we use the Everyday\n Robotsrobotwiththepickingcommandsandevaluateitbasedon 72 graspingtrials.\n Theresultsarepresentedin Table 10. Weobservethatthemodelthatmixesthe RT-1 dataandthe\n Kukadatahasonlyaminimaldecreaseintheoriginaltasks‚Äôperformance(i.e. Classroomeval),i.e.\n 2%. Even more importantly, in the Bin-picking eval, we observe that the model trained on multi-\n robotdataperformsat 39%comparedtothe 22%ofthemodelthatwastrainedonlyonthe RT-1 data.\n Thisisa 17%performancedifference(almost 2 x). Additionally,RT-1 trainedon Kukabin-picking\n data and evaluated on the bin-picking tasks with the Everyday Robots (EDR) robot achieves 0%\n performance, confirming that it is difficult to transfer a behavior from another robot morphology.\n However, mixing the data from both robots allows RT-1 to infer the correct actions of the EDR\n robot even when faced with the states observed by Kuka robots. This is achieved without explicit\n demonstrationsofbin-pickingon EDRrobotandbytakingadvantageofpastexperiencescollected\n by Kukarobots. Theseresultsindicatethat RT-1‚Äôsabsorptionpropertiesalsoincludetheabilityto\n 27 "
  },
  {
    "page_num": 28,
    "text": " \n \n Preprint \n \n \n \n \n 17.5% \n 15.0% \n 12.5% Models Training Data Classroomeval Bin-pickingeval\n 10.0% \n RT-1 Kukabin-pickingdata+EDRdata 90 39 \n 7.5% \n RT-1 EDRonlydata 92 22 \n 5.0% RT-1 Kukabin-pickingonlydata 0 0\n 2.5% \n 0.0% \n 2.5% \n Bin-picking Eval Classroom Eval\n yln O \n RDE \n ot \n derapmo C \n eta R \n sseccu S \n EDR +Kuka Data \n +17% \n -2% \n Table 10: Experimental results for mixing data from two different robots. Incorporating Kuka\n bin-picking data from QT-Opt (Kalashnikov et al., 2018) in RT-1 minimally impacts the standard\n classroomevaluationperformanceandresultsinalmosta 2 ximprovementingeneralizationtothe\n Bin-picking evaluation (that is similar to the setup in the Kuka data) on the Everyday Robots ma-\n nipulator. Thisdemonstratesaneffectivetransferacrosstwodifferentrobotmorphologies.\n acquire new skills through observing other robots‚Äô experiences and present an exciting avenue of\n futureworkwherewecombinemanymoremulti-robotdatasetstoenhancetherobotcapabilities.\n D.3 LONG-HORIZONEVALUATIONDETAILS \n Inadditiontoshort-horizonindividualskillevaluationsshowninprevioussections,wealsoevaluate\n how RT-1 performsinalong-horizonrealistickitchensettingthatchainsmultiplemanipulationand\n navigation skills to accomplish natural language instructions within the Say Can framework (Ahn\n etal.,2022). Alistoflong-horizoninstructionsusedfortheseevaluationsislistedin Table 12.\n Thesuccessrateoflong-horizontasksdecreasesexponentiallywiththelengthofthetask,sohigh\n successratesinmanipulationskillsareparticularlyimportant. Furthermore,asmobilemanipulation\n tasks require both navigation and manipulation, the policies ability to be robust to base position\n iscrucial. Since Say Cancombinesmanylow-levelinstructionstoperformhigh-levelinstructions,\n the number of possible high-level instructions increases combinatorially with instructions, so the\n skill-breadthof RT-1 canbefullyseen. \n Say Can works by grounding language models in robotic affordances and it leverages few-shot\n prompting to break down a long horizon task expressed in natural language to a sequence of low\n level skills. An example of long horizon task would be ‚ÄúBring me two different sodas‚Äù, and one\n feasibleplanwouldbe‚Äú1. findacoke,2. pickupthecoke,3. bringittoyou,4. putdownthecoke,\n 5. findapepsi,6. pickupthepepsi,7. bringittoyou,8. putdownthepepsi,9. done.‚Äù Toobtainthe\n affordancefunctionweusevaluefunctionstrainedwith MT-OPT(Kalashnikovetal.,2021 a). Fora\n detaileddescriptionof Say Canalgorithmpleasereferto (Ahnetal.,2022). \n Sincethefocusofthispaperisacquisitionofmanygeneralizableskills,wefocusourevaluationon\n onesubsetoftaskspresentedin Ahnetal.(2022).Itisthelong-horizonfamilyoftasks,involving 15\n instructions,eachinstructionrequiresanaverageof 9.6 stepstocomplete,andinvolvesanaverage\n of 2.4 manipulationskillsperinstruction. Afulllistoftheinstructionscanbefoundin Table 12.\n Wecompareagainst 3 baselines.1)Say Canwith BC-Z,whichuses Say Canplanningalgorithmwith\n BC-Z as manipulation policy, 2) Say Can with Gato, which uses Say Can planning algorithm with\n Gato as manipulation policy, 3) Originally reported Say Can results, which use Say Can planning\n algorithmwith BC-Z,butsinceitusesaslightlydifferentprompt,theplanningsuccessrateislower.\n Wereimplemented 3)in 1)forafaircomparison. \n Asshownin Table 11,exceptfororiginal Say Can,allmethodsget 87%asplanningsuccessrate,and\n RT-1 performsthebest,with 67%executionsuccessratein Kitchen 1. Kitchen 2 constitutesamuch\n morechallenginggeneralizationscene,sincethe Robot Classroomtrainingscenesaremodeledafter\n Kitchen 1 (see the pictures of the kitchens in Fig. 2). Due to this generalization difficulty, Say Can\n with Gato is not able to finish any long horizon task, and Say Can with BC-Z is able to achieve a\n success rate of 13%. The original Say Can paper did not evaluate performance in a new kitchen.\n Surprisingly,themanipulationperformancedoesnotseeavisibledropfrom Kitchen 1 to Kitchen 2\n 28 "
  },
  {
    "page_num": 29,
    "text": " \n \n Preprint \n \n \n \n forourmethod. Inthesupplementaryvideo,weshowthatthisenablesustooperateunseendrawers\n in Kitchen 2,andthatwecanuse Say Can-RT 1 toplanandexecuteultra-longhorizontasks,withas\n manyas 50 steps. \n Say Cantasksin Kitchen 1 Say Cantasksin Kitchen 2 \n Planning Execution Planning Execution \n Original Say Can(Ahnetal.,2022)‚àó 73 47 - - \n Say Canw/Gato(Reedetal.,2022) 87 33 87 0 \n Say Canw/BC-Z(Jangetal.,2021) 87 53 87 13 \n Say Canw/RT-1(ours) 87 67 87 67 \n Table 11: Say Canstylelonghorizontasksin Kitchen 1 and Kitchen 2. (*Original Say Canevaluses\n aslightlydifferentpromptsotheplanningsuccessrateislower.) \n \n \n D.4 MODELABLATIONS \n What are the important and practical decisions in the design of the model and how do they\n affectperformanceandgeneralization? \n \n To answer this question, we perform a set of ablations over different design decisions in RT-1.\n We aim to test a number of hypotheses that will help us disambiguate where the benefits of our\n methodcomefrom. Possiblehypothesesaboutthesourceofimprovementinclude: (i)thecapacity\n andexpressivenessofourmodel,whichweverifybyablatingthemodelsize,tryingotherarchitec-\n tures(e.g.,byremovingthe Transformercomponent);(ii)theparticularactionrepresentation,which\n makesiteasytorepresentcomplexmulti-modalactiondistributions,whichwetestbyswitchingto\n continuous(normallydistributed)actions,aswellasbyablatingtheauto-regressiveactionrepresen-\n tation;(iii)the Image Netpre-trainedinitializationofthecomponents,whichwetestbyinitializing\n themodel‚Äôsweightsrandomly;and(iv)accesstotheshorthistory,whichwetestbyexcludingob-\n servationhistory.Moreconcretely,weablateourmodelby(1)decreasingthemodelsize(from 35 M\n to 21 Mparameters),(2)removingthe Transformerarchitecture(usingapre-trained Efficient Netin-\n stead),(3)usingacontinuousinsteadofdiscreteactionspace(usingan MSElossandmultivariate\n normaloutput), (4)auto-regressivelyconditioningonactions, (5)removing Image Netpre-training\n of the Fi LM Efficient Net, and (6) removing history (reducing the sequence of six images as input\n to a single image). For each ablation we compare on the axes of performance on seen tasks, per-\n formanceonunseentasks,aswellasinferencespeedandrobustnesstodistractorsandbackgrounds\n (withamoredetaileddescriptionofeachcategoryin Section 6.1 and Appendix D.1).\n Table 13 shows the results of each ablation and the delta performance compared to the full RT-1.\n RT-1 achievesimpressiveperformanceontasksandnewenvironments,andparticularlyoutperforms\n baselines on the most challenging robustness problems. We also find that each design decision is\n important,thoughatvaryinglevels. Wefirstevaluateamodelthatreplacestheper-dimensiondis-\n cretizedactionrepresentationinourmodelwithamorestandardcontinuous Gaussiandistribution.\n We observe a significant decline in performance from this modification. The per-dimension dis-\n cretization allows our model to represent complex multi-modal distributions, while the Gaussian\n distributioncapturesonlyasinglemode. Theseresultssuggestthatthisstandardandpopularchoice\n ishighlysuboptimalwiththemorecomplexanddiversedemonstrationdatausedbyoursystem.Im-\n age Netpre-trainingisparticularlyimportantformodelgeneralizationandrobustness,decreasingthe\n unseentaskperformancerateby 33%, asaresultofthelargeanddiversevisualsofthe Image Net\n dataset. Adding history has an impact primarily on generalization to distractors, while removing\n the Transformercomponenthasauniformbutsmallnegativeimpactacrosstheseentasks, unseen\n tasksanddistractors. Inordertokeepthe Image Netpre-trainingwhilereducingthemodelsize,we\n reduce the number of parameters only by 40% (from 31 M to 25 M). Resulting performance drops\n across training and generalization tasks but not as much as in other ablations. Finally, autoregres-\n sivelyconditioningonactions, asusedin(Reedetal.,2022;Chenetal.,2021;Leeetal.,2022 a),\n didnotbenefitperformanceandslowedinferencebymorethan 2 x. \n Asdescribedin Sec.5.1,inordertorunlarge Transformermodelsonrealrobots,werequireamodel\n thatsupportsfastinferenceforreal-timeoperation. Notethatinordertoachieveourtargetcontrol\n rateof 3 Hz(describedin Sec.5.1),wealsoneedtoconsiderothersourcesoflatencyinthepipeline,\n such as the camera latency and communication overhead. However, these factors will be constant\n 29 "
  },
  {
    "page_num": 30,
    "text": " \n \n Preprint \n \n \n \n Instruction \n Howwouldyouputanenergybarandwaterbottleonthetable \n Howwouldyoubringmealimesodaandabagofchips \n Canyouthrowawaytheappleandbringmeacoke \n Howwouldyoubringmea 7 upcanandatea? \n Howwouldthrowawayalltheitemsonthetable? \n Howwouldyoumoveanmultigrainchipstothetableandanappletothefarcounter?\n Howwouldyoumovethelimesoda,thesponge,andthewaterbottletothetable?\n Howwouldyoubringmetwosodas? \n Howwouldyoumovethreecokestothetrashcan? \n Howwouldyouthrowawaytwocokes? \n Howwouldyoubringmetwodifferentsodas? \n Howwouldyoubringmeanapple,acoke,andwaterbottle? \n Ispilledmycokeonthetable,howwouldyouthrowitawayandthenbringmesomething\n tohelpclean? \n Ijustworkedout,canyoubringmeadrinkandasnacktorecover? \n Howwouldyoubringmeafruit,asoda,andabagofchipsforlunch \n Table 12: Listof Say Caninstructionsevaluatedin Sec.6.4 \n Distractors Backgrounds \n Model Seen Tasks Unseen Tasks All Easy Medium Hard All Inference Time(ms)\n Gato(Reedetal.,2022) 65(-32) 52(-24) 43(-40) 71 44 29 35(-24) 129 \n BC-Z(Jangetal.,2021) 72(-25) 19(-57) 47(-36) 100 67 7 41(-18) 5.3 \n BC-ZXL 56(-41) 43(-33) 23(-60) 57 33 0 35(-24) 5.9 \n RT-1(ours) 97 76 83 100 100 64 59 15 \n RT-1 w/obigmodel 89(-8) 62(-14) 77(-6) 100 100 50 53(-6) 13.5 \n RT-1 w/opre-training 84(-13) 43(-33) 60(-23) 100 67 36 41(-18) 15 \n RT-1 w/continuousactions 68(-29) 43(-33) 37(-46) 71 67 0 35(-24) 16 \n RT-1 w/auto-regressiveactions 85(-12) 71(-5) 67(-16) 100 78 43 65(+6) 36\n RT-1 w/ohistory 82(-15) 62(-14) 50(-33) 71 89 14 59(+0) 15 \n RT-1 w/o Transformer 86(-13) 62(-14) 67(-16) 100 100 29 59(+0) 26 \n \n \n \n \n \n \n \n \n \n \n \n Table 13: Various model ablations of RT-1 across seen tasks, generalization to unseen tasks, and\n robustnesstodistractorsandbackgrounds. \n \n for all the models, and therefore we focus our evaluation on just the network inference time. The\n last column of Table 13 shows the inference speed of all the models. RT-1 is almost an order of\n magnitudefasterthan Gatowithasimilarnumberofparameters,butitisalsoconsiderablyslower\n than a Res Net-based BC-Z. In terms of the different ablations of our model, we observe that the\n biggestslow-downiscausedbyincludingauto-regressiveactions(‚àº2 xslow-down),andsincethis\n doesnotsignificantlyinfluencetheperformance,thefinalversionof RT-1 doesnotgenerateactions\n auto-regressively. \n \n 30 \n "
  },
  {
    "page_num": 31,
    "text": " \n \n Preprint \n \n \n \n D.5 SUMMARYANDANALYSIS \n \n In this section, we summarize some of our findings and propose intuition for RT-1‚Äôs high perfor-\n mance,generalization,androbustness. First,Image Netpretraining(alongwith Universal Sentence\n Encoder language embedding) has a large impact particularly on unseen tasks. We observe that\n RT-1 inherits some of the knowledge that results from the generality and diversity of the datasets\n thesemodelsweretrainedon. Second,continuousactionshavealargeimpactacrossallaspectsof\n performance. This has been previously observed and may be due to the ability to represent more\n complexactiondistributions‚Äìtheper-dimensiondiscretizationallowsourmodeltorepresentcom-\n plexmulti-modaldistributions,whilethe Gaussiandistributioncapturesonlyasinglemode. Third,\n given such expressive multitask models, data diversity has a larger impact than data size. Indeed,\n even datasets collected in simulated environments or from different robotic embodiments can be\n leveragedby RT-1,openingavenuesfornewregimesofdatacollection. \n Finally,RT-1 fuseslanguageintotheimagepipelineearlyvia Fi LMconditioning,comparedtoe.g.,\n Gato‚Äôslatefusion. Thisenablesimagetokensthatfocusonlyonrelevantfeaturesfortheinstruction\n at hand, which may be the cause of poor distractor performance for Gato. Figure 13 visualizes\n theattentionduringrolloutsof RT-1. Weseethattheattentionisfocusedonrelevantfeaturesand\n particularlyoninteractionbetweenthegripperandtheobjectofinterest.Thebottleneckofattention\n layers such as these results in a compact representation which effectively ignores distractors and\n varyingbackgrounds. \n ‚Äúpick green \n jalapeno chip \n Layer 2, bag from middle \n Head 6 drawer and \n place on \n counter‚Äù \n ‚Äúplace rxbar \n Layer 2, \n blueberry in \n Head 6 \n bottom drawer‚Äù \n \n Layer 4, \n ‚Äúopen middle \n Head 2 \n drawer‚Äù \n \n Figure 13: Inthisfigureweshowtheattentionmapofthe RT-1 policy. Differentlayersandheads\n generallyfocusondifferentpartoftheimage. Mostcommonly,theyfocusonthepartsofthescene\n with the richest interaction affordances, such as graspable objets. For example, Layer 2 Head 6\n focuses on the jalapeno chips and pepsi can in grasping tasks; and Layer 4 Head 2 focuses on the\n drawerindraweropeningtasks. \n \n \n \n \n \n \n \n \n \n \n 31 \n "
  }
]