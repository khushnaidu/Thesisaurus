{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Testing LLM Wrapper\n",
        "## Test Llama models with 4-bit quantization\n",
        "\n",
        "**Make sure you're using a GPU runtime!**\n",
        "- Runtime → Change runtime type → T4 GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q transformers accelerate bitsandbytes torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo\n",
        "!git clone https://github.com/khushnaidu/Thesisaurus.git\n",
        "%cd Thesisaurus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phase3_llm.llm_wrapper import LLMWrapper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Load 8B Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = LLMWrapper(model_size=\"8b\")\n",
        "llm.load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Basic Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"What is machine learning?\"\n",
        "answer = llm.generate(prompt, max_tokens=100)\n",
        "print(f\"Q: {prompt}\")\n",
        "print(f\"A: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Explain robot learning in 2 sentences.\"\n",
        "answer = llm.generate(prompt, max_tokens=150, temperature=0.7)\n",
        "print(f\"Q: {prompt}\")\n",
        "print(f\"A: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Test 70B Model (needs Colab Pro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only run if you have Colab Pro with A100 GPU\n",
        "# llm_70b = LLMWrapper(model_size=\"70b\")\n",
        "# llm_70b.load()\n",
        "# answer = llm_70b.generate(\"What is robot learning?\", max_tokens=100)\n",
        "# print(answer)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
