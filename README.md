# NLP Project: Thesaurus Virtual Assistant for Robotics Research

Building a Retrieval-Augmented Generation (RAG) system for analyzing robotics research papers.

## ğŸ“ Project Structure

```
NLP_Project/
â”œâ”€â”€ phase1_data_preparation/     # âœ… Phase 1: Complete data pipeline (READY)
â”‚   â”œâ”€â”€ data/                    # Input: 18 papers (text + metadata)
â”‚   â”œâ”€â”€ scripts/                 # Pipeline: extract â†’ database â†’ vector index
â”‚   â”œâ”€â”€ outputs/                 # Generated: CSV, SQLite DB, FAISS index
â”‚   â”œâ”€â”€ README.md               # Full documentation
â”‚   â””â”€â”€ SUBMISSION_GUIDE.md     # Quick start guide
â”œâ”€â”€ papers/                      # Source: Raw PDF files (18 papers)
â”œâ”€â”€ processed_papers/            # Source: Extracted text from PDFs
â”œâ”€â”€ scripts/                     # Development utilities
â”‚   â”œâ”€â”€ research_pdf_parser_clean.py  # PDF parsing (if needed for more papers)
â”‚   â”œâ”€â”€ query_database.py             # Database query helper
â”‚   â””â”€â”€ thesisaurus_va.ipynb          # Development notebook
â”œâ”€â”€ venv/                        # Python virtual environment
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### Phase 1: Data Preparation (Complete âœ…)

All Phase 1 work is organized in the **`phase1_data_preparation/`** directory.

```bash
# Navigate to Phase 1
cd phase1_data_preparation

# Verify setup (< 30 seconds)
python verify_setup.py

# See full documentation
cat README.md

# Or jump straight to demo
cd scripts
python demo_queries.py
```

**What Phase 1 Includes:**
- âœ… Text extraction from 18 robotics papers
- âœ… Structured data extraction (26 fields per paper)
- âœ… SQLite database (11 normalized tables)
- âœ… FAISS vector index (406 chunks for semantic search)
- âœ… Complete documentation and demo scripts

**Phase 1 Statistics:**
- 18 papers processed (2017-2025)
- 11 unique datasets identified
- 5 robot platforms tracked
- 406 text chunks indexed
- < 0.1s semantic search latency

## ğŸ“Š Data Pipeline Summary

**Input:** Raw PDFs â†’ Extract text â†’ **Process:**

1. **Extract Structured Info** â†’ CSV with 26 fields
   - Datasets, models, hardware, training details
   
2. **Populate Database** â†’ SQLite with relational schema
   - Normalized tables, indexed for fast queries
   
3. **Build Vector Index** â†’ FAISS for semantic search
   - 512-word chunks, 384-dim embeddings

**Output:** Queryable database + semantic search

## ğŸ¯ Current Progress

- âœ… **Phase 1: Data Preparation** (Complete)
- ğŸ”„ **Phase 2: Tool Implementation** (Next)
  - Database query tool
  - Vector search tool
  - PDF snippet extraction
  - Web search integration
- â³ **Phase 3: LLM Integration**
- â³ **Phase 4: Evaluation & Security**

## ğŸ’» Technologies

- **Python 3.8+**
- **SQLite** - Relational database
- **FAISS** - Vector similarity search
- **Sentence Transformers** - Text embeddings
- **Regex** - Pattern-based extraction

## ğŸ“ Development Notes

### Source Data
- `papers/` - Original PDFs (kept for reference)
- `processed_papers/` - Extracted text and metadata

### Phase 1 Package
- All Phase 1 code is in `phase1_data_preparation/`
- Self-contained with full documentation
- Ready for testing/evaluation

### Development Scripts
- `scripts/research_pdf_parser_clean.py` - For processing additional papers
- `scripts/query_database.py` - Database utilities
- `scripts/thesisaurus_va.ipynb` - Experimentation notebook

