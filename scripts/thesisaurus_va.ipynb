{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thesisaurus: A Virtual Assistant for Curating and Synthesizing Thesis Research\n",
        "\n",
        "**CMPE 259 Project**  \n",
        "**Author:** Khush Naidu  \n",
        "**SJSU ID:** 015798328\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook implements a Virtual Assistant that helps researchers organize, synthesize, and query research papers. The VA uses:\n",
        "- **Large Model:** Llama-3.3-70B-Instruct\n",
        "- **Small Model:** Llama-3.1-8B-Instruct\n",
        "- **Tools:** Database queries, Vector search (RAG), PDF extraction, Web search\n",
        "- **Advanced Prompting:** Prompt chaining, Meta-prompting, Self-reflection\n",
        "- **Security:** Prompt injection detection and prevention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers torch accelerate bitsandbytes\n",
        "%pip install -q faiss-cpu sentence-transformers\n",
        "%pip install -q pypdf2 pdfplumber\n",
        "%pip install -q requests beautifulsoup4 lxml\n",
        "%pip install -q python-dotenv\n",
        "%pip install -q datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LLM and ML libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector store\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# PDF processing\n",
        "import pdfplumber\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration for the VA system\"\"\"\n",
        "    \n",
        "    # Model configurations\n",
        "    LARGE_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "    SMALL_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    \n",
        "    # Database\n",
        "    DB_PATH = \"papers.db\"\n",
        "    \n",
        "    # Vector store\n",
        "    FAISS_INDEX_PATH = \"faiss_index.bin\"\n",
        "    CHUNK_SIZE = 512\n",
        "    CHUNK_OVERLAP = 50\n",
        "    TOP_K = 5\n",
        "    \n",
        "    # Generation parameters\n",
        "    MAX_NEW_TOKENS = 512\n",
        "    TEMPERATURE = 0.7\n",
        "    TOP_P = 0.9\n",
        "    \n",
        "    # Caching\n",
        "    ENABLE_CACHE = True\n",
        "    CACHE_SIZE = 100\n",
        "    \n",
        "    # Security\n",
        "    ENABLE_SECURITY = True\n",
        "    \n",
        "config = Config()\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"Large Model: {config.LARGE_MODEL}\")\n",
        "print(f\"Small Model: {config.SMALL_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Database Tool - Paper Metadata Storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DatabaseTool:\n",
        "    \"\"\"Tool for querying structured paper metadata from SQLite database\"\"\"\n",
        "    \n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.conn = None\n",
        "        self._init_db()\n",
        "    \n",
        "    def _init_db(self):\n",
        "        \"\"\"Initialize database with schema\"\"\"\n",
        "        self.conn = sqlite3.connect(self.db_path)\n",
        "        cursor = self.conn.cursor()\n",
        "        \n",
        "        # Papers table\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS papers (\n",
        "                paper_id TEXT PRIMARY KEY,\n",
        "                title TEXT NOT NULL,\n",
        "                authors TEXT,\n",
        "                year INTEGER,\n",
        "                venue TEXT,\n",
        "                arxiv_id TEXT,\n",
        "                pdf_path TEXT,\n",
        "                abstract TEXT,\n",
        "                keywords TEXT\n",
        "            )\n",
        "        ''')\n",
        "        \n",
        "        # Datasets table\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS datasets (\n",
        "                dataset_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                paper_id TEXT,\n",
        "                dataset_name TEXT,\n",
        "                dataset_type TEXT,\n",
        "                FOREIGN KEY (paper_id) REFERENCES papers (paper_id)\n",
        "            )\n",
        "        ''')\n",
        "        \n",
        "        # Models table\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS models (\n",
        "                model_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                paper_id TEXT,\n",
        "                model_name TEXT,\n",
        "                model_type TEXT,\n",
        "                FOREIGN KEY (paper_id) REFERENCES papers (paper_id)\n",
        "            )\n",
        "        ''')\n",
        "        \n",
        "        # Experimental setups table\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS experiments (\n",
        "                exp_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                paper_id TEXT,\n",
        "                optimizer TEXT,\n",
        "                learning_rate REAL,\n",
        "                batch_size INTEGER,\n",
        "                epochs INTEGER,\n",
        "                augmentations TEXT,\n",
        "                pretrained_weights TEXT,\n",
        "                FOREIGN KEY (paper_id) REFERENCES papers (paper_id)\n",
        "            )\n",
        "        ''')\n",
        "        \n",
        "        # Limitations table\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS limitations (\n",
        "                limitation_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                paper_id TEXT,\n",
        "                limitation TEXT,\n",
        "                FOREIGN KEY (paper_id) REFERENCES papers (paper_id)\n",
        "            )\n",
        "        ''')\n",
        "        \n",
        "        self.conn.commit()\n",
        "        print(\"Database initialized successfully!\")\n",
        "    \n",
        "    def query(self, sql: str, params: tuple = ()) -> List[Dict]:\n",
        "        \"\"\"Execute a parameterized SQL query (safe from injection)\"\"\"\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "            cursor.execute(sql, params)\n",
        "            columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
        "            results = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Database query error: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def insert_paper(self, paper_data: Dict):\n",
        "        \"\"\"Insert a paper into the database\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            INSERT OR REPLACE INTO papers \n",
        "            (paper_id, title, authors, year, venue, arxiv_id, pdf_path, abstract, keywords)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (\n",
        "            paper_data.get('paper_id'),\n",
        "            paper_data.get('title'),\n",
        "            paper_data.get('authors'),\n",
        "            paper_data.get('year'),\n",
        "            paper_data.get('venue'),\n",
        "            paper_data.get('arxiv_id'),\n",
        "            paper_data.get('pdf_path'),\n",
        "            paper_data.get('abstract'),\n",
        "            paper_data.get('keywords')\n",
        "        ))\n",
        "        self.conn.commit()\n",
        "    \n",
        "    def get_all_datasets(self) -> List[str]:\n",
        "        \"\"\"Get all unique datasets from corpus\"\"\"\n",
        "        results = self.query(\"SELECT DISTINCT dataset_name FROM datasets ORDER BY dataset_name\")\n",
        "        return [r['dataset_name'] for r in results]\n",
        "    \n",
        "    def get_all_models(self) -> List[str]:\n",
        "        \"\"\"Get all unique models from corpus\"\"\"\n",
        "        results = self.query(\"SELECT DISTINCT model_name FROM models ORDER BY model_name\")\n",
        "        return [r['model_name'] for r in results]\n",
        "    \n",
        "    def get_papers_by_year(self) -> List[Dict]:\n",
        "        \"\"\"Get papers grouped by publication year\"\"\"\n",
        "        return self.query(\"\"\"\n",
        "            SELECT paper_id, title, year, venue \n",
        "            FROM papers \n",
        "            ORDER BY year DESC, title\n",
        "        \"\"\")\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close database connection\"\"\"\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "\n",
        "# Initialize database tool\n",
        "db_tool = DatabaseTool(config.DB_PATH)\n",
        "print(\"Database tool initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
